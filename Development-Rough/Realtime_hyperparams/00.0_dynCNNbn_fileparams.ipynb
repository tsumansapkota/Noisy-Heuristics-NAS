{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "import time\n",
    "import sys, io\n",
    "\n",
    "import mylibrary.datasets as datasets\n",
    "import mylibrary.nnlib as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hidden_neuron_number(i, o):\n",
    "    return (max(i,o)*(min(i,o)**2))**(1/3)\n",
    "\n",
    "\n",
    "class Shortcut_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=(3,3), stride=1):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self._kernel = np.array(kernel, dtype=int)\n",
    "        self._padding = tuple(((self._kernel-1)/2).astype(int))\n",
    "        self._stride = stride\n",
    "        _wd = nn.Conv2d(input_dim, output_dim, self._kernel, stride=self._stride,\n",
    "                        padding=self._padding, bias=False).weight.data\n",
    "        ## Shape = OutputDim, InputDim, Kernel0, Kernel1\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "        del _wd\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.shape[1] > 0 and self.weight.shape[0] > 0:\n",
    "#             out_dim = self.weight.shape[0]\n",
    "#             self.weight.data /= torch.norm(self.weight.data.reshape(out_dim, -1), dim=1).reshape(out_dim, 1, 1, 1)\n",
    "            \n",
    "            return F.conv2d(x, self.weight, stride=self._stride, padding=self._padding)\n",
    "        ### output dim is 0\n",
    "        elif self.weight.shape[0] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###       #out_dim #inp_dim            #kernel\n",
    "            w = torch.zeros(1, 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return torch.zeros(o.shape[0], 0, o.shape[2], o.shape[3], dtype=x.dtype, device=x.device)\n",
    "        ### input dim is 0\n",
    "        elif x.shape[1] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###             #out_dim            #inp_dim            #kernel\n",
    "            w = torch.zeros(self.weight.shape[0], 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return o.data\n",
    "        else:\n",
    "            raise(f\"Unknown shape of input {x.shape} or weight {self.weight.shape}\")\n",
    "\n",
    "#     def decay_std_ratio(self, factor):\n",
    "#         self.weight.data = self.weight.data - self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "        \n",
    "#     def decay_std_ratio_grad(self, factor):\n",
    "#         self.weight.grad = self.weight.grad + self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "    \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        pass\n",
    "    \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "        pass\n",
    "     \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "        _w = self.weight.data[remaining, :]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "        _w = self.weight.data[:, remaining]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        _w = torch.cat((self.weight.data, torch.zeros(o, num, k0, k1, dtype=self.weight.data.dtype,\n",
    "                                                      device=self.weight.data.device)), dim=1)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i)\n",
    "        _new = torch.empty(num, i, k0, k1, dtype=self.weight.data.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        _w = torch.cat((self.weight.data, _new), dim=0)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)        \n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}S▚:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Shortcut_Conv(\"tree\", 2, 5).weight.data # O, I, k1,k2\n",
    "# n = torch.norm(a.reshape(5, -1), dim=1, keepdim=True).unsqueeze(2).unsqueeze(2)\n",
    "n = torch.norm(a.reshape(5, -1), dim=1).reshape(5, 1, 1, 1)\n",
    "a_ = a/n\n",
    "torch.norm(a_.reshape(5, -1), dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NonLinearity_Conv(nn.Module):\n",
    "\n",
    "#     def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "#         super().__init__()\n",
    "#         self.tree = tree\n",
    "#         self.bias = nn.Parameter(torch.zeros(io_dim))\n",
    "#         self.actf = actf_obj\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.actf(x+self.bias.view(1,-1,1,1))\n",
    "\n",
    "#     def add_neuron(self, num):\n",
    "#         _b = torch.cat((self.bias.data, torch.zeros(num, dtype=self.bias.data.dtype,\n",
    "#                                                     device=self.bias.data.device)))\n",
    "#         del self.bias\n",
    "#         self.bias = nn.Parameter(_b)\n",
    "        \n",
    "#     def remove_neuron(self, remaining):\n",
    "#         _b = self.bias.data[remaining]\n",
    "#         del self.bias\n",
    "#         self.bias = nn.Parameter(_b)\n",
    "\n",
    "class NonLinearity_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.bn = nn.BatchNorm2d(io_dim)\n",
    "        self.actf = actf_obj\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## if empty tensor forward that to next layer.\n",
    "        if x.shape[1] < 1:\n",
    "            return x\n",
    "        return self.actf(self.bn(x))\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        ####https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d\n",
    "        ## running_mean\n",
    "        _rm = self.bn.running_mean\n",
    "        _rm = torch.cat((_rm, torch.zeros(num, dtype=_rm.dtype, device=_rm.device)))\n",
    "        self.bn.running_mean = _rm\n",
    "        \n",
    "        ## running_var\n",
    "        _rv = self.bn.running_var\n",
    "        _rv = torch.cat((_rv, torch.ones(num, dtype=_rv.dtype, device=_rv.device)))\n",
    "        self.bn.running_var = _rv\n",
    "        \n",
    "        ## weight\n",
    "        _w = self.bn.weight.data\n",
    "        _w = torch.cat((_w, torch.ones(num, dtype=_w.dtype, device=_w.device)*0.1))\n",
    "        del self.bn.weight\n",
    "        self.bn.weight = nn.Parameter(_w)\n",
    "        \n",
    "        ## bias\n",
    "        _b = self.bn.bias.data\n",
    "        _b = torch.cat((_b, torch.zeros(num, dtype=_b.dtype, device=_b.device)))\n",
    "        del self.bn.bias\n",
    "        self.bn.bias = nn.Parameter(_b)\n",
    "        \n",
    "        self.bn.num_features += num\n",
    "        return\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        ## running_mean\n",
    "        _rm = self.bn.running_mean[remaining]\n",
    "        self.bn.running_mean = _rm\n",
    "        \n",
    "        ## running_var\n",
    "        _rv = self.bn.running_var[remaining]\n",
    "        self.bn.running_var = _rv\n",
    "        \n",
    "        ## weight\n",
    "        _w = self.bn.weight.data[remaining]\n",
    "        del self.bn.weight\n",
    "        self.bn.weight = nn.Parameter(_w)\n",
    "        \n",
    "        ## bias\n",
    "        _b = self.bn.bias.data[remaining]\n",
    "        del self.bn.bias\n",
    "        self.bn.bias = nn.Parameter(_b)\n",
    "        \n",
    "        self.bn.num_features = len(remaining)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearity(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.bias = nn.Parameter(torch.zeros(io_dim))\n",
    "        self.actf = actf_obj\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actf(x+self.bias)\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        _b = torch.cat((self.bias.data, torch.zeros(num, dtype=self.bias.data.dtype,\n",
    "                                                    device=self.bias.data.device)))\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        _b = self.bias.data[remaining]\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, hidden_dim, output_dim, stride=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "#         self.stride = stride\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = HierarchicalResidual_Conv(self.tree, input_dim, hidden_dim, stride=stride, activation=activation) \n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = HierarchicalResidual_Conv(self.tree, hidden_dim, output_dim, activation=activation)\n",
    "        self.fc1.shortcut.weight.data *= 0.\n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "            self.apnz = self.apnz/self.count\n",
    "            self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "#         self.count = None\n",
    "\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "#             self.significance += z.pow(2).sum(dim=0)\n",
    "            self.significance += z.abs().sum(dim=0)\n",
    "#             self.significance += z.abs().pow(0.8).sum(dim=0)\n",
    "#             print(f\"SIG ACT:\\n{float(self.activations.abs().mean())}\")\n",
    "#             print(f\"GRAD Mean, Std:\\n{float(grad_output[0].data.abs().mean()), float(grad_output[0].data.std())}\")\n",
    "\n",
    "            if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "                self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "        #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "                self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below=None, above=None, mask=None):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(self.significance.numel(), dtype=torch.bool)\n",
    "        if below:\n",
    "            mask = torch.logical_or(mask,self.significance<=below)\n",
    "        if above:\n",
    "            mask = torch.logical_or(mask,self.significance>above)\n",
    "            \n",
    "        print(f\"Significance:\\n{self.significance}\\nPrune:\\n{mask}\")\n",
    "            \n",
    "        self.to_remove = torch.nonzero(mask).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "#         max_dim = np.ceil((self.tree.parent_dict[self].input_dim+\\\n",
    "#             self.tree.parent_dict[self].output_dim)/2)\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None:\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, stride=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.stride = 1\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, stride=self.stride).to(self.tree.device)\n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None ## this can be Residual Layer or None\n",
    "        ##### only one of shortcut or residual can be None at a time\n",
    "        self.forward = self.forward_shortcut\n",
    "        \n",
    "        self.std_ratio = 0. ## 0-> all variation due to shortcut, 1-> residual\n",
    "        self.target_std_ratio = 0. ##\n",
    "    \n",
    "    def forward_both(self, r):\n",
    "\n",
    "        s = self.shortcut(r)\n",
    "        r = self.residual(r)\n",
    "\n",
    "        if self.residual.hook is None: ### dont execute when computing significance\n",
    "            s_std = torch.std(s, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            r_std = torch.std(r, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            stdr = r_std/(s_std+r_std)\n",
    "\n",
    "            self.std_ratio = self.tree.beta_std_ratio*self.std_ratio + (1-self.tree.beta_std_ratio)*stdr.data\n",
    "            if r_std.min() > 1e-9:\n",
    "                ## recover for the fact that when decaying neurons, target ratio should also be reducing\n",
    "                if self.tree.total_decay_steps:\n",
    "                    i, o = self.shortcut.weight.shape[1],self.shortcut.weight.shape[0]\n",
    "                    if self.shortcut.to_remove is not None:\n",
    "                        i -= len(self.shortcut.to_remove)\n",
    "                    if self.shortcut.to_freeze is not None:\n",
    "                        o -= len(self.shortcut.to_freeze)\n",
    "                    h = self.residual.hidden_dim\n",
    "                    if self.residual.to_remove is not None:\n",
    "                        h -= len(self.residual.to_remove)\n",
    "                    \n",
    "#                     tr = h/np.ceil((i+o)/2 +1)\n",
    "                    tr = h/_get_hidden_neuron_number(i, o)\n",
    "                    self.compute_target_std_ratio(tr)\n",
    "                else:\n",
    "                    self.compute_target_std_ratio()\n",
    "                self.get_std_loss(stdr)\n",
    "        return s+r\n",
    "    \n",
    "    def forward_shortcut(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def forward_residual(self, x):\n",
    "        self.compute_target_std_ratio()\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def compute_target_std_ratio(self, tr = None):\n",
    "        if tr is None:\n",
    "#             tr = self.residual.hidden_dim/np.ceil((self.input_dim+self.output_dim)/2 +1)\n",
    "            tr = self.residual.hidden_dim/_get_hidden_neuron_number(self.input_dim, self.output_dim)\n",
    "#             tr = self.residual.hidden_dim/np.ceil(self.output_dim/2 +1)\n",
    "\n",
    "        tr = np.clip(tr, 0., 1.)\n",
    "        self.target_std_ratio = self.tree.beta_std_ratio*self.target_std_ratio +\\\n",
    "                                (1-self.tree.beta_std_ratio)*tr\n",
    "        pass        \n",
    "    \n",
    "    def get_std_loss(self, stdr):\n",
    "        del_std = self.target_std_ratio-stdr\n",
    "        del_std_loss = (del_std**2 + torch.abs(del_std)).mean()\n",
    "#         del_std_loss = (del_std**2).mean()\n",
    "        self.tree.std_loss += del_std_loss\n",
    "        return\n",
    "            \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_freezing_connection(to_freeze)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_decaying_connection(to_remove)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_freezed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.remove_freezed_connection(remaining)\n",
    "            if self.shortcut: self.std_ratio = self.std_ratio[:, remaining]\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_decayed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_input_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_output_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.add_output_connection(num)\n",
    "            # if torch.is_tensor(self.std_ratio):\n",
    "            if self.shortcut:\n",
    "                self.std_ratio = torch.cat((self.std_ratio, torch.zeros(1, num, device=self.tree.device)), dim=1)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        \n",
    "        if self.residual is None:\n",
    "            # print(f\"Adding {num} hidden units.. in new residual_layer\")\n",
    "            self.residual = Residual_Conv(self.tree, self.input_dim,\n",
    "                                          num, self.output_dim, stride=self.stride,\n",
    "                                          activation=self.activation).to(self.tree.device)\n",
    "            \n",
    "            self.tree.parent_dict[self.residual] = self\n",
    "            if self.shortcut is None:\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            else:\n",
    "                self.forward = self.forward_both\n",
    "                self.std_ratio = torch.zeros(1, self.output_dim, device=self.tree.device)\n",
    "                \n",
    "        else:\n",
    "            # print(f\"Adding {num} hidden units..\")\n",
    "            self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            if self.std_ratio.min()>0.98 and self.target_std_ratio>0.98:\n",
    "                del self.tree.parent_dict[self.shortcut]\n",
    "                del self.shortcut\n",
    "                self.shortcut = None\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            \n",
    "        elif self.target_std_ratio<0.95:\n",
    "            self.shortcut = Shortcut_Conv(self.tree, self.input_dim, self.output_dim, stride=self.stride)\n",
    "            self.shortcut.weight.data *= 0.\n",
    "            self.forward = self.forward_both\n",
    "            \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.residual.hidden_dim < 1:\n",
    "            del self.tree.parent_dict[self.residual]\n",
    "            del self.residual\n",
    "            ### its parent (Residual_Conv) removes it from dynamic list if possible\n",
    "            self.residual = None\n",
    "            self.forward = self.forward_shortcut\n",
    "            self.std_ratio = 0.\n",
    "            return\n",
    "        \n",
    "#         max_dim = np.ceil((self.input_dim+self.output_dim)/2)\n",
    "        # max_dim = min((self.input_dim, self.output_dim))+1\n",
    "        max_dim = _get_hidden_neuron_number(self.input_dim, self.output_dim) + 1 \n",
    "        # print(\"MaxDIM\", max_dim, self.residual.hidden_dim)\n",
    "        if self.residual.hidden_dim > max_dim:\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc0)\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc1)\n",
    "            # print(\"Added\", self.residual)\n",
    "            \n",
    "        # self.residual.fc0.morph_network()\n",
    "        # self.residual.fc1.morph_network()\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        stdr = self.std_ratio\n",
    "        if torch.is_tensor(self.std_ratio):\n",
    "            stdr = self.std_ratio.min()\n",
    "            \n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{self.target_std_ratio}, s:{stdr}\")\n",
    "        if self.shortcut:\n",
    "            self.shortcut.print_network_debug(depth+1)\n",
    "        if self.residual:\n",
    "            self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        if self.residual is None:\n",
    "            return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            print(f\"{pre_string}╠════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}║    \")\n",
    "            print(f\"{pre_string}╠════╝\")\n",
    "        else:\n",
    "            print(f\"{pre_string}╚════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}     \")\n",
    "            print(f\"{pre_string}╔════╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Conv Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation, hidden_dim, post_activation=None):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "        self.post_activation = post_activation\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = hrnet0\n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = hrnet1\n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        if self.post_activation:\n",
    "            x = self.post_activation(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "            self.apnz = self.apnz/self.count\n",
    "            self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "#         self.count = None\n",
    "\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "#             self.significance += z.pow(2).sum(dim=0)\n",
    "            self.significance += z.abs().sum(dim=0)\n",
    "#             self.significance += z.abs().pow(0.8).sum(dim=0)\n",
    "#             print(\"Current Significance \\n\", self.significance)\n",
    "#             print(f\"SIG ACT:\\n{float(self.activations.abs().mean())}\")\n",
    "#             print(f\"GRAD Mean, Std:\\n{float(grad_output[0].data.abs().mean()), float(grad_output[0].data.std())}\")\n",
    "\n",
    "            if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "                self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "        #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "                self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below=None, above=None, mask=None):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(self.significance.numel(), dtype=torch.bool)\n",
    "        if below:\n",
    "            mask = torch.logical_or(mask,self.significance<=below)\n",
    "        if above:\n",
    "            mask = torch.logical_or(mask,self.significance>above)\n",
    "            \n",
    "        print(f\"Significance:\\n{self.significance}\\nPrune:\\n{mask}\")\n",
    "        \n",
    "        self.to_remove = torch.nonzero(mask).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None:\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [3],\n",
       "        [7],\n",
       "        [8]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10)<0 \n",
    "b = torch.randn(10) > 0.5\n",
    "torch.nonzero(torch.logical_and(a,b), as_tuple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation=nn.ReLU(), post_activation=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = hrnet0.input_dim\n",
    "        self.output_dim = hrnet1.output_dim\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = None\n",
    "        self.residual = Residual_Conv_Connector(self.tree, hrnet0, hrnet1,\n",
    "                                                activation, hrnet0.output_dim, post_activation)\n",
    "        self.tree.parent_dict[self.residual] = self\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.residual.fc1.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.residual.fc1.add_output_connection(num)\n",
    "        \n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):  \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        print(f\"{pre_string}╚╗\")\n",
    "        self.residual.print_network(f\"{pre_string} \")\n",
    "        print(f\"{pre_string}╔╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut only Hierarchical Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        _wd = nn.Linear(input_dim, output_dim, bias=False).weight.data\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## input_dim        ## output_dim\n",
    "        if x.shape[1] + self.weight.shape[1] > 0:\n",
    "            return x.matmul(self.weight.t())\n",
    "        else:\n",
    "            # print(x.shape, self.weight.shape)\n",
    "            # print(x.matmul(self.weight.t()))\n",
    "            if x.shape[1] + self.weight.shape[1] == 0:\n",
    "                return torch.zeros(x.shape[0], self.weight.shape[0], dtype=x.dtype, device=x.device)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        pass\n",
    "    \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "        pass\n",
    "            \n",
    "     \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "        _w = self.weight.data[remaining, :]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "        _w = self.weight.data[:, remaining]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        _w = torch.zeros(o, num, dtype=self.weight.data.dtype, device=self.weight.data.device)\n",
    "#         _w += torch.randn_like(_w)\n",
    "        _w = torch.cat((self.weight.data, _w), dim=1)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i)\n",
    "        _new = torch.empty(num, i, dtype=self.bias.weight.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        _w = torch.cat((self.weight.data, _new), dim=0)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)        \n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}S:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n",
    "\n",
    "\n",
    "class HierarchicalResidual_Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=None, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        ## this can be Shortcut Layer or None\n",
    "        if kernel is None:\n",
    "            self.shortcut = Shortcut(tree, self.input_dim, self.output_dim) \n",
    "        else:\n",
    "            self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, kernel, stride) \n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.shortcut.start_freezing_connection(to_freeze)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.shortcut.start_decaying_connection(to_remove)\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.shortcut.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.shortcut.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.shortcut.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.shortcut.add_output_connection(num)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        print(\"Cannot Add Hidden neuron to Shortcut Only Layer\")\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        pass\n",
    "        \n",
    "    def morph_network(self):\n",
    "        pass\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.shortcut.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree and Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree_State():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.DYNAMIC_LIST = set() ## residual parent is added, to make code effecient.\n",
    "        ## the parents which is not intended to have residual connection should not be added.\n",
    "        self.beta_std_ratio = None\n",
    "        self.beta_del_neuron = None\n",
    "        self.device = 'cpu'\n",
    "    \n",
    "        self.parent_dict = {}\n",
    "    \n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual:set = None\n",
    "        self.freeze_connection_shortcut:set = None\n",
    "        self.decay_connection_shortcut:set = None\n",
    "\n",
    "        self.decay_rate_std = 0.001\n",
    "\n",
    "        self.add_to_remove_ratio = 2.\n",
    "        pass\n",
    "    \n",
    "    def get_decay_factor(self):\n",
    "        ratio = self.current_decay_step/self.total_decay_steps\n",
    "#         self.decay_factor = np.exp(-2*ratio)*(1-ratio)\n",
    "        self.decay_factor = (1-ratio)**2\n",
    "        pass\n",
    "    \n",
    "    def clear_decay_variables(self):\n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual = None\n",
    "        self.freeze_connection_shortcut = None\n",
    "        self.decay_connection_shortcut = None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constructing Hierarchical Residual CNN (Resnet Inspired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutActivation(nn.Module):\n",
    "    \n",
    "    def __init__(self, p=0.2, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout2d(p)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dropout(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, device, input_dim = 1, hidden_dims = [8, 16, 32, 64], output_dim = 10, final_activation=None,\n",
    "                 num_stat=5, num_std=100, decay_rate_std=0.001):\n",
    "        super().__init__()\n",
    "        self.tree = Tree_State()\n",
    "        self.tree.beta_del_neuron = (num_stat-1)/num_stat\n",
    "        self.tree.beta_std_ratio = (num_std-1)/num_std\n",
    "        self.tree.decay_rate_std = decay_rate_std\n",
    "        self.tree.device = device\n",
    "        \n",
    "        self.root_net = None\n",
    "        self._construct_root_net(input_dim, hidden_dims, output_dim)\n",
    "        \n",
    "#         self.tree.DYNAMIC_LIST.add(self.root_net)\n",
    "        self.tree.parent_dict[self.root_net] = None\n",
    "        \n",
    "        if final_activation is None:\n",
    "            final_activation = lambda x: x\n",
    "        self.non_linearity = NonLinearity(\"Root\", output_dim, final_activation)\n",
    "        \n",
    "        self.neurons_added = 0\n",
    "\n",
    "        self._remove_below = None ## temporary variable\n",
    "        \n",
    "    def _construct_root_net(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        actf = DropoutActivation()\n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 8, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 8, 8, activation=actf)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 8, 16, stride=2, activation=actf)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2, activation=actf)\n",
    "        hrn3 = HierarchicalResidual_Conv(self.tree, 32, 32, stride=2, activation=actf)\n",
    "\n",
    "    \n",
    "        actf = lambda x: x\n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0, actf)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1, actf)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2, actf)\n",
    "        hrnR0123 = HierarchicalResidual_Connector(self.tree, hrnR012, hrn3, actf)\n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 32, 10)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR0123fc = HierarchicalResidual_Connector(self.tree, hrnR0123, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR0123fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "        morphables = [self.root_net, hrnR0123, hrnR012, hrnR01, hrnR0, hrn3, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def _construct_root_net2(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        \n",
    "        \n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 16, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 16, 16)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 32, 64, stride=2)\n",
    "\n",
    "        actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "    \n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1, actf)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2, actf)\n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 64, 10)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR012fc = HierarchicalResidual_Connector(self.tree, hrnR012, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR012fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "        morphables = [hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrnR012, hrnR01, hrnR0, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.non_linearity(self.root_net(x))\n",
    "\n",
    "    def add_neurons(self, num):\n",
    "        num_stat = num//2\n",
    "        num_random = num - num_stat\n",
    "        \n",
    "        DL = list(self.tree.DYNAMIC_LIST)\n",
    "        if num_random>0:\n",
    "            rands = torch.randint(high=len(DL), size=(num_random,))\n",
    "            index, count = torch.unique(rands, sorted=False, return_counts=True)\n",
    "            for i, idx in enumerate(index):\n",
    "                DL[idx].add_hidden_neuron(int(count[i]))\n",
    "\n",
    "        if num_stat>0:\n",
    "            del_neurons = []\n",
    "            for hr in DL:\n",
    "                if hr.residual:\n",
    "                    del_neurons.append(hr.residual.del_neurons)#+1e-7)\n",
    "                else:\n",
    "                    del_neurons.append(0.)#1e-7) ## residual layer yet not created \n",
    "            \n",
    "            prob_stat = torch.tensor(del_neurons)\n",
    "            prob_stat = torch.log(torch.exp(prob_stat)+1.)\n",
    "            m = torch.distributions.multinomial.Multinomial(total_count=num_stat,\n",
    "                                                            probs= prob_stat)\n",
    "            count = m.sample()#.type(torch.long)\n",
    "            for i, hr in enumerate(DL):\n",
    "                if count[i] < 1: continue\n",
    "                hr.add_hidden_neuron(int(count[i]))\n",
    "        \n",
    "        self.neurons_added += num \n",
    "        pass\n",
    "\n",
    "    def identify_removable_neurons(self, num=None, threshold_min=0., threshold_max=1.):\n",
    "        \n",
    "        all_sig = []\n",
    "        self.all_sig_ = []\n",
    "        \n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                all_sig.append(hr.residual.significance)\n",
    "                \n",
    "        all_sigs = torch.cat(all_sig)\n",
    "        del all_sig\n",
    "        \n",
    "#         print(\"All_sigs\", all_sigs)\n",
    "        \n",
    "#         print(\"Normalization\", (all_sigs/all_sigs.sum()).sum())\n",
    "        \n",
    "        ### Normalizes such that importance 1 is average importance\n",
    "        normalizer = float(torch.sum(all_sigs))/len(all_sigs)\n",
    "        all_sig = all_sigs/normalizer\n",
    "\n",
    "        ### Normalizes to range [0, 1]\n",
    "#         max_sig = all_sigs.max()\n",
    "#         all_sig = all_sigs/(max_sig+1e-9)\n",
    "#         print(\"All_sig\", all_sig)\n",
    "#         print(\"Sig sum\", all_sig.sum())\n",
    "        print(f\"Significance Stat:\\nMin, Max: {float(all_sig.min()), float(all_sig.max())}\")\n",
    "        print(f\"Mean, Std: {float(all_sig.mean()), float(all_sig.std())}\")\n",
    "        all_sig = all_sig[all_sig<threshold_max]\n",
    "        if len(all_sig)<1: ## if all significance is above threshold max \n",
    "            return 0, None, all_sigs\n",
    "        all_sig = torch.sort(all_sig)[0] ### sorted significance scores\n",
    "        \n",
    "        self.all_sig_ = all_sig\n",
    "        \n",
    "        if not num:num = int(np.ceil(self.neurons_added/self.tree.add_to_remove_ratio))\n",
    "        ## reset the neurons_added number if decay is started\n",
    "\n",
    "        remove_below = threshold_min\n",
    "        if num>len(all_sig):\n",
    "            remove_below = float(all_sig[-1])\n",
    "        elif num>0:\n",
    "            remove_below = float(all_sig[num-1])\n",
    "        \n",
    "        ### sig < threshold_min is always removed; whatsoever\n",
    "        if remove_below < threshold_min:\n",
    "            remove_below = threshold_min\n",
    "            \n",
    "        print(\"remove_below\", remove_below)\n",
    "        remove_below *= normalizer\n",
    "#         remove_below *= max_sig\n",
    "#         print(\"remove_below\", remove_below)\n",
    "\n",
    "        self._remove_below = remove_below\n",
    "#         self._remove_above = remove_above*normalizer\n",
    "        self._remove_above = None\n",
    "\n",
    "        return remove_below, all_sigs\n",
    "\n",
    "    def decay_neuron_start(self, decay_steps=1000):\n",
    "        if self._remove_below is None: return 0\n",
    "        \n",
    "        self.neurons_added = 0 ## resetting this variable\n",
    "        \n",
    "        self.tree.total_decay_steps = decay_steps\n",
    "        self.tree.current_decay_step = 0\n",
    "        self.tree.remove_neuron_residual = set()\n",
    "        self.tree.freeze_connection_shortcut = set()\n",
    "        self.tree.decay_connection_shortcut = set()\n",
    "        \n",
    "        count_remove = 0\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                ### always prune 1 % of the neurons randomly. It might overlap with less significant neurons\n",
    "                mask = torch.bernoulli(torch.ones_like(hr.residual.significance)*0.05).type(torch.bool)\n",
    "                count_remove += hr.residual.identify_removable_neurons(below=self._remove_below,\n",
    "                                                                       above=self._remove_above,\n",
    "                                                                       mask = mask\n",
    "                                                                      )\n",
    "        if count_remove<1:\n",
    "            self.tree.clear_decay_variables()\n",
    "        return count_remove\n",
    "    \n",
    "    def decay_neuron_step(self):\n",
    "        if self.tree.total_decay_steps is None:\n",
    "            return\n",
    "        \n",
    "        self.tree.current_decay_step += 1\n",
    "        \n",
    "        if self.tree.current_decay_step < self.tree.total_decay_steps:\n",
    "            self.tree.get_decay_factor()\n",
    "            for sh in self.tree.decay_connection_shortcut:\n",
    "                sh.decay_connection_step()\n",
    "            for sh in self.tree.freeze_connection_shortcut:\n",
    "                sh.freeze_connection_step()\n",
    "        else:\n",
    "            for rs in self.tree.remove_neuron_residual:\n",
    "                rs.remove_decayed_neurons()\n",
    "            \n",
    "            self.tree.clear_decay_variables()\n",
    "            \n",
    "    def compute_del_neurons(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.compute_del_neurons()\n",
    "    \n",
    "    def maintain_network(self):\n",
    "        self.root_net.maintain_shortcut_connection()\n",
    "        self.root_net.morph_network()\n",
    "        \n",
    "    def start_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.start_computing_significance()\n",
    "\n",
    "    def finish_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.finish_computing_significance()\n",
    "            \n",
    "    def print_network_debug(self):\n",
    "        self.root_net.print_network_debug(0)\n",
    "        \n",
    "    def print_network(self):\n",
    "        print(self.root_net.input_dim)\n",
    "        self.root_net.print_network()\n",
    "        print(\"│\")\n",
    "        print(self.root_net.output_dim)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.binomial(1, 0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.bernoulli(torch.ones(10)*0.01).type(torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dycnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = transforms.Compose([\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "cifar_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet = Dynamic_CNN(device).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563, 313)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameters\n",
    "learning_rate = 0.001\n",
    "# learning_rate = 0.00003\n",
    "\n",
    "num_add_neuron = 50#25#10\n",
    "num_decay_steps = int(len(train_loader)*3)#3\n",
    "\n",
    "remove_above = 10\n",
    "threshold_max = 1\n",
    "threshold_min = 0.01\n",
    "\n",
    "train_epoch_min = 1 #1\n",
    "train_epoch_max = 6 #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.add_to_remove_ratio = 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4689"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decay_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = '00.0'\n",
    "name = 'dynCNNbn'\n",
    "exp_index = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = {\n",
    "    'learning_rate':learning_rate,\n",
    "    'num_add_neuron':num_add_neuron,\n",
    "    'num_decay_steps':num_decay_steps,\n",
    "    'remove_above':remove_above,\n",
    "    'threshold_max':threshold_max,\n",
    "    'train_epoch_min':train_epoch_min,\n",
    "    'train_epoch_max':train_epoch_max,\n",
    "    'add_to_remove_ratio':dynet.tree.add_to_remove_ratio,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_json = f'hyperparameters/{index}_hyp.json'\n",
    "with open(hyp_json, 'w') as fp:\n",
    "    json.dump(hyp, fp, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoTrainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_func = None\n",
    "        self.adding_func = None\n",
    "        self.pruning_func = None\n",
    "        self.maintainance_func = None\n",
    "        self.extra_func = None\n",
    "        \n",
    "        self.log_func = None\n",
    "        \n",
    "    def loop(self, count = 15):\n",
    "        cb = count\n",
    "        for i in range(count):\n",
    "            if i>-0.1:\n",
    "                self.adding_func()\n",
    "            else:\n",
    "                global optimizer, warmup\n",
    "                dynet.print_network()    \n",
    "                optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#                 optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "                warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader))\n",
    "            \n",
    "            self.training_func()\n",
    "            \n",
    "            if i>-0.1:\n",
    "                self.pruning_func()\n",
    "            \n",
    "            self.maintainance_func()\n",
    "            \n",
    "            self.log_func(i)\n",
    "            \n",
    "            if self.extra_func:\n",
    "                self.extra_func()\n",
    "            \n",
    "            print(f\"=====================\")\n",
    "            print(f\"===LOOPS FINISHED :{i} ===\")\n",
    "            print(f\"Pausing for 2 second to give user time to STOP PROCESS\")\n",
    "            time.sleep(2)\n",
    "        self.training_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when to stop training functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeff(num_iter, coeff0, coeff1, coeff2, coeff_opt, loss_list):\n",
    "    if len(loss_list)<10: return np.array([0]), np.array([0]), float(coeff0.data.cpu()[0])\n",
    "    \n",
    "    _t = torch.tensor(loss_list)\n",
    "    _t = (_t - _t[-1])/(_t[0]-_t.min()) ## normalize to make first point at 1 and last at 0 \n",
    "    _t = torch.clamp(_t, -1.1, 1.1)\n",
    "    _x = torch.linspace(0, 1, steps=len(_t))\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        coeff_opt.zero_grad()\n",
    "        _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "        _loss = ((_y - _t)**2).mean()\n",
    "        _loss.backward()\n",
    "        coeff_opt.step()\n",
    "\n",
    "        coeff0.data = torch.clamp(coeff0.data, -20., 20.)\n",
    "        coeff1.data = torch.clamp(coeff1.data, 0.7, 2.)\n",
    "        coeff2.data = torch.clamp(coeff2.data, -0.2,0.1)\n",
    "        \n",
    "    if torch.isnan(coeff0.data[0]):\n",
    "        coeff0.data[0] = 0.\n",
    "        coeff1.data[0] = 0.\n",
    "        coeff2.data[0] = 1. ## this gives signal\n",
    "        \n",
    "    _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "    return _x.numpy(), _t.numpy(), _y.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global variables\n",
    "optimizer = None\n",
    "warmup = None\n",
    "coeff_opt = None\n",
    "\n",
    "loss_all = []\n",
    "accs_all = []\n",
    "accs_test = []\n",
    "\n",
    "## for adam optimizer = \n",
    "# learning_rate *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupLR_Polynomial():\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_epoch, num_batch_in_epoch, power=5):\n",
    "        self.warmup_epoch = warmup_epoch\n",
    "        self.optimizer = optimizer\n",
    "        self.num_batch = num_batch_in_epoch\n",
    "        self.steps = 0\n",
    "        self.power = power\n",
    "        \n",
    "    def step(self):\n",
    "        steps = self.steps/self.num_batch\n",
    "        self.steps += 1\n",
    "        \n",
    "        factor = 1\n",
    "        warming = False\n",
    "        if steps<self.warmup_epoch:\n",
    "            factor = (steps/self.warmup_epoch)**self.power\n",
    "            warming = True\n",
    "            \n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] *= factor\n",
    "        \n",
    "        return warming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neurons_func():\n",
    "    global optimizer, warmup, added\n",
    "    ### number of neurons\n",
    "    count = 0\n",
    "    for hr in dynet.tree.DYNAMIC_LIST:\n",
    "        if hr.residual:\n",
    "            count += hr.residual.hidden_dim\n",
    "    ## add more neurons relatively (+x%)\n",
    "    adding = num_add_neuron+int(0.07*count)\n",
    "    dynet.add_neurons(adding)\n",
    "    print(f\"Adding {adding} Neurons\")\n",
    "    added = adding\n",
    "    dynet.print_network()    \n",
    "    optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader), power=1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_network_func():\n",
    "    global optimizer, warmup, loss_all, accs_all\n",
    "    \n",
    "    coeff0 = torch.zeros(1, requires_grad=True)\n",
    "    coeff1 = torch.zeros(1, requires_grad=True)\n",
    "    coeff2 = torch.zeros(1, requires_grad=True)\n",
    "    coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "    loss_list = []\n",
    "    prev_loss = None\n",
    "    beta_loss = (1000-1)/1000\n",
    "    loss_ = []\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    breakall=False\n",
    "    \n",
    "\n",
    "    steps_ = -1\n",
    "    for epoch in range(train_epoch_max):\n",
    "        \n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "        for train_x, train_y in train_loader:\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "            steps_ += 1\n",
    "            \n",
    "            dynet.decay_neuron_step()\n",
    "            dynet.tree.std_loss = 0.    \n",
    "\n",
    "            yout = dynet(train_x)\n",
    "            loss = criterion(yout, train_y) + dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "            \n",
    "            if steps_>100:\n",
    "                prev_loss = (1-beta_loss)*float(loss)+beta_loss*prev_loss\n",
    "                loss_list.append(prev_loss)\n",
    "            elif steps_ == 100:\n",
    "                loss_.append(float(loss))\n",
    "                prev_loss = np.mean(loss_)\n",
    "                loss_ = []\n",
    "            else:\n",
    "                loss_.append(float(loss))\n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=False)\n",
    "            warmup.step()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "            targets = train_y.data.cpu().numpy()\n",
    "\n",
    "            correct = (outputs == targets).sum()\n",
    "            train_acc += correct\n",
    "            train_count += len(outputs)\n",
    "\n",
    "            if steps_%100 == 0 and steps_>0:\n",
    "                if len(loss_list)>0:\n",
    "                    max_indx = np.argmax(loss_list)\n",
    "                    loss_list = loss_list[max_indx:]\n",
    "    #                 loss_all.append(float(loss))\n",
    "                \n",
    "                _x, _t, _y = update_coeff(50, coeff0, coeff1, coeff2, coeff_opt, loss_list)\n",
    "                _c = float(coeff0.data.cpu()[0])\n",
    "    #             if coeff2.data[0] > 0.5: ## this is a signal to reset optimizer\n",
    "                coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "                _info = f'ES: {epoch}:{steps_}, coeff:{_c:.3f}/{-5}, \\nLoss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "\n",
    "                ax.clear()\n",
    "                if len(_x)>0:\n",
    "                    ax.plot(_x, _t, c='c')\n",
    "                    ax.plot(_x, _y, c='m')\n",
    "                xmin, xmax = ax.get_xlim()\n",
    "                ymin, ymax = ax.get_ylim()\n",
    "                ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                ax2.clear()\n",
    "                if len(accs_all)>0:\n",
    "                    acc_tr = accs_all\n",
    "                    acc_te = accs_test\n",
    "                    if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                    if len(acc_te)>20: acc_te = acc_te[-20:]\n",
    "                    ax2.plot(acc_tr, label=\"train\")\n",
    "                    ax2.plot(acc_te, label=\"test\")\n",
    "                    ax2.legend(loc=\"lower right\")\n",
    "                    \n",
    "                    ymin, ymax = ax2.get_ylim()\n",
    "                    ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                    ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_te):.3f} end{acc_te[-1]:.3f}\")\n",
    "\n",
    "                \n",
    "                fig.canvas.draw()\n",
    "                plt.savefig(f\"./output/logs/_{index}_temp_train_plot.png\")\n",
    "\n",
    "                if _c < -5 and epoch>train_epoch_min: \n",
    "                    breakall=True\n",
    "                    break\n",
    "                    \n",
    "        if not breakall:\n",
    "            accs_all.append(train_acc/train_count*100.)\n",
    "            with torch.no_grad():\n",
    "                corrects = 0\n",
    "                dynet.eval()\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x  = test_x.to(device)\n",
    "                    yout = dynet.forward(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                    correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                    corrects += correct\n",
    "                dynet.train()\n",
    "                accs_test.append(corrects/len(test_dataset)*100)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_func():\n",
    "    global optimizer, warmup\n",
    "    optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader), power=0.5)\n",
    "    \n",
    "    \n",
    "    dynet.start_computing_significance()\n",
    "\n",
    "    for train_x, train_y in train_loader:\n",
    "        train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "        dynet.tree.std_loss = 0.    \n",
    "        yout = dynet(train_x)\n",
    "#         yout.backward(gradient=torch.ones_like(yout))\n",
    "        loss = criterion(yout, train_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=False)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    dynet.finish_computing_significance()\n",
    "    dynet.identify_removable_neurons(num=None,\n",
    "                                 threshold_min = threshold_min,\n",
    "                                 threshold_max = threshold_max)\n",
    "    num_remove = dynet.decay_neuron_start(decay_steps=num_decay_steps)\n",
    "    if num_remove > 0:\n",
    "        print(f\"pruning {num_remove} neurons.\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,4))\n",
    "        ax = fig.add_subplot(121)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        \n",
    "        loss_list = []\n",
    "        steps_ = -1\n",
    "        breakall=False\n",
    "        for epoch in range(train_epoch_max):\n",
    "            loss_ = []\n",
    "            train_acc = 0\n",
    "            train_count = 0\n",
    "            for train_x, train_y in train_loader:\n",
    "                train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "                steps_ += 1\n",
    "\n",
    "                dynet.decay_neuron_step()\n",
    "                dynet.tree.std_loss = 0.    \n",
    "\n",
    "                yout = dynet(train_x)\n",
    "                loss = criterion(yout, train_y) + dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "\n",
    "                loss_.append(float(loss))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=False)\n",
    "                warmup.step()\n",
    "                optimizer.step()\n",
    "\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                targets = train_y.data.cpu().numpy()\n",
    "                correct = (outputs == targets).sum()\n",
    "                train_acc += correct\n",
    "                train_count += len(outputs)\n",
    "\n",
    "                dynet.decay_neuron_step()\n",
    "                \n",
    "                if steps_%50 == 0 and steps_>0:\n",
    "                    loss = np.mean(loss_)\n",
    "                    loss_ = []\n",
    "                    loss_list.append(loss)\n",
    "                \n",
    "                if steps_%100 == 0 and steps_>0:\n",
    "                    \n",
    "                    _info = f'ES: {epoch}:{steps_}, Loss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "#                     print(_info)\n",
    "                    ax.clear()\n",
    "                    out = (yout.data.cpu().numpy()>0.5).astype(int)\n",
    "                    ax.plot(loss_list)\n",
    "                    \n",
    "                    xmin, xmax = ax.get_xlim()\n",
    "                    ymin, ymax = ax.get_ylim()\n",
    "                    ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                    ax2.clear()\n",
    "                    if len(accs_all)>0:\n",
    "                        acc_tr = accs_all\n",
    "                        acc_te = accs_test\n",
    "                        if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                        if len(acc_te)>20: acc_te = acc_te[-20:]\n",
    "                        ax2.plot(acc_tr, label=\"train\")\n",
    "                        ax2.plot(acc_te, label=\"test\")\n",
    "                        ax2.legend(loc=\"lower right\")\n",
    "\n",
    "                        ymin, ymax = ax2.get_ylim()\n",
    "                        ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                        ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_te):.3f} end{acc_te[-1]:.3f}\")\n",
    "\n",
    "                    \n",
    "                    fig.canvas.draw()\n",
    "                    plt.savefig(f\"./output/logs/_{index}_temp_prune_plot.png\")\n",
    "#                     plt.pause(0.01)\n",
    "#                     print(\"\\n\")\n",
    "                    \n",
    "                if steps_>num_decay_steps+int(num_decay_steps/2): breakall=True\n",
    "                if breakall: break\n",
    "            if breakall: break\n",
    "                \n",
    "        if not breakall:\n",
    "            accs_all.append(train_acc/train_count*100.)\n",
    "            with torch.no_grad():\n",
    "                corrects = 0\n",
    "                dynet.eval()\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x  = test_x.to(device)\n",
    "                    yout = dynet.forward(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                    correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                    corrects += correct\n",
    "                dynet.train()\n",
    "                accs_test.append(corrects/len(test_dataset)*100)\n",
    "        plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintain_network():\n",
    "    dynet.compute_del_neurons()\n",
    "    dynet.maintain_network()\n",
    "    dynet.print_network()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network_stat(loop_indx):\n",
    "    stdout = sys.stdout\n",
    "    s = io.StringIO(newline=\"\")\n",
    "    sys.stdout = s\n",
    "    dynet.print_network()\n",
    "    sys.stdout = stdout\n",
    "    s.seek(0)\n",
    "    # prints = s.read()\n",
    "    architecture = s.getvalue()\n",
    "    s.close()\n",
    "    \n",
    "    ### number of neurons\n",
    "    count = 0\n",
    "    for hr in dynet.tree.DYNAMIC_LIST:\n",
    "        if hr.residual:\n",
    "            count += hr.residual.hidden_dim\n",
    "    \n",
    "    with open(f\"output/logs/{index}_{name}_log_{exp_index}.txt\", \"a+\") as f:\n",
    "        if loop_indx == 0:\n",
    "            ### Print the configuration at top.\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "            \n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%B %d, %Y @ %H:%M:%S\")\n",
    "            f.write(f\"DateTime: {dt_string}\")\n",
    "            \n",
    "            f.write(f\"num_add_neuron :{num_add_neuron}\\n add_to_remove_ratio :{dynet.tree.add_to_remove_ratio}\\n\")\n",
    "            f.write(f\"learning_rate :{learning_rate}\\n num_decay_steps :{num_decay_steps}\\n\")\n",
    "            f.write(f\"threshold_max :{threshold_max}\\n threshold_min :{threshold_min}\\n\")\n",
    "            f.write(f\"train_epoch_min :{train_epoch_min}\\n threshold_max :{train_epoch_max}\\n\")\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "        \n",
    "        f.write(f\"####################| Loop:{loop_indx} | Epoch: {len(accs_all)} \\n\")\n",
    "        num_params = sum(p.numel() for p in dynet.parameters())\n",
    "        num_trainable = sum(p.numel() for p in dynet.parameters() if p.requires_grad)\n",
    "        f.write(f\"| Dynamic Neurons:{count} | Total Parameters: {num_params} | Trainable Parameters: {num_trainable}\\n\")\n",
    "        f.write(f\"| Train Acc:{accs_all[-1]:.3f} | Test Acc: {accs_test[-1]:.3f}\\n\")\n",
    "        f.write(architecture)\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyperparameters_from_json():\n",
    "    global learning_rate,num_add_neuron,num_decay_steps,\\\n",
    "            remove_above,threshold_max,train_epoch_min,train_epoch_max,\\\n",
    "            dynet\n",
    "    with open(hyp_json, 'r') as fp:\n",
    "        hyps = json.load(fp)\n",
    "        learning_rate = hyps['learning_rate']\n",
    "        num_add_neuron = hyps['num_add_neuron']\n",
    "        num_decay_steps = hyps['num_decay_steps']\n",
    "        threshold_max = hyps['threshold_max']\n",
    "        train_epoch_min = hyps['train_epoch_min']\n",
    "        train_epoch_max = hyps['train_epoch_max']\n",
    "        dynet.tree.add_to_remove_ratio = hyps['add_to_remove_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_func():\n",
    "    load_hyperparameters_from_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set all functions and begin automated loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AutoTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.adding_func = add_neurons_func\n",
    "trainer.training_func = training_network_func\n",
    "trainer.pruning_func = pruning_func\n",
    "trainer.maintainance_func = maintain_network\n",
    "trainer.log_func = save_network_stat\n",
    "trainer.extra_func = extra_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_neurons_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     8\n",
      "    ╔╝\n",
      "    8\n",
      "   ╔╝\n",
      "   16\n",
      "  ╔╝\n",
      "  32\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 56 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     20\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    15\n",
      "    ╠════╗\n",
      "    ║    5\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   20\n",
      "   ╠════╗\n",
      "   ║    5\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  36\n",
      "  ╠════╗\n",
      "  ║    11\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 37\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.17967960238456726, 5.320516109466553)\n",
      "Mean, Std: (1.0000001192092896, 0.9855154752731323)\n",
      "remove_below 0.3096477687358856\n",
      "Significance:\n",
      "tensor([299.2057, 138.4171, 124.9451, 228.7381,  71.1155], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 88.2262,  53.2452,  79.0213,  46.6073,  57.1214,  43.9829,  55.2252,\n",
      "         70.5724,  74.6714,  66.1216,  63.6045,  66.1883,  82.7493,  46.8760,\n",
      "        108.9046,  86.6028,  69.5759,  75.7971,  55.1729,  61.0102,  60.6324,\n",
      "         59.3218,  51.2186,  56.4702,  69.3757,  67.3667,  44.2929,  59.6115,\n",
      "         69.1819,  58.9155,  78.2391,  68.1083,  95.7594, 133.8967,  86.8942,\n",
      "        141.7176,  44.9154], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True, False, False,  True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([260.0374, 273.8337, 111.7061, 307.3742, 107.9651], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([124.6971, 174.2867, 150.5624, 108.1010, 157.6834, 143.3237,  96.4960,\n",
      "        122.8623,  46.1592, 111.6392,  74.4381], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "         True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([192.8567, 190.2104, 156.5830, 123.1045,  85.3334, 108.6217,  80.7428,\n",
      "        363.1187, 199.6420, 137.5304, 125.0798, 125.4354,  49.5571, 196.4817,\n",
      "        108.0415, 187.4590,  98.6766,  99.5087, 266.3069, 150.4548, 215.7340,\n",
      "         93.6883, 307.4424, 175.8286,  67.4022, 265.4214, 258.4127, 151.8612,\n",
      "        128.8687, 366.6701, 411.6708, 246.4286, 118.9539, 140.0053,  83.4548,\n",
      "         68.9959], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False,  True, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 360.8242,  486.3401,  190.9280,  739.8630,  639.1956,  319.5545,\n",
      "         384.6532,  384.9128,  874.5519,  327.2947,  181.6497,  264.1325,\n",
      "         181.4923,  452.3655, 1302.3820], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([296.4339, 326.2169, 164.3241], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 340.3853,  215.0117,  256.0529,  522.0649,  350.5248,  500.3481,\n",
      "         315.4470,  279.0681, 1203.8289,  751.5099, 1018.0947,  934.1844,\n",
      "         214.0782,  487.0779,  310.7504,  132.3894,  382.1361,  431.3054,\n",
      "        1132.9092,  560.3828], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([202.9452, 480.6163, 313.1692,  85.0630, 338.3642, 225.4531, 175.5911,\n",
      "        514.5038, 396.9970, 205.2871, 180.0692, 211.9133, 236.8129, 227.6392,\n",
      "        152.7265, 297.6460, 718.4164, 966.3004, 847.2861, 239.4751],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False,  True],\n",
      "       device='cuda:0')\n",
      "pruning 41 neurons.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-72e84a82c19d>:84: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  self.to_remove = torch.nonzero(mask).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     20\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    15\n",
      "    ╠════╗\n",
      "    ║    5\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   16\n",
      "   ╠════╗\n",
      "   ║    3\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :0 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 57 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     32\n",
      "     ╠════╗\n",
      "     ║    12\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    9\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  36\n",
      "  ╠════╗\n",
      "  ║    12\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-036bb0f9319e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-2b83527bc1a8>\u001b[0m in \u001b[0;36mloop\u001b[0;34m(self, count)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mwarmup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWarmupLR_Polynomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-6fb2bfc68281>\u001b[0m in \u001b[0;36mtraining_network_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mwarmup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.loop(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significance Stat:\n",
      "Min, Max: (0.09377192705869675, 8.59330940246582)\n",
      "Mean, Std: (1.0, 1.1411583423614502)\n",
      "remove_below 0.2752997875213623\n",
      "Significance:\n",
      "tensor([ 66.8951,  79.9173,  58.7942,  82.6012,  79.9783,  81.9402,  38.9842,\n",
      "         55.1065,  76.4500,  84.4421,  76.5988,  91.1477, 102.4111,  95.8581,\n",
      "         64.2486,  70.4334,  83.2980,  81.1182,  68.2141,  77.6980,  57.0823,\n",
      "         88.4452,  68.7478,  76.5936,  57.4599,  93.2435,  87.6101,  79.2936,\n",
      "         83.5295,  69.0583,  83.5971,  80.4612,  45.0809,  28.9465,  31.2496,\n",
      "         34.0118,  44.1813,  39.6092,  27.9103,  41.6908], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True,  True,  True, False,  True, False,  True,  True,  True, False,\n",
      "         True, False, False, False,  True,  True, False,  True,  True,  True,\n",
      "         True, False,  True,  True,  True, False, False,  True, False,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 219.7851,  414.3529,  400.9198,  262.8810,  577.2507,  543.6565,\n",
      "         623.5686,  614.1130, 1495.2745,   39.2629,  714.9155, 1395.2021,\n",
      "         110.4103], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 745.5691,  770.9700, 1926.7782,  485.7285,  330.6727,  608.2348,\n",
      "         634.8781,  676.1720, 2557.7117,  824.7438,  522.8613],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([153.4225, 157.6297, 206.2041,  93.1009, 170.7836, 179.5337, 145.9282,\n",
      "        233.7398], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([107.4099, 192.2452, 277.5740, 245.1981, 119.7246], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([118.4077, 120.1867, 105.7280, 138.7441, 106.0669, 234.0067],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([239.7757, 267.9015,  79.2809, 304.9558, 166.5075, 147.9782, 198.0863,\n",
      "        258.9722, 129.7808, 196.5428, 409.6802, 322.9768, 288.8892, 171.2477,\n",
      "        211.5477, 224.2108,  42.6557, 118.3373, 271.3690, 259.0655, 121.9063,\n",
      "        169.9981, 261.8765, 153.3487, 481.2382, 172.0875, 376.9563, 276.8899,\n",
      "        153.6199, 172.8810, 645.0677, 336.6094, 798.3557, 320.7003, 701.2107,\n",
      "        718.6799, 338.8727], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([270.7117, 238.2127, 379.6584, 402.4537, 380.0794, 434.5421, 304.3901,\n",
      "        622.0212, 310.0030, 671.5247, 593.5432, 195.7761, 200.4697, 359.0691,\n",
      "        440.0849, 472.6281, 248.0408, 639.5551, 913.8325, 736.2945, 153.8111,\n",
      "        469.9786, 656.2601], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([174.8540, 154.0283,  62.4338, 205.3641, 155.3280, 152.7145, 112.7208,\n",
      "        141.4182, 143.9144], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "pruning 36 neurons.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-1a8f1b484628>:87: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  self.to_remove = torch.nonzero(mask).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    6\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    11\n",
      "    ╠════╗\n",
      "    ║    5\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   22\n",
      "   ╠════╗\n",
      "   ║    8\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  33\n",
      "  ╠════╗\n",
      "  ║    7\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 12\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "trainer.training_func()\n",
    "trainer.pruning_func()\n",
    "trainer.maintainance_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.27"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    corrects = 0\n",
    "    dynet.eval()\n",
    "    for test_x, test_y in test_loader:\n",
    "        test_x  = test_x.to(device)\n",
    "        yout = dynet.forward(test_x)\n",
    "        outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "        correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "        corrects += correct\n",
    "    dynet.train()\n",
    "    acc = corrects/len(test_dataset)*100\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     corrects = 0\n",
    "#     dynet.train()\n",
    "#     for test_x, test_y in train_loader:\n",
    "#         test_x  = test_x.to(device)\n",
    "#         yout = dynet.forward(test_x)\n",
    "#         outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "#         correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "#         corrects += correct\n",
    "#     acc = corrects/len(train_dataset)*100\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-64-a4770090c0a9>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-64-a4770090c0a9>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    tr->41.786, 42.174, 41.692\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# tr->49.93, 49.852000000000004\n",
    "# te->53.2, 54.50000000000001\n",
    "# tr->49.984, 50.361999999999995\n",
    "# te->54.72, 54.22\n",
    "\n",
    "te->50.27, 50.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 58 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     18\n",
      "     ╠════╗\n",
      "     ║    13\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   29\n",
      "   ╠════╗\n",
      "   ║    15\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  40\n",
      "  ╠════╗\n",
      "  ║    14\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 15\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "trainer.adding_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.beta_del_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accs_all, label=\"train\")\n",
    "plt.plot(accs_test, label=\"test\")\n",
    "ymin, ymax = plt.gca().get_ylim()\n",
    "plt.text(0, 0.8*ymin+0.2*ymax, f\"Train-> max:{max(accs_all):.3f} end:{accs_all[-1]:.3f} \\nTest-> max:{max(accs_test):.3f} end:{accs_test[-1]:.3f}\")\n",
    "                    \n",
    "plt.legend()\n",
    "plt.savefig(f\"output/plots/{index}_{name}_cifar10_{exp_index}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.non_linearity.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
