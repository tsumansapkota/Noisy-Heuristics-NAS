{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "import time\n",
    "import sys, io\n",
    "\n",
    "import mylibrary.datasets as datasets\n",
    "import mylibrary.nnlib as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adam_custom\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.LongTensor([2,3])\n",
    "a += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes\n",
    "\n",
    "- Add BN after convolution directly.\n",
    "    - This helps keep weight norm uniform while changing the scaling parameter of BN\n",
    "    - This will help to make the weight gradient well behaved.\n",
    "    \n",
    "- Reuse Optimizer (Adam) for added or removed parameters\n",
    "    - This will (supposedly) remove unstable training\n",
    "    - Maybe we need to add different learning rate for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1]), tensor([2]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hidden_neuron_number(i, o):\n",
    "    nh =  (max(i,o)*(min(i,o)**2))**(1/3)\n",
    "#     return max(nh, 1)\n",
    "    return nh\n",
    "\n",
    "class Shortcut_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=(3,3), stride=1):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self._kernel = np.array(kernel, dtype=int)\n",
    "        self._padding = tuple(((self._kernel-1)/2).astype(int))\n",
    "        self._stride = stride\n",
    "        _wd = nn.Conv2d(input_dim, output_dim, self._kernel, stride=self._stride,\n",
    "                        padding=self._padding, bias=False).weight.data\n",
    "        ## Shape = OutputDim, InputDim, Kernel0, Kernel1\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "        del _wd\n",
    "        self.bn = nn.BatchNorm2d(output_dim)\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "        self.initial_freeze_bn = None\n",
    "        \n",
    "        self.add_parameters_to_optimizer()\n",
    "        return\n",
    "        \n",
    "    def add_parameters_to_optimizer(self):\n",
    "        ## internal optimizer\n",
    "#         print(list(self.parameters()))\n",
    "#         self.tree.optimizer.state[pp] = {'step':0, \"aa\":'hahaha'}\n",
    "\n",
    "# {'step': tensor([12, 12,  6,  6,  6]),\n",
    "#               'exp_avg': tensor([ 2.0893e-11,  7.7122e-10, -6.7105e-12, -5.0940e-10, -9.8008e-10]),\n",
    "#               'exp_avg_sq': tensor([2.3143e-19, 1.5871e-19, 3.0733e-20, 2.8796e-20, 4.1671e-20])}\n",
    "        \n",
    "        for p in self.parameters():\n",
    "#             self.tree.optimizer.state[p] = {}\n",
    "            self.tree.optimizer.param_groups[0]['params'].append(p)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.shape[1] > 0 and self.weight.shape[0] > 0:\n",
    "            out_dim = self.weight.shape[0]\n",
    "            self.weight.data /= torch.norm(self.weight.data.reshape(out_dim, -1), dim=1).reshape(out_dim, 1, 1, 1)\n",
    "            \n",
    "            return self.bn(F.conv2d(x, self.weight, stride=self._stride, padding=self._padding))\n",
    "        ### output dim is 0\n",
    "        elif self.weight.shape[0] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###       #out_dim #inp_dim            #kernel\n",
    "            w = torch.zeros(1, 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return torch.zeros(o.shape[0], 0, o.shape[2], o.shape[3], dtype=x.dtype, device=x.device)\n",
    "        ### input dim is 0\n",
    "        elif x.shape[1] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###             #out_dim            #inp_dim            #kernel\n",
    "            w = torch.zeros(self.weight.shape[0], 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return o.data\n",
    "        else:\n",
    "            raise(f\"Unknown shape of input {x.shape} or weight {self.weight.shape}\")\n",
    "\n",
    "#     def decay_std_ratio(self, factor):\n",
    "#         self.weight.data = self.weight.data - self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "        \n",
    "#     def decay_std_ratio_grad(self, factor):\n",
    "#         self.weight.grad = self.weight.grad + self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "    \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "#         self.initial_remove = torch.atan(self.weight.data[:, to_remove])\n",
    "\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.initial_freeze_bn = self.bn.weight.data[to_freeze], self.bn.bias.data[to_freeze]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    ## freeze output neuron's incoming weight \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        self.bn.weight.data[self.to_freeze] = self.initial_freeze_bn[0] \n",
    "        self.bn.bias.data[self.to_freeze] = self.initial_freeze_bn[1] \n",
    "        pass\n",
    "    \n",
    "    ## decay input neuron's outgoing weight \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "#         self.weight.data[:, self.to_remove] = torch.tan(self.initial_remove*self.tree.decay_factor)\n",
    "        pass\n",
    "     \n",
    "    ## remove output neuron \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "        \n",
    "        ### do the same thing to optimizer variables as well        \n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        self.weight.data = self.weight.data[remaining, :]\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        ops[self.weight][_var][remaining, :]\n",
    "        \n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        \n",
    "#         ## running_mean\n",
    "        _rm = self.bn.running_mean[remaining]\n",
    "        self.bn.running_mean = _rm\n",
    "        \n",
    "#         ## running_var\n",
    "        _rv = self.bn.running_var[remaining]\n",
    "        self.bn.running_var = _rv\n",
    "        \n",
    "#         ## weight\n",
    "        self.bn.weight.data = self.bn.weight.data[remaining]\n",
    "        self.bn.weight.grad = None\n",
    "        if len(ops[self.bn.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.bn.weight][_var] = \\\n",
    "                        ops[self.bn.weight][_var][remaining]\n",
    "\n",
    "        ## bias\n",
    "        self.bn.bias.data = self.bn.bias.data[remaining]\n",
    "        self.bn.bias.grad = None\n",
    "        if len(ops[self.bn.bias]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.bn.bias][_var] = \\\n",
    "                        ops[self.bn.bias][_var][remaining]\n",
    "        \n",
    "        self.bn.num_features = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    ## remove input neuron \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "#         print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "#         print(torch.count_nonzero(self.weight.data<1e-6))\n",
    "#         print(self.weight.data[:, self.to_remove])\n",
    "\n",
    "        ops = self.tree.optimizer.state\n",
    "\n",
    "        self.weight.data = self.weight.data[:, remaining]\n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        ops[self.weight][_var][:, remaining]\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        self.weight.data = torch.cat((self.weight.data, \\\n",
    "                                      torch.zeros(o, num, k0, k1, dtype=self.weight.data.dtype,\n",
    "                                      device=self.weight.data.device)), \n",
    "                                     dim=1)\n",
    "        self.weight.grad = None\n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        torch.cat((ops[self.weight][_var], \\\n",
    "                                  torch.zeros(o, num, k0, k1, dtype=ops[self.weight][_var].dtype,\n",
    "                                              device=ops[self.weight][_var].device)), \n",
    "                                  dim=1)\n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i) ### similar to Xavier init ?? !!\n",
    "#         stdv = torch.std(self.weight.data) ## if it does not work, revert it\n",
    "    \n",
    "        _new = torch.empty(num, i, k0, k1, dtype=self.weight.data.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        self.weight.data = torch.cat((self.weight.data, _new), dim=0)\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        torch.cat((ops[self.weight][_var], \\\n",
    "                                  torch.zeros(num, i, k0, k1, dtype=ops[self.weight][_var].dtype,\n",
    "                                              device=ops[self.weight][_var].device)), \n",
    "                                  dim=0)\n",
    "        \n",
    "        # print(self.weight.data.shape)\n",
    "                \n",
    "        ####https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d\n",
    "        ## running_mean\n",
    "        _rm = self.bn.running_mean\n",
    "        _rm = torch.cat((_rm, torch.zeros(num, dtype=_rm.dtype, device=_rm.device)))\n",
    "        self.bn.running_mean = _rm\n",
    "        \n",
    "        ## running_var\n",
    "        _rv = self.bn.running_var\n",
    "        _rv = torch.cat((_rv, torch.ones(num, dtype=_rv.dtype, device=_rv.device)))\n",
    "        self.bn.running_var = _rv\n",
    "        \n",
    "        ## weight\n",
    "        _w = self.bn.weight.data\n",
    "        _w = torch.cat((_w, torch.ones(num, dtype=_w.dtype, device=_w.device)))\n",
    "        self.bn.weight.data = _w\n",
    "        self.bn.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.bn.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.bn.weight][_var] = \\\n",
    "                        torch.cat((ops[self.bn.weight][_var], \\\n",
    "                                  torch.zeros(num, dtype=ops[self.bn.weight][_var].dtype,\n",
    "                                              device=ops[self.bn.weight][_var].device)), \n",
    "                                 )\n",
    "        \n",
    "        ## bias\n",
    "        _b = self.bn.bias.data\n",
    "        _b = torch.cat((_b, torch.zeros(num, dtype=_b.dtype, device=_b.device)))\n",
    "        self.bn.bias.data = _b\n",
    "        self.bn.bias.grad = None\n",
    "        \n",
    "        if len(ops[self.bn.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.bn.bias][_var] = \\\n",
    "                        torch.cat((ops[self.bn.bias][_var], \\\n",
    "                                  torch.zeros(num, dtype=ops[self.bn.bias][_var].dtype,\n",
    "                                              device=ops[self.bn.bias][_var].device)), \n",
    "                                 )\n",
    "        \n",
    "        self.bn.num_features += num\n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}S▚:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempTree():\n",
    "    def __init__(self):\n",
    "        self.optimizer = adam_custom.Adam([nn.Parameter(torch.Tensor(0))],\n",
    "                                          lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shortcut_Conv(\n",
       "  (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = TempTree()\n",
    "\n",
    "a = Shortcut_Conv(tree, 2, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\nTried to set an attribute: data on a non-class: Tensor:\n  File \"/tmp/ipykernel_721117/1549481170.py\", line 55\n            out_dim = self.weight.shape[0]\n            with torch.no_grad():\n                self.weight.data /= torch.norm(self.weight.data.reshape(out_dim, -1), dim=1).reshape(out_dim, 1, 1, 1)\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    #                 self.weight = self.weight/torch.norm(self.weight.reshape(out_dim, -1), dim=1).reshape(out_dim, 1, 1, 1)\n            \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/jit/_script.py:1265\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1264\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/jit/_recursive.py:454\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    453\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n\u001b[0;32m--> 520\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_stubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_stubs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;66;03m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;66;03m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/jit/_recursive.py:371\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    368\u001b[0m property_defs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mdef_ \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m    369\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[0;32m--> 371\u001b[0m \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nTried to set an attribute: data on a non-class: Tensor:\n  File \"/tmp/ipykernel_721117/1549481170.py\", line 55\n            out_dim = self.weight.shape[0]\n            with torch.no_grad():\n                self.weight.data /= torch.norm(self.weight.data.reshape(out_dim, -1), dim=1).reshape(out_dim, 1, 1, 1)\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    #                 self.weight = self.weight/torch.norm(self.weight.reshape(out_dim, -1), dim=1).reshape(out_dim, 1, 1, 1)\n            \n"
     ]
    }
   ],
   "source": [
    "torch.jit.script(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_hook_for_profile',\n",
       " '_zero_grad_profile_name',\n",
       " 'add_param_group',\n",
       " 'defaults',\n",
       " 'load_state_dict',\n",
       " 'param_groups',\n",
       " 'state',\n",
       " 'state_dict',\n",
       " 'step',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tree.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict, {})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.optimizer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 0.2281, -0.1195, -0.0211],\n",
       "           [ 0.0483,  0.1598, -0.2191],\n",
       "           [ 0.0193,  0.1493, -0.2305]],\n",
       " \n",
       "          [[ 0.0603,  0.1399, -0.0636],\n",
       "           [ 0.2329, -0.0229, -0.0310],\n",
       "           [ 0.0124,  0.1230, -0.0840]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.], requires_grad=True)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.optimizer.param_groups[0]['params'] ##.append(a.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tree\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 2\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# loss.backward(create_graph=False, retain_graph=False)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36mShortcut_Conv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m             out_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#             self.weight.data /= torch.norm(self.weight.data.reshape(out_dim, -1), dim=1).reshape(out_dim, 1, 1, 1)\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mreshape(out_dim, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(out_dim, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(F\u001b[38;5;241m.\u001b[39mconv2d(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stride, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_padding))\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;66;03m### output dim is 0\u001b[39;00m\n",
      "File \u001b[0;32m~/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1206\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1207\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Parameter or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1208\u001b[0m                         \u001b[38;5;241m.\u001b[39mformat(torch\u001b[38;5;241m.\u001b[39mtypename(value), name))\n\u001b[1;32m   1209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(name, value)\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "tree.optimizer.zero_grad()\n",
    "loss = a(torch.randn(3, 2, 4,4)).mean()\n",
    "# loss.backward(create_graph=False, retain_graph=False)\n",
    "loss.backward()\n",
    "tree.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = Shortcut_Conv(\"tree\", 2, 5).weight.data # O, I, k1,k2\n",
    "# # n = torch.norm(a.reshape(5, -1), dim=1, keepdim=True).unsqueeze(2).unsqueeze(2)\n",
    "# n = torch.norm(a.reshape(5, -1), dim=1).reshape(5, 1, 1, 1)\n",
    "# a_ = a/n\n",
    "# torch.norm(a_.reshape(5, -1), dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TempTree():\n",
    "#     def __init__(self, model):\n",
    "#         self.optimizer = adam_custom.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "tt = TempTree()\n",
    "a = Shortcut_Conv(tt, i, 5)\n",
    "# tt = TempTree(a)\n",
    "# a.tree = tt\n",
    "# tt.optimizer = adam_custom.Adam(a.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Optimizer.state_dict of Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.optimizer.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.optimizer.zero_grad()\n",
    "a(torch.randn(3, i, 4,4)).mean().backward()\n",
    "tt.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: torch.Size([5, 4, 3, 3])\n",
      "step : torch.Size([5, 4, 3, 3])\n",
      "exp_avg : torch.Size([5, 4, 3, 3])\n",
      "exp_avg_sq : torch.Size([5, 4, 3, 3])\n",
      "\n",
      "Key: torch.Size([5])\n",
      "step : torch.Size([5])\n",
      "exp_avg : torch.Size([5])\n",
      "exp_avg_sq : torch.Size([5])\n",
      "\n",
      "Key: torch.Size([5])\n",
      "step : torch.Size([5])\n",
      "exp_avg : torch.Size([5])\n",
      "exp_avg_sq : torch.Size([5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tt.optimizer.state[a.weight]['step']\n",
    "for k, v in tt.optimizer.state.items():\n",
    "    print('Key:', k.shape)\n",
    "    for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "        print(_var,':',v['step'].shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.bn.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.remove_freezed_connection([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.add_output_connection(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.add_input_connection(3)\n",
    "i += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.remove_decayed_connection([0, 1])\n",
    "i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.optimizer.zero_grad()\n",
    "loss = a(torch.randn(3, i, 4,4)).mean()\n",
    "loss.backward()\n",
    "# loss.backward(create_graph=False, retain_graph=False)\n",
    "tt.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 3, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {Parameter containing:\n",
       "             tensor([[[[-0.0168, -0.0777,  0.3159],\n",
       "                       [-0.1555, -0.2693, -0.3841],\n",
       "                       [ 0.3062,  0.0974, -0.0458]],\n",
       "             \n",
       "                      [[ 0.2564, -0.3905,  0.2313],\n",
       "                       [-0.1225, -0.0089, -0.0048],\n",
       "                       [-0.3576, -0.2372,  0.2707]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.2100,  0.0272, -0.1049],\n",
       "                       [ 0.0369, -0.1703, -0.3158],\n",
       "                       [ 0.3324, -0.3620, -0.0033]],\n",
       "             \n",
       "                      [[ 0.1970,  0.2450, -0.1308],\n",
       "                       [ 0.3127, -0.2900, -0.1631],\n",
       "                       [-0.3547, -0.2212,  0.2708]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.3423,  0.4263, -0.1677],\n",
       "                       [-0.1491,  0.1547,  0.1024],\n",
       "                       [ 0.1350,  0.1476,  0.2200]],\n",
       "             \n",
       "                      [[ 0.0464, -0.0258, -0.1098],\n",
       "                       [ 0.0819,  0.4025,  0.3158],\n",
       "                       [-0.2587,  0.3940, -0.1498]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.3937, -0.0624, -0.4071],\n",
       "                       [-0.1780, -0.0574, -0.2159],\n",
       "                       [-0.2386, -0.0918, -0.1386]],\n",
       "             \n",
       "                      [[ 0.1138, -0.3518,  0.0690],\n",
       "                       [ 0.0903, -0.1836,  0.4237],\n",
       "                       [ 0.3786,  0.0196, -0.0511]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.3330,  0.1087,  0.1946],\n",
       "                       [-0.1237, -0.2530,  0.2377],\n",
       "                       [ 0.2589,  0.3286, -0.2895]],\n",
       "             \n",
       "                      [[ 0.1216,  0.2684,  0.1313],\n",
       "                       [-0.2175, -0.2149, -0.3426],\n",
       "                       [ 0.1773,  0.0017, -0.3137]]]], requires_grad=True): {'step': tensor([[[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]],\n",
       "              \n",
       "              \n",
       "                      [[[1, 1, 1],\n",
       "                        [1, 1, 1],\n",
       "                        [1, 1, 1]],\n",
       "              \n",
       "                       [[1, 1, 1],\n",
       "                        [1, 1, 1],\n",
       "                        [1, 1, 1]]],\n",
       "              \n",
       "              \n",
       "                      [[[1, 1, 1],\n",
       "                        [1, 1, 1],\n",
       "                        [1, 1, 1]],\n",
       "              \n",
       "                       [[1, 1, 1],\n",
       "                        [1, 1, 1],\n",
       "                        [1, 1, 1]]],\n",
       "              \n",
       "              \n",
       "                      [[[1, 1, 1],\n",
       "                        [1, 1, 1],\n",
       "                        [1, 1, 1]],\n",
       "              \n",
       "                       [[1, 1, 1],\n",
       "                        [1, 1, 1],\n",
       "                        [1, 1, 1]]]]),\n",
       "              'exp_avg': tensor([[[[ 5.2761e-11,  2.7271e-11,  7.4345e-11],\n",
       "                        [-3.8394e-11, -1.8359e-10, -2.3686e-10],\n",
       "                        [ 1.4292e-10,  6.2095e-11,  7.5258e-11]],\n",
       "              \n",
       "                       [[ 6.9232e-11, -1.2067e-10,  1.0105e-10],\n",
       "                        [-3.7471e-11, -1.3258e-10, -8.7794e-11],\n",
       "                        [-3.3348e-11, -7.1177e-11,  7.1436e-11]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.3837e-11, -1.8908e-11, -1.5319e-11],\n",
       "                        [-7.4914e-12,  5.0667e-11,  1.0911e-10],\n",
       "                        [-6.8492e-11,  9.1967e-11,  3.0726e-11]],\n",
       "              \n",
       "                       [[-1.6610e-11, -7.4575e-11, -1.8152e-11],\n",
       "                        [-3.3966e-11,  1.1286e-10, -4.1826e-12],\n",
       "                        [ 1.0394e-10,  3.7981e-11, -4.3517e-11]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.2034e-11, -1.9094e-10,  1.2238e-10],\n",
       "                        [ 1.6021e-10, -2.1109e-10, -1.1094e-10],\n",
       "                        [ 6.3426e-12,  9.2491e-11, -2.0235e-10]],\n",
       "              \n",
       "                       [[-6.6419e-11, -4.9734e-11,  1.0548e-10],\n",
       "                        [-1.1563e-10, -2.9162e-10, -8.6434e-11],\n",
       "                        [ 1.1550e-10, -2.5554e-10,  1.3119e-10]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.8274e-10,  9.1034e-12,  1.5480e-10],\n",
       "                        [ 1.9605e-11, -7.7954e-11,  1.5021e-10],\n",
       "                        [ 3.2590e-11, -6.9519e-11,  7.7902e-11]],\n",
       "              \n",
       "                       [[-1.7873e-11,  1.6183e-10, -5.9303e-11],\n",
       "                        [-9.6993e-11,  1.6229e-10, -1.3677e-10],\n",
       "                        [-1.3297e-10,  7.6248e-11,  2.2102e-14]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.1951e-11, -1.7625e-11, -1.9466e-11],\n",
       "                        [ 4.4414e-11,  1.1545e-10, -3.7405e-11],\n",
       "                        [-1.5194e-11, -6.6427e-11,  2.0375e-11]],\n",
       "              \n",
       "                       [[-3.5811e-11, -6.7275e-11, -4.6003e-11],\n",
       "                        [ 8.7689e-11,  9.1433e-11,  9.2488e-11],\n",
       "                        [-4.4688e-11,  2.5924e-11,  4.9847e-11]]]]),\n",
       "              'exp_avg_sq': tensor([[[[3.8934e-22, 1.6049e-22, 5.8568e-22],\n",
       "                        [2.1987e-22, 3.5342e-21, 6.1193e-21],\n",
       "                        [2.2052e-21, 4.0869e-22, 9.6076e-22]],\n",
       "              \n",
       "                       [[5.4636e-22, 1.6020e-21, 9.7482e-22],\n",
       "                        [1.0083e-22, 1.7703e-21, 1.0756e-21],\n",
       "                        [1.0886e-22, 4.3839e-22, 4.0880e-22]]],\n",
       "              \n",
       "              \n",
       "                      [[[2.8984e-22, 3.5751e-23, 2.3466e-23],\n",
       "                        [5.6121e-24, 2.5672e-22, 1.1905e-21],\n",
       "                        [4.6912e-22, 8.4580e-22, 9.4407e-23]],\n",
       "              \n",
       "                       [[2.7590e-23, 5.5614e-22, 3.2948e-23],\n",
       "                        [1.1537e-22, 1.2737e-21, 1.7494e-24],\n",
       "                        [1.0804e-21, 1.4426e-22, 1.8937e-22]]],\n",
       "              \n",
       "              \n",
       "                      [[[6.7296e-22, 3.6458e-21, 1.4978e-21],\n",
       "                        [2.5668e-21, 4.4559e-21, 1.2307e-21],\n",
       "                        [4.0229e-24, 8.5546e-22, 4.0946e-21]],\n",
       "              \n",
       "                       [[4.4115e-22, 2.4735e-22, 1.1125e-21],\n",
       "                        [1.3371e-21, 8.5043e-21, 7.4709e-22],\n",
       "                        [1.3340e-21, 6.5301e-21, 1.7210e-21]]],\n",
       "              \n",
       "              \n",
       "                      [[[3.3392e-21, 8.2872e-24, 2.3964e-21],\n",
       "                        [3.8435e-23, 6.0768e-22, 2.2563e-21],\n",
       "                        [1.0621e-22, 4.8329e-22, 6.0687e-22]],\n",
       "              \n",
       "                       [[3.1946e-23, 2.6189e-21, 3.5169e-22],\n",
       "                        [9.4076e-22, 2.6338e-21, 1.8707e-21],\n",
       "                        [1.7681e-21, 5.8137e-22, 4.8852e-29]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.7599e-22, 3.1063e-23, 3.7891e-23],\n",
       "                        [1.9726e-22, 1.3328e-21, 1.3992e-22],\n",
       "                        [2.3087e-23, 4.4126e-22, 4.1512e-23]],\n",
       "              \n",
       "                       [[1.2824e-22, 4.5259e-22, 2.1163e-22],\n",
       "                        [7.6893e-22, 8.3600e-22, 8.5540e-22],\n",
       "                        [1.9970e-22, 6.7208e-23, 2.4848e-22]]]])},\n",
       "             Parameter containing:\n",
       "             tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], requires_grad=True): {'step': tensor([2, 2, 1, 1, 1]),\n",
       "              'exp_avg': tensor([-4.3311e-10,  2.2863e-10,  5.4565e-10,  3.8760e-10,  2.1731e-10]),\n",
       "              'exp_avg_sq': tensor([1.9971e-20, 5.2273e-21, 2.9774e-20, 1.5023e-20, 4.7222e-21])},\n",
       "             Parameter containing:\n",
       "             tensor([-2.0000e-04, -2.0000e-04, -9.9999e-05, -9.9999e-05, -9.9999e-05],\n",
       "                    requires_grad=True): {'step': tensor([2, 2, 1, 1, 1]),\n",
       "              'exp_avg': tensor([0.0380, 0.0380, 0.0200, 0.0200, 0.0200]),\n",
       "              'exp_avg_sq': tensor([7.9960e-05, 7.9960e-05, 4.0000e-05, 4.0000e-05, 4.0000e-05])}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.optimizer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.optimizer.zero_grad()\n",
    "a(torch.randn(3, i, 4,4)).mean().backward()\n",
    "tt.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0003, -0.0003, -0.0002, -0.0002, -0.0002], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.bn.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1.0001, 1.0000, 1.0000, 1.0000, 1.0000], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.bn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 3, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {Parameter containing:\n",
       "             tensor([[[[-0.0168, -0.0777,  0.3159],\n",
       "                       [-0.1555, -0.2693, -0.3841],\n",
       "                       [ 0.3062,  0.0974, -0.0458]],\n",
       "             \n",
       "                      [[ 0.2564, -0.3905,  0.2313],\n",
       "                       [-0.1225, -0.0089, -0.0048],\n",
       "                       [-0.3576, -0.2372,  0.2708]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.2100,  0.0272, -0.1049],\n",
       "                       [ 0.0369, -0.1703, -0.3158],\n",
       "                       [ 0.3324, -0.3620, -0.0033]],\n",
       "             \n",
       "                      [[ 0.1970,  0.2450, -0.1308],\n",
       "                       [ 0.3127, -0.2900, -0.1631],\n",
       "                       [-0.3547, -0.2212,  0.2708]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.3423,  0.4263, -0.1677],\n",
       "                       [-0.1491,  0.1547,  0.1024],\n",
       "                       [ 0.1350,  0.1476,  0.2200]],\n",
       "             \n",
       "                      [[ 0.0464, -0.0258, -0.1098],\n",
       "                       [ 0.0819,  0.4025,  0.3158],\n",
       "                       [-0.2587,  0.3940, -0.1498]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.3937, -0.0624, -0.4071],\n",
       "                       [-0.1780, -0.0574, -0.2159],\n",
       "                       [-0.2386, -0.0918, -0.1386]],\n",
       "             \n",
       "                      [[ 0.1138, -0.3518,  0.0690],\n",
       "                       [ 0.0903, -0.1836,  0.4237],\n",
       "                       [ 0.3786,  0.0196, -0.0511]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.3330,  0.1087,  0.1946],\n",
       "                       [-0.1237, -0.2531,  0.2377],\n",
       "                       [ 0.2589,  0.3286, -0.2895]],\n",
       "             \n",
       "                      [[ 0.1216,  0.2684,  0.1313],\n",
       "                       [-0.2175, -0.2149, -0.3426],\n",
       "                       [ 0.1773,  0.0017, -0.3137]]]], requires_grad=True): {'step': tensor([[[[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]],\n",
       "              \n",
       "                       [[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]]],\n",
       "              \n",
       "              \n",
       "                      [[[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]],\n",
       "              \n",
       "                       [[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]]]),\n",
       "              'exp_avg': tensor([[[[ 5.1803e-11,  3.6380e-11,  1.8502e-11],\n",
       "                        [-2.8003e-11, -1.3493e-10, -1.2974e-10],\n",
       "                        [ 1.2106e-10,  6.9523e-11,  5.1638e-11]],\n",
       "              \n",
       "                       [[ 4.2756e-11, -4.4246e-11,  7.7836e-11],\n",
       "                        [-2.0230e-11, -8.0620e-11, -6.4396e-11],\n",
       "                        [ 4.1732e-11, -9.0318e-12,  6.7171e-11]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.8453e-11, -1.7017e-11, -1.3787e-11],\n",
       "                        [-6.7423e-12,  4.5601e-11,  9.8201e-11],\n",
       "                        [-6.1643e-11,  8.2771e-11,  2.7653e-11]],\n",
       "              \n",
       "                       [[-1.4949e-11, -6.7117e-11, -1.6336e-11],\n",
       "                        [-3.0569e-11,  1.0157e-10, -3.7643e-12],\n",
       "                        [ 9.3547e-11,  3.4183e-11, -3.9165e-11]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.2510e-11, -8.4301e-11, -1.5070e-11],\n",
       "                        [ 1.0781e-10, -6.6687e-11, -8.3821e-11],\n",
       "                        [ 9.4142e-11,  5.2812e-12, -6.8527e-11]],\n",
       "              \n",
       "                       [[-4.3812e-11, -4.8985e-11,  9.1285e-11],\n",
       "                        [-5.8949e-12, -4.2884e-11,  8.8333e-11],\n",
       "                        [ 4.6101e-11, -1.1641e-10,  1.2153e-10]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.2159e-10,  6.4340e-11,  1.5723e-10],\n",
       "                        [ 6.1205e-11, -1.0513e-10,  1.7906e-10],\n",
       "                        [ 2.0512e-11, -3.2854e-11,  4.0073e-11]],\n",
       "              \n",
       "                       [[-2.4266e-11,  1.8231e-10, -3.0556e-11],\n",
       "                        [-5.2091e-11,  1.6771e-10, -1.9614e-10],\n",
       "                        [-1.7643e-10,  2.7582e-11,  4.8411e-12]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0253e-10, -6.3764e-12, -2.7417e-11],\n",
       "                        [ 6.5915e-11,  2.3033e-10, -1.3217e-10],\n",
       "                        [ 1.1229e-11, -1.2870e-10,  1.2146e-10]],\n",
       "              \n",
       "                       [[-8.2821e-11, -1.8209e-10, -6.0570e-11],\n",
       "                        [ 1.7450e-10,  2.4051e-10,  2.2206e-10],\n",
       "                        [-7.1001e-11,  4.0470e-11,  7.1391e-11]]]]),\n",
       "              'exp_avg_sq': tensor([[[[3.9082e-22, 1.7434e-22, 8.1944e-22],\n",
       "                        [2.2394e-22, 3.6225e-21, 6.8094e-21],\n",
       "                        [2.2088e-21, 4.2688e-22, 9.8570e-22]],\n",
       "              \n",
       "                       [[5.8405e-22, 2.0146e-21, 9.9103e-22],\n",
       "                        [1.1893e-22, 1.9183e-21, 1.0959e-21],\n",
       "                        [6.2348e-22, 7.4076e-22, 4.0922e-22]]],\n",
       "              \n",
       "              \n",
       "                      [[[2.8955e-22, 3.5715e-23, 2.3442e-23],\n",
       "                        [5.6065e-24, 2.5646e-22, 1.1894e-21],\n",
       "                        [4.6865e-22, 8.4495e-22, 9.4313e-23]],\n",
       "              \n",
       "                       [[2.7562e-23, 5.5558e-22, 3.2915e-23],\n",
       "                        [1.1525e-22, 1.2725e-21, 1.7476e-24],\n",
       "                        [1.0793e-21, 1.4411e-22, 1.8918e-22]]],\n",
       "              \n",
       "              \n",
       "                      [[[7.1774e-22, 4.4086e-21, 3.0642e-21],\n",
       "                        [2.6966e-21, 5.9716e-21, 1.2552e-21],\n",
       "                        [7.8608e-22, 1.4624e-21, 5.3807e-21]],\n",
       "              \n",
       "                       [[4.6619e-22, 2.4888e-22, 1.1127e-21],\n",
       "                        [2.2996e-21, 1.3317e-20, 3.5061e-21],\n",
       "                        [1.6673e-21, 7.8136e-21, 1.7205e-21]]],\n",
       "              \n",
       "              \n",
       "                      [[[3.6622e-21, 3.2353e-22, 2.4261e-21],\n",
       "                        [2.2815e-22, 7.2938e-22, 2.4465e-21],\n",
       "                        [1.1388e-22, 5.7110e-22, 6.9650e-22]],\n",
       "              \n",
       "                       [[3.8606e-23, 2.7507e-21, 4.0340e-22],\n",
       "                        [1.0637e-21, 2.6780e-21, 2.4024e-21],\n",
       "                        [2.0884e-21, 7.4923e-22, 2.3245e-24]]],\n",
       "              \n",
       "              \n",
       "                      [[[5.9538e-22, 4.0030e-23, 4.7649e-23],\n",
       "                        [2.6436e-22, 2.9299e-21, 1.1102e-21],\n",
       "                        [8.5087e-23, 9.1581e-22, 1.1049e-21]],\n",
       "              \n",
       "                       [[3.8406e-22, 1.9293e-21, 2.4815e-22],\n",
       "                        [1.6817e-21, 3.3386e-21, 2.7817e-21],\n",
       "                        [2.9426e-22, 9.6512e-23, 3.1861e-22]]]])},\n",
       "             Parameter containing:\n",
       "             tensor([1.0001, 1.0000, 1.0000, 1.0000, 1.0000], requires_grad=True): {'step': tensor([3, 3, 2, 2, 2]),\n",
       "              'exp_avg': tensor([-2.5886e-10,  2.0577e-10,  1.6456e-10,  4.6034e-10,  4.8010e-10]),\n",
       "              'exp_avg_sq': tensor([2.1666e-20, 5.2220e-21, 4.0406e-20, 1.6251e-20, 1.2813e-20])},\n",
       "             Parameter containing:\n",
       "             tensor([-0.0003, -0.0003, -0.0002, -0.0002, -0.0002], requires_grad=True): {'step': tensor([3, 3, 2, 2, 2]),\n",
       "              'exp_avg': tensor([0.0542, 0.0542, 0.0380, 0.0380, 0.0380]),\n",
       "              'exp_avg_sq': tensor([1.1988e-04, 1.1988e-04, 7.9960e-05, 7.9960e-05, 7.9960e-05])}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.optimizer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = nn.Parameter(torch.zeros(10))\n",
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.optimizer.state[pp] = {'step':0, \"aa\":'hahaha'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {Parameter containing:\n",
       "             tensor([[[[-0.0168, -0.0777,  0.3159],\n",
       "                       [-0.1555, -0.2693, -0.3841],\n",
       "                       [ 0.3062,  0.0974, -0.0458]],\n",
       "             \n",
       "                      [[ 0.2564, -0.3905,  0.2313],\n",
       "                       [-0.1225, -0.0089, -0.0048],\n",
       "                       [-0.3576, -0.2372,  0.2708]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.2100,  0.0272, -0.1049],\n",
       "                       [ 0.0369, -0.1703, -0.3158],\n",
       "                       [ 0.3324, -0.3620, -0.0033]],\n",
       "             \n",
       "                      [[ 0.1970,  0.2450, -0.1308],\n",
       "                       [ 0.3127, -0.2900, -0.1631],\n",
       "                       [-0.3547, -0.2212,  0.2708]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.3423,  0.4263, -0.1677],\n",
       "                       [-0.1491,  0.1547,  0.1024],\n",
       "                       [ 0.1350,  0.1476,  0.2200]],\n",
       "             \n",
       "                      [[ 0.0464, -0.0258, -0.1098],\n",
       "                       [ 0.0819,  0.4025,  0.3158],\n",
       "                       [-0.2587,  0.3940, -0.1498]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.3937, -0.0624, -0.4071],\n",
       "                       [-0.1780, -0.0574, -0.2159],\n",
       "                       [-0.2386, -0.0918, -0.1386]],\n",
       "             \n",
       "                      [[ 0.1138, -0.3518,  0.0690],\n",
       "                       [ 0.0903, -0.1836,  0.4237],\n",
       "                       [ 0.3786,  0.0196, -0.0511]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.3330,  0.1087,  0.1946],\n",
       "                       [-0.1237, -0.2531,  0.2377],\n",
       "                       [ 0.2589,  0.3286, -0.2895]],\n",
       "             \n",
       "                      [[ 0.1216,  0.2684,  0.1313],\n",
       "                       [-0.2175, -0.2149, -0.3426],\n",
       "                       [ 0.1773,  0.0017, -0.3137]]]], requires_grad=True): {'step': tensor([[[[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]],\n",
       "              \n",
       "                       [[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]]],\n",
       "              \n",
       "              \n",
       "                      [[[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]],\n",
       "              \n",
       "                       [[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]]]),\n",
       "              'exp_avg': tensor([[[[ 5.1803e-11,  3.6380e-11,  1.8502e-11],\n",
       "                        [-2.8003e-11, -1.3493e-10, -1.2974e-10],\n",
       "                        [ 1.2106e-10,  6.9523e-11,  5.1638e-11]],\n",
       "              \n",
       "                       [[ 4.2756e-11, -4.4246e-11,  7.7836e-11],\n",
       "                        [-2.0230e-11, -8.0620e-11, -6.4396e-11],\n",
       "                        [ 4.1732e-11, -9.0318e-12,  6.7171e-11]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.8453e-11, -1.7017e-11, -1.3787e-11],\n",
       "                        [-6.7423e-12,  4.5601e-11,  9.8201e-11],\n",
       "                        [-6.1643e-11,  8.2771e-11,  2.7653e-11]],\n",
       "              \n",
       "                       [[-1.4949e-11, -6.7117e-11, -1.6336e-11],\n",
       "                        [-3.0569e-11,  1.0157e-10, -3.7643e-12],\n",
       "                        [ 9.3547e-11,  3.4183e-11, -3.9165e-11]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.2510e-11, -8.4301e-11, -1.5070e-11],\n",
       "                        [ 1.0781e-10, -6.6687e-11, -8.3821e-11],\n",
       "                        [ 9.4142e-11,  5.2812e-12, -6.8527e-11]],\n",
       "              \n",
       "                       [[-4.3812e-11, -4.8985e-11,  9.1285e-11],\n",
       "                        [-5.8949e-12, -4.2884e-11,  8.8333e-11],\n",
       "                        [ 4.6101e-11, -1.1641e-10,  1.2153e-10]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.2159e-10,  6.4340e-11,  1.5723e-10],\n",
       "                        [ 6.1205e-11, -1.0513e-10,  1.7906e-10],\n",
       "                        [ 2.0512e-11, -3.2854e-11,  4.0073e-11]],\n",
       "              \n",
       "                       [[-2.4266e-11,  1.8231e-10, -3.0556e-11],\n",
       "                        [-5.2091e-11,  1.6771e-10, -1.9614e-10],\n",
       "                        [-1.7643e-10,  2.7582e-11,  4.8411e-12]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0253e-10, -6.3764e-12, -2.7417e-11],\n",
       "                        [ 6.5915e-11,  2.3033e-10, -1.3217e-10],\n",
       "                        [ 1.1229e-11, -1.2870e-10,  1.2146e-10]],\n",
       "              \n",
       "                       [[-8.2821e-11, -1.8209e-10, -6.0570e-11],\n",
       "                        [ 1.7450e-10,  2.4051e-10,  2.2206e-10],\n",
       "                        [-7.1001e-11,  4.0470e-11,  7.1391e-11]]]]),\n",
       "              'exp_avg_sq': tensor([[[[3.9082e-22, 1.7434e-22, 8.1944e-22],\n",
       "                        [2.2394e-22, 3.6225e-21, 6.8094e-21],\n",
       "                        [2.2088e-21, 4.2688e-22, 9.8570e-22]],\n",
       "              \n",
       "                       [[5.8405e-22, 2.0146e-21, 9.9103e-22],\n",
       "                        [1.1893e-22, 1.9183e-21, 1.0959e-21],\n",
       "                        [6.2348e-22, 7.4076e-22, 4.0922e-22]]],\n",
       "              \n",
       "              \n",
       "                      [[[2.8955e-22, 3.5715e-23, 2.3442e-23],\n",
       "                        [5.6065e-24, 2.5646e-22, 1.1894e-21],\n",
       "                        [4.6865e-22, 8.4495e-22, 9.4313e-23]],\n",
       "              \n",
       "                       [[2.7562e-23, 5.5558e-22, 3.2915e-23],\n",
       "                        [1.1525e-22, 1.2725e-21, 1.7476e-24],\n",
       "                        [1.0793e-21, 1.4411e-22, 1.8918e-22]]],\n",
       "              \n",
       "              \n",
       "                      [[[7.1774e-22, 4.4086e-21, 3.0642e-21],\n",
       "                        [2.6966e-21, 5.9716e-21, 1.2552e-21],\n",
       "                        [7.8608e-22, 1.4624e-21, 5.3807e-21]],\n",
       "              \n",
       "                       [[4.6619e-22, 2.4888e-22, 1.1127e-21],\n",
       "                        [2.2996e-21, 1.3317e-20, 3.5061e-21],\n",
       "                        [1.6673e-21, 7.8136e-21, 1.7205e-21]]],\n",
       "              \n",
       "              \n",
       "                      [[[3.6622e-21, 3.2353e-22, 2.4261e-21],\n",
       "                        [2.2815e-22, 7.2938e-22, 2.4465e-21],\n",
       "                        [1.1388e-22, 5.7110e-22, 6.9650e-22]],\n",
       "              \n",
       "                       [[3.8606e-23, 2.7507e-21, 4.0340e-22],\n",
       "                        [1.0637e-21, 2.6780e-21, 2.4024e-21],\n",
       "                        [2.0884e-21, 7.4923e-22, 2.3245e-24]]],\n",
       "              \n",
       "              \n",
       "                      [[[5.9538e-22, 4.0030e-23, 4.7649e-23],\n",
       "                        [2.6436e-22, 2.9299e-21, 1.1102e-21],\n",
       "                        [8.5087e-23, 9.1581e-22, 1.1049e-21]],\n",
       "              \n",
       "                       [[3.8406e-22, 1.9293e-21, 2.4815e-22],\n",
       "                        [1.6817e-21, 3.3386e-21, 2.7817e-21],\n",
       "                        [2.9426e-22, 9.6512e-23, 3.1861e-22]]]])},\n",
       "             Parameter containing:\n",
       "             tensor([1.0001, 1.0000, 1.0000, 1.0000, 1.0000], requires_grad=True): {'step': tensor([3, 3, 2, 2, 2]),\n",
       "              'exp_avg': tensor([-2.5886e-10,  2.0577e-10,  1.6456e-10,  4.6034e-10,  4.8010e-10]),\n",
       "              'exp_avg_sq': tensor([2.1666e-20, 5.2220e-21, 4.0406e-20, 1.6251e-20, 1.2813e-20])},\n",
       "             Parameter containing:\n",
       "             tensor([-0.0003, -0.0003, -0.0002, -0.0002, -0.0002], requires_grad=True): {'step': tensor([3, 3, 2, 2, 2]),\n",
       "              'exp_avg': tensor([0.0542, 0.0542, 0.0380, 0.0380, 0.0380]),\n",
       "              'exp_avg_sq': tensor([1.1988e-04, 1.1988e-04, 7.9960e-05, 7.9960e-05, 7.9960e-05])},\n",
       "             Parameter containing:\n",
       "             tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True): {'step': 0,\n",
       "              'aa': 'hahaha'}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.optimizer.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearity_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.actf = actf_obj\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actf(x)\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        pass\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearity(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.bias = nn.Parameter(torch.zeros(io_dim))\n",
    "        self.actf = actf_obj\n",
    "        \n",
    "        self.tree.optimizer.state[self.bias] = {}\n",
    "        tree.optimizer.param_groups[0]['params'].append(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actf(x+self.bias)\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        _b = torch.cat((self.bias.data, torch.zeros(num, dtype=self.bias.data.dtype,\n",
    "                                                    device=self.bias.data.device)))\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        _b = self.bias.data[remaining]\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, hidden_dim, output_dim, stride=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "#         self.stride = stride\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = HierarchicalResidual_Conv(self.tree, input_dim, hidden_dim, stride=stride, activation=activation) \n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = HierarchicalResidual_Conv(self.tree, hidden_dim, output_dim, activation=activation)\n",
    "        self.fc1.shortcut.bn.weight.data *= 0.        \n",
    "        self.fc1.shortcut.weight.data *= 0.1        \n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "            self.apnz = self.apnz/self.count\n",
    "#             self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "            self.significance = self.significance*(1-self.apnz**33) / 0.872 ## tried on desmos.\n",
    "\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "#         self.count = None\n",
    "\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "#             self.significance += z.pow(2).sum(dim=0)\n",
    "            self.significance += z.abs().sum(dim=0)\n",
    "#             self.significance += z.abs().pow(0.8).sum(dim=0)\n",
    "#             print(f\"SIG ACT:\\n{float(self.activations.abs().mean())}\")\n",
    "#             print(f\"GRAD Mean, Std:\\n{float(grad_output[0].data.abs().mean()), float(grad_output[0].data.std())}\")\n",
    "\n",
    "            if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "                self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "        #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "                self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below=None, above=None, mask=None):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(self.significance.numel(), dtype=torch.bool)\n",
    "        if below:\n",
    "            mask = torch.logical_or(mask,self.significance<=below)\n",
    "        if above:\n",
    "            mask = torch.logical_or(mask,self.significance>above)\n",
    "            \n",
    "        print(f\"Significance:\\n{self.significance}\\nPrune:\\n{mask}\")\n",
    "            \n",
    "        self.to_remove = torch.nonzero(mask).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "#         max_dim = np.ceil((self.tree.parent_dict[self].input_dim+\\\n",
    "#             self.tree.parent_dict[self].output_dim)/2)\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None: ## it is shortcut conv\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, stride=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.stride = 1\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, stride=self.stride).to(self.tree.device)\n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None ## this can be Residual Layer or None\n",
    "        ##### only one of shortcut or residual can be None at a time\n",
    "        self.forward = self.forward_shortcut\n",
    "        \n",
    "        self.std_ratio = 0. ## 0-> all variation due to shortcut, 1-> residual\n",
    "        self.target_std_ratio = 0. ##\n",
    "    \n",
    "    def forward_both(self, r):\n",
    "\n",
    "        s = self.shortcut(r)\n",
    "        r = self.residual(r)\n",
    "\n",
    "        if self.residual.hook is None: ### dont execute when computing significance\n",
    "            s_std = torch.std(s, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            r_std = torch.std(r, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            stdr = r_std/(s_std+r_std)\n",
    "\n",
    "            self.std_ratio = self.tree.beta_std_ratio*self.std_ratio + (1-self.tree.beta_std_ratio)*stdr.data\n",
    "            if r_std.min() > 1e-9:\n",
    "                ## recover for the fact that when decaying neurons, target ratio should also be reducing\n",
    "                if self.tree.total_decay_steps:\n",
    "                    i, o = self.shortcut.weight.shape[1],self.shortcut.weight.shape[0]\n",
    "                    if self.shortcut.to_remove is not None:\n",
    "                        i -= len(self.shortcut.to_remove)\n",
    "                    if self.shortcut.to_freeze is not None:\n",
    "                        o -= len(self.shortcut.to_freeze)\n",
    "                    h = self.residual.hidden_dim\n",
    "                    if self.residual.to_remove is not None:\n",
    "                        h -= len(self.residual.to_remove)\n",
    "                    \n",
    "#                     tr = h/np.ceil((i+o)/2 +1)\n",
    "                    tr = h/_get_hidden_neuron_number(i, o)\n",
    "                    self.compute_target_std_ratio(tr)\n",
    "                else:\n",
    "                    self.compute_target_std_ratio()\n",
    "                self.get_std_loss(stdr)\n",
    "        return s+r\n",
    "    \n",
    "    def forward_shortcut(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def forward_residual(self, x):\n",
    "        self.compute_target_std_ratio()\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def compute_target_std_ratio(self, tr = None):\n",
    "        if tr is None:\n",
    "#             tr = self.residual.hidden_dim/np.ceil((self.input_dim+self.output_dim)/2 +1)\n",
    "            tr = self.residual.hidden_dim/_get_hidden_neuron_number(self.input_dim, self.output_dim)\n",
    "#             tr = self.residual.hidden_dim/np.ceil(self.output_dim/2 +1)\n",
    "\n",
    "        tr = np.clip(tr, 0., 1.)\n",
    "        self.target_std_ratio = self.tree.beta_std_ratio*self.target_std_ratio +\\\n",
    "                                (1-self.tree.beta_std_ratio)*tr\n",
    "        pass        \n",
    "    \n",
    "    def get_std_loss(self, stdr):\n",
    "        del_std = self.target_std_ratio-stdr\n",
    "        del_std_loss = (del_std**2 + torch.abs(del_std)).mean()\n",
    "#         del_std_loss = (del_std**2).mean()\n",
    "        self.tree.std_loss += del_std_loss\n",
    "        return\n",
    "            \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_freezing_connection(to_freeze)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_decaying_connection(to_remove)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_freezed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.remove_freezed_connection(remaining)\n",
    "            if self.shortcut: self.std_ratio = self.std_ratio[:, remaining]\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_decayed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_input_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_output_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.add_output_connection(num)\n",
    "            # if torch.is_tensor(self.std_ratio):\n",
    "            if self.shortcut:\n",
    "                self.std_ratio = torch.cat((self.std_ratio, torch.zeros(1, num, device=self.tree.device)), dim=1)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        \n",
    "        if self.residual is None:\n",
    "            # print(f\"Adding {num} hidden units.. in new residual_layer\")\n",
    "            self.residual = Residual_Conv(self.tree, self.input_dim,\n",
    "                                          num, self.output_dim, stride=self.stride,\n",
    "                                          activation=self.activation).to(self.tree.device)\n",
    "            \n",
    "            self.tree.parent_dict[self.residual] = self\n",
    "            if self.shortcut is None:\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            else:\n",
    "                self.forward = self.forward_both\n",
    "                self.std_ratio = torch.zeros(1, self.output_dim, device=self.tree.device)\n",
    "                \n",
    "        else:\n",
    "            # print(f\"Adding {num} hidden units..\")\n",
    "            self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            if self.std_ratio.min()>0.98 and self.target_std_ratio>0.98:\n",
    "                del self.tree.parent_dict[self.shortcut]\n",
    "                del self.shortcut\n",
    "                self.shortcut = None\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            \n",
    "        elif self.target_std_ratio<0.95:\n",
    "            self.shortcut = Shortcut_Conv(self.tree, self.input_dim, self.output_dim, stride=self.stride)\n",
    "            self.shortcut.bn.weight.data *= 0.\n",
    "            self.shortcut.weight.data *= 0.1\n",
    "            self.forward = self.forward_both\n",
    "            \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.residual.hidden_dim < 1:\n",
    "            del self.tree.parent_dict[self.residual]\n",
    "            del self.residual\n",
    "            ### its parent (Residual_Conv) removes it from dynamic list if possible\n",
    "            self.residual = None\n",
    "            self.forward = self.forward_shortcut\n",
    "            self.std_ratio = 0.\n",
    "            return\n",
    "        \n",
    "#         max_dim = np.ceil((self.input_dim+self.output_dim)/2)\n",
    "        # max_dim = min((self.input_dim, self.output_dim))+1\n",
    "        max_dim = _get_hidden_neuron_number(self.input_dim, self.output_dim) + 1 \n",
    "        # print(\"MaxDIM\", max_dim, self.residual.hidden_dim)\n",
    "        if self.residual.hidden_dim > max_dim:\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc0)\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc1)\n",
    "            # print(\"Added\", self.residual)\n",
    "            \n",
    "        # self.residual.fc0.morph_network()\n",
    "        # self.residual.fc1.morph_network()\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        stdr = self.std_ratio\n",
    "        if torch.is_tensor(self.std_ratio):\n",
    "            stdr = self.std_ratio.min()\n",
    "            \n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{self.target_std_ratio}, s:{stdr}\")\n",
    "        if self.shortcut:\n",
    "            self.shortcut.print_network_debug(depth+1)\n",
    "        if self.residual:\n",
    "            self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        if self.residual is None:\n",
    "            return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            print(f\"{pre_string}╠════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}║    \")\n",
    "            print(f\"{pre_string}╠════╝\")\n",
    "        else:\n",
    "            print(f\"{pre_string}╚════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}     \")\n",
    "            print(f\"{pre_string}╔════╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Conv Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation, hidden_dim, post_activation=None):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "        self.post_activation = post_activation\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = hrnet0\n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = hrnet1\n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        if self.post_activation:\n",
    "            x = self.post_activation(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "            self.apnz = self.apnz/self.count\n",
    "#             self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "            self.significance = self.significance*(1-self.apnz**33) / 0.872 ## tried on desmos.\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "#         self.count = None\n",
    "\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "#             self.significance += z.pow(2).sum(dim=0)\n",
    "            self.significance += z.abs().sum(dim=0)\n",
    "#             self.significance += z.abs().pow(0.8).sum(dim=0)\n",
    "#             print(\"Current Significance \\n\", self.significance)\n",
    "#             print(f\"SIG ACT:\\n{float(self.activations.abs().mean())}\")\n",
    "#             print(f\"GRAD Mean, Std:\\n{float(grad_output[0].data.abs().mean()), float(grad_output[0].data.std())}\")\n",
    "\n",
    "            if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "                self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "        #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "                self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below=None, above=None, mask=None):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(self.significance.numel(), dtype=torch.bool)\n",
    "        if below:\n",
    "            mask = torch.logical_or(mask,self.significance<=below)\n",
    "        if above:\n",
    "            mask = torch.logical_or(mask,self.significance>above)\n",
    "            \n",
    "        print(f\"Significance:\\n{self.significance}\\nPrune:\\n{mask}\")\n",
    "        \n",
    "        self.to_remove = torch.nonzero(mask).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None:\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [8]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10)<0 \n",
    "b = torch.randn(10) > 0.5\n",
    "torch.nonzero(torch.logical_and(a,b), as_tuple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation=nn.ReLU(), post_activation=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = hrnet0.input_dim\n",
    "        self.output_dim = hrnet1.output_dim\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = None\n",
    "        self.residual = Residual_Conv_Connector(self.tree, hrnet0, hrnet1,\n",
    "                                                activation, hrnet0.output_dim, post_activation)\n",
    "        self.tree.parent_dict[self.residual] = self\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.residual.fc1.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.residual.fc1.add_output_connection(num)\n",
    "        \n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):  \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        print(f\"{pre_string}╚╗\")\n",
    "        self.residual.print_network(f\"{pre_string} \")\n",
    "        print(f\"{pre_string}╔╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut only Hierarchical Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        _wd = nn.Linear(input_dim, output_dim, bias=False).weight.data\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "        self.add_parameters_to_optimizer()\n",
    "        return\n",
    "        \n",
    "    def add_parameters_to_optimizer(self):\n",
    "        for p in self.parameters():\n",
    "#             self.tree.optimizer.state[p] = {}\n",
    "            self.tree.optimizer.param_groups[0]['params'].append(p)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## input_dim        ## output_dim\n",
    "        if x.shape[1] + self.weight.shape[1] > 0:\n",
    "            return x.matmul(self.weight.t())\n",
    "        else:\n",
    "            # print(x.shape, self.weight.shape)\n",
    "            # print(x.matmul(self.weight.t()))\n",
    "            if x.shape[1] + self.weight.shape[1] == 0:\n",
    "                return torch.zeros(x.shape[0], self.weight.shape[0], dtype=x.dtype, device=x.device)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        pass\n",
    "    \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "        pass\n",
    "            \n",
    "     \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "#         _w = self.weight.data[remaining, :]\n",
    "#         del self.weight\n",
    "#         self.weight = nn.Parameter(_w)\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        self.weight.data = self.weight.data[remaining, :]\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        ops[self.weight][_var][remaining, :]\n",
    "        \n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "#         _w = self.weight.data[:, remaining]\n",
    "#         del self.weight\n",
    "#         self.weight = nn.Parameter(_w)\n",
    "        ops = self.tree.optimizer.state\n",
    "\n",
    "        self.weight.data = self.weight.data[:, remaining]\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        ops[self.weight][_var][:, remaining]\n",
    "\n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        _w = torch.zeros(o, num, dtype=self.weight.data.dtype, device=self.weight.data.device)\n",
    "#         _w += torch.randn_like(_w)\n",
    "        _w = torch.cat((self.weight.data, _w), dim=1)\n",
    "        self.weight.data = _w\n",
    "        self.weight.grad = None\n",
    "                \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        torch.cat((ops[self.weight][_var], \\\n",
    "                                  torch.zeros(o, num, dtype=ops[self.weight][_var].dtype,\n",
    "                                              device=ops[self.weight][_var].device)), \n",
    "                                  dim=1)\n",
    "        \n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i)\n",
    "#         stdv = torch.std(self.weight.data)\n",
    "    \n",
    "        _new = torch.empty(num, i, dtype=self.weight.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        _w = torch.cat((self.weight.data, _new), dim=0)\n",
    "        self.weight.data = _w\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        torch.cat((ops[self.weight][_var], \\\n",
    "                                  torch.zeros(num, i, k0, k1, dtype=ops[self.weight][_var].dtype,\n",
    "                                              device=ops[self.weight][_var].device)), \n",
    "                                  dim=0)\n",
    "        \n",
    "        # print(self.weight.data.shape)        \n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}S:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n",
    "\n",
    "\n",
    "class HierarchicalResidual_Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=None, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        ## this can be Shortcut Layer or None\n",
    "        if kernel is None:\n",
    "            self.shortcut = Shortcut(tree, self.input_dim, self.output_dim) \n",
    "        else:\n",
    "            self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, kernel, stride) \n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.shortcut.start_freezing_connection(to_freeze)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.shortcut.start_decaying_connection(to_remove)\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.shortcut.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.shortcut.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.shortcut.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.shortcut.add_output_connection(num)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        print(\"Cannot Add Hidden neuron to Shortcut Only Layer\")\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        pass\n",
    "        \n",
    "    def morph_network(self):\n",
    "        pass\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.shortcut.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree and Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree_State():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.DYNAMIC_LIST = set() ## residual parent is added, to make code effecient.\n",
    "        ## the parents which is not intended to have residual connection should not be added.\n",
    "        self.beta_std_ratio = None\n",
    "        self.beta_del_neuron = None\n",
    "        self.device = 'cpu'\n",
    "    \n",
    "        self.parent_dict = {}\n",
    "    \n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual:set = None\n",
    "        self.freeze_connection_shortcut:set = None\n",
    "        self.decay_connection_shortcut:set = None\n",
    "\n",
    "        self.decay_rate_std = 0.001\n",
    "\n",
    "        self.add_to_remove_ratio = 2.\n",
    "        \n",
    "#         self.dummy_param = nn.Parameter(torch.Tensor([0]))\n",
    "#         self.optimizer = adam_custom.Adam([self.dummy_param])\n",
    "        self.optimizer = None\n",
    "    \n",
    "        pass\n",
    "    \n",
    "    def get_decay_factor(self):\n",
    "        ratio = self.current_decay_step/self.total_decay_steps\n",
    "#         self.decay_factor = np.exp(-2*ratio)*(1-ratio)\n",
    "        ratio = np.clip(ratio, 0, 1)\n",
    "        self.decay_factor = (1-ratio)**2\n",
    "#         self.decay_factor = (1-ratio)\n",
    "        pass\n",
    "    \n",
    "    def clear_decay_variables(self):\n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual = None\n",
    "        self.freeze_connection_shortcut = None\n",
    "        self.decay_connection_shortcut = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tree_State()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constructing Hierarchical Residual CNN (Resnet Inspired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutActivation(nn.Module):\n",
    "    \n",
    "    def __init__(self, p=0.1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout2d(p) ## ok to reuse dropout !! caution\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dropout(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, device, lr, input_dim = 1, hidden_dims = [8, 16, 32, 64], output_dim = 10, final_activation=None,\n",
    "                 num_stat=5, num_std=100, decay_rate_std=0.001):\n",
    "        super().__init__()\n",
    "        self.tree = Tree_State()\n",
    "        self.tree.beta_del_neuron = (num_stat-1)/num_stat\n",
    "        self.tree.beta_std_ratio = (num_std-1)/num_std\n",
    "        self.tree.decay_rate_std = decay_rate_std\n",
    "        self.tree.device = device\n",
    "        \n",
    "        \n",
    "        dummy_param = nn.Parameter(torch.Tensor([0]))\n",
    "        ############################################################\n",
    "        self.tree.optimizer = adam_custom.Adam([dummy_param], lr=lr, weight_decay=1e-5)\n",
    "        self.tree.optimizer.param_groups[0]['params'] = []\n",
    "        ############################################################\n",
    "        \n",
    "        \n",
    "        self.root_net = None\n",
    "        self._construct_root_net(input_dim, hidden_dims, output_dim)\n",
    "        \n",
    "#         self.tree.DYNAMIC_LIST.add(self.root_net)\n",
    "        self.tree.parent_dict[self.root_net] = None\n",
    "        \n",
    "        if final_activation is None:\n",
    "            final_activation = lambda x: x\n",
    "        self.non_linearity = NonLinearity(self.tree, output_dim, final_activation)\n",
    "        \n",
    "        self.neurons_added = 0\n",
    "\n",
    "        self._remove_below = None ## temporary variable\n",
    "        \n",
    "    def _construct_root_net(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        actf = DropoutActivation()\n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 8, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 8, 8, activation=actf)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 8, 16, stride=2, activation=actf)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2, activation=actf)\n",
    "        hrn3 = HierarchicalResidual_Conv(self.tree, 32, 32, stride=2, activation=actf)\n",
    "\n",
    "    \n",
    "        actf = lambda x: x\n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0, actf)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1, actf)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2, actf)\n",
    "        hrnR0123 = HierarchicalResidual_Connector(self.tree, hrnR012, hrn3, actf)\n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 32, 10)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR0123fc = HierarchicalResidual_Connector(self.tree, hrnR0123, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR0123fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "        morphables = [self.root_net, hrnR0123, hrnR012, hrnR01, hrnR0, hrn3, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def _construct_root_net2(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        \n",
    "        \n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 16, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 16, 16)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 32, 64, stride=2)\n",
    "\n",
    "        actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "    \n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1, actf)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2, actf)\n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 64, 10)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR012fc = HierarchicalResidual_Connector(self.tree, hrnR012, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR012fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "        morphables = [hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrnR012, hrnR01, hrnR0, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.non_linearity(self.root_net(x))\n",
    "\n",
    "    def add_neurons(self, num):\n",
    "        num_stat = int(num*0.7)\n",
    "        num_random = num - num_stat\n",
    "        \n",
    "        DL = list(self.tree.DYNAMIC_LIST)\n",
    "        if num_random>0:\n",
    "            rands = torch.randint(high=len(DL), size=(num_random,))\n",
    "            index, count = torch.unique(rands, sorted=False, return_counts=True)\n",
    "            for i, idx in enumerate(index):\n",
    "                DL[idx].add_hidden_neuron(int(count[i]))\n",
    "\n",
    "        if num_stat>0:\n",
    "            del_neurons = []\n",
    "            for hr in DL:\n",
    "                if hr.residual:\n",
    "                    del_neurons.append(hr.residual.del_neurons)#+1e-7)\n",
    "                else:\n",
    "                    del_neurons.append(0.)#1e-7) ## residual layer yet not created \n",
    "            \n",
    "            prob_stat = torch.tensor(del_neurons)\n",
    "            prob_stat = torch.log(torch.exp(prob_stat)+1.)\n",
    "            m = torch.distributions.multinomial.Multinomial(total_count=num_stat,\n",
    "                                                            probs= prob_stat)\n",
    "            count = m.sample()#.type(torch.long)\n",
    "            for i, hr in enumerate(DL):\n",
    "                if count[i] < 1: continue\n",
    "                hr.add_hidden_neuron(int(count[i]))\n",
    "        \n",
    "        self.neurons_added += num \n",
    "        pass\n",
    "\n",
    "    def identify_removable_neurons(self, num=None, threshold_min=0., threshold_max=1.):\n",
    "        \n",
    "        all_sig = []\n",
    "        self.all_sig_ = []\n",
    "        \n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                all_sig.append(hr.residual.significance)\n",
    "                \n",
    "        all_sigs = torch.cat(all_sig)\n",
    "        del all_sig\n",
    "        \n",
    "#         print(\"All_sigs\", all_sigs)\n",
    "        \n",
    "#         print(\"Normalization\", (all_sigs/all_sigs.sum()).sum())\n",
    "        \n",
    "        ### Normalizes such that importance 1 is average importance\n",
    "        normalizer = float(torch.sum(all_sigs))/len(all_sigs)\n",
    "        all_sig = all_sigs/normalizer\n",
    "\n",
    "        ### Normalizes to range [0, 1]\n",
    "#         max_sig = all_sigs.max()\n",
    "#         all_sig = all_sigs/(max_sig+1e-9)\n",
    "#         print(\"All_sig\", all_sig)\n",
    "#         print(\"Sig sum\", all_sig.sum())\n",
    "        print(f\"Significance Stat:\\nMin, Max: {float(all_sig.min()), float(all_sig.max())}\")\n",
    "        print(f\"Mean, Std: {float(all_sig.mean()), float(all_sig.std())}\")\n",
    "        all_sig = all_sig[all_sig<threshold_max]\n",
    "        if len(all_sig)<1: ## if all significance is above threshold max \n",
    "            return 0, None, all_sigs\n",
    "        all_sig = torch.sort(all_sig)[0] ### sorted significance scores\n",
    "        \n",
    "        self.all_sig_ = all_sig\n",
    "        \n",
    "        if not num:num = int(np.ceil(self.neurons_added/self.tree.add_to_remove_ratio))\n",
    "        ## reset the neurons_added number if decay is started\n",
    "\n",
    "        remove_below = threshold_min\n",
    "        if num>len(all_sig):\n",
    "            remove_below = float(all_sig[-1])\n",
    "        elif num>0:\n",
    "            remove_below = float(all_sig[num-1])\n",
    "        \n",
    "        ### sig < threshold_min is always removed; whatsoever\n",
    "        if remove_below < threshold_min:\n",
    "            remove_below = threshold_min\n",
    "            \n",
    "        print(\"remove_below\", remove_below, \"true:\", remove_below*normalizer)\n",
    "        remove_below *= normalizer\n",
    "#         remove_below *= max_sig\n",
    "#         print(\"remove_below\", remove_below)\n",
    "\n",
    "        self._remove_below = remove_below\n",
    "#         self._remove_above = remove_above*normalizer\n",
    "        self._remove_above = None\n",
    "\n",
    "        return remove_below, all_sigs\n",
    "\n",
    "    def decay_neuron_start(self, decay_steps=1000):\n",
    "        if self._remove_below is None: return 0\n",
    "        \n",
    "        self.neurons_added = 0 ## resetting this variable\n",
    "        \n",
    "        self.tree.total_decay_steps = decay_steps\n",
    "        self.tree.current_decay_step = 0\n",
    "        self.tree.remove_neuron_residual = set()\n",
    "        self.tree.freeze_connection_shortcut = set()\n",
    "        self.tree.decay_connection_shortcut = set()\n",
    "        \n",
    "        count_remove = 0\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                ### always prune 1 % of the neurons randomly. It might overlap with less significant neurons\n",
    "                mask = torch.bernoulli(torch.ones_like(hr.residual.significance)*0.05).type(torch.bool)\n",
    "                count_remove += hr.residual.identify_removable_neurons(below=self._remove_below,\n",
    "                                                                       above=self._remove_above,\n",
    "                                                                       mask = mask\n",
    "                                                                      )\n",
    "        if count_remove<1:\n",
    "            self.tree.clear_decay_variables()\n",
    "        return count_remove\n",
    "    \n",
    "    def decay_neuron_step(self):\n",
    "        if self.tree.total_decay_steps is None:\n",
    "            return 0\n",
    "        \n",
    "        self.tree.current_decay_step += 1\n",
    "        \n",
    "        if self.tree.current_decay_step < self.tree.total_decay_steps:\n",
    "            self.tree.get_decay_factor()\n",
    "            for sh in self.tree.decay_connection_shortcut:\n",
    "                sh.decay_connection_step()\n",
    "            for sh in self.tree.freeze_connection_shortcut:\n",
    "                sh.freeze_connection_step()\n",
    "            return 1\n",
    "        else:\n",
    "#             if self.tree.current_decay_step == self.tree.total_decay_steps:\n",
    "#                 for sh in self.tree.decay_connection_shortcut:\n",
    "#     #                 sh.decay_connection_step()\n",
    "#                     print(\"------------------\")\n",
    "#                     print(sh.weight.data.shape, \"removing decayed; \", sh.to_remove)\n",
    "#                     print(\"Small vals\", torch.count_nonzero(sh.weight.data<1e-6))\n",
    "#                     print(\"data\", sh.weight.data[:, sh.to_remove])\n",
    "#                     print(\"grads\", sh.weight.grad[:, sh.to_remove])\n",
    "#                     print(\"initial\", sh.initial_remove)\n",
    "#                     break\n",
    "\n",
    "            \n",
    "            \n",
    "#             for rs in self.tree.remove_neuron_residual:\n",
    "#                 rs.remove_decayed_neurons()\n",
    "                \n",
    "#             self.tree.clear_decay_variables()\n",
    "#             self.maintain_network()\n",
    "\n",
    "            ### need to decay and freeze all the time\n",
    "            for sh in self.tree.decay_connection_shortcut:\n",
    "                sh.decay_connection_step()\n",
    "            for sh in self.tree.freeze_connection_shortcut:\n",
    "                sh.freeze_connection_step()\n",
    "            return -1\n",
    "        \n",
    "    def remove_decayed_neurons(self):\n",
    "        for rs in self.tree.remove_neuron_residual:\n",
    "            rs.remove_decayed_neurons()\n",
    "                \n",
    "        self.tree.clear_decay_variables()\n",
    "        self.maintain_network()\n",
    "        return\n",
    "\n",
    "    def compute_del_neurons(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.compute_del_neurons()\n",
    "    \n",
    "    def maintain_network(self):\n",
    "        self.root_net.maintain_shortcut_connection()\n",
    "        self.root_net.morph_network()\n",
    "        \n",
    "    def start_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.start_computing_significance()\n",
    "\n",
    "    def finish_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.finish_computing_significance()\n",
    "            \n",
    "    def print_network_debug(self):\n",
    "        self.root_net.print_network_debug(0)\n",
    "        \n",
    "    def print_network(self):\n",
    "        print(self.root_net.input_dim)\n",
    "        self.root_net.print_network()\n",
    "        print(\"│\")\n",
    "        print(self.root_net.output_dim)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.binomial(1, 0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.bernoulli(torch.ones(10)*0.01).type(torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dycnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcifar-10-batches-py\u001b[0m/  \u001b[01;31mcifar-10-python.tar.gz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls \"../../_Datasets/cifar10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = transforms.Compose([\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "cifar_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameters\n",
    "# learning_rate = 0.001\n",
    "learning_rate = 0.0003\n",
    "\n",
    "num_add_neuron = 50 #50#25#10\n",
    "num_decay_steps = int(len(train_loader)*2)#3\n",
    "\n",
    "remove_above = 12 #10\n",
    "threshold_max = 0.5\n",
    "threshold_min = 0.01\n",
    "\n",
    "train_epoch_min = 1 #1\n",
    "train_epoch_max = 12 #10 #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet = Dynamic_CNN(device, learning_rate).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = dynet.tree.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563, 313)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.add_to_remove_ratio = 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.optimizer.param_groups[0]['lr'] = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necessary for initializing variables in Adam Optimizer\n",
    "dynet(torch.randn(1,3,32,32).to(device)).mean().backward()\n",
    "dynet.tree.optimizer.step()\n",
    "\n",
    "dynet.tree.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([-0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,\n",
       "         -0.0010, -0.0010], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 1.2233e-01, -1.0797e-01, -1.1560e-01],\n",
       "           [ 1.0054e-01,  1.6252e-01,  2.8502e-01],\n",
       "           [ 1.9072e-01, -2.9728e-01, -8.9353e-02]],\n",
       " \n",
       "          [[-1.9856e-01, -9.1499e-02, -1.7031e-01],\n",
       "           [-3.7699e-02, -2.2428e-01, -2.5555e-02],\n",
       "           [-2.4178e-01,  3.3122e-02,  2.2636e-01]],\n",
       " \n",
       "          [[ 2.3545e-01,  3.0003e-01, -2.1340e-01],\n",
       "           [-1.1353e-01,  2.3427e-01,  1.7870e-01],\n",
       "           [ 1.4887e-01,  2.6009e-01,  2.8229e-01]]],\n",
       " \n",
       " \n",
       "         [[[-7.5914e-02,  1.8779e-01, -5.8983e-02],\n",
       "           [-2.5899e-02, -6.1926e-02, -2.4166e-01],\n",
       "           [ 3.4330e-01,  1.1774e-01, -1.0435e-01]],\n",
       " \n",
       "          [[-3.3113e-01,  3.2410e-01,  1.9273e-01],\n",
       "           [-1.2584e-01, -2.5498e-01, -3.2613e-01],\n",
       "           [-1.4636e-01,  3.1716e-01, -3.9758e-02]],\n",
       " \n",
       "          [[-2.1444e-01, -3.3370e-04, -6.2067e-02],\n",
       "           [ 1.7349e-01, -1.9738e-01, -6.7500e-02],\n",
       "           [-1.4169e-01, -1.8716e-01, -1.2045e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.6432e-01,  6.5764e-02, -1.0571e-01],\n",
       "           [ 3.0477e-01,  2.7013e-01,  1.9134e-01],\n",
       "           [ 1.9773e-01,  1.4202e-01, -2.4380e-01]],\n",
       " \n",
       "          [[-2.8068e-01, -2.8282e-01,  2.2601e-01],\n",
       "           [ 2.1182e-01,  2.8515e-01, -1.6137e-01],\n",
       "           [-1.3436e-01,  1.3971e-02, -5.9268e-02]],\n",
       " \n",
       "          [[-2.7546e-01, -2.6605e-01,  1.0383e-02],\n",
       "           [-1.8733e-01, -1.9085e-01,  1.4405e-01],\n",
       "           [-5.9748e-02, -7.5328e-03,  2.9056e-02]]],\n",
       " \n",
       " \n",
       "         [[[-2.6458e-01,  3.1176e-01,  1.4254e-01],\n",
       "           [-2.2056e-01,  1.1775e-01,  5.1721e-03],\n",
       "           [-2.5706e-01,  8.7064e-02,  3.2318e-01]],\n",
       " \n",
       "          [[ 1.0737e-01,  5.7737e-02,  1.3161e-01],\n",
       "           [ 8.4253e-02,  2.0309e-01,  1.0775e-01],\n",
       "           [-2.2939e-01,  1.7117e-01, -2.2541e-01]],\n",
       " \n",
       "          [[-2.0114e-01,  1.0961e-01, -4.1199e-01],\n",
       "           [ 9.5966e-02, -2.1471e-03, -5.6395e-02],\n",
       "           [ 2.4724e-01, -1.9688e-01, -6.0399e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0826e-01,  9.8270e-02,  2.2055e-01],\n",
       "           [-3.1190e-01,  3.0177e-02,  2.7127e-01],\n",
       "           [-1.8063e-01,  3.1182e-01, -1.4948e-01]],\n",
       " \n",
       "          [[ 5.8237e-02, -2.6585e-01, -1.2489e-01],\n",
       "           [ 1.8842e-01, -1.0359e-01, -1.4133e-01],\n",
       "           [-1.3789e-01,  1.8560e-01, -2.5083e-01]],\n",
       " \n",
       "          [[-2.2624e-01, -1.5114e-01,  9.9086e-02],\n",
       "           [ 2.8583e-01,  6.9618e-02, -2.6597e-01],\n",
       "           [ 1.5515e-01,  1.1196e-01, -2.1585e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.5663e-01, -3.1774e-01, -3.7449e-02],\n",
       "           [-2.2345e-01,  7.9603e-02,  2.1506e-01],\n",
       "           [-2.6364e-01, -6.7569e-02, -1.1443e-01]],\n",
       " \n",
       "          [[ 1.2584e-01, -2.3501e-03,  2.3127e-01],\n",
       "           [ 3.2704e-01, -8.6569e-02,  7.5210e-02],\n",
       "           [-2.7844e-02, -1.0214e-01, -1.4932e-01]],\n",
       " \n",
       "          [[ 8.9557e-02,  3.0225e-01, -1.9086e-01],\n",
       "           [ 3.0879e-01,  3.6099e-02, -1.0697e-01],\n",
       "           [ 1.6451e-01, -2.7746e-01, -2.4977e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.1526e-01,  1.1080e-02,  1.7361e-02],\n",
       "           [ 3.6307e-02,  2.8767e-01,  2.8404e-01],\n",
       "           [ 2.8595e-01, -2.8179e-01,  8.3770e-02]],\n",
       " \n",
       "          [[-2.6417e-01,  2.7411e-01, -3.1225e-01],\n",
       "           [ 5.5032e-02,  5.5944e-02, -2.6682e-01],\n",
       "           [-4.5083e-02,  1.4721e-01,  2.5143e-02]],\n",
       " \n",
       "          [[-2.1594e-01, -2.9799e-01, -3.1916e-02],\n",
       "           [-4.9346e-02,  1.1174e-01, -5.0286e-02],\n",
       "           [ 2.2442e-01,  1.1262e-01, -2.2526e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.3656e-01,  1.1999e-01, -6.8009e-02],\n",
       "           [ 2.8413e-01,  1.3740e-01,  1.7116e-01],\n",
       "           [ 1.9399e-01, -8.2427e-02, -6.4057e-02]],\n",
       " \n",
       "          [[-2.2379e-01,  9.7441e-02, -2.0585e-01],\n",
       "           [ 3.9243e-02,  2.0372e-01, -1.9928e-01],\n",
       "           [-1.8606e-01, -1.8294e-01, -1.8116e-01]],\n",
       " \n",
       "          [[ 2.5686e-01,  1.4978e-01,  2.5665e-01],\n",
       "           [ 2.6907e-01,  2.5505e-01, -2.4254e-01],\n",
       "           [ 2.5893e-01,  1.8229e-01, -3.9904e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-4.4501e-07, -2.2539e-06,  1.2606e-06,  1.2836e-06, -1.3354e-06,\n",
       "          3.4596e-06, -2.4821e-06,  1.0208e-06], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-0.1094,  0.0477, -0.1304],\n",
       "           [ 0.1377,  0.1671, -0.0014],\n",
       "           [-0.0254, -0.1369, -0.1325]],\n",
       " \n",
       "          [[-0.1455, -0.0324, -0.1950],\n",
       "           [-0.0974, -0.0884, -0.1209],\n",
       "           [ 0.1504, -0.1873,  0.1776]],\n",
       " \n",
       "          [[ 0.0538, -0.0136, -0.1651],\n",
       "           [ 0.0709, -0.1367,  0.0947],\n",
       "           [ 0.1198, -0.0600, -0.1717]],\n",
       " \n",
       "          [[-0.1896, -0.0310, -0.0839],\n",
       "           [-0.0520, -0.0387,  0.1845],\n",
       "           [-0.0211,  0.0655,  0.0538]],\n",
       " \n",
       "          [[-0.0209,  0.0888, -0.1742],\n",
       "           [-0.1534, -0.1318,  0.0126],\n",
       "           [ 0.1834, -0.1970,  0.0903]],\n",
       " \n",
       "          [[-0.1570,  0.0254,  0.0743],\n",
       "           [-0.0194,  0.0372, -0.1622],\n",
       "           [ 0.1763, -0.0384, -0.1877]],\n",
       " \n",
       "          [[-0.1325, -0.1633,  0.0614],\n",
       "           [-0.0270,  0.0134,  0.0649],\n",
       "           [-0.0902,  0.1211, -0.0858]],\n",
       " \n",
       "          [[-0.0902, -0.1638, -0.0359],\n",
       "           [-0.0376, -0.1899,  0.0580],\n",
       "           [-0.0680,  0.0136,  0.1916]]],\n",
       " \n",
       " \n",
       "         [[[-0.1678,  0.0647,  0.1790],\n",
       "           [-0.0774,  0.1681,  0.1820],\n",
       "           [-0.0945, -0.1816,  0.1511]],\n",
       " \n",
       "          [[ 0.0404,  0.0036,  0.1706],\n",
       "           [-0.0140,  0.0586, -0.0987],\n",
       "           [-0.0862, -0.0816, -0.0122]],\n",
       " \n",
       "          [[ 0.1010,  0.0871,  0.0713],\n",
       "           [-0.0147,  0.1403, -0.0540],\n",
       "           [ 0.0641,  0.0940,  0.0933]],\n",
       " \n",
       "          [[ 0.1813, -0.1705,  0.1514],\n",
       "           [ 0.0643,  0.0492, -0.0042],\n",
       "           [ 0.0686,  0.0742,  0.0998]],\n",
       " \n",
       "          [[ 0.1424, -0.0487, -0.1640],\n",
       "           [-0.1664, -0.0377, -0.1758],\n",
       "           [ 0.0383, -0.0249, -0.1156]],\n",
       " \n",
       "          [[ 0.1309,  0.0653,  0.1421],\n",
       "           [ 0.0978,  0.0914, -0.1185],\n",
       "           [-0.0486,  0.0447, -0.0913]],\n",
       " \n",
       "          [[ 0.1836, -0.0687, -0.1711],\n",
       "           [ 0.0012, -0.1759,  0.1909],\n",
       "           [-0.1920, -0.1496, -0.1659]],\n",
       " \n",
       "          [[ 0.0951,  0.1370, -0.1494],\n",
       "           [ 0.0553, -0.1865, -0.0447],\n",
       "           [-0.1457, -0.0763,  0.0353]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1743,  0.1898,  0.0799],\n",
       "           [ 0.0154, -0.0028,  0.0088],\n",
       "           [ 0.0963,  0.1812,  0.0225]],\n",
       " \n",
       "          [[-0.1981, -0.0059, -0.0647],\n",
       "           [ 0.0162,  0.1008, -0.0751],\n",
       "           [ 0.1629, -0.0597,  0.1525]],\n",
       " \n",
       "          [[ 0.1824, -0.1354, -0.1851],\n",
       "           [-0.0878,  0.1976,  0.2052],\n",
       "           [-0.0418, -0.0005, -0.0128]],\n",
       " \n",
       "          [[ 0.0779,  0.0663, -0.0949],\n",
       "           [-0.1563, -0.0197,  0.1193],\n",
       "           [-0.1801,  0.0061, -0.1574]],\n",
       " \n",
       "          [[-0.0085,  0.0704,  0.0216],\n",
       "           [-0.0662,  0.0935, -0.0955],\n",
       "           [-0.1344,  0.0940, -0.0745]],\n",
       " \n",
       "          [[-0.0829, -0.1446, -0.1520],\n",
       "           [ 0.1606,  0.1886, -0.1921],\n",
       "           [-0.1417, -0.1395, -0.0154]],\n",
       " \n",
       "          [[ 0.0366,  0.0977,  0.1137],\n",
       "           [ 0.1406,  0.0559,  0.1265],\n",
       "           [-0.1545,  0.0132, -0.1726]],\n",
       " \n",
       "          [[ 0.0242,  0.1372,  0.0824],\n",
       "           [ 0.1873, -0.1073, -0.0233],\n",
       "           [-0.0660,  0.0941, -0.1118]]],\n",
       " \n",
       " \n",
       "         [[[-0.0036, -0.1321,  0.0265],\n",
       "           [ 0.0044, -0.1235,  0.0415],\n",
       "           [ 0.1807, -0.0853,  0.0422]],\n",
       " \n",
       "          [[-0.1585,  0.0471, -0.0008],\n",
       "           [-0.1157,  0.0630,  0.1464],\n",
       "           [ 0.0027, -0.0325, -0.0018]],\n",
       " \n",
       "          [[-0.1237,  0.1810, -0.0225],\n",
       "           [ 0.0465,  0.1673, -0.0849],\n",
       "           [ 0.0319, -0.0432,  0.1436]],\n",
       " \n",
       "          [[-0.1010,  0.0839, -0.1031],\n",
       "           [-0.1752,  0.1855, -0.1735],\n",
       "           [-0.1159,  0.1508, -0.1392]],\n",
       " \n",
       "          [[ 0.1759,  0.0099,  0.0775],\n",
       "           [ 0.1623, -0.0177,  0.1654],\n",
       "           [ 0.1797,  0.0326,  0.1668]],\n",
       " \n",
       "          [[ 0.1376, -0.1365,  0.1781],\n",
       "           [ 0.0873,  0.0738,  0.0113],\n",
       "           [-0.1016, -0.1338,  0.0131]],\n",
       " \n",
       "          [[-0.0338,  0.1475,  0.0991],\n",
       "           [-0.1416,  0.1888, -0.1740],\n",
       "           [ 0.1841, -0.0691,  0.1075]],\n",
       " \n",
       "          [[ 0.1892,  0.1213,  0.0431],\n",
       "           [-0.1201,  0.0330,  0.0900],\n",
       "           [ 0.1286,  0.1298,  0.1095]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0267,  0.1664,  0.1530],\n",
       "           [-0.1235, -0.1281,  0.0966],\n",
       "           [ 0.0599, -0.0394,  0.1763]],\n",
       " \n",
       "          [[ 0.1255,  0.0995,  0.1309],\n",
       "           [-0.1268,  0.1339,  0.1813],\n",
       "           [ 0.0854,  0.0343,  0.2017]],\n",
       " \n",
       "          [[-0.1079,  0.0040, -0.0453],\n",
       "           [-0.0908, -0.1642, -0.1346],\n",
       "           [-0.1486, -0.1078,  0.1396]],\n",
       " \n",
       "          [[ 0.1421,  0.1042, -0.0906],\n",
       "           [-0.1548,  0.1357,  0.0586],\n",
       "           [-0.0083, -0.1568, -0.1827]],\n",
       " \n",
       "          [[ 0.0674, -0.0513, -0.2000],\n",
       "           [-0.0864,  0.0226, -0.0136],\n",
       "           [ 0.0961,  0.0412,  0.0643]],\n",
       " \n",
       "          [[-0.0603,  0.1057, -0.0645],\n",
       "           [-0.0176, -0.1180, -0.0258],\n",
       "           [-0.0915, -0.2009,  0.1365]],\n",
       " \n",
       "          [[ 0.1358,  0.1480,  0.1600],\n",
       "           [-0.1937,  0.0829,  0.0082],\n",
       "           [ 0.0945, -0.1708,  0.0654]],\n",
       " \n",
       "          [[-0.0108,  0.0461,  0.0056],\n",
       "           [-0.0286, -0.1307, -0.1217],\n",
       "           [ 0.1641, -0.1778,  0.1271]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1752, -0.0827,  0.0356],\n",
       "           [-0.1209,  0.1683, -0.1464],\n",
       "           [ 0.1614,  0.1723,  0.0874]],\n",
       " \n",
       "          [[ 0.0489, -0.0017, -0.1709],\n",
       "           [-0.0718, -0.1718,  0.1481],\n",
       "           [-0.1876,  0.0939, -0.1043]],\n",
       " \n",
       "          [[ 0.1679,  0.0586, -0.0559],\n",
       "           [-0.1175,  0.0408, -0.0156],\n",
       "           [ 0.1581, -0.0078, -0.1821]],\n",
       " \n",
       "          [[ 0.0811,  0.0750, -0.1293],\n",
       "           [-0.0395,  0.1261,  0.1136],\n",
       "           [-0.1624, -0.0150,  0.0044]],\n",
       " \n",
       "          [[ 0.0771,  0.0844,  0.0623],\n",
       "           [ 0.1160, -0.0954, -0.0934],\n",
       "           [-0.1466, -0.1425, -0.1862]],\n",
       " \n",
       "          [[ 0.0397,  0.1634,  0.0310],\n",
       "           [-0.0022,  0.1677,  0.1544],\n",
       "           [ 0.0881, -0.0830, -0.0285]],\n",
       " \n",
       "          [[ 0.1621, -0.0746, -0.0057],\n",
       "           [-0.1688,  0.1034,  0.0552],\n",
       "           [-0.1641, -0.1067,  0.1381]],\n",
       " \n",
       "          [[ 0.1746, -0.0256, -0.0625],\n",
       "           [-0.0487,  0.0952, -0.1608],\n",
       "           [ 0.1820, -0.0538, -0.1415]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0093, -0.0098,  0.1415],\n",
       "           [ 0.1101,  0.1287, -0.1625],\n",
       "           [-0.0975,  0.1740, -0.0562]],\n",
       " \n",
       "          [[ 0.0031,  0.0465,  0.1671],\n",
       "           [-0.0045, -0.1272, -0.1607],\n",
       "           [ 0.1036,  0.1494,  0.0554]],\n",
       " \n",
       "          [[-0.0196,  0.0380, -0.1718],\n",
       "           [ 0.1197, -0.1396, -0.0888],\n",
       "           [-0.0367,  0.1168, -0.0290]],\n",
       " \n",
       "          [[-0.1760, -0.1106, -0.1435],\n",
       "           [-0.1507, -0.1566,  0.1508],\n",
       "           [ 0.1697,  0.0256,  0.1545]],\n",
       " \n",
       "          [[ 0.0868,  0.0998,  0.0896],\n",
       "           [ 0.0542,  0.1683, -0.0545],\n",
       "           [ 0.1126, -0.1002,  0.1612]],\n",
       " \n",
       "          [[ 0.1086,  0.1532, -0.0571],\n",
       "           [ 0.0475, -0.0071,  0.0755],\n",
       "           [ 0.1538,  0.1554,  0.0698]],\n",
       " \n",
       "          [[ 0.1461,  0.1382,  0.0343],\n",
       "           [ 0.0541, -0.1413, -0.1505],\n",
       "           [ 0.1323,  0.0929, -0.1131]],\n",
       " \n",
       "          [[ 0.1624, -0.1414,  0.1358],\n",
       "           [-0.1148, -0.0329, -0.1198],\n",
       "           [-0.1768,  0.0267, -0.1531]]],\n",
       " \n",
       " \n",
       "         [[[-0.1507,  0.1945, -0.0061],\n",
       "           [-0.1223,  0.1440,  0.1953],\n",
       "           [-0.0032, -0.0467,  0.1843]],\n",
       " \n",
       "          [[ 0.0642, -0.1290,  0.0217],\n",
       "           [ 0.1416,  0.1946,  0.1177],\n",
       "           [ 0.1501, -0.0710,  0.1466]],\n",
       " \n",
       "          [[ 0.1830, -0.1158, -0.1279],\n",
       "           [ 0.1057, -0.1152,  0.0736],\n",
       "           [-0.1130,  0.0702, -0.1538]],\n",
       " \n",
       "          [[ 0.0254,  0.1707, -0.0514],\n",
       "           [-0.0969, -0.1010, -0.0157],\n",
       "           [-0.0748,  0.1804, -0.0579]],\n",
       " \n",
       "          [[-0.0941, -0.1464, -0.0625],\n",
       "           [-0.0474,  0.1434, -0.1921],\n",
       "           [-0.1478,  0.1448, -0.1495]],\n",
       " \n",
       "          [[-0.1499, -0.1790,  0.0382],\n",
       "           [-0.0277,  0.1208,  0.0007],\n",
       "           [ 0.1220, -0.0990,  0.0858]],\n",
       " \n",
       "          [[-0.0597,  0.0992, -0.1443],\n",
       "           [ 0.0856, -0.0775,  0.1801],\n",
       "           [-0.0448, -0.1107, -0.0328]],\n",
       " \n",
       "          [[ 0.0582, -0.0410,  0.0748],\n",
       "           [ 0.1372,  0.0717, -0.0766],\n",
       "           [-0.1841,  0.0627,  0.0904]]]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 4.8866e-07,  1.1841e-06, -6.2245e-07,  1.8964e-07, -4.6248e-06,\n",
       "          1.2891e-06, -4.1887e-06, -2.1644e-06], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 1.0635e-01,  1.9060e-01, -7.8454e-02],\n",
       "           [-2.3215e-02, -3.6200e-02, -1.8921e-02],\n",
       "           [-4.5146e-03, -1.1549e-01,  1.7637e-01]],\n",
       " \n",
       "          [[-1.8185e-02,  2.0165e-01,  8.3150e-02],\n",
       "           [ 1.5513e-02, -1.0642e-01,  1.9328e-01],\n",
       "           [ 4.9810e-02,  1.7587e-01,  3.7231e-02]],\n",
       " \n",
       "          [[ 9.5331e-02, -1.0814e-01, -1.5141e-01],\n",
       "           [-1.6524e-01, -1.4073e-01,  3.3038e-02],\n",
       "           [ 3.7056e-02,  3.2101e-02,  3.6298e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.7013e-01, -5.3636e-03,  1.8894e-01],\n",
       "           [ 9.1932e-02,  6.0471e-02,  6.0471e-02],\n",
       "           [-1.2386e-01,  2.0363e-01, -2.2394e-02]],\n",
       " \n",
       "          [[-3.8919e-03, -1.1256e-01, -1.5077e-01],\n",
       "           [-4.6223e-02,  1.9148e-01,  1.3509e-01],\n",
       "           [-1.8662e-01,  1.5472e-01,  5.8691e-02]],\n",
       " \n",
       "          [[ 1.2529e-01,  7.4358e-05,  1.7410e-01],\n",
       "           [ 5.9954e-02,  1.0846e-01,  8.5738e-02],\n",
       "           [ 1.6165e-01,  1.5575e-01,  1.6251e-02]]],\n",
       " \n",
       " \n",
       "         [[[-5.3034e-04,  1.4990e-01, -1.4546e-01],\n",
       "           [-1.2899e-01, -1.0532e-01,  9.5060e-02],\n",
       "           [-7.1052e-02, -1.6902e-01,  1.4123e-01]],\n",
       " \n",
       "          [[ 4.2795e-02, -8.2319e-02,  1.2209e-01],\n",
       "           [ 1.1553e-01, -1.4206e-01, -1.8191e-01],\n",
       "           [ 2.6162e-02,  8.9395e-02, -5.4940e-02]],\n",
       " \n",
       "          [[-3.0834e-02,  6.8771e-02,  1.7198e-01],\n",
       "           [-1.0759e-01,  1.3363e-01,  1.2277e-01],\n",
       "           [-1.3228e-01, -1.7876e-01,  4.5876e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 9.5875e-02, -1.7292e-01, -1.7384e-01],\n",
       "           [-5.7801e-02, -6.9255e-02, -8.0834e-02],\n",
       "           [ 8.6357e-02, -7.0235e-02, -7.3633e-02]],\n",
       " \n",
       "          [[-1.4221e-01,  1.0876e-01,  3.5759e-02],\n",
       "           [ 1.8346e-01, -9.2328e-02,  3.1995e-02],\n",
       "           [-8.2773e-02, -6.7382e-02, -1.8233e-01]],\n",
       " \n",
       "          [[-8.9682e-02,  8.8345e-03, -5.3892e-02],\n",
       "           [ 8.9291e-02,  5.3500e-02, -1.3750e-01],\n",
       "           [ 1.3097e-01,  3.9502e-02, -1.8025e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.8069e-01,  8.4294e-02, -1.7466e-01],\n",
       "           [ 1.5447e-01, -6.6029e-02, -1.5819e-01],\n",
       "           [ 1.3377e-01,  1.9408e-02,  6.1315e-02]],\n",
       " \n",
       "          [[ 1.2253e-01,  2.8684e-02, -1.5054e-01],\n",
       "           [-1.5801e-01,  1.3146e-01,  1.7881e-01],\n",
       "           [-8.8034e-02, -1.5216e-01,  1.1855e-01]],\n",
       " \n",
       "          [[-1.2727e-01,  1.2085e-01, -1.7862e-01],\n",
       "           [-1.8179e-01, -1.1328e-03, -1.6249e-01],\n",
       "           [-1.8240e-01, -1.0790e-01, -1.4543e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-9.5802e-02, -1.0505e-01, -1.0717e-01],\n",
       "           [ 3.4889e-02, -1.5630e-01, -2.9784e-02],\n",
       "           [ 1.8235e-01, -1.8732e-01,  3.3444e-02]],\n",
       " \n",
       "          [[ 9.3966e-02,  1.0343e-01, -3.2535e-02],\n",
       "           [-1.3194e-01,  1.7779e-01, -1.0850e-01],\n",
       "           [ 1.0270e-02,  1.7786e-01, -1.1475e-01]],\n",
       " \n",
       "          [[ 1.7757e-01, -6.0830e-02, -1.0427e-01],\n",
       "           [ 1.4987e-01,  5.7600e-02,  8.5931e-02],\n",
       "           [ 6.7867e-02,  1.1781e-01, -2.9943e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.5834e-01, -1.9235e-01,  5.4715e-02],\n",
       "           [-6.4879e-02, -1.4939e-01, -1.5783e-01],\n",
       "           [-2.1378e-01, -1.9395e-01, -1.6190e-01]],\n",
       " \n",
       "          [[ 1.1175e-01,  5.0419e-02,  4.6070e-02],\n",
       "           [ 1.1403e-01, -8.5888e-02,  8.9724e-02],\n",
       "           [-1.9488e-01,  1.1339e-01,  1.1628e-02]],\n",
       " \n",
       "          [[-1.0994e-02,  2.3260e-02, -2.9554e-02],\n",
       "           [ 8.6178e-02, -1.3086e-01,  1.6836e-01],\n",
       "           [ 1.4781e-01,  1.4485e-01, -1.0650e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0571e-01, -5.2068e-02, -9.3555e-02],\n",
       "           [ 5.2385e-02, -4.2563e-03, -8.3273e-02],\n",
       "           [ 1.6074e-01,  1.4541e-01, -5.9887e-02]],\n",
       " \n",
       "          [[ 7.3584e-03, -7.8646e-02, -6.3913e-03],\n",
       "           [-1.8667e-01,  1.3417e-02,  2.0022e-01],\n",
       "           [ 8.9518e-02,  8.6905e-02, -5.3541e-02]],\n",
       " \n",
       "          [[ 9.6323e-04,  1.1291e-01,  1.6788e-01],\n",
       "           [-3.6510e-02, -7.7487e-02,  8.7166e-02],\n",
       "           [ 1.5017e-01, -2.5396e-02,  3.1981e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.5843e-01, -1.6237e-01,  9.8059e-02],\n",
       "           [ 9.0719e-02, -2.5686e-02,  1.7368e-01],\n",
       "           [-6.7550e-02, -1.5044e-01,  4.4057e-02]],\n",
       " \n",
       "          [[-1.5011e-01,  6.4690e-02,  6.1893e-02],\n",
       "           [-1.5164e-01, -9.5009e-02,  1.1989e-01],\n",
       "           [-3.9579e-04, -4.1715e-02, -1.3564e-01]],\n",
       " \n",
       "          [[ 5.1865e-02, -8.0841e-02, -1.6108e-01],\n",
       "           [-1.5143e-01, -6.9575e-02,  1.5320e-01],\n",
       "           [-1.5735e-01,  1.4610e-01,  1.3800e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.6892e-02,  6.8519e-02,  1.2724e-01],\n",
       "           [-1.3321e-01, -8.5373e-02, -4.2546e-02],\n",
       "           [-1.3432e-01,  7.8485e-02, -1.2776e-01]],\n",
       " \n",
       "          [[-8.6186e-02, -1.4207e-01,  1.0763e-01],\n",
       "           [ 1.7905e-01, -7.3547e-02,  1.3974e-01],\n",
       "           [-1.5891e-01, -6.1176e-02,  1.6729e-01]],\n",
       " \n",
       "          [[-1.2317e-01, -7.3587e-02,  1.7744e-01],\n",
       "           [-1.4278e-01, -1.5384e-01,  8.2101e-02],\n",
       "           [ 3.0518e-04, -1.2913e-01, -1.1947e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 7.6622e-02, -1.4633e-01,  9.6578e-02],\n",
       "           [-9.2232e-02,  5.6348e-02,  1.3592e-01],\n",
       "           [ 1.9390e-01,  1.6627e-01, -1.0032e-01]],\n",
       " \n",
       "          [[-5.9257e-02, -1.6760e-01,  8.0818e-02],\n",
       "           [-1.3233e-01,  1.9430e-01, -5.1566e-02],\n",
       "           [ 2.2294e-01,  7.1613e-02, -1.4133e-01]],\n",
       " \n",
       "          [[ 1.2790e-01, -4.0809e-02, -2.0408e-01],\n",
       "           [ 1.4337e-01,  1.1465e-01,  2.7806e-02],\n",
       "           [ 1.7017e-01,  3.6093e-02,  7.6696e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.3152e-03, -5.4402e-02,  3.2257e-02],\n",
       "           [ 1.7898e-02, -1.9865e-01,  1.0244e-01],\n",
       "           [-5.4609e-02, -1.7114e-01, -3.9173e-02]],\n",
       " \n",
       "          [[-2.3136e-02, -8.4636e-02, -2.3358e-02],\n",
       "           [ 8.7430e-02, -1.3486e-01,  3.7457e-02],\n",
       "           [-6.7237e-02,  1.2494e-01, -2.2367e-01]],\n",
       " \n",
       "          [[ 2.1185e-01, -7.1481e-02,  1.0434e-01],\n",
       "           [-5.4778e-02,  9.7401e-02,  1.8615e-01],\n",
       "           [-5.0614e-02, -2.5251e-02,  2.8698e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-4.2020e-06,  9.4549e-07, -3.7896e-06,  8.4918e-08, -9.3607e-07,\n",
       "          7.4030e-07,  4.4473e-06,  1.7969e-06,  1.9541e-06,  5.6487e-06,\n",
       "         -2.0300e-06, -2.7495e-06, -1.9099e-06, -8.9899e-07, -7.2531e-07,\n",
       "         -3.8294e-08], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 2.3320e-02,  5.8278e-02,  2.1913e-02],\n",
       "           [ 1.0780e-01, -1.2621e-01, -1.5261e-01],\n",
       "           [-5.8727e-02, -7.1870e-04,  6.6658e-03]],\n",
       " \n",
       "          [[ 1.1516e-01,  1.1550e-01, -1.1339e-01],\n",
       "           [-5.0120e-02,  1.3261e-01,  1.4528e-01],\n",
       "           [-3.8750e-02, -4.9018e-02, -9.9877e-02]],\n",
       " \n",
       "          [[-1.0855e-01, -3.1459e-02, -2.0521e-02],\n",
       "           [-1.2538e-01, -4.3097e-02, -2.2858e-02],\n",
       "           [-8.6533e-02,  1.1572e-01,  8.1870e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.1430e-01, -1.9040e-02, -1.2409e-01],\n",
       "           [-6.3435e-02,  3.7612e-02, -7.0888e-02],\n",
       "           [ 1.1234e-01,  6.8877e-02, -4.0181e-02]],\n",
       " \n",
       "          [[ 5.3448e-02,  1.0392e-01,  1.0198e-01],\n",
       "           [-1.2121e-02, -1.5430e-02, -7.5842e-02],\n",
       "           [-7.5652e-02, -7.8263e-02, -1.1098e-01]],\n",
       " \n",
       "          [[-9.2729e-02,  3.4844e-02, -2.3835e-02],\n",
       "           [ 6.5880e-02, -1.4028e-01, -1.4094e-01],\n",
       "           [ 1.0595e-01,  9.0912e-02, -3.4854e-03]]],\n",
       " \n",
       " \n",
       "         [[[-8.1880e-04,  5.9362e-02, -1.3318e-01],\n",
       "           [ 5.8124e-02,  1.0412e-01,  1.3306e-01],\n",
       "           [ 3.9635e-03,  9.3215e-02,  7.0479e-02]],\n",
       " \n",
       "          [[ 6.2421e-02,  6.0060e-02,  1.0880e-01],\n",
       "           [-8.5378e-03, -1.2351e-01,  7.2200e-02],\n",
       "           [-4.4593e-02, -6.4109e-02, -4.8819e-03]],\n",
       " \n",
       "          [[-8.8117e-02, -6.0037e-02, -8.6524e-03],\n",
       "           [-2.6853e-02,  1.2220e-01, -6.9315e-02],\n",
       "           [ 1.9544e-02, -1.0688e-01, -1.4169e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.3582e-01, -5.5153e-02,  4.8441e-03],\n",
       "           [-4.3756e-02, -7.5189e-02,  4.9890e-02],\n",
       "           [-4.8356e-02, -3.1583e-03,  9.9293e-02]],\n",
       " \n",
       "          [[-7.2338e-02, -7.4791e-02,  9.8975e-02],\n",
       "           [ 1.3891e-01, -1.2703e-01, -7.9911e-02],\n",
       "           [ 7.1626e-02, -3.2525e-05,  7.8405e-02]],\n",
       " \n",
       "          [[-1.1900e-01, -1.3239e-01, -9.7198e-02],\n",
       "           [ 2.0141e-04,  9.9731e-02, -5.3664e-02],\n",
       "           [-1.1615e-02,  9.7944e-02, -4.9798e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.8457e-02, -3.2829e-02,  4.6692e-02],\n",
       "           [ 2.9558e-02, -4.0326e-02,  6.3537e-02],\n",
       "           [ 3.0698e-02,  1.2447e-01, -1.2183e-01]],\n",
       " \n",
       "          [[-4.1252e-02,  5.7144e-02, -3.3021e-02],\n",
       "           [ 1.2226e-01,  6.6156e-03,  1.1323e-01],\n",
       "           [-3.0162e-02,  1.2324e-01,  9.3607e-02]],\n",
       " \n",
       "          [[ 2.2929e-02,  5.3653e-02,  4.7225e-03],\n",
       "           [-2.6756e-02,  1.2612e-01, -1.1354e-01],\n",
       "           [-1.2811e-01,  9.4408e-02,  5.3685e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.6038e-02,  1.1222e-01,  1.4215e-01],\n",
       "           [-1.2461e-01,  9.0963e-02,  4.3583e-02],\n",
       "           [ 9.8822e-02,  1.2231e-01, -6.6700e-02]],\n",
       " \n",
       "          [[ 3.0040e-02,  1.0379e-01,  1.8812e-02],\n",
       "           [-1.0485e-01,  4.8557e-02,  1.1072e-01],\n",
       "           [ 5.2530e-02, -8.9239e-02, -1.1146e-01]],\n",
       " \n",
       "          [[-1.1322e-01, -1.2220e-01, -7.4908e-02],\n",
       "           [-9.7898e-02, -1.1281e-01,  3.8691e-02],\n",
       "           [ 1.0792e-01,  6.1381e-02, -1.1061e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 2.2913e-02, -7.3010e-02,  1.1374e-01],\n",
       "           [-5.4716e-02, -7.1698e-02, -9.1502e-02],\n",
       "           [-6.9164e-02, -6.6752e-02,  3.1186e-02]],\n",
       " \n",
       "          [[-3.8754e-02, -1.1593e-01,  6.3641e-02],\n",
       "           [ 1.3793e-01,  2.9055e-02,  1.0970e-01],\n",
       "           [-7.6242e-02,  8.6206e-02, -1.1082e-01]],\n",
       " \n",
       "          [[-1.3520e-01,  1.1323e-01, -1.1425e-01],\n",
       "           [ 8.7512e-02,  9.0999e-02, -1.1420e-01],\n",
       "           [ 1.4235e-01, -5.2422e-02, -5.4441e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.7417e-02,  8.8134e-02,  1.2947e-01],\n",
       "           [-1.2379e-02,  1.1888e-01, -7.9912e-02],\n",
       "           [-5.2966e-02,  1.3131e-01,  2.5443e-03]],\n",
       " \n",
       "          [[ 6.9660e-02, -3.4221e-02, -2.1286e-02],\n",
       "           [ 1.0441e-01,  4.7275e-03,  5.0270e-02],\n",
       "           [-1.2704e-01, -8.9836e-02,  1.6273e-02]],\n",
       " \n",
       "          [[ 1.0085e-01, -9.0703e-02, -5.9582e-02],\n",
       "           [-6.1979e-03, -5.0744e-02,  8.9788e-03],\n",
       "           [-2.2147e-02, -6.1341e-02, -3.4915e-02]]],\n",
       " \n",
       " \n",
       "         [[[-7.9173e-02,  1.0203e-01, -9.8545e-02],\n",
       "           [-1.1140e-01, -4.3579e-02,  1.3002e-01],\n",
       "           [ 6.8384e-02,  1.4014e-01,  2.4411e-02]],\n",
       " \n",
       "          [[-1.1598e-01,  1.2445e-01, -4.4399e-03],\n",
       "           [ 2.4686e-02, -1.0966e-01,  4.4525e-02],\n",
       "           [-6.7641e-02, -6.2949e-02,  5.9019e-02]],\n",
       " \n",
       "          [[ 5.0199e-02, -1.2687e-01,  7.3455e-02],\n",
       "           [ 8.8037e-02,  1.0495e-01,  9.0475e-02],\n",
       "           [ 2.4475e-02, -1.0789e-01, -5.8583e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.5966e-02,  1.1624e-01, -9.1316e-02],\n",
       "           [ 2.3336e-02,  5.5963e-04,  1.0393e-01],\n",
       "           [-1.0541e-01, -1.3389e-01, -8.1914e-03]],\n",
       " \n",
       "          [[-4.8128e-03,  1.2437e-01, -9.4402e-02],\n",
       "           [ 3.7788e-02, -8.7934e-02,  1.4541e-01],\n",
       "           [-1.2637e-01,  1.3636e-01, -9.2408e-02]],\n",
       " \n",
       "          [[ 7.7760e-02,  7.3097e-02,  6.5494e-02],\n",
       "           [ 9.7212e-04,  5.7406e-02, -5.4047e-03],\n",
       "           [-9.6770e-02, -2.3242e-02,  1.2266e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.5359e-02,  1.1287e-01,  3.5499e-03],\n",
       "           [-5.2685e-02,  4.2325e-02, -4.5189e-02],\n",
       "           [ 6.5179e-03, -4.7369e-02,  5.8609e-02]],\n",
       " \n",
       "          [[ 8.6564e-02,  1.3674e-01,  1.7454e-02],\n",
       "           [ 1.1389e-01, -6.5724e-03,  3.9187e-02],\n",
       "           [ 1.0315e-01,  7.4955e-02, -9.6964e-02]],\n",
       " \n",
       "          [[ 8.3606e-02,  1.4753e-01, -9.4033e-02],\n",
       "           [-2.7168e-02, -8.6773e-02, -9.4039e-02],\n",
       "           [-4.9206e-02,  1.1053e-01, -2.6256e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.2399e-02, -1.0967e-01,  1.3374e-01],\n",
       "           [ 7.5766e-02, -1.3445e-01, -1.5234e-02],\n",
       "           [ 1.0744e-01,  6.6928e-02, -1.3212e-01]],\n",
       " \n",
       "          [[ 3.0932e-02, -1.0104e-01, -3.0344e-02],\n",
       "           [ 2.2360e-02, -3.4547e-02,  1.2956e-01],\n",
       "           [ 1.2841e-01, -2.8487e-02, -8.4738e-02]],\n",
       " \n",
       "          [[-4.6592e-02,  1.4451e-01,  8.3424e-02],\n",
       "           [-1.0040e-01,  1.0800e-01,  4.9241e-02],\n",
       "           [-1.0096e-01,  1.5026e-02, -8.3207e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-2.8580e-05, -5.7106e-05,  1.0160e-04, -4.5732e-05,  3.0459e-05,\n",
       "         -4.7348e-06, -2.6534e-05,  2.2065e-07, -1.7105e-05, -2.3418e-06,\n",
       "          2.0291e-05,  9.9398e-05, -2.2766e-05, -1.0606e-04,  2.6959e-05,\n",
       "         -3.2369e-05, -5.6509e-05,  4.6831e-05,  8.6772e-05,  7.6245e-05,\n",
       "         -4.8536e-05, -1.3738e-05, -2.0636e-05,  6.6789e-05, -8.3066e-06,\n",
       "         -5.7829e-05, -1.7894e-05, -2.2073e-05, -7.9000e-05, -2.9149e-05,\n",
       "         -4.5438e-05, -2.4339e-05], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-3.3688e-02, -3.8310e-02, -6.9611e-02],\n",
       "           [ 7.9456e-02, -3.1459e-02,  7.3094e-02],\n",
       "           [ 9.8591e-02,  7.8891e-02, -8.9042e-02]],\n",
       " \n",
       "          [[-9.8842e-02, -2.2763e-02, -4.2898e-03],\n",
       "           [ 5.7245e-02,  8.2270e-02,  4.5103e-02],\n",
       "           [-4.9365e-02,  8.6985e-02,  3.4662e-02]],\n",
       " \n",
       "          [[-3.6132e-03,  1.9394e-02,  7.4846e-02],\n",
       "           [ 8.6999e-02, -6.3950e-02,  6.7117e-02],\n",
       "           [-9.5327e-03,  7.4772e-02, -1.0364e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.8420e-02,  3.4460e-02, -2.0881e-02],\n",
       "           [-8.5756e-02,  2.2161e-03, -6.4460e-02],\n",
       "           [ 2.1849e-02, -3.6732e-03, -9.6448e-02]],\n",
       " \n",
       "          [[-2.6929e-02, -6.0994e-02, -6.2729e-02],\n",
       "           [ 6.9633e-02,  4.4524e-02, -2.0068e-02],\n",
       "           [-3.8892e-02,  1.0838e-02, -8.3537e-03]],\n",
       " \n",
       "          [[-9.4192e-02, -7.8317e-02,  4.5645e-02],\n",
       "           [ 7.5237e-02,  6.7152e-02,  5.5863e-03],\n",
       "           [-7.3464e-02,  4.9467e-02, -5.8591e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.1815e-02,  4.3723e-02, -1.6356e-02],\n",
       "           [-3.6561e-03,  4.1122e-02, -3.6133e-02],\n",
       "           [ 2.0249e-02,  3.0738e-02,  6.2230e-02]],\n",
       " \n",
       "          [[ 8.9326e-02,  2.5446e-02,  3.9456e-02],\n",
       "           [ 6.8884e-02,  7.9735e-02,  2.3961e-02],\n",
       "           [ 5.1999e-03, -2.5530e-02, -3.1137e-02]],\n",
       " \n",
       "          [[-4.6663e-02, -6.6538e-02, -9.2988e-02],\n",
       "           [ 7.4693e-02, -3.4452e-02,  6.8729e-02],\n",
       "           [ 1.0504e-01, -2.5272e-02,  2.3083e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.8496e-02,  1.2263e-02, -4.5522e-03],\n",
       "           [ 7.9843e-02,  3.5170e-02, -8.3222e-02],\n",
       "           [ 1.0044e-01,  8.4765e-02,  5.7128e-02]],\n",
       " \n",
       "          [[ 9.4084e-03,  9.4106e-02,  5.9008e-02],\n",
       "           [-8.5263e-02,  7.4211e-02, -1.7828e-02],\n",
       "           [-3.9896e-02, -9.0597e-02,  5.6784e-02]],\n",
       " \n",
       "          [[ 1.5422e-02, -2.6707e-02,  4.2987e-02],\n",
       "           [ 8.0566e-02,  6.1646e-02, -4.2621e-02],\n",
       "           [-4.8070e-03, -3.6476e-02, -2.6179e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 6.7115e-02, -4.7935e-02,  6.3907e-02],\n",
       "           [ 9.2522e-02,  9.2025e-02,  5.7704e-02],\n",
       "           [-7.8439e-03,  8.5366e-02,  9.1897e-02]],\n",
       " \n",
       "          [[-3.9897e-02, -3.3313e-02,  5.7170e-02],\n",
       "           [ 4.7031e-02,  6.0466e-02,  5.2821e-02],\n",
       "           [-5.8070e-02, -5.5911e-02, -7.2661e-02]],\n",
       " \n",
       "          [[-5.9516e-02,  3.8848e-02,  1.7563e-02],\n",
       "           [-3.1229e-02,  5.9037e-02,  4.1653e-02],\n",
       "           [-5.6168e-02,  7.3117e-02, -8.4985e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.6676e-03,  3.8301e-02,  8.0859e-02],\n",
       "           [-4.4131e-02, -5.2573e-02, -9.7383e-02],\n",
       "           [-7.1801e-02, -4.8217e-02, -2.7579e-02]],\n",
       " \n",
       "          [[-9.2187e-02, -2.9320e-02, -6.4818e-02],\n",
       "           [-1.3647e-02, -1.9500e-02,  8.3873e-03],\n",
       "           [-4.1475e-02,  2.2783e-02,  9.9772e-02]],\n",
       " \n",
       "          [[-2.5818e-02, -6.1879e-02, -2.1460e-02],\n",
       "           [ 5.6676e-02,  7.7172e-02, -4.4362e-02],\n",
       "           [ 3.4048e-02,  4.5650e-02,  6.1847e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-8.7530e-02, -8.9685e-02,  6.1771e-03],\n",
       "           [ 6.5225e-02,  9.6682e-02, -5.8146e-02],\n",
       "           [-6.3527e-05,  7.1921e-02,  9.2055e-02]],\n",
       " \n",
       "          [[-1.8184e-03, -7.1245e-02,  2.9431e-02],\n",
       "           [ 5.8899e-02, -4.5925e-02, -7.7455e-02],\n",
       "           [ 9.7059e-02,  2.3618e-02, -6.5647e-02]],\n",
       " \n",
       "          [[-4.7873e-02, -5.1207e-02,  6.8227e-02],\n",
       "           [ 1.0021e-01, -9.3839e-02,  3.2627e-02],\n",
       "           [ 2.5535e-03, -7.9053e-03,  3.5255e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.3751e-02,  5.2702e-02,  9.5215e-02],\n",
       "           [ 1.0366e-02, -1.7493e-02, -8.2366e-02],\n",
       "           [ 7.4448e-02, -7.0568e-02, -7.4838e-02]],\n",
       " \n",
       "          [[-2.6885e-02,  8.4368e-02, -2.5643e-02],\n",
       "           [-9.9412e-02,  7.2433e-02, -3.8417e-02],\n",
       "           [ 3.7166e-02, -5.1868e-02, -9.3603e-02]],\n",
       " \n",
       "          [[-6.2792e-02, -5.9253e-02, -7.9272e-02],\n",
       "           [-7.7499e-02, -9.1411e-02, -1.2506e-02],\n",
       "           [ 2.1484e-02, -7.7579e-02,  2.1324e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.3157e-02, -1.5213e-02,  5.8259e-02],\n",
       "           [ 1.0542e-01,  5.6660e-02, -7.3500e-02],\n",
       "           [ 3.2743e-02, -6.4410e-04,  4.4373e-02]],\n",
       " \n",
       "          [[-7.4654e-02,  9.8469e-02,  1.0389e-01],\n",
       "           [-3.6754e-02, -4.4285e-02,  7.7036e-02],\n",
       "           [ 9.1785e-02,  6.9707e-02,  6.5581e-02]],\n",
       " \n",
       "          [[-9.1807e-02,  5.0067e-02,  5.0839e-03],\n",
       "           [-7.5597e-02,  2.6834e-02, -7.6972e-02],\n",
       "           [-6.9621e-02, -5.6203e-02, -7.9741e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.8646e-02,  3.3364e-02, -1.8338e-03],\n",
       "           [-4.5991e-02,  6.5480e-02, -9.0995e-03],\n",
       "           [ 3.4265e-02, -2.6951e-02, -5.9921e-02]],\n",
       " \n",
       "          [[-9.6306e-02,  5.0985e-04,  7.9147e-02],\n",
       "           [-5.1029e-02,  8.0589e-02,  3.0560e-03],\n",
       "           [ 7.5441e-03, -3.1992e-02,  9.9390e-02]],\n",
       " \n",
       "          [[ 8.8676e-02, -4.1586e-02,  6.2362e-02],\n",
       "           [-3.8977e-02,  5.2655e-02,  8.8111e-02],\n",
       "           [ 7.9408e-02,  5.4279e-02, -3.4047e-03]]],\n",
       " \n",
       " \n",
       "         [[[-8.4449e-02, -8.1628e-02, -1.1939e-03],\n",
       "           [-7.3358e-02, -7.4481e-02,  9.9025e-03],\n",
       "           [ 3.0919e-02,  6.8269e-04,  7.7426e-02]],\n",
       " \n",
       "          [[ 5.2951e-02, -9.4957e-02,  9.9500e-03],\n",
       "           [-7.8490e-02, -3.9062e-02, -8.1130e-02],\n",
       "           [-1.8813e-02,  1.8985e-02, -2.2812e-02]],\n",
       " \n",
       "          [[ 5.5363e-02, -9.0396e-02, -3.8657e-02],\n",
       "           [ 8.9237e-02, -9.9412e-02,  5.2548e-03],\n",
       "           [-3.2639e-02, -3.2150e-02,  7.3497e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.0490e-03, -2.3747e-02,  5.8681e-02],\n",
       "           [-3.1602e-02,  5.7551e-02, -5.5329e-02],\n",
       "           [-4.0355e-02, -4.7725e-02,  8.8630e-02]],\n",
       " \n",
       "          [[-5.1444e-02, -6.4629e-02, -4.7848e-02],\n",
       "           [ 1.8816e-02, -5.9040e-03,  9.6782e-02],\n",
       "           [ 7.2345e-02, -3.8876e-02,  6.8742e-02]],\n",
       " \n",
       "          [[-3.0342e-02, -9.4876e-02,  4.3257e-02],\n",
       "           [-5.6013e-02, -1.7534e-02,  9.7478e-02],\n",
       "           [ 9.3880e-02,  5.1448e-02,  3.9457e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010, -0.0010,\n",
       "          0.0010,  0.0010,  0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,\n",
       "          0.0010,  0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,\n",
       "          0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,  0.0010],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1607, -0.1702,  0.0482, -0.0502,  0.1170,  0.1704,  0.1738,  0.1234,\n",
       "           0.0297,  0.0807,  0.0730,  0.1330,  0.0392,  0.0630,  0.0083, -0.0038,\n",
       "          -0.1553, -0.0699, -0.1508, -0.0710,  0.0745,  0.1405, -0.1480,  0.0259,\n",
       "           0.1096,  0.0516, -0.0808, -0.0014, -0.0402,  0.1346,  0.1531, -0.1756],\n",
       "         [ 0.1526, -0.0769,  0.1545,  0.0831, -0.0867,  0.1411, -0.0859,  0.1134,\n",
       "          -0.0043,  0.0052, -0.1514,  0.1092,  0.0661,  0.0408, -0.0281, -0.0157,\n",
       "           0.0439,  0.0159, -0.0375,  0.0377,  0.0143,  0.1069, -0.0689, -0.0609,\n",
       "          -0.0389, -0.1569, -0.1717,  0.0825, -0.0316,  0.1174,  0.1273,  0.0351],\n",
       "         [ 0.0302,  0.0189, -0.0392,  0.0912, -0.0451, -0.1495, -0.1331, -0.1639,\n",
       "           0.0546,  0.0299,  0.0545,  0.0533,  0.1567,  0.0567,  0.1380, -0.0467,\n",
       "           0.0280,  0.0092, -0.1536,  0.0825,  0.1636, -0.0972, -0.1546,  0.1368,\n",
       "           0.1401,  0.0574, -0.1750, -0.0469, -0.0716,  0.0004,  0.0687, -0.1313],\n",
       "         [-0.0208,  0.1660, -0.1226, -0.0971,  0.0058,  0.1049,  0.1075,  0.1193,\n",
       "          -0.1366,  0.0090,  0.0027, -0.1110, -0.0210, -0.0102,  0.0249, -0.0752,\n",
       "           0.0178, -0.0649,  0.1622, -0.0310,  0.0948, -0.0866,  0.0817,  0.0055,\n",
       "          -0.0769, -0.0798,  0.0224,  0.1465,  0.0893, -0.1206, -0.0649, -0.1265],\n",
       "         [-0.0468,  0.0607, -0.0020,  0.1178, -0.0199, -0.1038, -0.1465, -0.1164,\n",
       "           0.1499, -0.1192, -0.0971,  0.0968, -0.0514, -0.0528, -0.1635,  0.0926,\n",
       "          -0.0098, -0.0889, -0.0497,  0.1175, -0.0447, -0.1674, -0.1278, -0.0499,\n",
       "          -0.0798, -0.0562, -0.0248,  0.0255, -0.0358, -0.1008,  0.1502, -0.0219],\n",
       "         [ 0.1583, -0.1273,  0.0456, -0.0769,  0.0194,  0.0151,  0.0712, -0.0026,\n",
       "          -0.0899, -0.1460, -0.1171,  0.0558,  0.1444,  0.0925, -0.0144, -0.0127,\n",
       "           0.1526, -0.0477,  0.0173,  0.0625,  0.1189,  0.0759,  0.0309,  0.1629,\n",
       "          -0.1385, -0.0374,  0.0171,  0.1595, -0.1120,  0.1512, -0.0533,  0.1045],\n",
       "         [ 0.1047, -0.0128, -0.0270, -0.0027, -0.0048, -0.0352,  0.0278,  0.1151,\n",
       "          -0.0869, -0.1120,  0.0971, -0.0726, -0.1752,  0.0244,  0.0766,  0.0625,\n",
       "          -0.0946, -0.0516,  0.1676,  0.0080,  0.0296,  0.0657,  0.0618,  0.0003,\n",
       "          -0.0736, -0.1333,  0.1693,  0.0609,  0.0826, -0.0917,  0.0342,  0.0574],\n",
       "         [-0.0835,  0.1432, -0.1391,  0.0924,  0.1282, -0.0592,  0.0249,  0.0015,\n",
       "           0.1576,  0.0032, -0.1181,  0.1004,  0.0176, -0.0586, -0.0342, -0.0907,\n",
       "          -0.0077,  0.1036, -0.0601,  0.0725,  0.1681, -0.1313, -0.0588, -0.0238,\n",
       "          -0.1367,  0.1755,  0.0028,  0.0086, -0.1307,  0.1373,  0.0831, -0.1067],\n",
       "         [ 0.1105,  0.1445,  0.1345,  0.0280,  0.1203, -0.0034, -0.1202, -0.0560,\n",
       "          -0.1582, -0.0314, -0.1125,  0.0762, -0.0939,  0.1186,  0.0141, -0.0241,\n",
       "           0.0550, -0.0424,  0.0598, -0.1221, -0.1323, -0.0102, -0.0117,  0.0062,\n",
       "           0.0884, -0.0396, -0.0070, -0.0947,  0.0643, -0.0076,  0.1282,  0.0410],\n",
       "         [-0.0861, -0.0394,  0.0899, -0.1277, -0.0869, -0.0550, -0.1742,  0.1285,\n",
       "          -0.0266, -0.0224, -0.0054,  0.0944,  0.1006,  0.0629,  0.0816, -0.0502,\n",
       "          -0.0971, -0.1424,  0.1416, -0.0874,  0.1217,  0.0165,  0.1246, -0.1297,\n",
       "           0.1088,  0.1366,  0.0589, -0.0862,  0.1694, -0.1577, -0.0145,  0.0514]],\n",
       "        device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dynet.tree.optimizer.state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[ 1.2233e-01, -1.0797e-01, -1.1560e-01],\n",
       "           [ 1.0054e-01,  1.6252e-01,  2.8502e-01],\n",
       "           [ 1.9072e-01, -2.9728e-01, -8.9353e-02]],\n",
       " \n",
       "          [[-1.9856e-01, -9.1499e-02, -1.7031e-01],\n",
       "           [-3.7699e-02, -2.2428e-01, -2.5555e-02],\n",
       "           [-2.4178e-01,  3.3122e-02,  2.2636e-01]],\n",
       " \n",
       "          [[ 2.3545e-01,  3.0003e-01, -2.1340e-01],\n",
       "           [-1.1353e-01,  2.3427e-01,  1.7870e-01],\n",
       "           [ 1.4887e-01,  2.6009e-01,  2.8229e-01]]],\n",
       " \n",
       " \n",
       "         [[[-7.5914e-02,  1.8779e-01, -5.8983e-02],\n",
       "           [-2.5899e-02, -6.1926e-02, -2.4166e-01],\n",
       "           [ 3.4330e-01,  1.1774e-01, -1.0435e-01]],\n",
       " \n",
       "          [[-3.3113e-01,  3.2410e-01,  1.9273e-01],\n",
       "           [-1.2584e-01, -2.5498e-01, -3.2613e-01],\n",
       "           [-1.4636e-01,  3.1716e-01, -3.9758e-02]],\n",
       " \n",
       "          [[-2.1444e-01, -3.3370e-04, -6.2067e-02],\n",
       "           [ 1.7349e-01, -1.9738e-01, -6.7500e-02],\n",
       "           [-1.4169e-01, -1.8716e-01, -1.2045e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.6432e-01,  6.5764e-02, -1.0571e-01],\n",
       "           [ 3.0477e-01,  2.7013e-01,  1.9134e-01],\n",
       "           [ 1.9773e-01,  1.4202e-01, -2.4380e-01]],\n",
       " \n",
       "          [[-2.8068e-01, -2.8282e-01,  2.2601e-01],\n",
       "           [ 2.1182e-01,  2.8515e-01, -1.6137e-01],\n",
       "           [-1.3436e-01,  1.3971e-02, -5.9268e-02]],\n",
       " \n",
       "          [[-2.7546e-01, -2.6605e-01,  1.0383e-02],\n",
       "           [-1.8733e-01, -1.9085e-01,  1.4405e-01],\n",
       "           [-5.9748e-02, -7.5328e-03,  2.9056e-02]]],\n",
       " \n",
       " \n",
       "         [[[-2.6458e-01,  3.1176e-01,  1.4254e-01],\n",
       "           [-2.2056e-01,  1.1775e-01,  5.1721e-03],\n",
       "           [-2.5706e-01,  8.7064e-02,  3.2318e-01]],\n",
       " \n",
       "          [[ 1.0737e-01,  5.7737e-02,  1.3161e-01],\n",
       "           [ 8.4253e-02,  2.0309e-01,  1.0775e-01],\n",
       "           [-2.2939e-01,  1.7117e-01, -2.2541e-01]],\n",
       " \n",
       "          [[-2.0114e-01,  1.0961e-01, -4.1199e-01],\n",
       "           [ 9.5966e-02, -2.1471e-03, -5.6395e-02],\n",
       "           [ 2.4724e-01, -1.9688e-01, -6.0399e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0826e-01,  9.8270e-02,  2.2055e-01],\n",
       "           [-3.1190e-01,  3.0177e-02,  2.7127e-01],\n",
       "           [-1.8063e-01,  3.1182e-01, -1.4948e-01]],\n",
       " \n",
       "          [[ 5.8237e-02, -2.6585e-01, -1.2489e-01],\n",
       "           [ 1.8842e-01, -1.0359e-01, -1.4133e-01],\n",
       "           [-1.3789e-01,  1.8560e-01, -2.5083e-01]],\n",
       " \n",
       "          [[-2.2624e-01, -1.5114e-01,  9.9086e-02],\n",
       "           [ 2.8583e-01,  6.9618e-02, -2.6597e-01],\n",
       "           [ 1.5515e-01,  1.1196e-01, -2.1585e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.5663e-01, -3.1774e-01, -3.7449e-02],\n",
       "           [-2.2345e-01,  7.9603e-02,  2.1506e-01],\n",
       "           [-2.6364e-01, -6.7569e-02, -1.1443e-01]],\n",
       " \n",
       "          [[ 1.2584e-01, -2.3501e-03,  2.3127e-01],\n",
       "           [ 3.2704e-01, -8.6569e-02,  7.5210e-02],\n",
       "           [-2.7844e-02, -1.0214e-01, -1.4932e-01]],\n",
       " \n",
       "          [[ 8.9557e-02,  3.0225e-01, -1.9086e-01],\n",
       "           [ 3.0879e-01,  3.6099e-02, -1.0697e-01],\n",
       "           [ 1.6451e-01, -2.7746e-01, -2.4977e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.1526e-01,  1.1080e-02,  1.7361e-02],\n",
       "           [ 3.6307e-02,  2.8767e-01,  2.8404e-01],\n",
       "           [ 2.8595e-01, -2.8179e-01,  8.3770e-02]],\n",
       " \n",
       "          [[-2.6417e-01,  2.7411e-01, -3.1225e-01],\n",
       "           [ 5.5032e-02,  5.5944e-02, -2.6682e-01],\n",
       "           [-4.5083e-02,  1.4721e-01,  2.5143e-02]],\n",
       " \n",
       "          [[-2.1594e-01, -2.9799e-01, -3.1916e-02],\n",
       "           [-4.9346e-02,  1.1174e-01, -5.0286e-02],\n",
       "           [ 2.2442e-01,  1.1262e-01, -2.2526e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.3656e-01,  1.1999e-01, -6.8009e-02],\n",
       "           [ 2.8413e-01,  1.3740e-01,  1.7116e-01],\n",
       "           [ 1.9399e-01, -8.2427e-02, -6.4057e-02]],\n",
       " \n",
       "          [[-2.2379e-01,  9.7441e-02, -2.0585e-01],\n",
       "           [ 3.9243e-02,  2.0372e-01, -1.9928e-01],\n",
       "           [-1.8606e-01, -1.8294e-01, -1.8116e-01]],\n",
       " \n",
       "          [[ 2.5686e-01,  1.4978e-01,  2.5665e-01],\n",
       "           [ 2.6907e-01,  2.5505e-01, -2.4254e-01],\n",
       "           [ 2.5893e-01,  1.8229e-01, -3.9904e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-4.4501e-07, -2.2539e-06,  1.2606e-06,  1.2836e-06, -1.3354e-06,\n",
       "          3.4596e-06, -2.4821e-06,  1.0208e-06], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-0.1094,  0.0477, -0.1304],\n",
       "           [ 0.1377,  0.1671, -0.0014],\n",
       "           [-0.0254, -0.1369, -0.1325]],\n",
       " \n",
       "          [[-0.1455, -0.0324, -0.1950],\n",
       "           [-0.0974, -0.0884, -0.1209],\n",
       "           [ 0.1504, -0.1873,  0.1776]],\n",
       " \n",
       "          [[ 0.0538, -0.0136, -0.1651],\n",
       "           [ 0.0709, -0.1367,  0.0947],\n",
       "           [ 0.1198, -0.0600, -0.1717]],\n",
       " \n",
       "          [[-0.1896, -0.0310, -0.0839],\n",
       "           [-0.0520, -0.0387,  0.1845],\n",
       "           [-0.0211,  0.0655,  0.0538]],\n",
       " \n",
       "          [[-0.0209,  0.0888, -0.1742],\n",
       "           [-0.1534, -0.1318,  0.0126],\n",
       "           [ 0.1834, -0.1970,  0.0903]],\n",
       " \n",
       "          [[-0.1570,  0.0254,  0.0743],\n",
       "           [-0.0194,  0.0372, -0.1622],\n",
       "           [ 0.1763, -0.0384, -0.1877]],\n",
       " \n",
       "          [[-0.1325, -0.1633,  0.0614],\n",
       "           [-0.0270,  0.0134,  0.0649],\n",
       "           [-0.0902,  0.1211, -0.0858]],\n",
       " \n",
       "          [[-0.0902, -0.1638, -0.0359],\n",
       "           [-0.0376, -0.1899,  0.0580],\n",
       "           [-0.0680,  0.0136,  0.1916]]],\n",
       " \n",
       " \n",
       "         [[[-0.1678,  0.0647,  0.1790],\n",
       "           [-0.0774,  0.1681,  0.1820],\n",
       "           [-0.0945, -0.1816,  0.1511]],\n",
       " \n",
       "          [[ 0.0404,  0.0036,  0.1706],\n",
       "           [-0.0140,  0.0586, -0.0987],\n",
       "           [-0.0862, -0.0816, -0.0122]],\n",
       " \n",
       "          [[ 0.1010,  0.0871,  0.0713],\n",
       "           [-0.0147,  0.1403, -0.0540],\n",
       "           [ 0.0641,  0.0940,  0.0933]],\n",
       " \n",
       "          [[ 0.1813, -0.1705,  0.1514],\n",
       "           [ 0.0643,  0.0492, -0.0042],\n",
       "           [ 0.0686,  0.0742,  0.0998]],\n",
       " \n",
       "          [[ 0.1424, -0.0487, -0.1640],\n",
       "           [-0.1664, -0.0377, -0.1758],\n",
       "           [ 0.0383, -0.0249, -0.1156]],\n",
       " \n",
       "          [[ 0.1309,  0.0653,  0.1421],\n",
       "           [ 0.0978,  0.0914, -0.1185],\n",
       "           [-0.0486,  0.0447, -0.0913]],\n",
       " \n",
       "          [[ 0.1836, -0.0687, -0.1711],\n",
       "           [ 0.0012, -0.1759,  0.1909],\n",
       "           [-0.1920, -0.1496, -0.1659]],\n",
       " \n",
       "          [[ 0.0951,  0.1370, -0.1494],\n",
       "           [ 0.0553, -0.1865, -0.0447],\n",
       "           [-0.1457, -0.0763,  0.0353]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1743,  0.1898,  0.0799],\n",
       "           [ 0.0154, -0.0028,  0.0088],\n",
       "           [ 0.0963,  0.1812,  0.0225]],\n",
       " \n",
       "          [[-0.1981, -0.0059, -0.0647],\n",
       "           [ 0.0162,  0.1008, -0.0751],\n",
       "           [ 0.1629, -0.0597,  0.1525]],\n",
       " \n",
       "          [[ 0.1824, -0.1354, -0.1851],\n",
       "           [-0.0878,  0.1976,  0.2052],\n",
       "           [-0.0418, -0.0005, -0.0128]],\n",
       " \n",
       "          [[ 0.0779,  0.0663, -0.0949],\n",
       "           [-0.1563, -0.0197,  0.1193],\n",
       "           [-0.1801,  0.0061, -0.1574]],\n",
       " \n",
       "          [[-0.0085,  0.0704,  0.0216],\n",
       "           [-0.0662,  0.0935, -0.0955],\n",
       "           [-0.1344,  0.0940, -0.0745]],\n",
       " \n",
       "          [[-0.0829, -0.1446, -0.1520],\n",
       "           [ 0.1606,  0.1886, -0.1921],\n",
       "           [-0.1417, -0.1395, -0.0154]],\n",
       " \n",
       "          [[ 0.0366,  0.0977,  0.1137],\n",
       "           [ 0.1406,  0.0559,  0.1265],\n",
       "           [-0.1545,  0.0132, -0.1726]],\n",
       " \n",
       "          [[ 0.0242,  0.1372,  0.0824],\n",
       "           [ 0.1873, -0.1073, -0.0233],\n",
       "           [-0.0660,  0.0941, -0.1118]]],\n",
       " \n",
       " \n",
       "         [[[-0.0036, -0.1321,  0.0265],\n",
       "           [ 0.0044, -0.1235,  0.0415],\n",
       "           [ 0.1807, -0.0853,  0.0422]],\n",
       " \n",
       "          [[-0.1585,  0.0471, -0.0008],\n",
       "           [-0.1157,  0.0630,  0.1464],\n",
       "           [ 0.0027, -0.0325, -0.0018]],\n",
       " \n",
       "          [[-0.1237,  0.1810, -0.0225],\n",
       "           [ 0.0465,  0.1673, -0.0849],\n",
       "           [ 0.0319, -0.0432,  0.1436]],\n",
       " \n",
       "          [[-0.1010,  0.0839, -0.1031],\n",
       "           [-0.1752,  0.1855, -0.1735],\n",
       "           [-0.1159,  0.1508, -0.1392]],\n",
       " \n",
       "          [[ 0.1759,  0.0099,  0.0775],\n",
       "           [ 0.1623, -0.0177,  0.1654],\n",
       "           [ 0.1797,  0.0326,  0.1668]],\n",
       " \n",
       "          [[ 0.1376, -0.1365,  0.1781],\n",
       "           [ 0.0873,  0.0738,  0.0113],\n",
       "           [-0.1016, -0.1338,  0.0131]],\n",
       " \n",
       "          [[-0.0338,  0.1475,  0.0991],\n",
       "           [-0.1416,  0.1888, -0.1740],\n",
       "           [ 0.1841, -0.0691,  0.1075]],\n",
       " \n",
       "          [[ 0.1892,  0.1213,  0.0431],\n",
       "           [-0.1201,  0.0330,  0.0900],\n",
       "           [ 0.1286,  0.1298,  0.1095]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0267,  0.1664,  0.1530],\n",
       "           [-0.1235, -0.1281,  0.0966],\n",
       "           [ 0.0599, -0.0394,  0.1763]],\n",
       " \n",
       "          [[ 0.1255,  0.0995,  0.1309],\n",
       "           [-0.1268,  0.1339,  0.1813],\n",
       "           [ 0.0854,  0.0343,  0.2017]],\n",
       " \n",
       "          [[-0.1079,  0.0040, -0.0453],\n",
       "           [-0.0908, -0.1642, -0.1346],\n",
       "           [-0.1486, -0.1078,  0.1396]],\n",
       " \n",
       "          [[ 0.1421,  0.1042, -0.0906],\n",
       "           [-0.1548,  0.1357,  0.0586],\n",
       "           [-0.0083, -0.1568, -0.1827]],\n",
       " \n",
       "          [[ 0.0674, -0.0513, -0.2000],\n",
       "           [-0.0864,  0.0226, -0.0136],\n",
       "           [ 0.0961,  0.0412,  0.0643]],\n",
       " \n",
       "          [[-0.0603,  0.1057, -0.0645],\n",
       "           [-0.0176, -0.1180, -0.0258],\n",
       "           [-0.0915, -0.2009,  0.1365]],\n",
       " \n",
       "          [[ 0.1358,  0.1480,  0.1600],\n",
       "           [-0.1937,  0.0829,  0.0082],\n",
       "           [ 0.0945, -0.1708,  0.0654]],\n",
       " \n",
       "          [[-0.0108,  0.0461,  0.0056],\n",
       "           [-0.0286, -0.1307, -0.1217],\n",
       "           [ 0.1641, -0.1778,  0.1271]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1752, -0.0827,  0.0356],\n",
       "           [-0.1209,  0.1683, -0.1464],\n",
       "           [ 0.1614,  0.1723,  0.0874]],\n",
       " \n",
       "          [[ 0.0489, -0.0017, -0.1709],\n",
       "           [-0.0718, -0.1718,  0.1481],\n",
       "           [-0.1876,  0.0939, -0.1043]],\n",
       " \n",
       "          [[ 0.1679,  0.0586, -0.0559],\n",
       "           [-0.1175,  0.0408, -0.0156],\n",
       "           [ 0.1581, -0.0078, -0.1821]],\n",
       " \n",
       "          [[ 0.0811,  0.0750, -0.1293],\n",
       "           [-0.0395,  0.1261,  0.1136],\n",
       "           [-0.1624, -0.0150,  0.0044]],\n",
       " \n",
       "          [[ 0.0771,  0.0844,  0.0623],\n",
       "           [ 0.1160, -0.0954, -0.0934],\n",
       "           [-0.1466, -0.1425, -0.1862]],\n",
       " \n",
       "          [[ 0.0397,  0.1634,  0.0310],\n",
       "           [-0.0022,  0.1677,  0.1544],\n",
       "           [ 0.0881, -0.0830, -0.0285]],\n",
       " \n",
       "          [[ 0.1621, -0.0746, -0.0057],\n",
       "           [-0.1688,  0.1034,  0.0552],\n",
       "           [-0.1641, -0.1067,  0.1381]],\n",
       " \n",
       "          [[ 0.1746, -0.0256, -0.0625],\n",
       "           [-0.0487,  0.0952, -0.1608],\n",
       "           [ 0.1820, -0.0538, -0.1415]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0093, -0.0098,  0.1415],\n",
       "           [ 0.1101,  0.1287, -0.1625],\n",
       "           [-0.0975,  0.1740, -0.0562]],\n",
       " \n",
       "          [[ 0.0031,  0.0465,  0.1671],\n",
       "           [-0.0045, -0.1272, -0.1607],\n",
       "           [ 0.1036,  0.1494,  0.0554]],\n",
       " \n",
       "          [[-0.0196,  0.0380, -0.1718],\n",
       "           [ 0.1197, -0.1396, -0.0888],\n",
       "           [-0.0367,  0.1168, -0.0290]],\n",
       " \n",
       "          [[-0.1760, -0.1106, -0.1435],\n",
       "           [-0.1507, -0.1566,  0.1508],\n",
       "           [ 0.1697,  0.0256,  0.1545]],\n",
       " \n",
       "          [[ 0.0868,  0.0998,  0.0896],\n",
       "           [ 0.0542,  0.1683, -0.0545],\n",
       "           [ 0.1126, -0.1002,  0.1612]],\n",
       " \n",
       "          [[ 0.1086,  0.1532, -0.0571],\n",
       "           [ 0.0475, -0.0071,  0.0755],\n",
       "           [ 0.1538,  0.1554,  0.0698]],\n",
       " \n",
       "          [[ 0.1461,  0.1382,  0.0343],\n",
       "           [ 0.0541, -0.1413, -0.1505],\n",
       "           [ 0.1323,  0.0929, -0.1131]],\n",
       " \n",
       "          [[ 0.1624, -0.1414,  0.1358],\n",
       "           [-0.1148, -0.0329, -0.1198],\n",
       "           [-0.1768,  0.0267, -0.1531]]],\n",
       " \n",
       " \n",
       "         [[[-0.1507,  0.1945, -0.0061],\n",
       "           [-0.1223,  0.1440,  0.1953],\n",
       "           [-0.0032, -0.0467,  0.1843]],\n",
       " \n",
       "          [[ 0.0642, -0.1290,  0.0217],\n",
       "           [ 0.1416,  0.1946,  0.1177],\n",
       "           [ 0.1501, -0.0710,  0.1466]],\n",
       " \n",
       "          [[ 0.1830, -0.1158, -0.1279],\n",
       "           [ 0.1057, -0.1152,  0.0736],\n",
       "           [-0.1130,  0.0702, -0.1538]],\n",
       " \n",
       "          [[ 0.0254,  0.1707, -0.0514],\n",
       "           [-0.0969, -0.1010, -0.0157],\n",
       "           [-0.0748,  0.1804, -0.0579]],\n",
       " \n",
       "          [[-0.0941, -0.1464, -0.0625],\n",
       "           [-0.0474,  0.1434, -0.1921],\n",
       "           [-0.1478,  0.1448, -0.1495]],\n",
       " \n",
       "          [[-0.1499, -0.1790,  0.0382],\n",
       "           [-0.0277,  0.1208,  0.0007],\n",
       "           [ 0.1220, -0.0990,  0.0858]],\n",
       " \n",
       "          [[-0.0597,  0.0992, -0.1443],\n",
       "           [ 0.0856, -0.0775,  0.1801],\n",
       "           [-0.0448, -0.1107, -0.0328]],\n",
       " \n",
       "          [[ 0.0582, -0.0410,  0.0748],\n",
       "           [ 0.1372,  0.0717, -0.0766],\n",
       "           [-0.1841,  0.0627,  0.0904]]]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 4.8866e-07,  1.1841e-06, -6.2245e-07,  1.8964e-07, -4.6248e-06,\n",
       "          1.2891e-06, -4.1887e-06, -2.1644e-06], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 1.0635e-01,  1.9060e-01, -7.8454e-02],\n",
       "           [-2.3215e-02, -3.6200e-02, -1.8921e-02],\n",
       "           [-4.5146e-03, -1.1549e-01,  1.7637e-01]],\n",
       " \n",
       "          [[-1.8185e-02,  2.0165e-01,  8.3150e-02],\n",
       "           [ 1.5513e-02, -1.0642e-01,  1.9328e-01],\n",
       "           [ 4.9810e-02,  1.7587e-01,  3.7231e-02]],\n",
       " \n",
       "          [[ 9.5331e-02, -1.0814e-01, -1.5141e-01],\n",
       "           [-1.6524e-01, -1.4073e-01,  3.3038e-02],\n",
       "           [ 3.7056e-02,  3.2101e-02,  3.6298e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.7013e-01, -5.3636e-03,  1.8894e-01],\n",
       "           [ 9.1932e-02,  6.0471e-02,  6.0471e-02],\n",
       "           [-1.2386e-01,  2.0363e-01, -2.2394e-02]],\n",
       " \n",
       "          [[-3.8919e-03, -1.1256e-01, -1.5077e-01],\n",
       "           [-4.6223e-02,  1.9148e-01,  1.3509e-01],\n",
       "           [-1.8662e-01,  1.5472e-01,  5.8691e-02]],\n",
       " \n",
       "          [[ 1.2529e-01,  7.4358e-05,  1.7410e-01],\n",
       "           [ 5.9954e-02,  1.0846e-01,  8.5738e-02],\n",
       "           [ 1.6165e-01,  1.5575e-01,  1.6251e-02]]],\n",
       " \n",
       " \n",
       "         [[[-5.3034e-04,  1.4990e-01, -1.4546e-01],\n",
       "           [-1.2899e-01, -1.0532e-01,  9.5060e-02],\n",
       "           [-7.1052e-02, -1.6902e-01,  1.4123e-01]],\n",
       " \n",
       "          [[ 4.2795e-02, -8.2319e-02,  1.2209e-01],\n",
       "           [ 1.1553e-01, -1.4206e-01, -1.8191e-01],\n",
       "           [ 2.6162e-02,  8.9395e-02, -5.4940e-02]],\n",
       " \n",
       "          [[-3.0834e-02,  6.8771e-02,  1.7198e-01],\n",
       "           [-1.0759e-01,  1.3363e-01,  1.2277e-01],\n",
       "           [-1.3228e-01, -1.7876e-01,  4.5876e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 9.5875e-02, -1.7292e-01, -1.7384e-01],\n",
       "           [-5.7801e-02, -6.9255e-02, -8.0834e-02],\n",
       "           [ 8.6357e-02, -7.0235e-02, -7.3633e-02]],\n",
       " \n",
       "          [[-1.4221e-01,  1.0876e-01,  3.5759e-02],\n",
       "           [ 1.8346e-01, -9.2328e-02,  3.1995e-02],\n",
       "           [-8.2773e-02, -6.7382e-02, -1.8233e-01]],\n",
       " \n",
       "          [[-8.9682e-02,  8.8345e-03, -5.3892e-02],\n",
       "           [ 8.9291e-02,  5.3500e-02, -1.3750e-01],\n",
       "           [ 1.3097e-01,  3.9502e-02, -1.8025e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.8069e-01,  8.4294e-02, -1.7466e-01],\n",
       "           [ 1.5447e-01, -6.6029e-02, -1.5819e-01],\n",
       "           [ 1.3377e-01,  1.9408e-02,  6.1315e-02]],\n",
       " \n",
       "          [[ 1.2253e-01,  2.8684e-02, -1.5054e-01],\n",
       "           [-1.5801e-01,  1.3146e-01,  1.7881e-01],\n",
       "           [-8.8034e-02, -1.5216e-01,  1.1855e-01]],\n",
       " \n",
       "          [[-1.2727e-01,  1.2085e-01, -1.7862e-01],\n",
       "           [-1.8179e-01, -1.1328e-03, -1.6249e-01],\n",
       "           [-1.8240e-01, -1.0790e-01, -1.4543e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-9.5802e-02, -1.0505e-01, -1.0717e-01],\n",
       "           [ 3.4889e-02, -1.5630e-01, -2.9784e-02],\n",
       "           [ 1.8235e-01, -1.8732e-01,  3.3444e-02]],\n",
       " \n",
       "          [[ 9.3966e-02,  1.0343e-01, -3.2535e-02],\n",
       "           [-1.3194e-01,  1.7779e-01, -1.0850e-01],\n",
       "           [ 1.0270e-02,  1.7786e-01, -1.1475e-01]],\n",
       " \n",
       "          [[ 1.7757e-01, -6.0830e-02, -1.0427e-01],\n",
       "           [ 1.4987e-01,  5.7600e-02,  8.5931e-02],\n",
       "           [ 6.7867e-02,  1.1781e-01, -2.9943e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.5834e-01, -1.9235e-01,  5.4715e-02],\n",
       "           [-6.4879e-02, -1.4939e-01, -1.5783e-01],\n",
       "           [-2.1378e-01, -1.9395e-01, -1.6190e-01]],\n",
       " \n",
       "          [[ 1.1175e-01,  5.0419e-02,  4.6070e-02],\n",
       "           [ 1.1403e-01, -8.5888e-02,  8.9724e-02],\n",
       "           [-1.9488e-01,  1.1339e-01,  1.1628e-02]],\n",
       " \n",
       "          [[-1.0994e-02,  2.3260e-02, -2.9554e-02],\n",
       "           [ 8.6178e-02, -1.3086e-01,  1.6836e-01],\n",
       "           [ 1.4781e-01,  1.4485e-01, -1.0650e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0571e-01, -5.2068e-02, -9.3555e-02],\n",
       "           [ 5.2385e-02, -4.2563e-03, -8.3273e-02],\n",
       "           [ 1.6074e-01,  1.4541e-01, -5.9887e-02]],\n",
       " \n",
       "          [[ 7.3584e-03, -7.8646e-02, -6.3913e-03],\n",
       "           [-1.8667e-01,  1.3417e-02,  2.0022e-01],\n",
       "           [ 8.9518e-02,  8.6905e-02, -5.3541e-02]],\n",
       " \n",
       "          [[ 9.6323e-04,  1.1291e-01,  1.6788e-01],\n",
       "           [-3.6510e-02, -7.7487e-02,  8.7166e-02],\n",
       "           [ 1.5017e-01, -2.5396e-02,  3.1981e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.5843e-01, -1.6237e-01,  9.8059e-02],\n",
       "           [ 9.0719e-02, -2.5686e-02,  1.7368e-01],\n",
       "           [-6.7550e-02, -1.5044e-01,  4.4057e-02]],\n",
       " \n",
       "          [[-1.5011e-01,  6.4690e-02,  6.1893e-02],\n",
       "           [-1.5164e-01, -9.5009e-02,  1.1989e-01],\n",
       "           [-3.9579e-04, -4.1715e-02, -1.3564e-01]],\n",
       " \n",
       "          [[ 5.1865e-02, -8.0841e-02, -1.6108e-01],\n",
       "           [-1.5143e-01, -6.9575e-02,  1.5320e-01],\n",
       "           [-1.5735e-01,  1.4610e-01,  1.3800e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.6892e-02,  6.8519e-02,  1.2724e-01],\n",
       "           [-1.3321e-01, -8.5373e-02, -4.2546e-02],\n",
       "           [-1.3432e-01,  7.8485e-02, -1.2776e-01]],\n",
       " \n",
       "          [[-8.6186e-02, -1.4207e-01,  1.0763e-01],\n",
       "           [ 1.7905e-01, -7.3547e-02,  1.3974e-01],\n",
       "           [-1.5891e-01, -6.1176e-02,  1.6729e-01]],\n",
       " \n",
       "          [[-1.2317e-01, -7.3587e-02,  1.7744e-01],\n",
       "           [-1.4278e-01, -1.5384e-01,  8.2101e-02],\n",
       "           [ 3.0518e-04, -1.2913e-01, -1.1947e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 7.6622e-02, -1.4633e-01,  9.6578e-02],\n",
       "           [-9.2232e-02,  5.6348e-02,  1.3592e-01],\n",
       "           [ 1.9390e-01,  1.6627e-01, -1.0032e-01]],\n",
       " \n",
       "          [[-5.9257e-02, -1.6760e-01,  8.0818e-02],\n",
       "           [-1.3233e-01,  1.9430e-01, -5.1566e-02],\n",
       "           [ 2.2294e-01,  7.1613e-02, -1.4133e-01]],\n",
       " \n",
       "          [[ 1.2790e-01, -4.0809e-02, -2.0408e-01],\n",
       "           [ 1.4337e-01,  1.1465e-01,  2.7806e-02],\n",
       "           [ 1.7017e-01,  3.6093e-02,  7.6696e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.3152e-03, -5.4402e-02,  3.2257e-02],\n",
       "           [ 1.7898e-02, -1.9865e-01,  1.0244e-01],\n",
       "           [-5.4609e-02, -1.7114e-01, -3.9173e-02]],\n",
       " \n",
       "          [[-2.3136e-02, -8.4636e-02, -2.3358e-02],\n",
       "           [ 8.7430e-02, -1.3486e-01,  3.7457e-02],\n",
       "           [-6.7237e-02,  1.2494e-01, -2.2367e-01]],\n",
       " \n",
       "          [[ 2.1185e-01, -7.1481e-02,  1.0434e-01],\n",
       "           [-5.4778e-02,  9.7401e-02,  1.8615e-01],\n",
       "           [-5.0614e-02, -2.5251e-02,  2.8698e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-4.2020e-06,  9.4549e-07, -3.7896e-06,  8.4918e-08, -9.3607e-07,\n",
       "          7.4030e-07,  4.4473e-06,  1.7969e-06,  1.9541e-06,  5.6487e-06,\n",
       "         -2.0300e-06, -2.7495e-06, -1.9099e-06, -8.9899e-07, -7.2531e-07,\n",
       "         -3.8294e-08], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 2.3320e-02,  5.8278e-02,  2.1913e-02],\n",
       "           [ 1.0780e-01, -1.2621e-01, -1.5261e-01],\n",
       "           [-5.8727e-02, -7.1870e-04,  6.6658e-03]],\n",
       " \n",
       "          [[ 1.1516e-01,  1.1550e-01, -1.1339e-01],\n",
       "           [-5.0120e-02,  1.3261e-01,  1.4528e-01],\n",
       "           [-3.8750e-02, -4.9018e-02, -9.9877e-02]],\n",
       " \n",
       "          [[-1.0855e-01, -3.1459e-02, -2.0521e-02],\n",
       "           [-1.2538e-01, -4.3097e-02, -2.2858e-02],\n",
       "           [-8.6533e-02,  1.1572e-01,  8.1870e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.1430e-01, -1.9040e-02, -1.2409e-01],\n",
       "           [-6.3435e-02,  3.7612e-02, -7.0888e-02],\n",
       "           [ 1.1234e-01,  6.8877e-02, -4.0181e-02]],\n",
       " \n",
       "          [[ 5.3448e-02,  1.0392e-01,  1.0198e-01],\n",
       "           [-1.2121e-02, -1.5430e-02, -7.5842e-02],\n",
       "           [-7.5652e-02, -7.8263e-02, -1.1098e-01]],\n",
       " \n",
       "          [[-9.2729e-02,  3.4844e-02, -2.3835e-02],\n",
       "           [ 6.5880e-02, -1.4028e-01, -1.4094e-01],\n",
       "           [ 1.0595e-01,  9.0912e-02, -3.4854e-03]]],\n",
       " \n",
       " \n",
       "         [[[-8.1880e-04,  5.9362e-02, -1.3318e-01],\n",
       "           [ 5.8124e-02,  1.0412e-01,  1.3306e-01],\n",
       "           [ 3.9635e-03,  9.3215e-02,  7.0479e-02]],\n",
       " \n",
       "          [[ 6.2421e-02,  6.0060e-02,  1.0880e-01],\n",
       "           [-8.5378e-03, -1.2351e-01,  7.2200e-02],\n",
       "           [-4.4593e-02, -6.4109e-02, -4.8819e-03]],\n",
       " \n",
       "          [[-8.8117e-02, -6.0037e-02, -8.6524e-03],\n",
       "           [-2.6853e-02,  1.2220e-01, -6.9315e-02],\n",
       "           [ 1.9544e-02, -1.0688e-01, -1.4169e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.3582e-01, -5.5153e-02,  4.8441e-03],\n",
       "           [-4.3756e-02, -7.5189e-02,  4.9890e-02],\n",
       "           [-4.8356e-02, -3.1583e-03,  9.9293e-02]],\n",
       " \n",
       "          [[-7.2338e-02, -7.4791e-02,  9.8975e-02],\n",
       "           [ 1.3891e-01, -1.2703e-01, -7.9911e-02],\n",
       "           [ 7.1626e-02, -3.2525e-05,  7.8405e-02]],\n",
       " \n",
       "          [[-1.1900e-01, -1.3239e-01, -9.7198e-02],\n",
       "           [ 2.0141e-04,  9.9731e-02, -5.3664e-02],\n",
       "           [-1.1615e-02,  9.7944e-02, -4.9798e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.8457e-02, -3.2829e-02,  4.6692e-02],\n",
       "           [ 2.9558e-02, -4.0326e-02,  6.3537e-02],\n",
       "           [ 3.0698e-02,  1.2447e-01, -1.2183e-01]],\n",
       " \n",
       "          [[-4.1252e-02,  5.7144e-02, -3.3021e-02],\n",
       "           [ 1.2226e-01,  6.6156e-03,  1.1323e-01],\n",
       "           [-3.0162e-02,  1.2324e-01,  9.3607e-02]],\n",
       " \n",
       "          [[ 2.2929e-02,  5.3653e-02,  4.7225e-03],\n",
       "           [-2.6756e-02,  1.2612e-01, -1.1354e-01],\n",
       "           [-1.2811e-01,  9.4408e-02,  5.3685e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.6038e-02,  1.1222e-01,  1.4215e-01],\n",
       "           [-1.2461e-01,  9.0963e-02,  4.3583e-02],\n",
       "           [ 9.8822e-02,  1.2231e-01, -6.6700e-02]],\n",
       " \n",
       "          [[ 3.0040e-02,  1.0379e-01,  1.8812e-02],\n",
       "           [-1.0485e-01,  4.8557e-02,  1.1072e-01],\n",
       "           [ 5.2530e-02, -8.9239e-02, -1.1146e-01]],\n",
       " \n",
       "          [[-1.1322e-01, -1.2220e-01, -7.4908e-02],\n",
       "           [-9.7898e-02, -1.1281e-01,  3.8691e-02],\n",
       "           [ 1.0792e-01,  6.1381e-02, -1.1061e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 2.2913e-02, -7.3010e-02,  1.1374e-01],\n",
       "           [-5.4716e-02, -7.1698e-02, -9.1502e-02],\n",
       "           [-6.9164e-02, -6.6752e-02,  3.1186e-02]],\n",
       " \n",
       "          [[-3.8754e-02, -1.1593e-01,  6.3641e-02],\n",
       "           [ 1.3793e-01,  2.9055e-02,  1.0970e-01],\n",
       "           [-7.6242e-02,  8.6206e-02, -1.1082e-01]],\n",
       " \n",
       "          [[-1.3520e-01,  1.1323e-01, -1.1425e-01],\n",
       "           [ 8.7512e-02,  9.0999e-02, -1.1420e-01],\n",
       "           [ 1.4235e-01, -5.2422e-02, -5.4441e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.7417e-02,  8.8134e-02,  1.2947e-01],\n",
       "           [-1.2379e-02,  1.1888e-01, -7.9912e-02],\n",
       "           [-5.2966e-02,  1.3131e-01,  2.5443e-03]],\n",
       " \n",
       "          [[ 6.9660e-02, -3.4221e-02, -2.1286e-02],\n",
       "           [ 1.0441e-01,  4.7275e-03,  5.0270e-02],\n",
       "           [-1.2704e-01, -8.9836e-02,  1.6273e-02]],\n",
       " \n",
       "          [[ 1.0085e-01, -9.0703e-02, -5.9582e-02],\n",
       "           [-6.1979e-03, -5.0744e-02,  8.9788e-03],\n",
       "           [-2.2147e-02, -6.1341e-02, -3.4915e-02]]],\n",
       " \n",
       " \n",
       "         [[[-7.9173e-02,  1.0203e-01, -9.8545e-02],\n",
       "           [-1.1140e-01, -4.3579e-02,  1.3002e-01],\n",
       "           [ 6.8384e-02,  1.4014e-01,  2.4411e-02]],\n",
       " \n",
       "          [[-1.1598e-01,  1.2445e-01, -4.4399e-03],\n",
       "           [ 2.4686e-02, -1.0966e-01,  4.4525e-02],\n",
       "           [-6.7641e-02, -6.2949e-02,  5.9019e-02]],\n",
       " \n",
       "          [[ 5.0199e-02, -1.2687e-01,  7.3455e-02],\n",
       "           [ 8.8037e-02,  1.0495e-01,  9.0475e-02],\n",
       "           [ 2.4475e-02, -1.0789e-01, -5.8583e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.5966e-02,  1.1624e-01, -9.1316e-02],\n",
       "           [ 2.3336e-02,  5.5963e-04,  1.0393e-01],\n",
       "           [-1.0541e-01, -1.3389e-01, -8.1914e-03]],\n",
       " \n",
       "          [[-4.8128e-03,  1.2437e-01, -9.4402e-02],\n",
       "           [ 3.7788e-02, -8.7934e-02,  1.4541e-01],\n",
       "           [-1.2637e-01,  1.3636e-01, -9.2408e-02]],\n",
       " \n",
       "          [[ 7.7760e-02,  7.3097e-02,  6.5494e-02],\n",
       "           [ 9.7212e-04,  5.7406e-02, -5.4047e-03],\n",
       "           [-9.6770e-02, -2.3242e-02,  1.2266e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.5359e-02,  1.1287e-01,  3.5499e-03],\n",
       "           [-5.2685e-02,  4.2325e-02, -4.5189e-02],\n",
       "           [ 6.5179e-03, -4.7369e-02,  5.8609e-02]],\n",
       " \n",
       "          [[ 8.6564e-02,  1.3674e-01,  1.7454e-02],\n",
       "           [ 1.1389e-01, -6.5724e-03,  3.9187e-02],\n",
       "           [ 1.0315e-01,  7.4955e-02, -9.6964e-02]],\n",
       " \n",
       "          [[ 8.3606e-02,  1.4753e-01, -9.4033e-02],\n",
       "           [-2.7168e-02, -8.6773e-02, -9.4039e-02],\n",
       "           [-4.9206e-02,  1.1053e-01, -2.6256e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.2399e-02, -1.0967e-01,  1.3374e-01],\n",
       "           [ 7.5766e-02, -1.3445e-01, -1.5234e-02],\n",
       "           [ 1.0744e-01,  6.6928e-02, -1.3212e-01]],\n",
       " \n",
       "          [[ 3.0932e-02, -1.0104e-01, -3.0344e-02],\n",
       "           [ 2.2360e-02, -3.4547e-02,  1.2956e-01],\n",
       "           [ 1.2841e-01, -2.8487e-02, -8.4738e-02]],\n",
       " \n",
       "          [[-4.6592e-02,  1.4451e-01,  8.3424e-02],\n",
       "           [-1.0040e-01,  1.0800e-01,  4.9241e-02],\n",
       "           [-1.0096e-01,  1.5026e-02, -8.3207e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-2.8580e-05, -5.7106e-05,  1.0160e-04, -4.5732e-05,  3.0459e-05,\n",
       "         -4.7348e-06, -2.6534e-05,  2.2065e-07, -1.7105e-05, -2.3418e-06,\n",
       "          2.0291e-05,  9.9398e-05, -2.2766e-05, -1.0606e-04,  2.6959e-05,\n",
       "         -3.2369e-05, -5.6509e-05,  4.6831e-05,  8.6772e-05,  7.6245e-05,\n",
       "         -4.8536e-05, -1.3738e-05, -2.0636e-05,  6.6789e-05, -8.3066e-06,\n",
       "         -5.7829e-05, -1.7894e-05, -2.2073e-05, -7.9000e-05, -2.9149e-05,\n",
       "         -4.5438e-05, -2.4339e-05], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-3.3688e-02, -3.8310e-02, -6.9611e-02],\n",
       "           [ 7.9456e-02, -3.1459e-02,  7.3094e-02],\n",
       "           [ 9.8591e-02,  7.8891e-02, -8.9042e-02]],\n",
       " \n",
       "          [[-9.8842e-02, -2.2763e-02, -4.2898e-03],\n",
       "           [ 5.7245e-02,  8.2270e-02,  4.5103e-02],\n",
       "           [-4.9365e-02,  8.6985e-02,  3.4662e-02]],\n",
       " \n",
       "          [[-3.6132e-03,  1.9394e-02,  7.4846e-02],\n",
       "           [ 8.6999e-02, -6.3950e-02,  6.7117e-02],\n",
       "           [-9.5327e-03,  7.4772e-02, -1.0364e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.8420e-02,  3.4460e-02, -2.0881e-02],\n",
       "           [-8.5756e-02,  2.2161e-03, -6.4460e-02],\n",
       "           [ 2.1849e-02, -3.6732e-03, -9.6448e-02]],\n",
       " \n",
       "          [[-2.6929e-02, -6.0994e-02, -6.2729e-02],\n",
       "           [ 6.9633e-02,  4.4524e-02, -2.0068e-02],\n",
       "           [-3.8892e-02,  1.0838e-02, -8.3537e-03]],\n",
       " \n",
       "          [[-9.4192e-02, -7.8317e-02,  4.5645e-02],\n",
       "           [ 7.5237e-02,  6.7152e-02,  5.5863e-03],\n",
       "           [-7.3464e-02,  4.9467e-02, -5.8591e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.1815e-02,  4.3723e-02, -1.6356e-02],\n",
       "           [-3.6561e-03,  4.1122e-02, -3.6133e-02],\n",
       "           [ 2.0249e-02,  3.0738e-02,  6.2230e-02]],\n",
       " \n",
       "          [[ 8.9326e-02,  2.5446e-02,  3.9456e-02],\n",
       "           [ 6.8884e-02,  7.9735e-02,  2.3961e-02],\n",
       "           [ 5.1999e-03, -2.5530e-02, -3.1137e-02]],\n",
       " \n",
       "          [[-4.6663e-02, -6.6538e-02, -9.2988e-02],\n",
       "           [ 7.4693e-02, -3.4452e-02,  6.8729e-02],\n",
       "           [ 1.0504e-01, -2.5272e-02,  2.3083e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.8496e-02,  1.2263e-02, -4.5522e-03],\n",
       "           [ 7.9843e-02,  3.5170e-02, -8.3222e-02],\n",
       "           [ 1.0044e-01,  8.4765e-02,  5.7128e-02]],\n",
       " \n",
       "          [[ 9.4084e-03,  9.4106e-02,  5.9008e-02],\n",
       "           [-8.5263e-02,  7.4211e-02, -1.7828e-02],\n",
       "           [-3.9896e-02, -9.0597e-02,  5.6784e-02]],\n",
       " \n",
       "          [[ 1.5422e-02, -2.6707e-02,  4.2987e-02],\n",
       "           [ 8.0566e-02,  6.1646e-02, -4.2621e-02],\n",
       "           [-4.8070e-03, -3.6476e-02, -2.6179e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 6.7115e-02, -4.7935e-02,  6.3907e-02],\n",
       "           [ 9.2522e-02,  9.2025e-02,  5.7704e-02],\n",
       "           [-7.8439e-03,  8.5366e-02,  9.1897e-02]],\n",
       " \n",
       "          [[-3.9897e-02, -3.3313e-02,  5.7170e-02],\n",
       "           [ 4.7031e-02,  6.0466e-02,  5.2821e-02],\n",
       "           [-5.8070e-02, -5.5911e-02, -7.2661e-02]],\n",
       " \n",
       "          [[-5.9516e-02,  3.8848e-02,  1.7563e-02],\n",
       "           [-3.1229e-02,  5.9037e-02,  4.1653e-02],\n",
       "           [-5.6168e-02,  7.3117e-02, -8.4985e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.6676e-03,  3.8301e-02,  8.0859e-02],\n",
       "           [-4.4131e-02, -5.2573e-02, -9.7383e-02],\n",
       "           [-7.1801e-02, -4.8217e-02, -2.7579e-02]],\n",
       " \n",
       "          [[-9.2187e-02, -2.9320e-02, -6.4818e-02],\n",
       "           [-1.3647e-02, -1.9500e-02,  8.3873e-03],\n",
       "           [-4.1475e-02,  2.2783e-02,  9.9772e-02]],\n",
       " \n",
       "          [[-2.5818e-02, -6.1879e-02, -2.1460e-02],\n",
       "           [ 5.6676e-02,  7.7172e-02, -4.4362e-02],\n",
       "           [ 3.4048e-02,  4.5650e-02,  6.1847e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-8.7530e-02, -8.9685e-02,  6.1771e-03],\n",
       "           [ 6.5225e-02,  9.6682e-02, -5.8146e-02],\n",
       "           [-6.3527e-05,  7.1921e-02,  9.2055e-02]],\n",
       " \n",
       "          [[-1.8184e-03, -7.1245e-02,  2.9431e-02],\n",
       "           [ 5.8899e-02, -4.5925e-02, -7.7455e-02],\n",
       "           [ 9.7059e-02,  2.3618e-02, -6.5647e-02]],\n",
       " \n",
       "          [[-4.7873e-02, -5.1207e-02,  6.8227e-02],\n",
       "           [ 1.0021e-01, -9.3839e-02,  3.2627e-02],\n",
       "           [ 2.5535e-03, -7.9053e-03,  3.5255e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.3751e-02,  5.2702e-02,  9.5215e-02],\n",
       "           [ 1.0366e-02, -1.7493e-02, -8.2366e-02],\n",
       "           [ 7.4448e-02, -7.0568e-02, -7.4838e-02]],\n",
       " \n",
       "          [[-2.6885e-02,  8.4368e-02, -2.5643e-02],\n",
       "           [-9.9412e-02,  7.2433e-02, -3.8417e-02],\n",
       "           [ 3.7166e-02, -5.1868e-02, -9.3603e-02]],\n",
       " \n",
       "          [[-6.2792e-02, -5.9253e-02, -7.9272e-02],\n",
       "           [-7.7499e-02, -9.1411e-02, -1.2506e-02],\n",
       "           [ 2.1484e-02, -7.7579e-02,  2.1324e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.3157e-02, -1.5213e-02,  5.8259e-02],\n",
       "           [ 1.0542e-01,  5.6660e-02, -7.3500e-02],\n",
       "           [ 3.2743e-02, -6.4410e-04,  4.4373e-02]],\n",
       " \n",
       "          [[-7.4654e-02,  9.8469e-02,  1.0389e-01],\n",
       "           [-3.6754e-02, -4.4285e-02,  7.7036e-02],\n",
       "           [ 9.1785e-02,  6.9707e-02,  6.5581e-02]],\n",
       " \n",
       "          [[-9.1807e-02,  5.0067e-02,  5.0839e-03],\n",
       "           [-7.5597e-02,  2.6834e-02, -7.6972e-02],\n",
       "           [-6.9621e-02, -5.6203e-02, -7.9741e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.8646e-02,  3.3364e-02, -1.8338e-03],\n",
       "           [-4.5991e-02,  6.5480e-02, -9.0995e-03],\n",
       "           [ 3.4265e-02, -2.6951e-02, -5.9921e-02]],\n",
       " \n",
       "          [[-9.6306e-02,  5.0985e-04,  7.9147e-02],\n",
       "           [-5.1029e-02,  8.0589e-02,  3.0560e-03],\n",
       "           [ 7.5441e-03, -3.1992e-02,  9.9390e-02]],\n",
       " \n",
       "          [[ 8.8676e-02, -4.1586e-02,  6.2362e-02],\n",
       "           [-3.8977e-02,  5.2655e-02,  8.8111e-02],\n",
       "           [ 7.9408e-02,  5.4279e-02, -3.4047e-03]]],\n",
       " \n",
       " \n",
       "         [[[-8.4449e-02, -8.1628e-02, -1.1939e-03],\n",
       "           [-7.3358e-02, -7.4481e-02,  9.9025e-03],\n",
       "           [ 3.0919e-02,  6.8269e-04,  7.7426e-02]],\n",
       " \n",
       "          [[ 5.2951e-02, -9.4957e-02,  9.9500e-03],\n",
       "           [-7.8490e-02, -3.9062e-02, -8.1130e-02],\n",
       "           [-1.8813e-02,  1.8985e-02, -2.2812e-02]],\n",
       " \n",
       "          [[ 5.5363e-02, -9.0396e-02, -3.8657e-02],\n",
       "           [ 8.9237e-02, -9.9412e-02,  5.2548e-03],\n",
       "           [-3.2639e-02, -3.2150e-02,  7.3497e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.0490e-03, -2.3747e-02,  5.8681e-02],\n",
       "           [-3.1602e-02,  5.7551e-02, -5.5329e-02],\n",
       "           [-4.0355e-02, -4.7725e-02,  8.8630e-02]],\n",
       " \n",
       "          [[-5.1444e-02, -6.4629e-02, -4.7848e-02],\n",
       "           [ 1.8816e-02, -5.9040e-03,  9.6782e-02],\n",
       "           [ 7.2345e-02, -3.8876e-02,  6.8742e-02]],\n",
       " \n",
       "          [[-3.0342e-02, -9.4876e-02,  4.3257e-02],\n",
       "           [-5.6013e-02, -1.7534e-02,  9.7478e-02],\n",
       "           [ 9.3880e-02,  5.1448e-02,  3.9457e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010, -0.0010,\n",
       "          0.0010,  0.0010,  0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,\n",
       "          0.0010,  0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,\n",
       "          0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,  0.0010],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1607, -0.1702,  0.0482, -0.0502,  0.1170,  0.1704,  0.1738,  0.1234,\n",
       "           0.0297,  0.0807,  0.0730,  0.1330,  0.0392,  0.0630,  0.0083, -0.0038,\n",
       "          -0.1553, -0.0699, -0.1508, -0.0710,  0.0745,  0.1405, -0.1480,  0.0259,\n",
       "           0.1096,  0.0516, -0.0808, -0.0014, -0.0402,  0.1346,  0.1531, -0.1756],\n",
       "         [ 0.1526, -0.0769,  0.1545,  0.0831, -0.0867,  0.1411, -0.0859,  0.1134,\n",
       "          -0.0043,  0.0052, -0.1514,  0.1092,  0.0661,  0.0408, -0.0281, -0.0157,\n",
       "           0.0439,  0.0159, -0.0375,  0.0377,  0.0143,  0.1069, -0.0689, -0.0609,\n",
       "          -0.0389, -0.1569, -0.1717,  0.0825, -0.0316,  0.1174,  0.1273,  0.0351],\n",
       "         [ 0.0302,  0.0189, -0.0392,  0.0912, -0.0451, -0.1495, -0.1331, -0.1639,\n",
       "           0.0546,  0.0299,  0.0545,  0.0533,  0.1567,  0.0567,  0.1380, -0.0467,\n",
       "           0.0280,  0.0092, -0.1536,  0.0825,  0.1636, -0.0972, -0.1546,  0.1368,\n",
       "           0.1401,  0.0574, -0.1750, -0.0469, -0.0716,  0.0004,  0.0687, -0.1313],\n",
       "         [-0.0208,  0.1660, -0.1226, -0.0971,  0.0058,  0.1049,  0.1075,  0.1193,\n",
       "          -0.1366,  0.0090,  0.0027, -0.1110, -0.0210, -0.0102,  0.0249, -0.0752,\n",
       "           0.0178, -0.0649,  0.1622, -0.0310,  0.0948, -0.0866,  0.0817,  0.0055,\n",
       "          -0.0769, -0.0798,  0.0224,  0.1465,  0.0893, -0.1206, -0.0649, -0.1265],\n",
       "         [-0.0468,  0.0607, -0.0020,  0.1178, -0.0199, -0.1038, -0.1465, -0.1164,\n",
       "           0.1499, -0.1192, -0.0971,  0.0968, -0.0514, -0.0528, -0.1635,  0.0926,\n",
       "          -0.0098, -0.0889, -0.0497,  0.1175, -0.0447, -0.1674, -0.1278, -0.0499,\n",
       "          -0.0798, -0.0562, -0.0248,  0.0255, -0.0358, -0.1008,  0.1502, -0.0219],\n",
       "         [ 0.1583, -0.1273,  0.0456, -0.0769,  0.0194,  0.0151,  0.0712, -0.0026,\n",
       "          -0.0899, -0.1460, -0.1171,  0.0558,  0.1444,  0.0925, -0.0144, -0.0127,\n",
       "           0.1526, -0.0477,  0.0173,  0.0625,  0.1189,  0.0759,  0.0309,  0.1629,\n",
       "          -0.1385, -0.0374,  0.0171,  0.1595, -0.1120,  0.1512, -0.0533,  0.1045],\n",
       "         [ 0.1047, -0.0128, -0.0270, -0.0027, -0.0048, -0.0352,  0.0278,  0.1151,\n",
       "          -0.0869, -0.1120,  0.0971, -0.0726, -0.1752,  0.0244,  0.0766,  0.0625,\n",
       "          -0.0946, -0.0516,  0.1676,  0.0080,  0.0296,  0.0657,  0.0618,  0.0003,\n",
       "          -0.0736, -0.1333,  0.1693,  0.0609,  0.0826, -0.0917,  0.0342,  0.0574],\n",
       "         [-0.0835,  0.1432, -0.1391,  0.0924,  0.1282, -0.0592,  0.0249,  0.0015,\n",
       "           0.1576,  0.0032, -0.1181,  0.1004,  0.0176, -0.0586, -0.0342, -0.0907,\n",
       "          -0.0077,  0.1036, -0.0601,  0.0725,  0.1681, -0.1313, -0.0588, -0.0238,\n",
       "          -0.1367,  0.1755,  0.0028,  0.0086, -0.1307,  0.1373,  0.0831, -0.1067],\n",
       "         [ 0.1105,  0.1445,  0.1345,  0.0280,  0.1203, -0.0034, -0.1202, -0.0560,\n",
       "          -0.1582, -0.0314, -0.1125,  0.0762, -0.0939,  0.1186,  0.0141, -0.0241,\n",
       "           0.0550, -0.0424,  0.0598, -0.1221, -0.1323, -0.0102, -0.0117,  0.0062,\n",
       "           0.0884, -0.0396, -0.0070, -0.0947,  0.0643, -0.0076,  0.1282,  0.0410],\n",
       "         [-0.0861, -0.0394,  0.0899, -0.1277, -0.0869, -0.0550, -0.1742,  0.1285,\n",
       "          -0.0266, -0.0224, -0.0054,  0.0944,  0.1006,  0.0629,  0.0816, -0.0502,\n",
       "          -0.0971, -0.1424,  0.1416, -0.0874,  0.1217,  0.0165,  0.1246, -0.1297,\n",
       "           0.1088,  0.1366,  0.0589, -0.0862,  0.1694, -0.1577, -0.0145,  0.0514]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,\n",
       "         -0.0010, -0.0010], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynet.tree.optimizer.param_groups[0]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[ 1.2233e-01, -1.0797e-01, -1.1560e-01],\n",
       "           [ 1.0054e-01,  1.6252e-01,  2.8502e-01],\n",
       "           [ 1.9072e-01, -2.9728e-01, -8.9353e-02]],\n",
       " \n",
       "          [[-1.9856e-01, -9.1499e-02, -1.7031e-01],\n",
       "           [-3.7699e-02, -2.2428e-01, -2.5555e-02],\n",
       "           [-2.4178e-01,  3.3122e-02,  2.2636e-01]],\n",
       " \n",
       "          [[ 2.3545e-01,  3.0003e-01, -2.1340e-01],\n",
       "           [-1.1353e-01,  2.3427e-01,  1.7870e-01],\n",
       "           [ 1.4887e-01,  2.6009e-01,  2.8229e-01]]],\n",
       " \n",
       " \n",
       "         [[[-7.5914e-02,  1.8779e-01, -5.8983e-02],\n",
       "           [-2.5899e-02, -6.1926e-02, -2.4166e-01],\n",
       "           [ 3.4330e-01,  1.1774e-01, -1.0435e-01]],\n",
       " \n",
       "          [[-3.3113e-01,  3.2410e-01,  1.9273e-01],\n",
       "           [-1.2584e-01, -2.5498e-01, -3.2613e-01],\n",
       "           [-1.4636e-01,  3.1716e-01, -3.9758e-02]],\n",
       " \n",
       "          [[-2.1444e-01, -3.3370e-04, -6.2067e-02],\n",
       "           [ 1.7349e-01, -1.9738e-01, -6.7500e-02],\n",
       "           [-1.4169e-01, -1.8716e-01, -1.2045e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.6432e-01,  6.5764e-02, -1.0571e-01],\n",
       "           [ 3.0477e-01,  2.7013e-01,  1.9134e-01],\n",
       "           [ 1.9773e-01,  1.4202e-01, -2.4380e-01]],\n",
       " \n",
       "          [[-2.8068e-01, -2.8282e-01,  2.2601e-01],\n",
       "           [ 2.1182e-01,  2.8515e-01, -1.6137e-01],\n",
       "           [-1.3436e-01,  1.3971e-02, -5.9268e-02]],\n",
       " \n",
       "          [[-2.7546e-01, -2.6605e-01,  1.0383e-02],\n",
       "           [-1.8733e-01, -1.9085e-01,  1.4405e-01],\n",
       "           [-5.9748e-02, -7.5328e-03,  2.9056e-02]]],\n",
       " \n",
       " \n",
       "         [[[-2.6458e-01,  3.1176e-01,  1.4254e-01],\n",
       "           [-2.2056e-01,  1.1775e-01,  5.1721e-03],\n",
       "           [-2.5706e-01,  8.7064e-02,  3.2318e-01]],\n",
       " \n",
       "          [[ 1.0737e-01,  5.7737e-02,  1.3161e-01],\n",
       "           [ 8.4253e-02,  2.0309e-01,  1.0775e-01],\n",
       "           [-2.2939e-01,  1.7117e-01, -2.2541e-01]],\n",
       " \n",
       "          [[-2.0114e-01,  1.0961e-01, -4.1199e-01],\n",
       "           [ 9.5966e-02, -2.1471e-03, -5.6395e-02],\n",
       "           [ 2.4724e-01, -1.9688e-01, -6.0399e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0826e-01,  9.8270e-02,  2.2055e-01],\n",
       "           [-3.1190e-01,  3.0177e-02,  2.7127e-01],\n",
       "           [-1.8063e-01,  3.1182e-01, -1.4948e-01]],\n",
       " \n",
       "          [[ 5.8237e-02, -2.6585e-01, -1.2489e-01],\n",
       "           [ 1.8842e-01, -1.0359e-01, -1.4133e-01],\n",
       "           [-1.3789e-01,  1.8560e-01, -2.5083e-01]],\n",
       " \n",
       "          [[-2.2624e-01, -1.5114e-01,  9.9086e-02],\n",
       "           [ 2.8583e-01,  6.9618e-02, -2.6597e-01],\n",
       "           [ 1.5515e-01,  1.1196e-01, -2.1585e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.5663e-01, -3.1774e-01, -3.7449e-02],\n",
       "           [-2.2345e-01,  7.9603e-02,  2.1506e-01],\n",
       "           [-2.6364e-01, -6.7569e-02, -1.1443e-01]],\n",
       " \n",
       "          [[ 1.2584e-01, -2.3501e-03,  2.3127e-01],\n",
       "           [ 3.2704e-01, -8.6569e-02,  7.5210e-02],\n",
       "           [-2.7844e-02, -1.0214e-01, -1.4932e-01]],\n",
       " \n",
       "          [[ 8.9557e-02,  3.0225e-01, -1.9086e-01],\n",
       "           [ 3.0879e-01,  3.6099e-02, -1.0697e-01],\n",
       "           [ 1.6451e-01, -2.7746e-01, -2.4977e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.1526e-01,  1.1080e-02,  1.7361e-02],\n",
       "           [ 3.6307e-02,  2.8767e-01,  2.8404e-01],\n",
       "           [ 2.8595e-01, -2.8179e-01,  8.3770e-02]],\n",
       " \n",
       "          [[-2.6417e-01,  2.7411e-01, -3.1225e-01],\n",
       "           [ 5.5032e-02,  5.5944e-02, -2.6682e-01],\n",
       "           [-4.5083e-02,  1.4721e-01,  2.5143e-02]],\n",
       " \n",
       "          [[-2.1594e-01, -2.9799e-01, -3.1916e-02],\n",
       "           [-4.9346e-02,  1.1174e-01, -5.0286e-02],\n",
       "           [ 2.2442e-01,  1.1262e-01, -2.2526e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.3656e-01,  1.1999e-01, -6.8009e-02],\n",
       "           [ 2.8413e-01,  1.3740e-01,  1.7116e-01],\n",
       "           [ 1.9399e-01, -8.2427e-02, -6.4057e-02]],\n",
       " \n",
       "          [[-2.2379e-01,  9.7441e-02, -2.0585e-01],\n",
       "           [ 3.9243e-02,  2.0372e-01, -1.9928e-01],\n",
       "           [-1.8606e-01, -1.8294e-01, -1.8116e-01]],\n",
       " \n",
       "          [[ 2.5686e-01,  1.4978e-01,  2.5665e-01],\n",
       "           [ 2.6907e-01,  2.5505e-01, -2.4254e-01],\n",
       "           [ 2.5893e-01,  1.8229e-01, -3.9904e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-4.4501e-07, -2.2539e-06,  1.2606e-06,  1.2836e-06, -1.3354e-06,\n",
       "          3.4596e-06, -2.4821e-06,  1.0208e-06], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-0.1094,  0.0477, -0.1304],\n",
       "           [ 0.1377,  0.1671, -0.0014],\n",
       "           [-0.0254, -0.1369, -0.1325]],\n",
       " \n",
       "          [[-0.1455, -0.0324, -0.1950],\n",
       "           [-0.0974, -0.0884, -0.1209],\n",
       "           [ 0.1504, -0.1873,  0.1776]],\n",
       " \n",
       "          [[ 0.0538, -0.0136, -0.1651],\n",
       "           [ 0.0709, -0.1367,  0.0947],\n",
       "           [ 0.1198, -0.0600, -0.1717]],\n",
       " \n",
       "          [[-0.1896, -0.0310, -0.0839],\n",
       "           [-0.0520, -0.0387,  0.1845],\n",
       "           [-0.0211,  0.0655,  0.0538]],\n",
       " \n",
       "          [[-0.0209,  0.0888, -0.1742],\n",
       "           [-0.1534, -0.1318,  0.0126],\n",
       "           [ 0.1834, -0.1970,  0.0903]],\n",
       " \n",
       "          [[-0.1570,  0.0254,  0.0743],\n",
       "           [-0.0194,  0.0372, -0.1622],\n",
       "           [ 0.1763, -0.0384, -0.1877]],\n",
       " \n",
       "          [[-0.1325, -0.1633,  0.0614],\n",
       "           [-0.0270,  0.0134,  0.0649],\n",
       "           [-0.0902,  0.1211, -0.0858]],\n",
       " \n",
       "          [[-0.0902, -0.1638, -0.0359],\n",
       "           [-0.0376, -0.1899,  0.0580],\n",
       "           [-0.0680,  0.0136,  0.1916]]],\n",
       " \n",
       " \n",
       "         [[[-0.1678,  0.0647,  0.1790],\n",
       "           [-0.0774,  0.1681,  0.1820],\n",
       "           [-0.0945, -0.1816,  0.1511]],\n",
       " \n",
       "          [[ 0.0404,  0.0036,  0.1706],\n",
       "           [-0.0140,  0.0586, -0.0987],\n",
       "           [-0.0862, -0.0816, -0.0122]],\n",
       " \n",
       "          [[ 0.1010,  0.0871,  0.0713],\n",
       "           [-0.0147,  0.1403, -0.0540],\n",
       "           [ 0.0641,  0.0940,  0.0933]],\n",
       " \n",
       "          [[ 0.1813, -0.1705,  0.1514],\n",
       "           [ 0.0643,  0.0492, -0.0042],\n",
       "           [ 0.0686,  0.0742,  0.0998]],\n",
       " \n",
       "          [[ 0.1424, -0.0487, -0.1640],\n",
       "           [-0.1664, -0.0377, -0.1758],\n",
       "           [ 0.0383, -0.0249, -0.1156]],\n",
       " \n",
       "          [[ 0.1309,  0.0653,  0.1421],\n",
       "           [ 0.0978,  0.0914, -0.1185],\n",
       "           [-0.0486,  0.0447, -0.0913]],\n",
       " \n",
       "          [[ 0.1836, -0.0687, -0.1711],\n",
       "           [ 0.0012, -0.1759,  0.1909],\n",
       "           [-0.1920, -0.1496, -0.1659]],\n",
       " \n",
       "          [[ 0.0951,  0.1370, -0.1494],\n",
       "           [ 0.0553, -0.1865, -0.0447],\n",
       "           [-0.1457, -0.0763,  0.0353]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1743,  0.1898,  0.0799],\n",
       "           [ 0.0154, -0.0028,  0.0088],\n",
       "           [ 0.0963,  0.1812,  0.0225]],\n",
       " \n",
       "          [[-0.1981, -0.0059, -0.0647],\n",
       "           [ 0.0162,  0.1008, -0.0751],\n",
       "           [ 0.1629, -0.0597,  0.1525]],\n",
       " \n",
       "          [[ 0.1824, -0.1354, -0.1851],\n",
       "           [-0.0878,  0.1976,  0.2052],\n",
       "           [-0.0418, -0.0005, -0.0128]],\n",
       " \n",
       "          [[ 0.0779,  0.0663, -0.0949],\n",
       "           [-0.1563, -0.0197,  0.1193],\n",
       "           [-0.1801,  0.0061, -0.1574]],\n",
       " \n",
       "          [[-0.0085,  0.0704,  0.0216],\n",
       "           [-0.0662,  0.0935, -0.0955],\n",
       "           [-0.1344,  0.0940, -0.0745]],\n",
       " \n",
       "          [[-0.0829, -0.1446, -0.1520],\n",
       "           [ 0.1606,  0.1886, -0.1921],\n",
       "           [-0.1417, -0.1395, -0.0154]],\n",
       " \n",
       "          [[ 0.0366,  0.0977,  0.1137],\n",
       "           [ 0.1406,  0.0559,  0.1265],\n",
       "           [-0.1545,  0.0132, -0.1726]],\n",
       " \n",
       "          [[ 0.0242,  0.1372,  0.0824],\n",
       "           [ 0.1873, -0.1073, -0.0233],\n",
       "           [-0.0660,  0.0941, -0.1118]]],\n",
       " \n",
       " \n",
       "         [[[-0.0036, -0.1321,  0.0265],\n",
       "           [ 0.0044, -0.1235,  0.0415],\n",
       "           [ 0.1807, -0.0853,  0.0422]],\n",
       " \n",
       "          [[-0.1585,  0.0471, -0.0008],\n",
       "           [-0.1157,  0.0630,  0.1464],\n",
       "           [ 0.0027, -0.0325, -0.0018]],\n",
       " \n",
       "          [[-0.1237,  0.1810, -0.0225],\n",
       "           [ 0.0465,  0.1673, -0.0849],\n",
       "           [ 0.0319, -0.0432,  0.1436]],\n",
       " \n",
       "          [[-0.1010,  0.0839, -0.1031],\n",
       "           [-0.1752,  0.1855, -0.1735],\n",
       "           [-0.1159,  0.1508, -0.1392]],\n",
       " \n",
       "          [[ 0.1759,  0.0099,  0.0775],\n",
       "           [ 0.1623, -0.0177,  0.1654],\n",
       "           [ 0.1797,  0.0326,  0.1668]],\n",
       " \n",
       "          [[ 0.1376, -0.1365,  0.1781],\n",
       "           [ 0.0873,  0.0738,  0.0113],\n",
       "           [-0.1016, -0.1338,  0.0131]],\n",
       " \n",
       "          [[-0.0338,  0.1475,  0.0991],\n",
       "           [-0.1416,  0.1888, -0.1740],\n",
       "           [ 0.1841, -0.0691,  0.1075]],\n",
       " \n",
       "          [[ 0.1892,  0.1213,  0.0431],\n",
       "           [-0.1201,  0.0330,  0.0900],\n",
       "           [ 0.1286,  0.1298,  0.1095]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0267,  0.1664,  0.1530],\n",
       "           [-0.1235, -0.1281,  0.0966],\n",
       "           [ 0.0599, -0.0394,  0.1763]],\n",
       " \n",
       "          [[ 0.1255,  0.0995,  0.1309],\n",
       "           [-0.1268,  0.1339,  0.1813],\n",
       "           [ 0.0854,  0.0343,  0.2017]],\n",
       " \n",
       "          [[-0.1079,  0.0040, -0.0453],\n",
       "           [-0.0908, -0.1642, -0.1346],\n",
       "           [-0.1486, -0.1078,  0.1396]],\n",
       " \n",
       "          [[ 0.1421,  0.1042, -0.0906],\n",
       "           [-0.1548,  0.1357,  0.0586],\n",
       "           [-0.0083, -0.1568, -0.1827]],\n",
       " \n",
       "          [[ 0.0674, -0.0513, -0.2000],\n",
       "           [-0.0864,  0.0226, -0.0136],\n",
       "           [ 0.0961,  0.0412,  0.0643]],\n",
       " \n",
       "          [[-0.0603,  0.1057, -0.0645],\n",
       "           [-0.0176, -0.1180, -0.0258],\n",
       "           [-0.0915, -0.2009,  0.1365]],\n",
       " \n",
       "          [[ 0.1358,  0.1480,  0.1600],\n",
       "           [-0.1937,  0.0829,  0.0082],\n",
       "           [ 0.0945, -0.1708,  0.0654]],\n",
       " \n",
       "          [[-0.0108,  0.0461,  0.0056],\n",
       "           [-0.0286, -0.1307, -0.1217],\n",
       "           [ 0.1641, -0.1778,  0.1271]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1752, -0.0827,  0.0356],\n",
       "           [-0.1209,  0.1683, -0.1464],\n",
       "           [ 0.1614,  0.1723,  0.0874]],\n",
       " \n",
       "          [[ 0.0489, -0.0017, -0.1709],\n",
       "           [-0.0718, -0.1718,  0.1481],\n",
       "           [-0.1876,  0.0939, -0.1043]],\n",
       " \n",
       "          [[ 0.1679,  0.0586, -0.0559],\n",
       "           [-0.1175,  0.0408, -0.0156],\n",
       "           [ 0.1581, -0.0078, -0.1821]],\n",
       " \n",
       "          [[ 0.0811,  0.0750, -0.1293],\n",
       "           [-0.0395,  0.1261,  0.1136],\n",
       "           [-0.1624, -0.0150,  0.0044]],\n",
       " \n",
       "          [[ 0.0771,  0.0844,  0.0623],\n",
       "           [ 0.1160, -0.0954, -0.0934],\n",
       "           [-0.1466, -0.1425, -0.1862]],\n",
       " \n",
       "          [[ 0.0397,  0.1634,  0.0310],\n",
       "           [-0.0022,  0.1677,  0.1544],\n",
       "           [ 0.0881, -0.0830, -0.0285]],\n",
       " \n",
       "          [[ 0.1621, -0.0746, -0.0057],\n",
       "           [-0.1688,  0.1034,  0.0552],\n",
       "           [-0.1641, -0.1067,  0.1381]],\n",
       " \n",
       "          [[ 0.1746, -0.0256, -0.0625],\n",
       "           [-0.0487,  0.0952, -0.1608],\n",
       "           [ 0.1820, -0.0538, -0.1415]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0093, -0.0098,  0.1415],\n",
       "           [ 0.1101,  0.1287, -0.1625],\n",
       "           [-0.0975,  0.1740, -0.0562]],\n",
       " \n",
       "          [[ 0.0031,  0.0465,  0.1671],\n",
       "           [-0.0045, -0.1272, -0.1607],\n",
       "           [ 0.1036,  0.1494,  0.0554]],\n",
       " \n",
       "          [[-0.0196,  0.0380, -0.1718],\n",
       "           [ 0.1197, -0.1396, -0.0888],\n",
       "           [-0.0367,  0.1168, -0.0290]],\n",
       " \n",
       "          [[-0.1760, -0.1106, -0.1435],\n",
       "           [-0.1507, -0.1566,  0.1508],\n",
       "           [ 0.1697,  0.0256,  0.1545]],\n",
       " \n",
       "          [[ 0.0868,  0.0998,  0.0896],\n",
       "           [ 0.0542,  0.1683, -0.0545],\n",
       "           [ 0.1126, -0.1002,  0.1612]],\n",
       " \n",
       "          [[ 0.1086,  0.1532, -0.0571],\n",
       "           [ 0.0475, -0.0071,  0.0755],\n",
       "           [ 0.1538,  0.1554,  0.0698]],\n",
       " \n",
       "          [[ 0.1461,  0.1382,  0.0343],\n",
       "           [ 0.0541, -0.1413, -0.1505],\n",
       "           [ 0.1323,  0.0929, -0.1131]],\n",
       " \n",
       "          [[ 0.1624, -0.1414,  0.1358],\n",
       "           [-0.1148, -0.0329, -0.1198],\n",
       "           [-0.1768,  0.0267, -0.1531]]],\n",
       " \n",
       " \n",
       "         [[[-0.1507,  0.1945, -0.0061],\n",
       "           [-0.1223,  0.1440,  0.1953],\n",
       "           [-0.0032, -0.0467,  0.1843]],\n",
       " \n",
       "          [[ 0.0642, -0.1290,  0.0217],\n",
       "           [ 0.1416,  0.1946,  0.1177],\n",
       "           [ 0.1501, -0.0710,  0.1466]],\n",
       " \n",
       "          [[ 0.1830, -0.1158, -0.1279],\n",
       "           [ 0.1057, -0.1152,  0.0736],\n",
       "           [-0.1130,  0.0702, -0.1538]],\n",
       " \n",
       "          [[ 0.0254,  0.1707, -0.0514],\n",
       "           [-0.0969, -0.1010, -0.0157],\n",
       "           [-0.0748,  0.1804, -0.0579]],\n",
       " \n",
       "          [[-0.0941, -0.1464, -0.0625],\n",
       "           [-0.0474,  0.1434, -0.1921],\n",
       "           [-0.1478,  0.1448, -0.1495]],\n",
       " \n",
       "          [[-0.1499, -0.1790,  0.0382],\n",
       "           [-0.0277,  0.1208,  0.0007],\n",
       "           [ 0.1220, -0.0990,  0.0858]],\n",
       " \n",
       "          [[-0.0597,  0.0992, -0.1443],\n",
       "           [ 0.0856, -0.0775,  0.1801],\n",
       "           [-0.0448, -0.1107, -0.0328]],\n",
       " \n",
       "          [[ 0.0582, -0.0410,  0.0748],\n",
       "           [ 0.1372,  0.0717, -0.0766],\n",
       "           [-0.1841,  0.0627,  0.0904]]]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 4.8866e-07,  1.1841e-06, -6.2245e-07,  1.8964e-07, -4.6248e-06,\n",
       "          1.2891e-06, -4.1887e-06, -2.1644e-06], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 1.0635e-01,  1.9060e-01, -7.8454e-02],\n",
       "           [-2.3215e-02, -3.6200e-02, -1.8921e-02],\n",
       "           [-4.5146e-03, -1.1549e-01,  1.7637e-01]],\n",
       " \n",
       "          [[-1.8185e-02,  2.0165e-01,  8.3150e-02],\n",
       "           [ 1.5513e-02, -1.0642e-01,  1.9328e-01],\n",
       "           [ 4.9810e-02,  1.7587e-01,  3.7231e-02]],\n",
       " \n",
       "          [[ 9.5331e-02, -1.0814e-01, -1.5141e-01],\n",
       "           [-1.6524e-01, -1.4073e-01,  3.3038e-02],\n",
       "           [ 3.7056e-02,  3.2101e-02,  3.6298e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.7013e-01, -5.3636e-03,  1.8894e-01],\n",
       "           [ 9.1932e-02,  6.0471e-02,  6.0471e-02],\n",
       "           [-1.2386e-01,  2.0363e-01, -2.2394e-02]],\n",
       " \n",
       "          [[-3.8919e-03, -1.1256e-01, -1.5077e-01],\n",
       "           [-4.6223e-02,  1.9148e-01,  1.3509e-01],\n",
       "           [-1.8662e-01,  1.5472e-01,  5.8691e-02]],\n",
       " \n",
       "          [[ 1.2529e-01,  7.4358e-05,  1.7410e-01],\n",
       "           [ 5.9954e-02,  1.0846e-01,  8.5738e-02],\n",
       "           [ 1.6165e-01,  1.5575e-01,  1.6251e-02]]],\n",
       " \n",
       " \n",
       "         [[[-5.3034e-04,  1.4990e-01, -1.4546e-01],\n",
       "           [-1.2899e-01, -1.0532e-01,  9.5060e-02],\n",
       "           [-7.1052e-02, -1.6902e-01,  1.4123e-01]],\n",
       " \n",
       "          [[ 4.2795e-02, -8.2319e-02,  1.2209e-01],\n",
       "           [ 1.1553e-01, -1.4206e-01, -1.8191e-01],\n",
       "           [ 2.6162e-02,  8.9395e-02, -5.4940e-02]],\n",
       " \n",
       "          [[-3.0834e-02,  6.8771e-02,  1.7198e-01],\n",
       "           [-1.0759e-01,  1.3363e-01,  1.2277e-01],\n",
       "           [-1.3228e-01, -1.7876e-01,  4.5876e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 9.5875e-02, -1.7292e-01, -1.7384e-01],\n",
       "           [-5.7801e-02, -6.9255e-02, -8.0834e-02],\n",
       "           [ 8.6357e-02, -7.0235e-02, -7.3633e-02]],\n",
       " \n",
       "          [[-1.4221e-01,  1.0876e-01,  3.5759e-02],\n",
       "           [ 1.8346e-01, -9.2328e-02,  3.1995e-02],\n",
       "           [-8.2773e-02, -6.7382e-02, -1.8233e-01]],\n",
       " \n",
       "          [[-8.9682e-02,  8.8345e-03, -5.3892e-02],\n",
       "           [ 8.9291e-02,  5.3500e-02, -1.3750e-01],\n",
       "           [ 1.3097e-01,  3.9502e-02, -1.8025e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.8069e-01,  8.4294e-02, -1.7466e-01],\n",
       "           [ 1.5447e-01, -6.6029e-02, -1.5819e-01],\n",
       "           [ 1.3377e-01,  1.9408e-02,  6.1315e-02]],\n",
       " \n",
       "          [[ 1.2253e-01,  2.8684e-02, -1.5054e-01],\n",
       "           [-1.5801e-01,  1.3146e-01,  1.7881e-01],\n",
       "           [-8.8034e-02, -1.5216e-01,  1.1855e-01]],\n",
       " \n",
       "          [[-1.2727e-01,  1.2085e-01, -1.7862e-01],\n",
       "           [-1.8179e-01, -1.1328e-03, -1.6249e-01],\n",
       "           [-1.8240e-01, -1.0790e-01, -1.4543e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-9.5802e-02, -1.0505e-01, -1.0717e-01],\n",
       "           [ 3.4889e-02, -1.5630e-01, -2.9784e-02],\n",
       "           [ 1.8235e-01, -1.8732e-01,  3.3444e-02]],\n",
       " \n",
       "          [[ 9.3966e-02,  1.0343e-01, -3.2535e-02],\n",
       "           [-1.3194e-01,  1.7779e-01, -1.0850e-01],\n",
       "           [ 1.0270e-02,  1.7786e-01, -1.1475e-01]],\n",
       " \n",
       "          [[ 1.7757e-01, -6.0830e-02, -1.0427e-01],\n",
       "           [ 1.4987e-01,  5.7600e-02,  8.5931e-02],\n",
       "           [ 6.7867e-02,  1.1781e-01, -2.9943e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.5834e-01, -1.9235e-01,  5.4715e-02],\n",
       "           [-6.4879e-02, -1.4939e-01, -1.5783e-01],\n",
       "           [-2.1378e-01, -1.9395e-01, -1.6190e-01]],\n",
       " \n",
       "          [[ 1.1175e-01,  5.0419e-02,  4.6070e-02],\n",
       "           [ 1.1403e-01, -8.5888e-02,  8.9724e-02],\n",
       "           [-1.9488e-01,  1.1339e-01,  1.1628e-02]],\n",
       " \n",
       "          [[-1.0994e-02,  2.3260e-02, -2.9554e-02],\n",
       "           [ 8.6178e-02, -1.3086e-01,  1.6836e-01],\n",
       "           [ 1.4781e-01,  1.4485e-01, -1.0650e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0571e-01, -5.2068e-02, -9.3555e-02],\n",
       "           [ 5.2385e-02, -4.2563e-03, -8.3273e-02],\n",
       "           [ 1.6074e-01,  1.4541e-01, -5.9887e-02]],\n",
       " \n",
       "          [[ 7.3584e-03, -7.8646e-02, -6.3913e-03],\n",
       "           [-1.8667e-01,  1.3417e-02,  2.0022e-01],\n",
       "           [ 8.9518e-02,  8.6905e-02, -5.3541e-02]],\n",
       " \n",
       "          [[ 9.6323e-04,  1.1291e-01,  1.6788e-01],\n",
       "           [-3.6510e-02, -7.7487e-02,  8.7166e-02],\n",
       "           [ 1.5017e-01, -2.5396e-02,  3.1981e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.5843e-01, -1.6237e-01,  9.8059e-02],\n",
       "           [ 9.0719e-02, -2.5686e-02,  1.7368e-01],\n",
       "           [-6.7550e-02, -1.5044e-01,  4.4057e-02]],\n",
       " \n",
       "          [[-1.5011e-01,  6.4690e-02,  6.1893e-02],\n",
       "           [-1.5164e-01, -9.5009e-02,  1.1989e-01],\n",
       "           [-3.9579e-04, -4.1715e-02, -1.3564e-01]],\n",
       " \n",
       "          [[ 5.1865e-02, -8.0841e-02, -1.6108e-01],\n",
       "           [-1.5143e-01, -6.9575e-02,  1.5320e-01],\n",
       "           [-1.5735e-01,  1.4610e-01,  1.3800e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.6892e-02,  6.8519e-02,  1.2724e-01],\n",
       "           [-1.3321e-01, -8.5373e-02, -4.2546e-02],\n",
       "           [-1.3432e-01,  7.8485e-02, -1.2776e-01]],\n",
       " \n",
       "          [[-8.6186e-02, -1.4207e-01,  1.0763e-01],\n",
       "           [ 1.7905e-01, -7.3547e-02,  1.3974e-01],\n",
       "           [-1.5891e-01, -6.1176e-02,  1.6729e-01]],\n",
       " \n",
       "          [[-1.2317e-01, -7.3587e-02,  1.7744e-01],\n",
       "           [-1.4278e-01, -1.5384e-01,  8.2101e-02],\n",
       "           [ 3.0518e-04, -1.2913e-01, -1.1947e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 7.6622e-02, -1.4633e-01,  9.6578e-02],\n",
       "           [-9.2232e-02,  5.6348e-02,  1.3592e-01],\n",
       "           [ 1.9390e-01,  1.6627e-01, -1.0032e-01]],\n",
       " \n",
       "          [[-5.9257e-02, -1.6760e-01,  8.0818e-02],\n",
       "           [-1.3233e-01,  1.9430e-01, -5.1566e-02],\n",
       "           [ 2.2294e-01,  7.1613e-02, -1.4133e-01]],\n",
       " \n",
       "          [[ 1.2790e-01, -4.0809e-02, -2.0408e-01],\n",
       "           [ 1.4337e-01,  1.1465e-01,  2.7806e-02],\n",
       "           [ 1.7017e-01,  3.6093e-02,  7.6696e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.3152e-03, -5.4402e-02,  3.2257e-02],\n",
       "           [ 1.7898e-02, -1.9865e-01,  1.0244e-01],\n",
       "           [-5.4609e-02, -1.7114e-01, -3.9173e-02]],\n",
       " \n",
       "          [[-2.3136e-02, -8.4636e-02, -2.3358e-02],\n",
       "           [ 8.7430e-02, -1.3486e-01,  3.7457e-02],\n",
       "           [-6.7237e-02,  1.2494e-01, -2.2367e-01]],\n",
       " \n",
       "          [[ 2.1185e-01, -7.1481e-02,  1.0434e-01],\n",
       "           [-5.4778e-02,  9.7401e-02,  1.8615e-01],\n",
       "           [-5.0614e-02, -2.5251e-02,  2.8698e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-4.2020e-06,  9.4549e-07, -3.7896e-06,  8.4918e-08, -9.3607e-07,\n",
       "          7.4030e-07,  4.4473e-06,  1.7969e-06,  1.9541e-06,  5.6487e-06,\n",
       "         -2.0300e-06, -2.7495e-06, -1.9099e-06, -8.9899e-07, -7.2531e-07,\n",
       "         -3.8294e-08], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 2.3320e-02,  5.8278e-02,  2.1913e-02],\n",
       "           [ 1.0780e-01, -1.2621e-01, -1.5261e-01],\n",
       "           [-5.8727e-02, -7.1870e-04,  6.6658e-03]],\n",
       " \n",
       "          [[ 1.1516e-01,  1.1550e-01, -1.1339e-01],\n",
       "           [-5.0120e-02,  1.3261e-01,  1.4528e-01],\n",
       "           [-3.8750e-02, -4.9018e-02, -9.9877e-02]],\n",
       " \n",
       "          [[-1.0855e-01, -3.1459e-02, -2.0521e-02],\n",
       "           [-1.2538e-01, -4.3097e-02, -2.2858e-02],\n",
       "           [-8.6533e-02,  1.1572e-01,  8.1870e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.1430e-01, -1.9040e-02, -1.2409e-01],\n",
       "           [-6.3435e-02,  3.7612e-02, -7.0888e-02],\n",
       "           [ 1.1234e-01,  6.8877e-02, -4.0181e-02]],\n",
       " \n",
       "          [[ 5.3448e-02,  1.0392e-01,  1.0198e-01],\n",
       "           [-1.2121e-02, -1.5430e-02, -7.5842e-02],\n",
       "           [-7.5652e-02, -7.8263e-02, -1.1098e-01]],\n",
       " \n",
       "          [[-9.2729e-02,  3.4844e-02, -2.3835e-02],\n",
       "           [ 6.5880e-02, -1.4028e-01, -1.4094e-01],\n",
       "           [ 1.0595e-01,  9.0912e-02, -3.4854e-03]]],\n",
       " \n",
       " \n",
       "         [[[-8.1880e-04,  5.9362e-02, -1.3318e-01],\n",
       "           [ 5.8124e-02,  1.0412e-01,  1.3306e-01],\n",
       "           [ 3.9635e-03,  9.3215e-02,  7.0479e-02]],\n",
       " \n",
       "          [[ 6.2421e-02,  6.0060e-02,  1.0880e-01],\n",
       "           [-8.5378e-03, -1.2351e-01,  7.2200e-02],\n",
       "           [-4.4593e-02, -6.4109e-02, -4.8819e-03]],\n",
       " \n",
       "          [[-8.8117e-02, -6.0037e-02, -8.6524e-03],\n",
       "           [-2.6853e-02,  1.2220e-01, -6.9315e-02],\n",
       "           [ 1.9544e-02, -1.0688e-01, -1.4169e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.3582e-01, -5.5153e-02,  4.8441e-03],\n",
       "           [-4.3756e-02, -7.5189e-02,  4.9890e-02],\n",
       "           [-4.8356e-02, -3.1583e-03,  9.9293e-02]],\n",
       " \n",
       "          [[-7.2338e-02, -7.4791e-02,  9.8975e-02],\n",
       "           [ 1.3891e-01, -1.2703e-01, -7.9911e-02],\n",
       "           [ 7.1626e-02, -3.2525e-05,  7.8405e-02]],\n",
       " \n",
       "          [[-1.1900e-01, -1.3239e-01, -9.7198e-02],\n",
       "           [ 2.0141e-04,  9.9731e-02, -5.3664e-02],\n",
       "           [-1.1615e-02,  9.7944e-02, -4.9798e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.8457e-02, -3.2829e-02,  4.6692e-02],\n",
       "           [ 2.9558e-02, -4.0326e-02,  6.3537e-02],\n",
       "           [ 3.0698e-02,  1.2447e-01, -1.2183e-01]],\n",
       " \n",
       "          [[-4.1252e-02,  5.7144e-02, -3.3021e-02],\n",
       "           [ 1.2226e-01,  6.6156e-03,  1.1323e-01],\n",
       "           [-3.0162e-02,  1.2324e-01,  9.3607e-02]],\n",
       " \n",
       "          [[ 2.2929e-02,  5.3653e-02,  4.7225e-03],\n",
       "           [-2.6756e-02,  1.2612e-01, -1.1354e-01],\n",
       "           [-1.2811e-01,  9.4408e-02,  5.3685e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.6038e-02,  1.1222e-01,  1.4215e-01],\n",
       "           [-1.2461e-01,  9.0963e-02,  4.3583e-02],\n",
       "           [ 9.8822e-02,  1.2231e-01, -6.6700e-02]],\n",
       " \n",
       "          [[ 3.0040e-02,  1.0379e-01,  1.8812e-02],\n",
       "           [-1.0485e-01,  4.8557e-02,  1.1072e-01],\n",
       "           [ 5.2530e-02, -8.9239e-02, -1.1146e-01]],\n",
       " \n",
       "          [[-1.1322e-01, -1.2220e-01, -7.4908e-02],\n",
       "           [-9.7898e-02, -1.1281e-01,  3.8691e-02],\n",
       "           [ 1.0792e-01,  6.1381e-02, -1.1061e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 2.2913e-02, -7.3010e-02,  1.1374e-01],\n",
       "           [-5.4716e-02, -7.1698e-02, -9.1502e-02],\n",
       "           [-6.9164e-02, -6.6752e-02,  3.1186e-02]],\n",
       " \n",
       "          [[-3.8754e-02, -1.1593e-01,  6.3641e-02],\n",
       "           [ 1.3793e-01,  2.9055e-02,  1.0970e-01],\n",
       "           [-7.6242e-02,  8.6206e-02, -1.1082e-01]],\n",
       " \n",
       "          [[-1.3520e-01,  1.1323e-01, -1.1425e-01],\n",
       "           [ 8.7512e-02,  9.0999e-02, -1.1420e-01],\n",
       "           [ 1.4235e-01, -5.2422e-02, -5.4441e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.7417e-02,  8.8134e-02,  1.2947e-01],\n",
       "           [-1.2379e-02,  1.1888e-01, -7.9912e-02],\n",
       "           [-5.2966e-02,  1.3131e-01,  2.5443e-03]],\n",
       " \n",
       "          [[ 6.9660e-02, -3.4221e-02, -2.1286e-02],\n",
       "           [ 1.0441e-01,  4.7275e-03,  5.0270e-02],\n",
       "           [-1.2704e-01, -8.9836e-02,  1.6273e-02]],\n",
       " \n",
       "          [[ 1.0085e-01, -9.0703e-02, -5.9582e-02],\n",
       "           [-6.1979e-03, -5.0744e-02,  8.9788e-03],\n",
       "           [-2.2147e-02, -6.1341e-02, -3.4915e-02]]],\n",
       " \n",
       " \n",
       "         [[[-7.9173e-02,  1.0203e-01, -9.8545e-02],\n",
       "           [-1.1140e-01, -4.3579e-02,  1.3002e-01],\n",
       "           [ 6.8384e-02,  1.4014e-01,  2.4411e-02]],\n",
       " \n",
       "          [[-1.1598e-01,  1.2445e-01, -4.4399e-03],\n",
       "           [ 2.4686e-02, -1.0966e-01,  4.4525e-02],\n",
       "           [-6.7641e-02, -6.2949e-02,  5.9019e-02]],\n",
       " \n",
       "          [[ 5.0199e-02, -1.2687e-01,  7.3455e-02],\n",
       "           [ 8.8037e-02,  1.0495e-01,  9.0475e-02],\n",
       "           [ 2.4475e-02, -1.0789e-01, -5.8583e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.5966e-02,  1.1624e-01, -9.1316e-02],\n",
       "           [ 2.3336e-02,  5.5963e-04,  1.0393e-01],\n",
       "           [-1.0541e-01, -1.3389e-01, -8.1914e-03]],\n",
       " \n",
       "          [[-4.8128e-03,  1.2437e-01, -9.4402e-02],\n",
       "           [ 3.7788e-02, -8.7934e-02,  1.4541e-01],\n",
       "           [-1.2637e-01,  1.3636e-01, -9.2408e-02]],\n",
       " \n",
       "          [[ 7.7760e-02,  7.3097e-02,  6.5494e-02],\n",
       "           [ 9.7212e-04,  5.7406e-02, -5.4047e-03],\n",
       "           [-9.6770e-02, -2.3242e-02,  1.2266e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.5359e-02,  1.1287e-01,  3.5499e-03],\n",
       "           [-5.2685e-02,  4.2325e-02, -4.5189e-02],\n",
       "           [ 6.5179e-03, -4.7369e-02,  5.8609e-02]],\n",
       " \n",
       "          [[ 8.6564e-02,  1.3674e-01,  1.7454e-02],\n",
       "           [ 1.1389e-01, -6.5724e-03,  3.9187e-02],\n",
       "           [ 1.0315e-01,  7.4955e-02, -9.6964e-02]],\n",
       " \n",
       "          [[ 8.3606e-02,  1.4753e-01, -9.4033e-02],\n",
       "           [-2.7168e-02, -8.6773e-02, -9.4039e-02],\n",
       "           [-4.9206e-02,  1.1053e-01, -2.6256e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.2399e-02, -1.0967e-01,  1.3374e-01],\n",
       "           [ 7.5766e-02, -1.3445e-01, -1.5234e-02],\n",
       "           [ 1.0744e-01,  6.6928e-02, -1.3212e-01]],\n",
       " \n",
       "          [[ 3.0932e-02, -1.0104e-01, -3.0344e-02],\n",
       "           [ 2.2360e-02, -3.4547e-02,  1.2956e-01],\n",
       "           [ 1.2841e-01, -2.8487e-02, -8.4738e-02]],\n",
       " \n",
       "          [[-4.6592e-02,  1.4451e-01,  8.3424e-02],\n",
       "           [-1.0040e-01,  1.0800e-01,  4.9241e-02],\n",
       "           [-1.0096e-01,  1.5026e-02, -8.3207e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-2.8580e-05, -5.7106e-05,  1.0160e-04, -4.5732e-05,  3.0459e-05,\n",
       "         -4.7348e-06, -2.6534e-05,  2.2065e-07, -1.7105e-05, -2.3418e-06,\n",
       "          2.0291e-05,  9.9398e-05, -2.2766e-05, -1.0606e-04,  2.6959e-05,\n",
       "         -3.2369e-05, -5.6509e-05,  4.6831e-05,  8.6772e-05,  7.6245e-05,\n",
       "         -4.8536e-05, -1.3738e-05, -2.0636e-05,  6.6789e-05, -8.3066e-06,\n",
       "         -5.7829e-05, -1.7894e-05, -2.2073e-05, -7.9000e-05, -2.9149e-05,\n",
       "         -4.5438e-05, -2.4339e-05], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-3.3688e-02, -3.8310e-02, -6.9611e-02],\n",
       "           [ 7.9456e-02, -3.1459e-02,  7.3094e-02],\n",
       "           [ 9.8591e-02,  7.8891e-02, -8.9042e-02]],\n",
       " \n",
       "          [[-9.8842e-02, -2.2763e-02, -4.2898e-03],\n",
       "           [ 5.7245e-02,  8.2270e-02,  4.5103e-02],\n",
       "           [-4.9365e-02,  8.6985e-02,  3.4662e-02]],\n",
       " \n",
       "          [[-3.6132e-03,  1.9394e-02,  7.4846e-02],\n",
       "           [ 8.6999e-02, -6.3950e-02,  6.7117e-02],\n",
       "           [-9.5327e-03,  7.4772e-02, -1.0364e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.8420e-02,  3.4460e-02, -2.0881e-02],\n",
       "           [-8.5756e-02,  2.2161e-03, -6.4460e-02],\n",
       "           [ 2.1849e-02, -3.6732e-03, -9.6448e-02]],\n",
       " \n",
       "          [[-2.6929e-02, -6.0994e-02, -6.2729e-02],\n",
       "           [ 6.9633e-02,  4.4524e-02, -2.0068e-02],\n",
       "           [-3.8892e-02,  1.0838e-02, -8.3537e-03]],\n",
       " \n",
       "          [[-9.4192e-02, -7.8317e-02,  4.5645e-02],\n",
       "           [ 7.5237e-02,  6.7152e-02,  5.5863e-03],\n",
       "           [-7.3464e-02,  4.9467e-02, -5.8591e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.1815e-02,  4.3723e-02, -1.6356e-02],\n",
       "           [-3.6561e-03,  4.1122e-02, -3.6133e-02],\n",
       "           [ 2.0249e-02,  3.0738e-02,  6.2230e-02]],\n",
       " \n",
       "          [[ 8.9326e-02,  2.5446e-02,  3.9456e-02],\n",
       "           [ 6.8884e-02,  7.9735e-02,  2.3961e-02],\n",
       "           [ 5.1999e-03, -2.5530e-02, -3.1137e-02]],\n",
       " \n",
       "          [[-4.6663e-02, -6.6538e-02, -9.2988e-02],\n",
       "           [ 7.4693e-02, -3.4452e-02,  6.8729e-02],\n",
       "           [ 1.0504e-01, -2.5272e-02,  2.3083e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.8496e-02,  1.2263e-02, -4.5522e-03],\n",
       "           [ 7.9843e-02,  3.5170e-02, -8.3222e-02],\n",
       "           [ 1.0044e-01,  8.4765e-02,  5.7128e-02]],\n",
       " \n",
       "          [[ 9.4084e-03,  9.4106e-02,  5.9008e-02],\n",
       "           [-8.5263e-02,  7.4211e-02, -1.7828e-02],\n",
       "           [-3.9896e-02, -9.0597e-02,  5.6784e-02]],\n",
       " \n",
       "          [[ 1.5422e-02, -2.6707e-02,  4.2987e-02],\n",
       "           [ 8.0566e-02,  6.1646e-02, -4.2621e-02],\n",
       "           [-4.8070e-03, -3.6476e-02, -2.6179e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 6.7115e-02, -4.7935e-02,  6.3907e-02],\n",
       "           [ 9.2522e-02,  9.2025e-02,  5.7704e-02],\n",
       "           [-7.8439e-03,  8.5366e-02,  9.1897e-02]],\n",
       " \n",
       "          [[-3.9897e-02, -3.3313e-02,  5.7170e-02],\n",
       "           [ 4.7031e-02,  6.0466e-02,  5.2821e-02],\n",
       "           [-5.8070e-02, -5.5911e-02, -7.2661e-02]],\n",
       " \n",
       "          [[-5.9516e-02,  3.8848e-02,  1.7563e-02],\n",
       "           [-3.1229e-02,  5.9037e-02,  4.1653e-02],\n",
       "           [-5.6168e-02,  7.3117e-02, -8.4985e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.6676e-03,  3.8301e-02,  8.0859e-02],\n",
       "           [-4.4131e-02, -5.2573e-02, -9.7383e-02],\n",
       "           [-7.1801e-02, -4.8217e-02, -2.7579e-02]],\n",
       " \n",
       "          [[-9.2187e-02, -2.9320e-02, -6.4818e-02],\n",
       "           [-1.3647e-02, -1.9500e-02,  8.3873e-03],\n",
       "           [-4.1475e-02,  2.2783e-02,  9.9772e-02]],\n",
       " \n",
       "          [[-2.5818e-02, -6.1879e-02, -2.1460e-02],\n",
       "           [ 5.6676e-02,  7.7172e-02, -4.4362e-02],\n",
       "           [ 3.4048e-02,  4.5650e-02,  6.1847e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-8.7530e-02, -8.9685e-02,  6.1771e-03],\n",
       "           [ 6.5225e-02,  9.6682e-02, -5.8146e-02],\n",
       "           [-6.3527e-05,  7.1921e-02,  9.2055e-02]],\n",
       " \n",
       "          [[-1.8184e-03, -7.1245e-02,  2.9431e-02],\n",
       "           [ 5.8899e-02, -4.5925e-02, -7.7455e-02],\n",
       "           [ 9.7059e-02,  2.3618e-02, -6.5647e-02]],\n",
       " \n",
       "          [[-4.7873e-02, -5.1207e-02,  6.8227e-02],\n",
       "           [ 1.0021e-01, -9.3839e-02,  3.2627e-02],\n",
       "           [ 2.5535e-03, -7.9053e-03,  3.5255e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.3751e-02,  5.2702e-02,  9.5215e-02],\n",
       "           [ 1.0366e-02, -1.7493e-02, -8.2366e-02],\n",
       "           [ 7.4448e-02, -7.0568e-02, -7.4838e-02]],\n",
       " \n",
       "          [[-2.6885e-02,  8.4368e-02, -2.5643e-02],\n",
       "           [-9.9412e-02,  7.2433e-02, -3.8417e-02],\n",
       "           [ 3.7166e-02, -5.1868e-02, -9.3603e-02]],\n",
       " \n",
       "          [[-6.2792e-02, -5.9253e-02, -7.9272e-02],\n",
       "           [-7.7499e-02, -9.1411e-02, -1.2506e-02],\n",
       "           [ 2.1484e-02, -7.7579e-02,  2.1324e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.3157e-02, -1.5213e-02,  5.8259e-02],\n",
       "           [ 1.0542e-01,  5.6660e-02, -7.3500e-02],\n",
       "           [ 3.2743e-02, -6.4410e-04,  4.4373e-02]],\n",
       " \n",
       "          [[-7.4654e-02,  9.8469e-02,  1.0389e-01],\n",
       "           [-3.6754e-02, -4.4285e-02,  7.7036e-02],\n",
       "           [ 9.1785e-02,  6.9707e-02,  6.5581e-02]],\n",
       " \n",
       "          [[-9.1807e-02,  5.0067e-02,  5.0839e-03],\n",
       "           [-7.5597e-02,  2.6834e-02, -7.6972e-02],\n",
       "           [-6.9621e-02, -5.6203e-02, -7.9741e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.8646e-02,  3.3364e-02, -1.8338e-03],\n",
       "           [-4.5991e-02,  6.5480e-02, -9.0995e-03],\n",
       "           [ 3.4265e-02, -2.6951e-02, -5.9921e-02]],\n",
       " \n",
       "          [[-9.6306e-02,  5.0985e-04,  7.9147e-02],\n",
       "           [-5.1029e-02,  8.0589e-02,  3.0560e-03],\n",
       "           [ 7.5441e-03, -3.1992e-02,  9.9390e-02]],\n",
       " \n",
       "          [[ 8.8676e-02, -4.1586e-02,  6.2362e-02],\n",
       "           [-3.8977e-02,  5.2655e-02,  8.8111e-02],\n",
       "           [ 7.9408e-02,  5.4279e-02, -3.4047e-03]]],\n",
       " \n",
       " \n",
       "         [[[-8.4449e-02, -8.1628e-02, -1.1939e-03],\n",
       "           [-7.3358e-02, -7.4481e-02,  9.9025e-03],\n",
       "           [ 3.0919e-02,  6.8269e-04,  7.7426e-02]],\n",
       " \n",
       "          [[ 5.2951e-02, -9.4957e-02,  9.9500e-03],\n",
       "           [-7.8490e-02, -3.9062e-02, -8.1130e-02],\n",
       "           [-1.8813e-02,  1.8985e-02, -2.2812e-02]],\n",
       " \n",
       "          [[ 5.5363e-02, -9.0396e-02, -3.8657e-02],\n",
       "           [ 8.9237e-02, -9.9412e-02,  5.2548e-03],\n",
       "           [-3.2639e-02, -3.2150e-02,  7.3497e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.0490e-03, -2.3747e-02,  5.8681e-02],\n",
       "           [-3.1602e-02,  5.7551e-02, -5.5329e-02],\n",
       "           [-4.0355e-02, -4.7725e-02,  8.8630e-02]],\n",
       " \n",
       "          [[-5.1444e-02, -6.4629e-02, -4.7848e-02],\n",
       "           [ 1.8816e-02, -5.9040e-03,  9.6782e-02],\n",
       "           [ 7.2345e-02, -3.8876e-02,  6.8742e-02]],\n",
       " \n",
       "          [[-3.0342e-02, -9.4876e-02,  4.3257e-02],\n",
       "           [-5.6013e-02, -1.7534e-02,  9.7478e-02],\n",
       "           [ 9.3880e-02,  5.1448e-02,  3.9457e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "         0.9990, 0.9990, 0.9990, 0.9990, 0.9990], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010, -0.0010,\n",
       "          0.0010,  0.0010,  0.0010, -0.0010, -0.0010, -0.0010, -0.0010,  0.0010,\n",
       "          0.0010,  0.0010, -0.0010, -0.0010, -0.0010,  0.0010,  0.0010, -0.0010,\n",
       "          0.0010,  0.0010,  0.0010, -0.0010,  0.0010, -0.0010, -0.0010,  0.0010],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1607, -0.1702,  0.0482, -0.0502,  0.1170,  0.1704,  0.1738,  0.1234,\n",
       "           0.0297,  0.0807,  0.0730,  0.1330,  0.0392,  0.0630,  0.0083, -0.0038,\n",
       "          -0.1553, -0.0699, -0.1508, -0.0710,  0.0745,  0.1405, -0.1480,  0.0259,\n",
       "           0.1096,  0.0516, -0.0808, -0.0014, -0.0402,  0.1346,  0.1531, -0.1756],\n",
       "         [ 0.1526, -0.0769,  0.1545,  0.0831, -0.0867,  0.1411, -0.0859,  0.1134,\n",
       "          -0.0043,  0.0052, -0.1514,  0.1092,  0.0661,  0.0408, -0.0281, -0.0157,\n",
       "           0.0439,  0.0159, -0.0375,  0.0377,  0.0143,  0.1069, -0.0689, -0.0609,\n",
       "          -0.0389, -0.1569, -0.1717,  0.0825, -0.0316,  0.1174,  0.1273,  0.0351],\n",
       "         [ 0.0302,  0.0189, -0.0392,  0.0912, -0.0451, -0.1495, -0.1331, -0.1639,\n",
       "           0.0546,  0.0299,  0.0545,  0.0533,  0.1567,  0.0567,  0.1380, -0.0467,\n",
       "           0.0280,  0.0092, -0.1536,  0.0825,  0.1636, -0.0972, -0.1546,  0.1368,\n",
       "           0.1401,  0.0574, -0.1750, -0.0469, -0.0716,  0.0004,  0.0687, -0.1313],\n",
       "         [-0.0208,  0.1660, -0.1226, -0.0971,  0.0058,  0.1049,  0.1075,  0.1193,\n",
       "          -0.1366,  0.0090,  0.0027, -0.1110, -0.0210, -0.0102,  0.0249, -0.0752,\n",
       "           0.0178, -0.0649,  0.1622, -0.0310,  0.0948, -0.0866,  0.0817,  0.0055,\n",
       "          -0.0769, -0.0798,  0.0224,  0.1465,  0.0893, -0.1206, -0.0649, -0.1265],\n",
       "         [-0.0468,  0.0607, -0.0020,  0.1178, -0.0199, -0.1038, -0.1465, -0.1164,\n",
       "           0.1499, -0.1192, -0.0971,  0.0968, -0.0514, -0.0528, -0.1635,  0.0926,\n",
       "          -0.0098, -0.0889, -0.0497,  0.1175, -0.0447, -0.1674, -0.1278, -0.0499,\n",
       "          -0.0798, -0.0562, -0.0248,  0.0255, -0.0358, -0.1008,  0.1502, -0.0219],\n",
       "         [ 0.1583, -0.1273,  0.0456, -0.0769,  0.0194,  0.0151,  0.0712, -0.0026,\n",
       "          -0.0899, -0.1460, -0.1171,  0.0558,  0.1444,  0.0925, -0.0144, -0.0127,\n",
       "           0.1526, -0.0477,  0.0173,  0.0625,  0.1189,  0.0759,  0.0309,  0.1629,\n",
       "          -0.1385, -0.0374,  0.0171,  0.1595, -0.1120,  0.1512, -0.0533,  0.1045],\n",
       "         [ 0.1047, -0.0128, -0.0270, -0.0027, -0.0048, -0.0352,  0.0278,  0.1151,\n",
       "          -0.0869, -0.1120,  0.0971, -0.0726, -0.1752,  0.0244,  0.0766,  0.0625,\n",
       "          -0.0946, -0.0516,  0.1676,  0.0080,  0.0296,  0.0657,  0.0618,  0.0003,\n",
       "          -0.0736, -0.1333,  0.1693,  0.0609,  0.0826, -0.0917,  0.0342,  0.0574],\n",
       "         [-0.0835,  0.1432, -0.1391,  0.0924,  0.1282, -0.0592,  0.0249,  0.0015,\n",
       "           0.1576,  0.0032, -0.1181,  0.1004,  0.0176, -0.0586, -0.0342, -0.0907,\n",
       "          -0.0077,  0.1036, -0.0601,  0.0725,  0.1681, -0.1313, -0.0588, -0.0238,\n",
       "          -0.1367,  0.1755,  0.0028,  0.0086, -0.1307,  0.1373,  0.0831, -0.1067],\n",
       "         [ 0.1105,  0.1445,  0.1345,  0.0280,  0.1203, -0.0034, -0.1202, -0.0560,\n",
       "          -0.1582, -0.0314, -0.1125,  0.0762, -0.0939,  0.1186,  0.0141, -0.0241,\n",
       "           0.0550, -0.0424,  0.0598, -0.1221, -0.1323, -0.0102, -0.0117,  0.0062,\n",
       "           0.0884, -0.0396, -0.0070, -0.0947,  0.0643, -0.0076,  0.1282,  0.0410],\n",
       "         [-0.0861, -0.0394,  0.0899, -0.1277, -0.0869, -0.0550, -0.1742,  0.1285,\n",
       "          -0.0266, -0.0224, -0.0054,  0.0944,  0.1006,  0.0629,  0.0816, -0.0502,\n",
       "          -0.0971, -0.1424,  0.1416, -0.0874,  0.1217,  0.0165,  0.1246, -0.1297,\n",
       "           0.1088,  0.1366,  0.0589, -0.0862,  0.1694, -0.1577, -0.0145,  0.0514]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,\n",
       "         -0.0010, -0.0010], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynet.tree.optimizer.param_groups[0]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " 'exp_avg': tensor([[ 1.6167e-07, -1.7124e-07,  4.9155e-08, -5.1114e-08,  1.1790e-07,\n",
       "           1.7141e-07,  1.7477e-07,  1.2435e-07,  3.0645e-08,  8.1681e-08,\n",
       "           7.4002e-08,  1.3397e-07,  4.0235e-08,  6.4017e-08,  9.1048e-09,\n",
       "          -4.6264e-09, -1.5634e-07, -7.0888e-08, -1.5177e-07, -7.2024e-08,\n",
       "           7.5563e-08,  1.4152e-07, -1.4896e-07,  2.6907e-08,  1.1054e-07,\n",
       "           5.2619e-08, -8.1770e-08, -2.0274e-09, -4.1141e-08,  1.3568e-07,\n",
       "           1.5410e-07, -1.7662e-07],\n",
       "         [ 1.5360e-07, -7.7877e-08,  1.5553e-07,  8.4158e-08, -8.7740e-08,\n",
       "           1.4212e-07, -8.6871e-08,  1.1437e-07, -5.1806e-09,  6.1025e-09,\n",
       "          -1.5231e-07,  1.1014e-07,  6.7142e-08,  4.1818e-08, -2.9075e-08,\n",
       "          -1.6640e-08,  4.4893e-08,  1.6918e-08, -3.8509e-08,  3.8727e-08,\n",
       "           1.5225e-08,  1.0787e-07, -6.9820e-08, -6.1830e-08, -3.9935e-08,\n",
       "          -1.5783e-07, -1.7266e-07,  8.3518e-08, -3.2593e-08,  1.1847e-07,\n",
       "           1.2833e-07,  3.6046e-08],\n",
       "         [ 3.1132e-08,  1.9823e-08, -4.0146e-08,  9.2216e-08, -4.6164e-08,\n",
       "          -1.5049e-07, -1.3406e-07, -1.6491e-07,  5.5576e-08,  3.0829e-08,\n",
       "           5.5472e-08,  5.4224e-08,  1.5776e-07,  5.7709e-08,  1.3898e-07,\n",
       "          -4.7628e-08,  2.8953e-08,  1.0132e-08, -1.5459e-07,  8.3455e-08,\n",
       "           1.6462e-07, -9.8229e-08, -1.5559e-07,  1.3779e-07,  1.4110e-07,\n",
       "           5.8416e-08, -1.7597e-07, -4.7860e-08, -7.2618e-08,  9.0187e-10,\n",
       "           6.9684e-08, -1.3231e-07],\n",
       "         [-2.1808e-08,  1.6696e-07, -1.2353e-07, -9.8048e-08,  6.6111e-09,\n",
       "           1.0590e-07,  1.0851e-07,  1.2027e-07, -1.3760e-07,  9.8687e-09,\n",
       "           3.5185e-09, -1.1201e-07, -2.1894e-08, -1.1117e-08,  2.5837e-08,\n",
       "          -7.6146e-08,  1.8706e-08, -6.5800e-08,  1.6322e-07, -3.2009e-08,\n",
       "           9.5861e-08, -8.7607e-08,  8.2739e-08,  6.3508e-09, -7.7933e-08,\n",
       "          -8.0764e-08,  2.3396e-08,  1.4748e-07,  9.0249e-08, -1.2152e-07,\n",
       "          -6.5842e-08, -1.2746e-07],\n",
       "         [-4.7756e-08,  6.1618e-08, -2.6662e-09,  1.1883e-07, -2.0944e-08,\n",
       "          -1.0475e-07, -1.4750e-07, -1.1739e-07,  1.5085e-07, -1.2017e-07,\n",
       "          -9.8040e-08,  9.7745e-08, -5.2326e-08, -5.3795e-08, -1.6452e-07,\n",
       "           9.3628e-08, -1.0698e-08, -8.9889e-08, -5.0657e-08,  1.1848e-07,\n",
       "          -4.5644e-08, -1.6837e-07, -1.2882e-07, -5.0846e-08, -8.0772e-08,\n",
       "          -5.7138e-08, -2.5758e-08,  2.6449e-08, -3.6819e-08, -1.0175e-07,\n",
       "           1.5118e-07, -2.2803e-08],\n",
       "         [ 1.5926e-07, -1.2829e-07,  4.6646e-08, -7.7813e-08,  2.0260e-08,\n",
       "           1.6026e-08,  7.2192e-08, -3.3452e-09, -9.0919e-08, -1.4699e-07,\n",
       "          -1.1801e-07,  5.6787e-08,  1.4536e-07,  9.3525e-08, -1.5363e-08,\n",
       "          -1.3617e-08,  1.5353e-07, -4.8635e-08,  1.8267e-08,  6.3478e-08,\n",
       "           1.1991e-07,  7.6907e-08,  3.1926e-08,  1.6389e-07, -1.3948e-07,\n",
       "          -3.8325e-08,  1.8132e-08,  1.6053e-07, -1.1298e-07,  1.5229e-07,\n",
       "          -5.4295e-08,  1.0549e-07],\n",
       "         [ 1.0567e-07, -1.3742e-08, -2.7973e-08, -3.4334e-09, -5.7125e-09,\n",
       "          -3.6183e-08,  2.8749e-08,  1.1604e-07, -8.7895e-08, -1.1300e-07,\n",
       "           9.8088e-08, -7.3579e-08, -1.7616e-07,  2.5395e-08,  7.7516e-08,\n",
       "           6.3459e-08, -9.5642e-08, -5.2552e-08,  1.6856e-07,  8.9279e-09,\n",
       "           3.0633e-08,  6.6691e-08,  6.2763e-08,  7.2931e-10, -7.4633e-08,\n",
       "          -1.3421e-07,  1.7035e-07,  6.1922e-08,  8.3582e-08, -9.2631e-08,\n",
       "           3.5161e-08,  5.8382e-08],\n",
       "         [-8.4511e-08,  1.4420e-07, -1.4008e-07,  9.3400e-08,  1.2913e-07,\n",
       "          -6.0210e-08,  2.5885e-08,  2.1875e-09,  1.5854e-07,  3.9879e-09,\n",
       "          -1.1903e-07,  1.0138e-07,  1.8608e-08, -5.9525e-08, -3.5269e-08,\n",
       "          -9.1701e-08, -8.6175e-09,  1.0463e-07, -6.1083e-08,  7.3516e-08,\n",
       "           1.6916e-07, -1.3228e-07, -5.9794e-08, -2.4758e-08, -1.3777e-07,\n",
       "           1.7654e-07,  3.6306e-09,  9.4817e-09, -1.3170e-07,  1.3831e-07,\n",
       "           8.4081e-08, -1.0771e-07],\n",
       "         [ 1.1150e-07,  1.4546e-07,  1.3548e-07,  2.9032e-08,  1.2124e-07,\n",
       "          -4.2007e-09, -1.2122e-07, -5.6962e-08, -1.5924e-07, -3.2417e-08,\n",
       "          -1.1343e-07,  7.7168e-08, -9.4823e-08,  1.1959e-07,  1.5014e-08,\n",
       "          -2.5090e-08,  5.5972e-08, -4.3368e-08,  6.0766e-08, -1.2310e-07,\n",
       "          -1.3328e-07, -1.1166e-08, -1.2608e-08,  7.0612e-09,  8.9399e-08,\n",
       "          -4.0481e-08, -7.8447e-09, -9.5673e-08,  6.5280e-08, -8.4187e-09,\n",
       "           1.2916e-07,  4.1977e-08],\n",
       "         [-8.7092e-08, -4.0358e-08,  9.0902e-08, -1.2866e-07, -8.7913e-08,\n",
       "          -5.5952e-08, -1.7517e-07,  1.2952e-07, -2.7620e-08, -2.3350e-08,\n",
       "          -6.1868e-09,  9.5393e-08,  1.0159e-07,  6.3898e-08,  8.2570e-08,\n",
       "          -5.1154e-08, -9.8130e-08, -1.4336e-07,  1.4258e-07, -8.8434e-08,\n",
       "           1.2272e-07,  1.7433e-08,  1.2557e-07, -1.3068e-07,  1.0973e-07,\n",
       "           1.3765e-07,  5.9926e-08, -8.7193e-08,  1.7041e-07, -1.5868e-07,\n",
       "          -1.5489e-08,  5.2382e-08]], device='cuda:0'),\n",
       " 'exp_avg_sq': tensor([[2.6139e-15, 2.9323e-15, 2.4162e-16, 2.6127e-16, 1.3901e-15, 2.9382e-15,\n",
       "          3.0546e-15, 1.5462e-15, 9.3910e-17, 6.6718e-16, 5.4763e-16, 1.7947e-15,\n",
       "          1.6188e-16, 4.0982e-16, 8.2897e-18, 2.1403e-18, 2.4442e-15, 5.0252e-16,\n",
       "          2.3035e-15, 5.1875e-16, 5.7097e-16, 2.0028e-15, 2.2188e-15, 7.2400e-17,\n",
       "          1.2219e-15, 2.7687e-16, 6.6864e-16, 4.1105e-19, 1.6926e-16, 1.8410e-15,\n",
       "          2.3748e-15, 3.1194e-15],\n",
       "         [2.3594e-15, 6.0648e-16, 2.4189e-15, 7.0826e-16, 7.6983e-16, 2.0198e-15,\n",
       "          7.5466e-16, 1.3081e-15, 2.6839e-18, 3.7241e-18, 2.3200e-15, 1.2130e-15,\n",
       "          4.5081e-16, 1.7487e-16, 8.4535e-17, 2.7690e-17, 2.0154e-16, 2.8621e-17,\n",
       "          1.4830e-16, 1.4998e-16, 2.3179e-17, 1.1635e-15, 4.8749e-16, 3.8230e-16,\n",
       "          1.5948e-16, 2.4910e-15, 2.9812e-15, 6.9753e-16, 1.0623e-16, 1.4034e-15,\n",
       "          1.6469e-15, 1.2993e-16],\n",
       "         [9.6923e-17, 3.9295e-17, 1.6117e-16, 8.5038e-16, 2.1312e-16, 2.2646e-15,\n",
       "          1.7972e-15, 2.7195e-15, 3.0887e-16, 9.5043e-17, 3.0771e-16, 2.9402e-16,\n",
       "          2.4888e-15, 3.3303e-16, 1.9315e-15, 2.2684e-16, 8.3827e-17, 1.0265e-17,\n",
       "          2.3897e-15, 6.9647e-16, 2.7098e-15, 9.6489e-16, 2.4207e-15, 1.8987e-15,\n",
       "          1.9908e-15, 3.4125e-16, 3.0964e-15, 2.2905e-16, 5.2734e-16, 8.1337e-20,\n",
       "          4.8558e-16, 1.7506e-15],\n",
       "         [4.7559e-17, 2.7876e-15, 1.5260e-15, 9.6133e-16, 4.3706e-18, 1.1215e-15,\n",
       "          1.1775e-15, 1.4465e-15, 1.8935e-15, 9.7391e-18, 1.2380e-18, 1.2547e-15,\n",
       "          4.7933e-17, 1.2358e-17, 6.6758e-17, 5.7982e-16, 3.4991e-17, 4.3296e-16,\n",
       "          2.6640e-15, 1.0246e-16, 9.1894e-16, 7.6750e-16, 6.8458e-16, 4.0333e-18,\n",
       "          6.0735e-16, 6.5229e-16, 5.4739e-17, 2.1751e-15, 8.1449e-16, 1.4767e-15,\n",
       "          4.3352e-16, 1.6247e-15],\n",
       "         [2.2806e-16, 3.7968e-16, 7.1085e-19, 1.4120e-15, 4.3864e-17, 1.0972e-15,\n",
       "          2.1758e-15, 1.3779e-15, 2.2756e-15, 1.4442e-15, 9.6118e-16, 9.5542e-16,\n",
       "          2.7380e-16, 2.8939e-16, 2.7067e-15, 8.7662e-16, 1.1444e-17, 8.0800e-16,\n",
       "          2.5661e-16, 1.4037e-15, 2.0833e-16, 2.8348e-15, 1.6594e-15, 2.5853e-16,\n",
       "          6.5241e-16, 3.2648e-16, 6.6350e-17, 6.9957e-17, 1.3557e-16, 1.0354e-15,\n",
       "          2.2854e-15, 5.1996e-17],\n",
       "         [2.5365e-15, 1.6458e-15, 2.1759e-16, 6.0549e-16, 4.1046e-17, 2.5682e-17,\n",
       "          5.2117e-16, 1.1190e-18, 8.2662e-16, 2.1607e-15, 1.3926e-15, 3.2247e-16,\n",
       "          2.1131e-15, 8.7468e-16, 2.3602e-17, 1.8543e-17, 2.3573e-15, 2.3653e-16,\n",
       "          3.3368e-17, 4.0295e-16, 1.4378e-15, 5.9147e-16, 1.0193e-16, 2.6860e-15,\n",
       "          1.9454e-15, 1.4688e-16, 3.2878e-17, 2.5769e-15, 1.2764e-15, 2.3193e-15,\n",
       "          2.9480e-16, 1.1128e-15],\n",
       "         [1.1166e-15, 1.8883e-17, 7.8246e-17, 1.1788e-18, 3.2632e-18, 1.3092e-16,\n",
       "          8.2652e-17, 1.3466e-15, 7.7255e-16, 1.2769e-15, 9.6212e-16, 5.4138e-16,\n",
       "          3.1033e-15, 6.4492e-17, 6.0087e-16, 4.0270e-16, 9.1473e-16, 2.7617e-16,\n",
       "          2.8413e-15, 7.9708e-18, 9.3836e-17, 4.4477e-16, 3.9392e-16, 5.3189e-20,\n",
       "          5.5701e-16, 1.8013e-15, 2.9020e-15, 3.8343e-16, 6.9860e-16, 8.5805e-16,\n",
       "          1.2363e-16, 3.4084e-16],\n",
       "         [7.1421e-16, 2.0793e-15, 1.9622e-15, 8.7236e-16, 1.6673e-15, 3.6252e-16,\n",
       "          6.7001e-17, 4.7853e-19, 2.5134e-15, 1.5903e-18, 1.4168e-15, 1.0279e-15,\n",
       "          3.4627e-17, 3.5433e-16, 1.2439e-16, 8.4090e-16, 7.4261e-18, 1.0947e-15,\n",
       "          3.7311e-16, 5.4046e-16, 2.8615e-15, 1.7497e-15, 3.5754e-16, 6.1295e-17,\n",
       "          1.8980e-15, 3.1165e-15, 1.3181e-18, 8.9903e-18, 1.7344e-15, 1.9129e-15,\n",
       "          7.0696e-16, 1.1601e-15],\n",
       "         [1.2432e-15, 2.1159e-15, 1.8353e-15, 8.4284e-17, 1.4700e-15, 1.7646e-18,\n",
       "          1.4694e-15, 3.2446e-16, 2.5357e-15, 1.0509e-16, 1.2865e-15, 5.9548e-16,\n",
       "          8.9914e-16, 1.4301e-15, 2.2542e-17, 6.2952e-17, 3.1329e-16, 1.8808e-16,\n",
       "          3.6925e-16, 1.5154e-15, 1.7763e-15, 1.2467e-17, 1.5897e-17, 4.9860e-18,\n",
       "          7.9923e-16, 1.6387e-16, 6.1540e-18, 9.1534e-16, 4.2614e-16, 7.0874e-18,\n",
       "          1.6682e-15, 1.7621e-16],\n",
       "         [7.5850e-16, 1.6287e-16, 8.2632e-16, 1.6553e-15, 7.7288e-16, 3.1306e-16,\n",
       "          3.0684e-15, 1.6775e-15, 7.6289e-17, 5.4524e-17, 3.8277e-18, 9.0998e-16,\n",
       "          1.0322e-15, 4.0830e-16, 6.8179e-16, 2.6167e-16, 9.6296e-16, 2.0552e-15,\n",
       "          2.0330e-15, 7.8205e-16, 1.5060e-15, 3.0390e-17, 1.5767e-15, 1.7076e-15,\n",
       "          1.2041e-15, 1.8946e-15, 3.5911e-16, 7.6026e-16, 2.9040e-15, 2.5178e-15,\n",
       "          2.3990e-17, 2.7439e-16]], device='cuda:0')}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynet.tree.optimizer.state[dynet.root_net.residual.fc1.shortcut.weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.root_net.residual.fc1.shortcut.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3126"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decay_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = '00.2'\n",
    "name = 'dynCNN_reuse_optim_v0'\n",
    "exp_index = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = {\n",
    "    'learning_rate':learning_rate,\n",
    "    'num_add_neuron':num_add_neuron,\n",
    "    'num_decay_steps':num_decay_steps,\n",
    "    'remove_above':remove_above,\n",
    "    'threshold_max':threshold_max,\n",
    "    'threshold_min':threshold_min,\n",
    "    'train_epoch_min':train_epoch_min,\n",
    "    'train_epoch_max':train_epoch_max,\n",
    "    'add_to_remove_ratio':dynet.tree.add_to_remove_ratio,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_json = f'hyperparameters/{index}_hyp_exp_{exp_index}.json'\n",
    "with open(hyp_json, 'w') as fp:\n",
    "    json.dump(hyp, fp, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoTrainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_func = None\n",
    "        self.adding_func = None\n",
    "        self.pruning_func = None\n",
    "        self.maintainance_func = None\n",
    "        self.extra_func = None\n",
    "        \n",
    "        self.log_func = None\n",
    "        \n",
    "    def loop(self, count = 15):\n",
    "        cb = count\n",
    "        for i in range(count):\n",
    "            if i>-0.1:\n",
    "                self.adding_func()\n",
    "            else:\n",
    "#                 global optimizer, warmup\n",
    "                dynet.print_network()    \n",
    "                \n",
    "                reset_optimizer()\n",
    "#                 optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#                 optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#                 warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader))\n",
    "            \n",
    "            \n",
    "            self.training_func()\n",
    "\n",
    "            self.log_func(i)\n",
    "            if self.extra_func:\n",
    "                self.extra_func()\n",
    "            \n",
    "            if i>-0.1:\n",
    "                self.pruning_func()\n",
    "            self.maintainance_func()\n",
    "            \n",
    "            self.log_func(i)\n",
    "            if self.extra_func:\n",
    "                self.extra_func()\n",
    "            \n",
    "            print(f\"=====================\")\n",
    "            print(f\"===LOOPS FINISHED :{i} ===\")\n",
    "            print(f\"Pausing for 2 second to give user time to STOP PROCESS\")\n",
    "            time.sleep(2)\n",
    "        self.training_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when to stop training functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeff(num_iter, coeff0, coeff1, coeff2, coeff_opt, loss_list):\n",
    "    if len(loss_list)<10: return np.array([0]), np.array([0]), float(coeff0.data.cpu()[0])\n",
    "    \n",
    "    _t = torch.tensor(loss_list)\n",
    "    _t = (_t - _t[-1])/(_t[0]-_t.min()) ## normalize to make first point at 1 and last at 0 \n",
    "    _t = torch.clamp(_t, -1.1, 1.1)\n",
    "    _x = torch.linspace(0, 1, steps=len(_t))\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        coeff_opt.zero_grad()\n",
    "        _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "        _loss = ((_y - _t)**2).mean()\n",
    "        _loss.backward()\n",
    "        coeff_opt.step()\n",
    "\n",
    "        coeff0.data = torch.clamp(coeff0.data, -20., 20.)\n",
    "        coeff1.data = torch.clamp(coeff1.data, 0.7, 2.)\n",
    "        coeff2.data = torch.clamp(coeff2.data, -0.2,0.1)\n",
    "        \n",
    "    if torch.isnan(coeff0.data[0]):\n",
    "        coeff0.data[0] = 0.\n",
    "        coeff1.data[0] = 0.\n",
    "        coeff2.data[0] = 1. ## this gives signal\n",
    "        \n",
    "    _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "    return _x.numpy(), _t.numpy(), _y.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global variables\n",
    "optimizer = None\n",
    "warmup = None\n",
    "coeff_opt = None\n",
    "\n",
    "loss_all = []\n",
    "accs_all = []\n",
    "accs_test = []\n",
    "events_all = []\n",
    "\n",
    "## for adam optimizer = \n",
    "# learning_rate *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reset_optimizer():\n",
    "#     global optimizer, warmup\n",
    "# #     optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "#     optimizer = adam_custom.Adam(dynet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# #     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "#     warmup = WarmupLR_Polynomial(optimizer, 0.5, len(train_loader), power=2)\n",
    "# #     warmup = WarmupLR_Polynomial(optimizer, 10/len(train_loader), len(train_loader))\n",
    "# #     get_bn_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_optimizer():\n",
    "    global dynet, warmup, optimizer\n",
    "\n",
    "    ## there are no param groups, but consider there are len=1\n",
    "    pg = dynet.tree.optimizer.param_groups\n",
    "    for i in range(len(pg)):\n",
    "        pg[i]['lr'] = learning_rate\n",
    "    \n",
    "        \n",
    "#     optimizer = dynet.tree.optimizer   \n",
    "#     dynet.tree.optimizer = adam_custom.Adam(dynet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    warmup = WarmupLR_Polynomial(dynet.tree.optimizer, 0.5, len(train_loader), power=2)\n",
    "    \n",
    "#     copy_optimizer()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def copy_optimizer():\n",
    "#     global dynet\n",
    "#     old_optim = dynet.tree.optimizer\n",
    "#     new_optim = adam_custom.Adam(dynet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "#     found=False\n",
    "#     for p in dynet.parameters():\n",
    "#         for _p in old_optim.param_groups[0]['params']:\n",
    "#             if _p is p:\n",
    "#                 found = True\n",
    "#                 new_optim.state[p] = old_optim.state[p]\n",
    "#     if not found:\n",
    "#         raise ValueError(\"Parameter could not be found\")\n",
    "    \n",
    "#     dynet.tree.optimizer = new_optim\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = dynet.tree.optimizer\n",
    "\n",
    "# copy_optimizer(optimizer)\n",
    "\n",
    "# dynet.tree.optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupLR_Polynomial():\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_epoch, num_batch_in_epoch, power=5):\n",
    "        self.warmup_epoch = warmup_epoch\n",
    "        self.optimizer = optimizer\n",
    "        self.num_batch = num_batch_in_epoch\n",
    "        self.steps = 0\n",
    "        self.power = power\n",
    "        self.backup_lr = []\n",
    "        for group in self.optimizer.param_groups:\n",
    "            self.backup_lr.append(float(group['lr']))\n",
    "        \n",
    "    def step(self):\n",
    "        self.steps += 1\n",
    "        steps = self.steps/self.num_batch\n",
    "        \n",
    "        factor = 1\n",
    "        warming = False\n",
    "        if steps<self.warmup_epoch:\n",
    "            factor = (steps/self.warmup_epoch)**self.power\n",
    "            warming = True\n",
    "            \n",
    "        for group, bkp_lr in zip(self.optimizer.param_groups, self.backup_lr):\n",
    "            group['lr'] = bkp_lr*factor\n",
    "        \n",
    "        return warming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neurons_func():\n",
    "    global optimizer, warmup, added, events_all\n",
    "    \n",
    "    ######################################33\n",
    "    ################# CHECK IF ADDING NEURONS CHANGES ACCURACY #####################\n",
    "#     with torch.no_grad():\n",
    "#         corrects = 0\n",
    "#         for test_x, test_y in train_loader:\n",
    "#             test_x  = test_x.to(device)\n",
    "#             yout = dynet.forward(test_x)\n",
    "#             outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "#             correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "#             corrects += correct\n",
    "#         accs_all.append(corrects/len(train_dataset)*100)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         corrects = 0\n",
    "#         dynet.eval()\n",
    "#         for test_x, test_y in test_loader:\n",
    "#             test_x  = test_x.to(device)\n",
    "#             yout = dynet.forward(test_x)\n",
    "#             outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "#             correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "#             corrects += correct\n",
    "#         dynet.train()\n",
    "#         accs_test.append(corrects/len(test_dataset)*100)\n",
    "    ######################################33\n",
    "    \n",
    "    \n",
    "    ### number of neurons\n",
    "    count = 0\n",
    "    for hr in dynet.tree.DYNAMIC_LIST:\n",
    "        if hr.residual:\n",
    "            count += hr.residual.hidden_dim\n",
    "    ## add more neurons relatively (+x%)\n",
    "    adding = num_add_neuron+int(0.07*count)\n",
    "    dynet.add_neurons(adding)\n",
    "    print(f\"Adding {adding} Neurons\")\n",
    "    added = adding\n",
    "    dynet.print_network()    \n",
    "    \n",
    "    reset_optimizer()\n",
    "#     optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#     warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader), power=1)\n",
    "    \n",
    "            \n",
    "    ######################################33\n",
    "    if len(accs_all)>0:\n",
    "        \n",
    "#         accs_all.append(accs_all[-1])\n",
    "        with torch.no_grad():\n",
    "            corrects = 0\n",
    "            for test_x, test_y in train_loader:\n",
    "                test_x  = test_x.to(device)\n",
    "                yout = dynet.forward(test_x)\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                corrects += correct\n",
    "            accs_all.append(corrects/len(train_dataset)*100)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            corrects = 0\n",
    "            dynet.eval()\n",
    "            for test_x, test_y in test_loader:\n",
    "                test_x  = test_x.to(device)\n",
    "                yout = dynet.forward(test_x)\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                corrects += correct\n",
    "            dynet.train()\n",
    "            accs_test.append(corrects/len(test_dataset)*100)\n",
    "    ######################################33\n",
    "    \n",
    "    events_all.append((len(accs_all), \"neurons added\"))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_children(module):\n",
    "    child = list(module.children())\n",
    "    if len(child) == 0:\n",
    "        return [module]\n",
    "    children = []\n",
    "    for ch in child:\n",
    "        grand_ch = get_children(ch)\n",
    "        children+=grand_ch\n",
    "    return children\n",
    "\n",
    "bn_params = []\n",
    "def get_bn_params():\n",
    "    global dynet, bn_params\n",
    "    bn_params = []\n",
    "    for module in get_children(dynet):\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            bn_params.append(module.weight)\n",
    "            bn_params.append(module.bias)\n",
    "            \n",
    "def clip_bn_weight_grads(val=0.05):\n",
    "    global bn_params\n",
    "    for bnp in bn_params:\n",
    "        bnp.grad = torch.clamp(bnp.grad, -val, val)\n",
    "        \n",
    "def get_bn_params_grads(val=0.05):\n",
    "    global bn_params\n",
    "    for bnp in bn_params:\n",
    "        if bnp.grad.abs().max() > val:\n",
    "            print(\"Batch Norm receiving high gradients!!\")\n",
    "            print(bnp.grad)\n",
    "            print()\n",
    "            \n",
    "def decay_bn_params(val=5e-5):\n",
    "    global bn_params\n",
    "    for bnp in bn_params:\n",
    "        bnp.data -= torch.sign(bnp.data)*val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_step(xx, yy):\n",
    "#     global dynet\n",
    "    \n",
    "#     yout = dynet(xx)\n",
    "#     loss = criterion(yout, yy) #+ dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "\n",
    "#     dynet.tree.optimizer.zero_grad(set_to_none = True)\n",
    "    \n",
    "#     loss.backward(create_graph=False, retain_graph=False)\n",
    "#     clip_bn_weight_grads()\n",
    "\n",
    "#     dynet.tree.optimizer.step()\n",
    "# #     dynet.zero_grad(True)\n",
    "    \n",
    "#     return yout, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_network_func():\n",
    "    global optimizer, warmup, loss_all, accs_all\n",
    "    \n",
    "    coeff0 = torch.zeros(1, requires_grad=True)\n",
    "    coeff1 = torch.zeros(1, requires_grad=True)\n",
    "    coeff2 = torch.zeros(1, requires_grad=True)\n",
    "    coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "    loss_list = []\n",
    "    prev_loss = None\n",
    "    beta_loss = (1000-1)/1000\n",
    "    loss_ = []\n",
    "    optimizer = dynet.tree.optimizer\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    breakall=False\n",
    "    \n",
    "    steps_ = -1\n",
    "    for epoch in range(train_epoch_max):\n",
    "        \n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "        for train_x, train_y in train_loader:\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "            steps_ += 1\n",
    "            \n",
    "#             dynet.decay_neuron_step()\n",
    "            dynet.tree.std_loss = 0.    \n",
    "\n",
    "            yout = dynet(train_x)\n",
    "            loss = criterion(yout, train_y) #+ dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "                    \n",
    "#             dynet.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=False)\n",
    "            \n",
    "            clip_bn_weight_grads()\n",
    "            optimizer.step()\n",
    "#             yout, loss = train_step(train_x, train_y)\n",
    "            \n",
    "            warmup.step()\n",
    "            \n",
    "            if steps_>100:\n",
    "                prev_loss = (1-beta_loss)*float(loss)+beta_loss*prev_loss\n",
    "                loss_list.append(prev_loss)\n",
    "            elif steps_ == 100:\n",
    "                loss_.append(float(loss))\n",
    "                prev_loss = np.mean(loss_)\n",
    "                loss_ = []\n",
    "            else:\n",
    "                loss_.append(float(loss))\n",
    "            \n",
    "            \n",
    "#             decay_bn_params()\n",
    "            \n",
    "            outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "            targets = train_y.data.cpu().numpy()\n",
    "\n",
    "            correct = (outputs == targets).sum()\n",
    "            train_acc += correct\n",
    "            train_count += len(outputs)\n",
    "\n",
    "            if steps_%100 == 0 and steps_>0:\n",
    "                if len(loss_list)>0:\n",
    "                    max_indx = np.argmax(loss_list)\n",
    "                    loss_list = loss_list[max_indx:]\n",
    "    #                 loss_all.append(float(loss))\n",
    "                \n",
    "                _x, _t, _y = update_coeff(50, coeff0, coeff1, coeff2, coeff_opt, loss_list)\n",
    "                _c = float(coeff0.data.cpu()[0])\n",
    "    #             if coeff2.data[0] > 0.5: ## this is a signal to reset optimizer\n",
    "                coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "                _info = f'ES: {epoch}:{steps_}, coeff:{_c:.3f}/{-5}, \\nLoss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "\n",
    "                ax.clear()\n",
    "                if len(_x)>0:\n",
    "                    ax.plot(_x, _t, c='c')\n",
    "                    ax.plot(_x, _y, c='m')\n",
    "                xmin, xmax = ax.get_xlim()\n",
    "                ymin, ymax = ax.get_ylim()\n",
    "                ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                ax2.clear()\n",
    "                if len(accs_all)>0:\n",
    "                    acc_tr = accs_all\n",
    "                    acc_te = accs_test\n",
    "                    if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                    if len(acc_te)>20: acc_te = acc_te[-20:]\n",
    "                    ax2.plot(acc_tr, marker='.', label=\"train\")\n",
    "                    ax2.plot(acc_te, marker='.', label=\"test\")\n",
    "                    ax2.legend(loc=\"lower right\")\n",
    "                    \n",
    "                    ymin, ymax = ax2.get_ylim()\n",
    "                    ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                    ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_te):.3f} end{acc_te[-1]:.3f}\")\n",
    "\n",
    "                \n",
    "                fig.canvas.draw()\n",
    "                plt.savefig(f\"./output/logs/_{index}_temp_train_plot.png\")\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                if _c < -5 and epoch>train_epoch_min: \n",
    "                    breakall=True\n",
    "                    break\n",
    "                    \n",
    "        if not breakall:\n",
    "            accs_all.append(train_acc/train_count*100.)\n",
    "            with torch.no_grad():\n",
    "                corrects = 0\n",
    "                dynet.eval()\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x  = test_x.to(device)\n",
    "                    yout = dynet.forward(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                    correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                    corrects += correct\n",
    "                dynet.train()\n",
    "                accs_test.append(corrects/len(test_dataset)*100)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_func():\n",
    "    global optimizer, warmup\n",
    "    reset_optimizer()\n",
    "#     optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#     warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader), power=0.5)\n",
    "    \n",
    "    optimizer = dynet.tree.optimizer\n",
    "    \n",
    "    \n",
    "    print(f\"Computing Network Siginificance\")\n",
    "    \n",
    "    dynet.eval()\n",
    "    dynet.start_computing_significance()\n",
    "\n",
    "    for train_x, train_y in train_loader:\n",
    "        train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "        dynet.tree.std_loss = 0.    \n",
    "        yout = dynet(train_x)\n",
    "#         yout.backward(gradient=torch.ones_like(yout))\n",
    "        loss = criterion(yout, train_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=False)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    dynet.finish_computing_significance()\n",
    "    \n",
    "    dynet.identify_removable_neurons(num=None,\n",
    "                                 threshold_min = threshold_min,\n",
    "                                 threshold_max = threshold_max)\n",
    "    num_remove = dynet.decay_neuron_start(decay_steps=num_decay_steps)\n",
    "    \n",
    "    dynet.train()\n",
    "    \n",
    "    if num_remove > 0:\n",
    "#     if num_remove < 0:\n",
    "        decayed = False\n",
    "        print(f\"pruning {num_remove} neurons.\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,4))\n",
    "        ax = fig.add_subplot(121)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        \n",
    "        loss_list = []\n",
    "        steps_ = -1\n",
    "        breakall=False\n",
    "\n",
    "        for epoch in range(train_epoch_max+int(np.ceil(num_decay_steps/len(train_loader)))):\n",
    "            loss_ = []\n",
    "            train_acc = 0\n",
    "            train_count = 0\n",
    "            \n",
    "            for train_x, train_y in train_loader:\n",
    "                train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "                steps_ += 1\n",
    "                \n",
    "#                 with torch.no_grad():\n",
    "                ret = dynet.decay_neuron_step()\n",
    "                dynet.tree.std_loss = 0.    \n",
    "        \n",
    "                if ret == -1 and not decayed:\n",
    "                    events_all.append((len(accs_all), \"neurons decayed\"))\n",
    "                    decayed = True\n",
    "                \n",
    "#                     copy_optimizer()\n",
    "#                     breakall = True\n",
    "#                     break\n",
    "\n",
    "                yout = dynet(train_x)\n",
    "                loss = criterion(yout, train_y) #+ dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "                \n",
    "                optimizer.zero_grad() ##set_to_none = True\n",
    "                loss.backward(retain_graph=False)\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss = float(loss)\n",
    "#                 yout, loss = train_step(train_x, train_y)\n",
    "                                \n",
    "                warmup.step()\n",
    "#                 decay_bn_params()\n",
    "                loss_.append(float(loss))\n",
    "                \n",
    "\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                targets = train_y.data.cpu().numpy()\n",
    "                correct = (outputs == targets).sum()\n",
    "                train_acc += correct\n",
    "                train_count += len(outputs)\n",
    "\n",
    "#                 dynet.decay_neuron_step()\n",
    "                \n",
    "                if steps_%50 == 0 and steps_>0:\n",
    "                    loss = np.mean(loss_)\n",
    "                    loss_ = []\n",
    "                    loss_list.append(loss)\n",
    "                \n",
    "                if steps_%100 == 0 and steps_>0:\n",
    "                    \n",
    "                    _info = f'ES: {epoch}:{steps_}, Loss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "#                     print(_info)\n",
    "                    ax.clear()\n",
    "                    out = (yout.data.cpu().numpy()>0.5).astype(int)\n",
    "                    ax.plot(loss_list)\n",
    "                    \n",
    "                    xmin, xmax = ax.get_xlim()\n",
    "                    ymin, ymax = ax.get_ylim()\n",
    "                    ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                    ax2.clear()\n",
    "                    if len(accs_all)>0:\n",
    "                        acc_tr = accs_all\n",
    "                        acc_te = accs_test\n",
    "                        if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                        if len(acc_te)>20: acc_te = acc_te[-20:]\n",
    "                        ax2.plot(acc_tr, marker='.', label=\"train\")\n",
    "                        ax2.plot(acc_te, marker='.', label=\"test\")\n",
    "                        ax2.legend(loc=\"lower right\")\n",
    "\n",
    "                        ymin, ymax = ax2.get_ylim()\n",
    "                        ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                        ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_te):.3f} end{acc_te[-1]:.3f}\")\n",
    "\n",
    "                    \n",
    "                    fig.canvas.draw()\n",
    "                    plt.savefig(f\"./output/logs/_{index}_temp_prune_plot.png\")\n",
    "#                     plt.pause(0.01)\n",
    "#                     print(\"\\n\")\n",
    "                    \n",
    "#                 if steps_>num_decay_steps+int(num_decay_steps/2): breakall=True\n",
    "#                 if steps_>(num_decay_steps+int(len(train_loader)*2.05)): breakall=True\n",
    "#                 if breakall: break\n",
    "\n",
    "#             if steps_>=(num_decay_steps):\n",
    "            if epoch >= (num_decay_steps/len(train_loader))+1.99:\n",
    "                breakall = True\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                corrects = 0\n",
    "                ret = dynet.decay_neuron_step()\n",
    "                dynet.eval()\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x  = test_x.to(device)\n",
    "                    yout = dynet.forward(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                    correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                    corrects += correct\n",
    "                dynet.train()\n",
    "                accs_test.append(corrects/len(test_dataset)*100)        \n",
    "\n",
    "            accs_all.append(train_acc/train_count*100.)\n",
    "\n",
    "#             if not breakall:\n",
    "#                 accs_all.append(train_acc/train_count*100.)\n",
    "#             else:\n",
    "#                 accs_all.append(accs_all[-1])\n",
    "#                 break\n",
    "            if breakall: break\n",
    "\n",
    "        plt.close()\n",
    "    \n",
    "    dynet.remove_decayed_neurons()\n",
    "    events_all.append((len(accs_all), \"neurons pruned\"))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1563"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintain_network():\n",
    "    dynet.compute_del_neurons()\n",
    "    dynet.maintain_network()\n",
    "    dynet.print_network()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network_stat(loop_indx):\n",
    "    stdout = sys.stdout\n",
    "    s = io.StringIO(newline=\"\")\n",
    "    sys.stdout = s\n",
    "    dynet.print_network()\n",
    "    sys.stdout = stdout\n",
    "    s.seek(0)\n",
    "    # prints = s.read()\n",
    "    architecture = s.getvalue()\n",
    "    s.close()\n",
    "    \n",
    "    ### number of neurons\n",
    "    count = 0\n",
    "    for hr in dynet.tree.DYNAMIC_LIST:\n",
    "        if hr.residual:\n",
    "            count += hr.residual.hidden_dim\n",
    "    \n",
    "    with open(f\"output/logs/{index}_{name}_log_{exp_index}.txt\", \"a+\") as f:\n",
    "        ### Print the configuration at top.\n",
    "#         if loop_indx == 0:\n",
    "        \n",
    "        if loop_indx >= 0:\n",
    "    \n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "            \n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%B %d, %Y @ %H:%M:%S\")\n",
    "            f.write(f\"DateTime: {dt_string}\")\n",
    "            \n",
    "            f.write(f\"num_add_neuron :{num_add_neuron}\\n add_to_remove_ratio :{dynet.tree.add_to_remove_ratio}\\n\")\n",
    "            f.write(f\"learning_rate :{learning_rate}\\n num_decay_steps :{num_decay_steps}\\n\")\n",
    "            f.write(f\"threshold_max :{threshold_max}\\n threshold_min :{threshold_min}\\n\")\n",
    "            f.write(f\"train_epoch_min :{train_epoch_min}\\n threshold_max :{train_epoch_max}\\n\")\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "        \n",
    "        f.write(f\"####################| Loop:{loop_indx} | Epoch: {len(accs_all)} \\n\")\n",
    "        num_params = sum(p.numel() for p in dynet.parameters())\n",
    "        num_trainable = sum(p.numel() for p in dynet.parameters() if p.requires_grad)\n",
    "        f.write(f\"| Dynamic Neurons:{count} | Total Parameters: {num_params} | Trainable Parameters: {num_trainable}\\n\")\n",
    "        f.write(f\"| Train Acc:{accs_all[-1]:.3f} | Test Acc: {accs_test[-1]:.3f}\\n\")\n",
    "        f.write(architecture)\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyperparameters_from_json():\n",
    "    global learning_rate,num_add_neuron,num_decay_steps,\\\n",
    "            remove_above,threshold_max,threshold_min,train_epoch_min,train_epoch_max,\\\n",
    "            dynet\n",
    "    with open(hyp_json, 'r') as fp:\n",
    "        hyps = json.load(fp)\n",
    "        learning_rate = hyps['learning_rate']\n",
    "        num_add_neuron = hyps['num_add_neuron']\n",
    "        num_decay_steps = hyps['num_decay_steps']\n",
    "        threshold_max = hyps['threshold_max']\n",
    "        threshold_min = hyps['threshold_min']\n",
    "        train_epoch_min = hyps['train_epoch_min']\n",
    "        train_epoch_max = hyps['train_epoch_max']\n",
    "        dynet.tree.add_to_remove_ratio = hyps['add_to_remove_ratio']\n",
    "        \n",
    "        \n",
    "    ############### Modifying to automate some meta/hyper-parameters\n",
    "#     epochs = len(accs_all)\n",
    "#     ###\n",
    "#     if epochs<100:\n",
    "#         learning_rate = 0.0009\n",
    "#         dynet.tree.add_to_remove_ratio = 2.0\n",
    "#         train_epoch_max = 12\n",
    "#     if epochs<200:\n",
    "#         learning_rate = 0.0005\n",
    "#         train_epoch_max = 12\n",
    "#         dynet.tree.add_to_remove_ratio = 2.5\n",
    "#     if epochs<400:\n",
    "#         learning_rate = 0.0003\n",
    "#         train_epoch_max = 15\n",
    "#     if epochs<600:\n",
    "#         learning_rate = 0.0003\n",
    "#         train_epoch_max = 17\n",
    "#         dynet.tree.add_to_remove_ratio = 0.9\n",
    "#     else:\n",
    "#         learning_rate = 0.00012\n",
    "#         train_epoch_max = 17\n",
    "#         dynet.tree.add_to_remove_ratio = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accs_save():\n",
    "    plt.plot(accs_all, label=\"train\")\n",
    "    plt.plot(accs_test, label=\"test\")\n",
    "    ymin, ymax = plt.gca().get_ylim()\n",
    "    plt.text(0, 0.8*ymin+0.2*ymax, f\"Train-> max:{max(accs_all):.3f} end:{accs_all[-1]:.3f} \\nTest-> max:{max(accs_test):.3f} end:{accs_test[-1]:.3f}\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"output/plots/{index}_{name}_cifar10_{exp_index}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    with open(f\"output/plots/{index}_{name}_cifar10_{exp_index}_event_dict.json\", 'w') as f:\n",
    "        d = {\n",
    "            \"train_accs\":accs_all,\n",
    "            \"test_accs\":accs_test,\n",
    "            \"event_dict\":events_all,\n",
    "        }\n",
    "        json.dump(d, f, indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduled_hyperparams():\n",
    "    hyp = {\n",
    "    'learning_rate':learning_rate,\n",
    "    'num_add_neuron':num_add_neuron,\n",
    "    'num_decay_steps':num_decay_steps,\n",
    "    'remove_above':remove_above,\n",
    "    'threshold_max':threshold_max,\n",
    "    'threshold_min':threshold_min,\n",
    "    'train_epoch_min':train_epoch_min,\n",
    "    'train_epoch_max':train_epoch_max,\n",
    "    'add_to_remove_ratio':dynet.tree.add_to_remove_ratio,\n",
    "    }\n",
    "    \n",
    "    ###### Schedule here\n",
    "        \n",
    "    \n",
    "    \n",
    "    ######################\n",
    "    hyp_json = f'hyperparameters/{index}_hyp_exp_{exp_index}.json'\n",
    "    with open(hyp_json, 'w') as fp:\n",
    "        json.dump(hyp, fp, indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_func():\n",
    "    load_hyperparameters_from_json()\n",
    "    plot_accs_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set all functions and begin automated loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AutoTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.adding_func = add_neurons_func\n",
    "trainer.training_func = training_network_func\n",
    "trainer.pruning_func = pruning_func\n",
    "trainer.maintainance_func = maintain_network\n",
    "trainer.log_func = save_network_stat\n",
    "trainer.extra_func = extra_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_neurons_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     8\n",
      "    ╔╝\n",
      "    8\n",
      "   ╔╝\n",
      "   16\n",
      "  ╔╝\n",
      "  32\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 56 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    8\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    14\n",
      "    ╠════╗\n",
      "    ║    5\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   22\n",
      "   ╠════╗\n",
      "   ║    6\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  41\n",
      "  ╠════╗\n",
      "  ║    5\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 39\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsuman/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significance Stat:\n",
      "Min, Max: (0.12826065719127655, 11.092744827270508)\n",
      "Mean, Std: (1.0, 1.3416097164154053)\n",
      "remove_below 0.2550874352455139 true: 77.07044157602726\n",
      "Significance:\n",
      "tensor([ 67.4062,  73.5764,  70.1190,  56.5900,  44.8301,  79.8592,  78.9389,\n",
      "         78.0007,  92.7123,  78.5168,  65.1355,  78.8198,  70.6237,  54.3473,\n",
      "         46.1785,  63.2722,  61.6082,  55.0093,  72.5021,  60.9426,  82.6729,\n",
      "         89.4161,  69.3275,  88.4484,  62.9747, 104.0415,  70.9720,  50.3373,\n",
      "         80.8957,  78.9744,  61.3377,  86.5592,  40.1867,  41.5220,  39.2094,\n",
      "         42.2890,  46.5260,  38.7518,  39.1228], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True,  True,  True,  True,  True, False, False, False, False, False,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False, False,  True, False,  True, False,  True,  True, False, False,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 56.6958, 102.5507, 153.1883, 102.5202, 104.2173, 203.0042, 111.2028,\n",
      "        169.2653,  80.4065, 245.9800,  64.8604, 170.1116, 106.1688, 194.7003,\n",
      "         63.2516,  77.0704, 189.3482, 112.8863, 115.3558, 322.1468,  78.6137,\n",
      "        128.4010, 114.6597,  47.8363, 110.0045, 107.7266, 258.8237,  67.6391,\n",
      "        356.7209, 102.5671, 107.0709, 119.1184, 381.6138, 335.2946, 227.6358,\n",
      "        227.3195, 198.1675, 270.1629, 232.9633, 274.7233, 780.8433],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False,  True,  True, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 97.8222, 265.3278,  49.3872, 153.1620, 213.4237], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([493.2021, 282.8423, 360.8333, 239.6743, 303.1845, 417.9938, 369.6912,\n",
      "        284.7890, 210.6282, 659.0669, 457.1249, 410.0320], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([165.0686, 186.0570, 204.1390, 149.2806, 284.4529, 149.3264, 143.8697,\n",
      "        182.9161], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([501.2542, 274.6230, 285.5131, 384.1441, 255.5856, 252.7584],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([233.6798, 229.8220, 169.3404, 237.2739, 128.8971], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1132.6051,  235.6026,  400.5230,  438.7378,  346.5626,  451.1811,\n",
      "         388.9677,  284.6178,  587.8589,  648.8040,  550.6238,  286.4631,\n",
      "         398.3553,  783.0162,  426.5614,  674.9136,  367.5362,  290.5634,\n",
      "        1179.8687,  803.9514,  459.8105,  489.8224], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 259.3824, 1406.7853,  512.2612, 1230.1005,  569.1912,  421.7971,\n",
      "         366.9997,  463.3230,  597.2344,  770.2312, 3351.4890, 1440.8805,\n",
      "        2457.3088,  928.7698], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False], device='cuda:0')\n",
      "pruning 35 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    6\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    14\n",
      "    ╠════╗\n",
      "    ║    5\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   22\n",
      "   ╠════╗\n",
      "   ║    6\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  35\n",
      "  ╠════╗\n",
      "  ║    4\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :0 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 58 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    11\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    24\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   28\n",
      "   ╠════╗\n",
      "   ║    16\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  42\n",
      "  ╠════╗\n",
      "  ║    10\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 15\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.06651939451694489, 7.46662712097168)\n",
      "Mean, Std: (1.0, 1.1169103384017944)\n",
      "remove_below 0.3578045666217804 true: 107.42825739457605\n",
      "Significance:\n",
      "tensor([184.4965, 166.7239, 184.9507, 197.7437, 290.8745, 232.8596, 296.5729,\n",
      "        264.3078, 198.4352, 198.5280, 213.3776, 226.9073, 229.6406,  34.4481,\n",
      "         32.0012], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 68.2835, 137.2846, 105.6466,  79.7091, 139.8283, 143.5186, 156.6080,\n",
      "        105.1520, 311.4923,  71.9662, 274.1456, 185.5858, 144.6332, 156.6717,\n",
      "        175.7359, 297.2200, 129.0827, 137.8267,  82.4857, 307.7249, 269.3771,\n",
      "        446.1249, 495.4528, 113.6830, 198.6391, 175.4749, 507.8141, 481.2852,\n",
      "        255.6561, 426.5013, 248.1808, 180.0569, 260.1685, 242.1589, 470.3322,\n",
      "         57.4761, 163.8584,  80.2691,  56.7896, 113.3631,  84.2084, 169.5208],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False,  True,  True, False, False, False,  True, False,  True,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False,  True, False, False,  True,  True,  True,  True, False,\n",
      "         True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 57.7138,  77.9713, 173.4453,  99.4169, 107.4283, 134.9169,  21.5324,\n",
      "        115.1497, 158.8972, 186.2243], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True,  True, False,  True,  True, False,  True, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 376.9687,  319.4740, 1006.3973,  303.0385,  459.4906,  359.1143,\n",
      "        1544.0989,  441.5706,  404.4531,  455.5790,  459.5906, 1086.5304,\n",
      "         104.4239,   19.9720,   77.4100], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([130.0466, 170.0439, 150.3479, 143.0522, 143.2922, 132.8917, 192.7700,\n",
      "        109.4951, 154.7358, 126.9597, 133.5773], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 93.8484, 216.1656, 183.4599, 175.5297, 232.6876, 146.6624, 157.3987,\n",
      "        179.6359, 140.4651, 187.7516, 145.1125, 150.1975, 140.6937, 134.7069,\n",
      "        189.4288,  73.2709], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([304.4643, 162.3809,  79.1574, 146.9712, 136.1569,  91.3143, 150.5508,\n",
      "         94.9162, 152.5147, 178.9242, 107.0863,  91.6154, 157.9660,  83.1124],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False,  True, False,  True, False, False,\n",
      "         True,  True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1128.7415,  245.3237,  364.6189,  363.1830,  240.3951,  566.2513,\n",
      "         562.4787,  279.5397,  309.1566,  596.6788,  521.9572,  306.4388,\n",
      "         203.2607,  893.5911,  478.3693,  703.7560,  294.0900,  184.5902,\n",
      "         825.4807,  777.6038,  485.0574,  505.6205,  151.6595,  200.6835,\n",
      "         172.9824,  126.1827,  128.8753,   91.7207], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 616.6129, 1149.3593,  445.7531, 1726.5559,  578.9717,  297.5762,\n",
      "         487.3655,  629.0996,  601.1960, 1071.2498, 2241.8013, 1019.8409,\n",
      "        2058.9675,  590.7301,   56.2631,   92.1090,   88.3974,  145.7519,\n",
      "         127.9858,  163.3438,   68.6788,   98.6901,   87.1653,  109.7161],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False,  True,  True,  True, False, False, False,\n",
      "         True,  True,  True, False], device='cuda:0')\n",
      "pruning 41 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    11\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    17\n",
      "    ╠════╗\n",
      "    ║    8\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   27\n",
      "   ╠════╗\n",
      "   ║    13\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  29\n",
      "  ╠════╗\n",
      "  ║    5\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 12\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :1 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 59 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     17\n",
      "     ╠════╗\n",
      "     ║    17\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    27\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   38\n",
      "   ╠════╗\n",
      "   ║    24\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  33\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 14\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.00742300832644105, 8.113249778747559)\n",
      "Mean, Std: (0.9999998807907104, 1.2195221185684204)\n",
      "remove_below 0.29759481549263 true: 76.55669393346928\n",
      "Significance:\n",
      "tensor([236.0526, 191.9834, 195.3504, 233.0020, 252.4088, 285.9362, 211.7669,\n",
      "        229.0097, 191.2633, 279.8403, 218.1770, 229.6564,   8.3584,   5.0359],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([104.6482, 169.7905, 281.5243,  96.1033, 269.0892, 196.2884, 273.6703,\n",
      "        171.8159, 293.5569,  98.3347, 270.0356, 288.4097, 131.0300, 345.3578,\n",
      "        204.8972, 927.2125, 690.1163, 117.4732, 216.0712, 226.5753, 285.7072,\n",
      "        356.3004, 255.4840, 345.2015, 213.7962, 271.4040, 375.0569, 150.5194,\n",
      "        189.4859,  62.6644,  41.9799,  39.5319,  68.3403], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False,  True, False, False, False, False,  True,\n",
      "         True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([149.5545, 138.9623, 158.5061, 150.1911, 147.1440,  88.8463,  88.9986,\n",
      "        123.6793, 100.4678], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 280.5349,  452.1785, 1360.2493,  398.3713,  490.5121,  248.6493,\n",
      "        1834.8273,  467.3186,  429.0440,  580.8187,  391.9632, 1260.1124,\n",
      "           1.9096,   24.9611,    3.6033,   39.9838,   18.5812],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False,  True,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([130.0329, 152.7782, 146.6241,  93.1809, 180.3865, 111.6760, 150.1826,\n",
      "         63.0054, 170.6582,  86.1342, 114.1871,  95.5654, 127.4428,  98.1102,\n",
      "         71.9920,  72.4019, 116.9985], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False,  True, False, False,  True, False, False,\n",
      "        False, False, False, False,  True,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([136.5005, 202.4073, 212.7285, 129.2814, 195.3591, 110.2803,  76.5567,\n",
      "         88.6934, 110.4151, 113.9935, 142.1297,  71.6774, 185.9023, 107.6621,\n",
      "         83.3689,  90.3048,  70.8284,  79.8631,  82.2999, 133.4817, 163.2863,\n",
      "         87.5914,  98.6911,  97.4473], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False,\n",
      "        False,  True, False, False, False, False,  True, False, False,  True,\n",
      "        False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([186.3271, 118.4802, 164.1827, 160.3662, 182.4869,  88.1412, 205.6940,\n",
      "        175.4842, 148.0972, 120.4719,  89.2528,  81.9884,  87.4283, 109.2521],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([607.9359, 226.5076, 135.2504, 216.8736, 295.2712, 309.1591, 354.4174,\n",
      "        220.8544, 198.9831, 722.9744, 488.2333, 236.6826, 190.3182, 874.2848,\n",
      "        282.3190, 618.0178, 166.0976, 174.9782, 419.7491, 535.9161, 361.6967,\n",
      "        469.1008, 175.0833, 296.7323, 183.9761, 144.1497, 185.2003,  77.8703,\n",
      "         57.3525,  70.1347,  43.8421,  75.6232,  49.4529,  68.3040,  75.7231,\n",
      "         36.3501,  93.3112,  81.6189], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 595.6550,  759.5385,  202.1984, 1392.5420,  463.9462,  282.8096,\n",
      "         506.6749,  639.8900,  672.0167, 1056.5063, 2087.1450, 1940.8046,\n",
      "         831.6924,  157.0726,  220.5269,  238.7433,  213.5931,   52.9871,\n",
      "          75.5975,   46.7802,   68.2354,   68.6153,   52.8153,   38.2620,\n",
      "          57.9139,   72.9746,   38.6000], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True], device='cuda:0')\n",
      "pruning 46 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    12\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    15\n",
      "    ╠════╗\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   28\n",
      "   ╠════╗\n",
      "   ║    20\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  27\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 12\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :2 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 60 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     16\n",
      "     ╠════╗\n",
      "     ║    20\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    22\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   35\n",
      "   ╠════╗\n",
      "   ║    34\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  32\n",
      "  ╠════╗\n",
      "  ║    13\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.005163060966879129, 8.67380428314209)\n",
      "Mean, Std: (1.0, 1.3716901540756226)\n",
      "remove_below 0.26607275009155273 true: 72.67564378836268\n",
      "Significance:\n",
      "tensor([254.7823, 229.8597, 209.2805, 217.1361, 214.3722, 303.0735, 227.5161,\n",
      "        215.9943, 210.0065, 300.9862, 188.1518, 347.3871,   9.5896],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([251.0152, 252.9431, 294.3458, 106.1933, 177.5886, 214.3625, 289.6324,\n",
      "        141.3877, 381.4670, 148.8855, 226.5622, 433.0447, 212.4873, 296.1389,\n",
      "        229.8009, 779.0522, 896.9317, 241.1936, 251.5962, 893.4510, 366.6473,\n",
      "        460.6917, 452.1788, 205.7901, 244.5649, 112.2264, 417.0927,  33.2985,\n",
      "         62.5241, 118.2411,  42.9731,  24.4958], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True,  True, False,\n",
      "         True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 78.8054,  64.6318, 103.6338, 116.8832, 142.0074, 115.8888, 135.7756,\n",
      "         83.7416,  99.5397, 113.2387, 101.2889,  84.9211, 100.2802],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([3.6868e+02, 4.3709e+02, 1.8493e+03, 6.2409e+02, 1.8194e+02, 2.2160e+03,\n",
      "        4.8770e+02, 4.2382e+02, 6.5866e+02, 5.9550e+02, 1.7535e+03, 5.5137e+00,\n",
      "        4.1504e+01, 2.2193e+01, 2.8968e+00, 1.4102e+00], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 72.6756, 176.6439,  89.1023,  98.4468, 209.2879, 193.9420,  78.1861,\n",
      "         91.0284, 102.2014,  94.7592,  85.4214, 165.2491,  95.4114,  60.2274,\n",
      "        141.5349, 127.7384, 120.7761,  98.6478, 147.4793,  63.8524],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([326.3044, 199.8273, 212.1349, 100.1012, 136.8969,  87.9635, 168.4340,\n",
      "        105.0441,  80.4375, 129.0767,  97.8752,  65.8793, 155.0609, 129.1012,\n",
      "         77.1980,  56.2946, 112.8140,  67.4702, 133.9659,  69.4028, 118.1054,\n",
      "        113.3392,  89.0113,  98.4777,  76.6680, 148.7200, 134.8164, 172.3698,\n",
      "         83.5672,  81.2733,  66.9505, 116.5724, 160.0323,  85.2023],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False,  True, False,  True, False,  True, False,  True,\n",
      "        False, False, False,  True, False, False,  True, False, False, False,\n",
      "         True, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([180.9010,  91.4763,  98.1278, 175.2103,  65.1789, 174.3994, 131.0092,\n",
      "        111.0595,  55.1032,  63.3773,  85.5610,  99.4314,  77.7620,  37.1658,\n",
      "        142.1655,  49.7501,  56.6079,  95.2487,  80.5312,  80.6611,  77.7534,\n",
      "         90.0362], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False,  True, False,  True,  True,\n",
      "        False, False, False,  True, False,  True,  True, False, False,  True,\n",
      "        False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([728.9683, 259.4085, 182.4017, 231.0553, 239.5338, 255.7444, 394.4219,\n",
      "        180.9994, 778.0464, 603.0919, 263.9372, 228.2972, 966.9678, 362.2487,\n",
      "        656.4623, 269.3884, 283.9065, 508.9472, 694.2197, 576.2894, 540.0779,\n",
      "        243.1111, 352.1808, 215.8894, 237.2645, 324.0890, 173.5679, 158.5756,\n",
      "         43.8809,  59.4976,  61.8288, 140.3413,  54.3621,  40.2253,  80.3284],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 907.4529,  285.3600, 1920.7845,  524.1211,  423.7417,  461.5035,\n",
      "         795.5746, 1142.5793, 2369.1802, 2350.0898,  693.3488,  308.1540,\n",
      "         326.8500,  239.8310,  343.7823,   48.2559,   34.0946,   34.1187,\n",
      "          53.6442,   87.2001,   48.4184,   67.8062], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False,  True, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True,  True, False,\n",
      "         True,  True], device='cuda:0')\n",
      "pruning 48 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    17\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    14\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   28\n",
      "   ╠════╗\n",
      "   ║    26\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  27\n",
      "  ╠════╗\n",
      "  ║    11\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 11\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :3 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 61 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     17\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ║    21\n",
      "     ║    ╠════╗\n",
      "     ║    ║    6\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    20\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   39\n",
      "   ╠════╗\n",
      "   ║    37\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  30\n",
      "  ╠════╗\n",
      "  ║    17\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 11\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.001074771280400455, 9.493549346923828)\n",
      "Mean, Std: (0.9999999403953552, 1.3864378929138184)\n",
      "remove_below 0.22489358484745026 true: 53.59285204787792\n",
      "Significance:\n",
      "tensor([247.4422, 218.6078, 221.2809, 226.7949, 268.9646, 265.3576, 253.2589,\n",
      "        201.4901, 272.8151, 185.7693, 273.2441], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([397.4634, 224.7571, 144.8741, 179.7491, 181.9615, 207.8529, 128.0078,\n",
      "        504.6496, 161.1445, 285.6605, 570.9297, 268.1292, 319.9245, 328.7996,\n",
      "        831.4922, 579.6236, 267.7126, 150.5068, 813.8760, 248.0247, 639.4599,\n",
      "        427.6287, 263.6325, 621.8047, 117.1190, 450.4514, 263.3082,  25.2527,\n",
      "         54.3104,  29.6962], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False,  True,  True, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([134.2458,  53.3942, 155.8717,  92.8969,  78.6083,  66.8194,  77.7014,\n",
      "        114.7291, 113.7196, 122.0296, 231.8135,  72.6564,  75.1263,  48.0250,\n",
      "         62.9976,  51.9317,  61.9412], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([3.0408e+02, 3.9539e+02, 2.0500e+03, 4.1737e+02, 2.0185e+02, 1.9552e+03,\n",
      "        5.2639e+02, 3.6033e+02, 5.5420e+02, 5.5378e+02, 1.4088e+03, 1.1844e+00,\n",
      "        2.5612e-01, 5.1854e+00, 3.1549e+00, 4.8077e+00, 9.5255e-01],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True,  True,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([128.8819,  80.1117,  99.0305, 157.3825, 160.2573,  80.8524, 105.1324,\n",
      "        111.2532,  85.8036,  80.3040, 134.0667,  75.6820, 115.2846, 131.1769,\n",
      "        107.4330, 124.9710, 117.9579,  85.9412,  72.9449,  57.6594,  53.5928],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([277.2855, 111.9036,  97.2094,  58.2455, 110.7937, 151.2080,  31.7488,\n",
      "         74.2744, 101.2427,  97.7932,  94.9129, 125.7352,  52.2353,  78.4270,\n",
      "        113.3096,  82.2729,  50.9609,  52.9117,  60.2856,  56.3463,  75.9562,\n",
      "        102.8618, 158.4305,  92.0274,  56.7037,  74.7483,  79.0162,  26.5651,\n",
      "         65.9408, 109.3345,  88.4953,  54.0328,  11.6620, 141.4516,  46.0480,\n",
      "        109.4553, 118.7346], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False,\n",
      "         True, False,  True, False, False, False,  True,  True, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False,  True, False,  True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([107.5535, 125.1196,  82.9745, 130.1674, 203.0543, 134.0425,  97.8693,\n",
      "         65.5621, 106.3121, 101.1128, 119.7750,  83.7504,  92.8726, 143.8567,\n",
      "         56.8355, 109.5578,  49.6802,  42.3277,  63.6945,  67.3396],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([507.6036, 403.3728, 201.9994, 288.4248, 253.1052, 275.2428, 423.5617,\n",
      "        215.7785, 735.4869, 284.7727, 118.1947, 683.3323, 350.3987, 576.1467,\n",
      "        217.5777, 238.3700, 621.2489, 615.1010, 257.1153, 473.5222, 189.0673,\n",
      "        262.0566, 258.8185, 335.7256, 285.4797, 139.8666, 179.4677, 110.8671,\n",
      "         35.9835,  33.1943,  22.6066,  37.5144,  25.6568,  32.3604,  47.0486,\n",
      "         15.2581,  27.8460,  21.5350,  57.7241], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([41.9346, 88.3497, 48.8249, 32.5598, 75.5516, 46.0201], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False,  True,  True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 857.8676,  347.4587, 1510.3134,  337.4574,  630.0065,  883.4147,\n",
      "        1923.6473, 2262.3428,  849.9313,  412.7209,  300.9048,  384.1280,\n",
      "         269.0987,  298.5355,  111.1262,  178.5775,   89.8796,  122.8869,\n",
      "         120.5762], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([102.6707,  48.7340,  72.6914], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False], device='cuda:0')\n",
      "pruning 40 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    2\n",
      "     ║    ╠════╝\n",
      "     ║    20\n",
      "     ║    ╠════╗\n",
      "     ║    ║    2\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    18\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   29\n",
      "   ╠════╗\n",
      "   ║    29\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  26\n",
      "  ╠════╗\n",
      "  ║    14\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 11\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :4 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 62 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    24\n",
      "     ║    ╠════╗\n",
      "     ║    ║    6\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    23\n",
      "    ╠════╗\n",
      "    ║    23\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   35\n",
      "   ╠════╗\n",
      "   ║    ╠════╗\n",
      "   ║    ║    2\n",
      "   ║    ╠════╝\n",
      "   ║    41\n",
      "   ║    ╠════╗\n",
      "   ║    ║    5\n",
      "   ║    ╠════╝\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  29\n",
      "  ╠════╗\n",
      "  ║    23\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 12\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.005058723501861095, 10.516327857971191)\n",
      "Mean, Std: (0.9999999403953552, 1.4535679817199707)\n",
      "remove_below 0.2201673984527588 true: 48.992886097328196\n",
      "Significance:\n",
      "tensor([207.8996, 231.6347, 237.8504, 229.0594, 297.1783, 304.2198, 286.1101,\n",
      "        191.3970, 225.9721, 216.0484, 280.2292,   9.6422], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 419.2305,  454.6172,  157.3266,  179.3204,  121.7711,  108.7334,\n",
      "         110.1193,  539.1961,  215.7775,  368.8929,  619.1882,  254.5753,\n",
      "         366.3568,  365.8321, 1376.8783,  398.2682,  170.5677,  230.7383,\n",
      "         817.5040,  264.7642,  794.5648,  664.0648,  365.4904,  142.0838,\n",
      "         343.6647,  111.3669,   92.7360,   88.8303,   46.5300],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([127.3191,  78.9881,  62.2173,  50.9448,  35.6299, 193.9569, 132.7518,\n",
      "        115.3961, 127.0487, 114.2103,  40.2081,  93.7215, 125.8637,  53.5559,\n",
      "         37.4684, 185.4757, 153.8096,  75.9675,  53.8783, 130.9133,  30.6281,\n",
      "         72.5988,  97.9609], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "         True, False, False, False,  True, False,  True, False,  True, False,\n",
      "         True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([2.8032e+02, 4.2269e+02, 1.9946e+03, 4.5920e+02, 1.7624e+02, 2.3402e+03,\n",
      "        4.9584e+02, 5.1570e+02, 5.2101e+02, 5.3868e+02, 1.5803e+03, 1.1257e+00],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False,  True, False, False,\n",
      "        False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([113.3936,  82.5759,  98.9563, 136.3895, 151.1204, 100.1324,  74.4844,\n",
      "         63.9095,  93.8162,  81.5758, 109.1081,  59.5321, 106.4231, 164.1301,\n",
      "        115.4144, 118.1904, 126.7416,  95.5096,  67.1888,  45.6192,  48.9929,\n",
      "         64.8527,  58.8083,  79.7974], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "         True, False,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([178.0024,  50.4930, 121.8988,  82.2737,  70.4334,  89.1020,  89.3657,\n",
      "         89.8794,  92.7210,  94.9905,  55.0491,  48.1106, 116.2021,  87.4166,\n",
      "         59.9683,  74.4822, 181.9857,  79.4143,  64.6738,  46.5341,  75.8428,\n",
      "         62.5816, 140.3341, 115.2183,  80.0085,  90.6553, 110.0065,  59.7384,\n",
      "         86.9468,  44.1448,  67.5649,  66.1352,  51.0442,  19.1770,  40.8587,\n",
      "         50.1512,  94.5033, 105.3841,  68.6331,  66.9548,  49.3813],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False,  True, False,  True,  True, False, False, False, False, False,\n",
      "        False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([113.5830, 132.9965,  62.6710,  90.5493, 181.0824,  93.4465,  66.9546,\n",
      "         42.5522,  97.5488,  88.3195, 125.3670,  65.8408,  81.6612, 125.2840,\n",
      "         58.3183,  45.7975,  96.4683,  76.5107,  69.5854,  67.3507,  28.8328,\n",
      "         82.7643,  87.2453], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "         True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([20.2593, 21.9217, 30.9160, 21.9238, 24.9623], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True, True, True, True, True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([565.1973, 430.6360, 215.7276, 219.5827, 297.9268, 308.9961, 412.0822,\n",
      "        166.0298, 611.2675, 377.6667, 143.7643, 687.8206, 325.7725, 465.7634,\n",
      "        301.8238, 254.2931, 701.8733, 663.1985, 239.2069, 547.2599, 172.5506,\n",
      "        347.4951, 196.2543, 326.1274, 308.5541, 175.6270, 250.9342,  90.7526,\n",
      "         59.1350,  26.1309,  40.5981,  44.9179,   8.9258,  50.1609,  20.5297],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([41.4246, 34.3009], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True, True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([112.8039, 104.8385,  81.3974,  54.4676,  51.1530,  56.2132],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 647.1288,  322.4896, 1149.7428,  314.4479,  579.0744,  575.5778,\n",
      "        1948.9598, 2094.3325,  371.0747,  382.9132,  358.3873,  313.4816,\n",
      "         279.9110,  362.0931,  177.1724,  169.5072,  327.0816,  228.0769,\n",
      "          30.5593,   38.7188,   38.4080,   44.9937,   37.8759],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True,  True, False, False,\n",
      "        False, False, False, False, False, False,  True, False,  True,  True,\n",
      "         True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([60.1775, 70.8187, 51.9186, 47.9626, 36.0818, 46.7270, 55.9974],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True, False], device='cuda:0')\n",
      "pruning 49 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     9\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    4\n",
      "     ║    ╠════╝\n",
      "     ║    21\n",
      "     ║    ╠════╗\n",
      "     ║    ║    6\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    15\n",
      "    ╠════╗\n",
      "    ║    20\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   28\n",
      "   ╠════╗\n",
      "   ║    35\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  28\n",
      "  ╠════╗\n",
      "  ║    17\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 10\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :5 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 63 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     10\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ║    28\n",
      "     ║    ╠════╗\n",
      "     ║    ║    9\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    20\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    2\n",
      "    ║    ╠════╝\n",
      "    ║    30\n",
      "    ║    ╠════╗\n",
      "    ║    ║    1\n",
      "    ║    ╠════╝\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   30\n",
      "   ╠════╗\n",
      "   ║    ╠════╗\n",
      "   ║    ║    3\n",
      "   ║    ╠════╝\n",
      "   ║    44\n",
      "   ║    ╠════╗\n",
      "   ║    ║    2\n",
      "   ║    ╠════╝\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    3\n",
      "  ║    ╠════╝\n",
      "  ║    23\n",
      "  ║    ╠════╗\n",
      "  ║    ║    3\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 12\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.0001826809166232124, 12.370390892028809)\n",
      "Mean, Std: (1.0, 1.4701883792877197)\n",
      "remove_below 0.19167546927928925 true: 40.071585397503895\n",
      "Significance:\n",
      "tensor([2.4091e+02, 2.1182e+02, 2.4445e+02, 2.7127e+02, 2.8031e+02, 3.1012e+02,\n",
      "        2.1904e+02, 1.8902e+02, 2.4607e+02, 2.7749e+02, 3.8191e-02, 1.5728e-01],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([67.4826, 44.7173], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 372.5736,  407.9323,  318.4057,  169.6782,  158.2557,  296.0450,\n",
      "         454.1724,  701.9410,  255.3447,  345.1548,  750.8831,  361.9875,\n",
      "         293.1056,  518.0622,  955.6694,  452.2209,  139.6044,  188.8844,\n",
      "        1201.2141,  701.0506,  738.4424,  499.9371,  694.5107,  246.5381,\n",
      "         179.3464,  107.0251,  123.5338,  125.1041,  117.2058,   89.7676,\n",
      "          45.8207], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False,  True,\n",
      "        False,  True, False, False, False,  True, False, False, False, False,\n",
      "        False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 96.2056, 109.3422,  51.0287,  73.2245, 127.1972, 111.3047,  80.8255,\n",
      "        138.1321, 102.8237,  72.3100,  73.8688, 124.9116,  74.2379,  61.9360,\n",
      "         43.8867, 110.3742,  76.9255,  21.7455,  16.2518,   1.8055,  77.1204,\n",
      "         54.1445,  73.9366], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True,  True,  True,\n",
      "        False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([8.0340, 7.5931, 7.5019], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True, True, True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 335.9132, 2281.8472,  434.5749,  153.9062, 2586.1482,  359.7455,\n",
      "         582.1501,  721.7855, 1230.4377,    6.7892], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([109.5970,  74.3886,  69.3631, 105.8325, 127.6789,  87.4899,  58.8094,\n",
      "         66.7756,  70.2260,  85.0399, 107.6302,  58.7963, 107.3618, 111.3639,\n",
      "        106.9776, 120.2184, 106.9374, 124.5105,  73.1878,  54.8915, 111.5072,\n",
      "         59.5465,  37.2783,  88.3397,  55.7491,  71.9947,  60.2517,  50.7995],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False,  True, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 78.7525,  71.8000,  67.0285,  99.1271,  79.7163,  84.0550,  47.5532,\n",
      "        153.4310,  91.0430,  65.9759,  61.9845, 120.4036,  54.1268,  94.3403,\n",
      "         49.7126,  79.4296,  45.8738,  73.6473,  57.6535,  74.9574,  37.9274,\n",
      "         56.8033,  93.3951,  58.1587,  76.9431,  62.7769,  47.7398,  62.8426,\n",
      "         72.1777,  39.9822,  42.9383, 123.1223,  57.2388,  66.4885, 106.6770,\n",
      "         24.5819,  28.0611,  34.9428,  35.4198,  19.5489,  63.3427,  31.9285,\n",
      "         36.3354,  33.3776], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([118.2509,  90.6442,  44.0416,  83.2160, 154.2668,  92.6477,  73.1050,\n",
      "         81.2228, 116.6933,  78.7039,  75.4729,  73.1887,  99.2901,  81.8619,\n",
      "         78.6148,  85.6644,  47.8594,  70.0227,  62.7843,  72.5053,  19.6331,\n",
      "         63.4956,  58.9716,  17.3457,  40.0716,  50.4445,  31.0791,  13.1738,\n",
      "         22.8455,  12.3665], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False,  True,  True, False,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([29.5673, 15.0150], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True, True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([533.2313, 451.2110, 300.6854, 208.8627, 246.8432, 290.9835, 498.9915,\n",
      "        188.0078, 712.7034, 328.1256, 848.2891, 289.3296, 539.6370, 271.7837,\n",
      "        238.5747, 701.9141, 784.2555, 299.4902, 706.3373, 182.9649, 357.9421,\n",
      "        194.3334, 259.9513, 415.1089, 150.0469, 246.1685,  95.6422,  56.3482,\n",
      "         36.9354,  26.7473], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 33.8691, 216.7335,  33.8345], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([20.8481], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 98.8066, 114.7548,  96.0009,  36.9619,  68.0806,  84.5117,  61.9583,\n",
      "         73.1396,  73.8032], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False,  True, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([181.2466,  34.2644,  18.0335], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 894.2380,  478.7211,  834.1259,  403.3640,  469.3479,  601.0778,\n",
      "         666.6448,  324.3460,  320.7370,  298.8802,  287.1670, 1299.4355,\n",
      "         397.9144,  207.2808, 1272.0265,   51.8223,   47.7029,   45.0751,\n",
      "          84.3077,   33.8687], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([69.8776, 82.7033, 92.8270, 84.3306, 80.8761], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False], device='cuda:0')\n",
      "pruning 50 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     8\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ║    25\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    2\n",
      "    ║    ╠════╝\n",
      "    ║    21\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   28\n",
      "   ╠════╗\n",
      "   ║    ╠════╗\n",
      "   ║    ║    1\n",
      "   ║    ╠════╝\n",
      "   ║    33\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  27\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    1\n",
      "  ║    ╠════╝\n",
      "  ║    19\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 10\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :6 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 64 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    11\n",
      "     ║    ╠════╝\n",
      "     ║    34\n",
      "     ║    ╠════╗\n",
      "     ║    ║    12\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    31\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   32\n",
      "   ╠════╗\n",
      "   ║    ╠════╗\n",
      "   ║    ║    2\n",
      "   ║    ╠════╝\n",
      "   ║    40\n",
      "   ║    ╠════╗\n",
      "   ║    ║    2\n",
      "   ║    ╠════╝\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  28\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    3\n",
      "  ║    ╠════╝\n",
      "  ║    23\n",
      "  ║    ╠════╗\n",
      "  ║    ║    4\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 10\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.009520367719233036, 11.876483917236328)\n",
      "Mean, Std: (1.0, 1.4058972597122192)\n",
      "remove_below 0.17688114941120148 true: 33.88407667896805\n",
      "Significance:\n",
      "tensor([271.5366, 197.4172, 227.6405, 341.3146, 267.7436, 329.8972, 284.3086,\n",
      "        199.5246, 237.1721, 222.6336], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([38.7199, 44.8341, 45.8933, 53.4759, 29.7483], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 455.8025,  497.7055,  304.4690,  192.2207,  394.8607,  256.1620,\n",
      "         423.8646,  602.5496,  270.9863,  332.4480,  640.1234,  333.2491,\n",
      "         311.9380,  629.3256, 1081.5551, 1027.9656,  178.1754, 1258.1357,\n",
      "         282.6435,  506.4922,  112.2722,  139.8118,  196.9600,  225.7888,\n",
      "         239.9773,  248.0702,  103.3380,   32.2326], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False,  True, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([108.5860,  85.9202,  37.1743, 166.7706, 177.3185,  64.8313, 100.2899,\n",
      "         57.4524, 105.1794,  97.4744, 113.0145, 149.2685,  36.3892,  94.4437,\n",
      "        108.6058, 112.5236,  41.7316,  52.5196,  53.9809,  60.2030,  64.4937,\n",
      "         76.4923,  33.2441], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([23.0056, 10.2147, 10.9958,  8.5155], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True, True, True, True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([3.6567e+02, 2.2751e+03, 4.2933e+02, 3.3041e+02, 4.8375e+02, 5.1530e+02,\n",
      "        5.1667e+02, 1.6535e+03, 2.5656e+01, 1.6132e+02, 1.8238e+00],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "         True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 94.4629,  57.5745,  60.8852, 117.7773, 115.3580,  68.2482,  61.4810,\n",
      "        102.4082,  96.0132, 106.4650,  54.7881, 113.3997,  84.1298,  82.3067,\n",
      "         74.0921,  97.9709,  66.0115,  56.3014,  43.8478,  76.0252,  94.4161,\n",
      "         67.3054,  99.0720,  74.5729,  62.6262,  25.5575,  28.6076,  68.9633,\n",
      "         57.5510,  50.1922,  42.0943,  37.6432,  61.7435,  47.8998],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True,  True, False, False, False,\n",
      "         True, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 99.8331,  62.4268,  90.1554,  50.0949,  46.4665,  41.6859,  99.6137,\n",
      "         90.2698,  61.1917, 100.5590,  86.1772,  24.0614, 123.3284,  54.3423,\n",
      "        104.5979,  85.2070,  98.2748, 117.0149,  40.3415,  68.1404,  37.3560,\n",
      "         58.2807,  75.6789, 104.0998, 121.4854,  80.9753,  42.5876, 105.6844,\n",
      "         88.5616,  60.5135,  70.5237,  67.0575,  32.7467,  40.6158,  11.3085,\n",
      "         19.6864,  67.8995,  39.5076,  60.0960,   9.8832], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False,  True,  True, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([101.8680,  42.4519,  75.8467, 156.8930, 101.5210,  62.2803, 128.6048,\n",
      "         77.0593,  62.5116,  83.7312, 124.9168,  76.2523,  74.3951, 102.3950,\n",
      "         81.5565,  92.8988, 125.8168,  25.1827,  64.6909,  54.6665,  48.6795,\n",
      "         48.4235,  36.7511,  27.6346,  19.4129,  42.6858,  27.8922,  23.2291,\n",
      "         18.0243,  29.7571,  31.0289], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False,  True, False, False,\n",
      "        False, False, False,  True,  True, False,  True,  True,  True,  True,\n",
      "         True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([24.3152, 25.8660], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True, True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([388.1269, 388.7854, 340.3953, 271.5415, 336.1530, 356.0113, 525.9142,\n",
      "        209.5902, 715.8862, 314.6422, 782.7382, 234.7091, 521.8242, 371.1848,\n",
      "        233.2954, 843.9052, 701.5983, 182.4747, 860.3118, 235.7451, 324.7025,\n",
      "        175.3461, 272.8120, 320.4796, 164.8526, 273.9122, 112.8743,  65.1444,\n",
      "         30.3814,  26.6946,  22.4084,  19.1365], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([98.9939, 18.1192], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([120.4847, 118.9672,  91.6382,  84.8873,  81.7069, 102.3502,  79.0093,\n",
      "         69.4130,  62.5371,  90.3632,  78.8851,  48.4137], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([249.6265,  26.6369,  29.2812], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 383.9594,  555.9504,  768.5164,  424.1151,  434.1613,  581.2608,\n",
      "         569.5888,  366.5159,  252.7715,  439.9800,  250.9252, 1505.4030,\n",
      "         381.3904,  311.6407, 1112.5704,  150.7460,  111.3090,  119.3391,\n",
      "          96.6594,   33.1439,   25.2965,   26.4126], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([48.9773, 64.8399, 73.2044, 63.4013, 59.0960, 33.8841, 45.3908, 60.2639,\n",
      "        33.9128, 28.4930, 48.7555], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False, False, False,  True,\n",
      "        False], device='cuda:0')\n",
      "pruning 51 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     9\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    9\n",
      "     ║    ╠════╝\n",
      "     ║    30\n",
      "     ║    ╠════╗\n",
      "     ║    ║    11\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    17\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    4\n",
      "    ║    ╠════╝\n",
      "    ║    22\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   27\n",
      "   ╠════╗\n",
      "   ║    ╠════╗\n",
      "   ║    ║    1\n",
      "   ║    ╠════╝\n",
      "   ║    34\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  24\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    1\n",
      "  ║    ╠════╝\n",
      "  ║    21\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :7 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 65 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     10\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    12\n",
      "     ║    ╠════╝\n",
      "     ║    37\n",
      "     ║    ╠════╗\n",
      "     ║    ║    18\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    7\n",
      "    ║    ╠════╝\n",
      "    ║    27\n",
      "    ║    ╠════╗\n",
      "    ║    ║    1\n",
      "    ║    ╠════╝\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   31\n",
      "   ╠════╗\n",
      "   ║    ╠════╗\n",
      "   ║    ║    5\n",
      "   ║    ╠════╝\n",
      "   ║    39\n",
      "   ║    ╠════╗\n",
      "   ║    ║    3\n",
      "   ║    ╠════╝\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  26\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    5\n",
      "  ║    ╠════╝\n",
      "  ║    27\n",
      "  ║    ╠════╗\n",
      "  ║    ║    4\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 10\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.0004940674989484251, 13.561148643493652)\n",
      "Mean, Std: (1.0, 1.5538184642791748)\n",
      "remove_below 0.18462856113910675 true: 30.890165862884547\n",
      "Significance:\n",
      "tensor([2.2461e+02, 2.5322e+02, 2.1204e+02, 2.4308e+02, 2.6089e+02, 2.4736e+02,\n",
      "        2.0776e+02, 2.2593e+02, 2.6677e+02, 8.7717e-02], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([41.7089, 71.1766, 46.3959, 60.3105, 27.0505, 39.7217, 16.0223],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 491.5561,  344.4166,  417.4255,  166.6537,  681.5891,  526.0920,\n",
      "         393.2086,  201.7684,  366.7503,  532.2910,  307.7950,  222.9916,\n",
      "         428.2858, 1085.6548, 1429.1105,  168.2980,  248.5989,  220.9085,\n",
      "         606.0411,  156.6392,  129.4303,  175.9287,  208.6048,  306.4436,\n",
      "          59.2689,   25.3033], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 48.4992,  33.6242,  42.5414,  75.8223,  84.1786,  82.9808,  76.0572,\n",
      "         72.2220,  86.3860,  37.7939, 119.0846,  44.6984, 105.7753,  57.5951,\n",
      "         62.8244,  38.3110,  45.4780,  57.0150,  65.4690,  51.4533,  34.3186,\n",
      "         11.8445,  94.1424,  21.4230,  30.3629,   6.4896,  49.9873],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False,  True,  True,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([14.9550,  8.7446,  2.1656,  0.0827], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True, True, True, True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 288.2128, 2268.9128,  380.6276,  227.8431,  402.7083,  479.1963,\n",
      "         624.3250, 1444.2640,  337.0965,    6.0270], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 98.0982,  55.4255,  55.3314, 102.2421,  83.2231,  54.4227,  67.5624,\n",
      "         85.1456,  73.7893, 114.3272,  63.3984,  95.5866,  76.1384,  99.9641,\n",
      "         70.8843,  95.8345,  61.2190,  46.1494,  54.0120,  59.2616,  69.9127,\n",
      "         82.0588,  98.6645,  59.0517,  57.2777,  89.4380,  83.5874,  38.3743,\n",
      "         68.7329,  80.8166,  52.3763,  21.6132,  32.0886,  13.5614,  20.7933,\n",
      "         40.6647,  34.8313], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False,  True,  True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 89.6404,  64.6956, 107.5178,  46.7101,  88.8181,  46.8082, 127.1944,\n",
      "         82.8834,  47.2637, 104.3155,  87.6507,  65.4872,  54.0402,  78.5175,\n",
      "         81.7085,  70.8541, 107.6569,  72.9019, 118.9018,  79.9982,  64.5114,\n",
      "         71.4516,  63.2903,  83.7571,  61.9274,  65.8034,  69.0858,  33.5322,\n",
      "        132.0568,  51.9660,  64.5576,  74.2583,  32.0851, 130.1457,  23.2739,\n",
      "         32.6264,   0.5351,  10.6302,  40.1601], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False,  True, False,  True,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 84.9077,  48.1002, 110.0826, 155.0518,  83.1578,  55.7068, 119.1408,\n",
      "         83.3177,  59.6263,  69.9825, 102.4967,  63.3749,  82.1289, 103.9519,\n",
      "         38.0630,  87.3437,  58.9467,  49.7761,  52.8238,  61.6418,  87.7510,\n",
      "         35.1231,  29.3947,  21.7431,  15.2467,  70.4089,  35.8268],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True,  True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([21.5868, 35.6538, 22.1716], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 427.7668,  362.8967,  278.0109,  232.0815,  212.9857,  250.8695,\n",
      "         476.2488,  195.5409,  575.5572,  215.9613,  722.1205,  242.5691,\n",
      "         435.1926,  346.4359,  165.2482,  772.0483,  634.2690, 1036.5051,\n",
      "         182.8605,  316.1499,  177.0338,  269.3790,  279.4727,  220.6991,\n",
      "         262.4379,  142.0061,   78.0843,   19.9972,   19.8742,   12.9722,\n",
      "          52.1367], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False,  True, False,  True,  True,  True,\n",
      "        False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([66.0635, 59.4732, 52.7920, 30.8902, 24.3375], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([46.5964], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([97.6495, 66.8849, 56.2920, 72.4659, 56.8138, 74.2402, 62.7453, 53.1315,\n",
      "        72.7267, 69.2337, 57.6575, 35.8170, 28.1186, 35.6894, 51.6021, 40.6860,\n",
      "        35.3566, 19.8931], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([37.6897, 26.9464, 20.6721, 33.2668, 24.4558], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 465.9961,  287.9830,  678.7356,  339.5189,  372.5229,  400.6460,\n",
      "         459.8331,  292.4750,  424.6331,  292.1176, 1503.1168,  462.1093,\n",
      "         274.0753, 1457.3385,  179.7645,  140.4129,  210.9180,   28.0260,\n",
      "          26.8513,   18.0918,   50.3243,   15.6495], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False,  True,  True,  True,\n",
      "        False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([56.1456, 46.8829, 61.4755, 62.5215, 85.6229, 40.9489, 53.1006, 38.3766,\n",
      "        59.0203, 42.1617, 45.3994, 21.2644], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False,  True, False, False, False, False, False,\n",
      "        False,  True], device='cuda:0')\n",
      "pruning 48 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     8\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    9\n",
      "     ║    ╠════╝\n",
      "     ║    32\n",
      "     ║    ╠════╗\n",
      "     ║    ║    16\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    17\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    24\n",
      "    ║    ╠════╗\n",
      "    ║    ║    1\n",
      "    ║    ╠════╝\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   26\n",
      "   ╠════╗\n",
      "   ║    ╠════╗\n",
      "   ║    ║    3\n",
      "   ║    ╠════╝\n",
      "   ║    35\n",
      "   ║    ╠════╗\n",
      "   ║    ║    1\n",
      "   ║    ╠════╝\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  25\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    2\n",
      "  ║    ╠════╝\n",
      "  ║    23\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :8 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 66 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    11\n",
      "     ║    ╠════╝\n",
      "     ║    42\n",
      "     ║    ╠════╗\n",
      "     ║    ║    23\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    8\n",
      "    ║    ╠════╝\n",
      "    ║    30\n",
      "    ║    ╠════╗\n",
      "    ║    ║    2\n",
      "    ║    ╠════╝\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   29\n",
      "   ╠════╗\n",
      "   ║    ╠════╗\n",
      "   ║    ║    6\n",
      "   ║    ╠════╝\n",
      "   ║    39\n",
      "   ║    ╠════╗\n",
      "   ║    ║    5\n",
      "   ║    ╠════╝\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  27\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    3\n",
      "  ║    ╠════╝\n",
      "  ║    30\n",
      "  ║    ╠════╗\n",
      "  ║    ║    4\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 14\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (1.4796640243730508e-05, 14.918195724487305)\n",
      "Mean, Std: (0.9999998807907104, 1.6553763151168823)\n",
      "remove_below 0.14694537222385406 true: 25.285061563867444\n",
      "Significance:\n",
      "tensor([2.2543e+02, 2.4553e+02, 2.2207e+02, 2.8096e+02, 3.0144e+02, 2.8816e+02,\n",
      "        2.3163e+02, 2.4303e+02, 2.5486e+02, 1.0202e-02, 1.0644e-01, 1.6342e-02,\n",
      "        2.5461e-03, 1.9137e-02], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([67.7728, 48.8646, 42.4209, 48.4741, 51.6722, 34.8312, 23.6388, 23.3300],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 556.5536,  368.5884,  537.2289,  152.1510, 1055.1926,  214.5980,\n",
      "         519.3106,  306.7009,  377.0522,  957.6103,  394.5449,  234.9060,\n",
      "         849.6401, 1081.5208, 1900.2169,  168.7630,  864.7422,  286.4107,\n",
      "         956.4648,  101.7105,  206.0802,  118.7079,  143.9435,  492.4110,\n",
      "          83.2702,   46.0902,   32.4395], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([105.0938,  67.9790,  48.7686,  88.5287,  55.4698,  62.9990,  72.9270,\n",
      "        112.7425,  66.0551,  48.7458,  72.5880,  79.8068,  52.9706, 114.8146,\n",
      "         46.6004,  49.1755,  42.5013, 173.2056, 117.3711,  65.9186,  77.6491,\n",
      "         68.1753,  86.7714,  60.7484,  19.0791,  86.3989,  49.0717,  40.4996,\n",
      "         12.0601,   0.5424], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True, False, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 7.7728,  2.0371, 16.2999,  5.8262], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True, True, True, True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 303.9475, 2566.9915,  360.1507,  376.0176,  501.7163,  663.0162,\n",
      "        1366.4243,  322.3963,    8.7118,   19.7481,   48.4764],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False, False,  True,  True,\n",
      "        False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([116.1318,  54.7042,  91.9857,  96.1191,  62.8560,  58.7092,  70.1858,\n",
      "         63.8199, 107.3106,  45.9251,  85.2506, 101.3801, 104.7128,  78.2320,\n",
      "         57.8167,  40.9651,  35.8297,  60.3282,  91.3075,  74.7993,  90.9485,\n",
      "         54.5476, 104.6543,  95.2332,  83.0339,  62.2792,  84.4473,  84.0124,\n",
      "         49.2008,  51.8114,  87.3608,  64.8123,  56.8360,  33.9668,  15.2841,\n",
      "         45.7494,  11.3184,  31.8197,  23.1155,  22.1280,  31.5114,  25.8175],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False,  True, False,  True, False,  True,  True,\n",
      "        False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 99.6511,  22.4874,  69.9169,  40.0143,  73.5080,  62.2473,  96.1624,\n",
      "         60.6051,  42.7251,  62.1362,  90.0113,  75.0715,  62.7294,  96.0789,\n",
      "         59.6640,  55.8658,  36.5990,  59.2380,  86.7087,  27.5090, 123.8255,\n",
      "         99.0241,  74.2221,  48.6692, 120.3526,  73.4518,  55.3430,  56.8053,\n",
      "         57.7182,  59.1932,  49.3470,  70.0805,  68.2262, 131.5525,  46.5733,\n",
      "          9.1346,  52.3889,  23.2485,  66.3537], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 71.4002,  38.8259, 108.2067, 115.0168,  61.0644,  68.0043, 112.0794,\n",
      "         71.2797,  96.4612,  62.4514, 114.8618,  54.2270,  73.9330,  75.6907,\n",
      "         66.6929, 117.2763,  49.0517,  53.0049,  65.6937,  66.9097,  68.4173,\n",
      "         76.8153,  50.5729,  48.8765,  32.0351,  30.8731,  25.2851,  12.1318,\n",
      "         30.6556,  48.8101], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([39.2923, 20.1830, 16.1176,  9.6113, 30.6827], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 415.9093,  473.7338,  349.5791,  311.6464,  265.9745,  292.9007,\n",
      "         351.6339,  175.1248,  758.6462,  343.1823,  763.9681,  244.9363,\n",
      "         356.6511,  394.0455,  195.5441,  622.4026,  833.0547, 1126.0557,\n",
      "         195.3497,  355.5698,  235.5065,  303.2459,  200.3776,  330.8556,\n",
      "          73.4208,   30.3426,   22.9160,   62.6660,   15.9638],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([66.0634, 58.4548, 35.0565, 27.4407, 18.4772, 23.7606], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([56.0627, 35.5338], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 84.4008,  81.5893,  48.8736,  84.8578, 118.0473,  52.7854,  54.8154,\n",
      "         44.8301,  77.6973,  51.1087,  67.7081,  64.4745,  41.1864,  40.0769,\n",
      "         52.8040,  50.1862,  22.7758,  34.3317,  24.6416,  40.5640,   9.3949,\n",
      "         18.3664,  24.8212], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False,  True, False,  True, False,  True, False,\n",
      "         True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([54.8134, 50.3327, 23.3500], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 486.8919,  320.3732,  639.8053,  287.0407,  407.2331,  448.6538,\n",
      "         614.8757,  290.4610,  501.4270,  319.9786, 1457.0603,  309.6696,\n",
      "         301.1722, 1136.2073,  197.8221,  199.9067,   87.2863,   22.8038],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([66.9642, 57.4621, 55.4896, 54.5563, 56.8113, 55.5283, 66.8964, 56.7250,\n",
      "        42.4843, 29.2657, 42.6533], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False], device='cuda:0')\n",
      "pruning 51 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     8\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    11\n",
      "     ║    ╠════╝\n",
      "     ║    36\n",
      "     ║    ╠════╗\n",
      "     ║    ║    16\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    16\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    6\n",
      "    ║    ╠════╝\n",
      "    ║    26\n",
      "    ║    ╠════╗\n",
      "    ║    ║    2\n",
      "    ║    ╠════╝\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   27\n",
      "   ╠════╗\n",
      "   ║    ╠════╗\n",
      "   ║    ║    3\n",
      "   ║    ╠════╝\n",
      "   ║    36\n",
      "   ║    ╠════╗\n",
      "   ║    ║    2\n",
      "   ║    ╠════╝\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  27\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    1\n",
      "  ║    ╠════╝\n",
      "  ║    25\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :9 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 67 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     10\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    17\n",
      "     ║    ╠════╝\n",
      "     ║    40\n",
      "     ║    ╠════╗\n",
      "     ║    ║    23\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    13\n",
      "    ║    ╠════╝\n",
      "    ║    30\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   30\n",
      "   ╠════╗\n",
      "   ║    ╠════╗\n",
      "   ║    ║    4\n",
      "   ║    ╠════╝\n",
      "   ║    43\n",
      "   ║    ╠════╗\n",
      "   ║    ║    6\n",
      "   ║    ╠════╝\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  30\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    2\n",
      "  ║    ╠════╝\n",
      "  ║    32\n",
      "  ║    ╠════╗\n",
      "  ║    ║    3\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 12\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (8.691347466083243e-05, 12.941884994506836)\n",
      "Mean, Std: (0.9999999403953552, 1.6335608959197998)\n",
      "remove_below 0.1268160492181778 true: 21.788750696929505\n",
      "Significance:\n",
      "tensor([2.0873e+02, 2.9372e+02, 2.3456e+02, 2.7223e+02, 3.0208e+02, 3.0387e+02,\n",
      "        2.3695e+02, 2.3644e+02, 3.0873e+02, 2.3828e-02, 1.4933e-02, 7.1718e-01],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([54.8133, 46.5744, 55.2276, 65.2197, 95.8274, 36.2811, 18.8746, 12.0484,\n",
      "        25.9469, 32.8575,  9.7747,  8.2983,  7.6162], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False,  True,  True, False, False,\n",
      "         True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 486.6786,  354.8371,  436.7845,  320.7468,  945.8263,  376.1658,\n",
      "         409.7362,  332.2637,  433.3467, 1112.1216,  279.9411,  266.4049,\n",
      "         905.6855, 1216.0242, 1828.0927,  180.9636, 1021.3773,  613.0183,\n",
      "         986.5173,  133.0969,  176.3694,  163.2624,  151.1618,  378.4240,\n",
      "         112.6213,   38.8197,   77.4828,   20.3387,   20.8016,   54.1630],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False,  True, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([157.6324,  30.4500,  91.1015, 106.1462,  87.1876, 126.3575, 135.6760,\n",
      "         67.4629,  85.8135, 130.7364,  48.6393,  99.3926, 103.5625,  68.0736,\n",
      "         88.6538,  69.7114,  61.6841,  96.6787,  77.7949,  63.4430,  80.1786,\n",
      "        104.8267, 107.4139,  62.4154,  38.5135,  41.1366,  32.6504,  53.9209,\n",
      "         41.9742,  50.9056,  27.4457,  95.6036], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([12.4897,  7.0839,  0.6696], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True, True, True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 298.4238, 2223.5947,  627.7573,  366.5053,  546.6891, 1022.0927,\n",
      "         448.5002,  498.9247,   25.9253,  192.0130], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 89.7188,  40.6608,  99.5166,  78.9662,  65.0239,  49.7976,  64.6964,\n",
      "        104.9970,  44.2295,  97.1411,  95.0900,  98.7474,  98.2863,  70.1384,\n",
      "         39.3414,  47.6807,  84.3589,  86.3473,  74.6912,  98.6544,  58.0486,\n",
      "         84.9003,  87.6163, 118.9436, 104.2489,  85.7347,  49.8586,  80.9329,\n",
      "         75.5072,  68.0213,  68.4838,  43.7184,  67.6538,  51.0294,  91.8462,\n",
      "         75.7267,  22.7964,  35.7686,  51.5320,  28.2186], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([128.6286,  84.7361,  41.0053,  61.0685,  42.0311,  72.9863,  39.7516,\n",
      "         68.8961,  41.6869,  57.1982,  69.8172, 108.3237,  89.3033,  61.8799,\n",
      "         39.2062,  77.3002,  45.1751,  76.3288,  47.4443,  90.8667, 105.4706,\n",
      "         43.4450,  44.7616,  86.8894,  67.1205,  76.0887,  43.9424,  74.7571,\n",
      "         81.0058,  81.3232,  68.9173,  60.8948,  58.9366,  28.5996,  60.0892,\n",
      "         28.7077,  19.0226,  29.3178,  24.7333,  30.1946,  26.3942,  19.9761,\n",
      "         50.2213], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 69.8811,  32.2168,  71.5628, 154.4691,  84.2261,  53.2579,  99.2963,\n",
      "        102.9852,  67.0285,  89.2277,  43.4104,  82.3682,  88.3693,  63.3892,\n",
      "        111.3224,  70.9886,  43.5463,  50.1604,  66.6513,  40.3325,  80.3601,\n",
      "         55.2881,  50.8438,  59.1062,  64.4132,  51.7560,  48.1312,  48.2133,\n",
      "         33.4851,  17.4993], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([29.9491, 40.3921,  7.8324,  7.4704,  4.8457,  4.4300], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 395.0183,  403.9000,  229.1202,  258.3574,  322.9604,  262.0004,\n",
      "         540.4789,  154.7561,  667.1866,  286.2834, 1113.6926,  221.4177,\n",
      "         356.1610,  369.5981,  212.3924,  749.5352, 1073.8503, 1410.6543,\n",
      "         199.7650,  496.3785,  239.0705,  284.9782,  182.8701,  325.2940,\n",
      "         111.9289,   50.5600,   48.2503,   74.7217,   21.8971,   35.4659],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([55.9293, 59.8225, 43.7266, 24.2997], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([71.0872, 64.2982, 26.1154, 21.7887, 49.1998], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([72.1960, 62.3966, 50.7287, 85.7791, 54.8364, 66.7898, 59.1201, 79.1437,\n",
      "        74.2770, 78.0565, 66.4648, 54.4372, 66.7708, 58.9027, 39.8688, 54.8413,\n",
      "        21.1362, 36.5698, 34.5444, 62.3542, 32.0718, 28.4982, 35.3173],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False,  True, False, False, False,\n",
      "        False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([51.6213, 38.0125], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 342.9444,  232.1689,  348.5084,  436.0204,  373.9468,  414.4260,\n",
      "         532.5769,  319.7783,  509.6906,  300.7885, 1609.8408,  310.5460,\n",
      "         364.3232, 1277.7522,  302.5761,  222.1295,   63.0043,   35.3130],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([59.5765, 53.3302, 66.9794, 45.6563, 54.8907, 38.3195, 54.2646, 47.4490,\n",
      "        49.5499, 39.0270, 44.7262, 19.8416, 31.6530, 37.0683, 27.0755, 32.8561,\n",
      "        32.4970], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False,\n",
      "        False,  True, False, False, False, False, False], device='cuda:0')\n",
      "pruning 44 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     9\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    15\n",
      "     ║    ╠════╝\n",
      "     ║    37\n",
      "     ║    ╠════╗\n",
      "     ║    ║    20\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    7\n",
      "    ║    ╠════╝\n",
      "    ║    27\n",
      "    ║    ╠════╗\n",
      "    ║    ║    4\n",
      "    ║    ╠════╝\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   28\n",
      "   ╠════╗\n",
      "   ║    ╠════╗\n",
      "   ║    ║    4\n",
      "   ║    ╠════╝\n",
      "   ║    36\n",
      "   ║    ╠════╗\n",
      "   ║    ║    2\n",
      "   ║    ╠════╝\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  25\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    2\n",
      "  ║    ╠════╝\n",
      "  ║    31\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :10 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 69 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     13\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    22\n",
      "     ║    ╠════╝\n",
      "     ║    48\n",
      "     ║    ╠════╗\n",
      "     ║    ║    22\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    8\n",
      "    ║    ╠════╝\n",
      "    ║    34\n",
      "    ║    ╠════╗\n",
      "    ║    ║    9\n",
      "    ║    ╠════╝\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   31\n",
      "   ╠════╗\n",
      "   ║    ╠════╗\n",
      "   ║    ║    5\n",
      "   ║    ╠════╝\n",
      "   ║    37\n",
      "   ║    ╠════╗\n",
      "   ║    ║    7\n",
      "   ║    ╠════╝\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  30\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    5\n",
      "  ║    ╠════╝\n",
      "  ║    39\n",
      "  ║    ╠════╗\n",
      "  ║    ║    4\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 11\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (1.552089088363573e-06, 16.518220901489258)\n",
      "Mean, Std: (1.0, 1.8473525047302246)\n",
      "remove_below 0.10386548191308975 true: 15.387198943570434\n",
      "Significance:\n",
      "tensor([1.8785e+02, 2.5528e+02, 2.0710e+02, 2.5367e+02, 2.6387e+02, 2.8760e+02,\n",
      "        2.3258e+02, 2.0282e+02, 2.7798e+02, 1.8922e-02, 2.5554e-01],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "         True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([66.7124, 48.6432, 60.1535, 42.3947, 45.0866, 58.4490, 49.5290, 25.6102],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 409.8956,  435.3098,  233.4818,  249.1782, 1063.5911,  503.7173,\n",
      "         533.9044,  402.3920,  397.1678,  272.2091,  236.1238,  889.2645,\n",
      "        2211.2769, 1470.5133,  603.6674, 1118.9987,  209.2728,  139.4980,\n",
      "         178.5582,  195.0276,  404.3779,  138.9387,   96.7915,  109.3540,\n",
      "         174.7968,   20.6805,   37.0316,   32.1221,   29.8807,   16.2774],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([5.9026e+01, 5.1759e+01, 3.8157e+01, 4.4041e+01, 5.4558e+01, 7.7537e+01,\n",
      "        6.1204e+01, 1.3561e+02, 6.4093e+01, 3.5612e+01, 4.0344e+01, 8.3493e+01,\n",
      "        9.5494e+01, 5.6879e+01, 2.2801e+01, 4.0379e+01, 5.9789e+01, 7.2354e+01,\n",
      "        5.3590e+01, 5.3414e+01, 3.4985e+01, 3.4895e+01, 9.4276e+01, 9.5302e+01,\n",
      "        4.6216e+01, 4.6644e+01, 5.1501e+01, 2.9763e+01, 3.8109e+01, 4.3139e+01,\n",
      "        2.3084e+01, 3.2036e+01, 1.3986e+00, 3.3160e-03, 1.5387e+01, 2.4496e+01,\n",
      "        2.2993e-04, 4.7591e-04, 3.4643e+01], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False,  True,  True,  True, False,  True,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([8.1909, 8.4327, 0.0870, 1.7620], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True, True, True, True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([2447.0991,  580.6578,  396.9816,  572.7126,  795.7239,  504.0723,\n",
      "         636.9624,  218.1030,  243.3192,    3.7983,    3.1107,   14.8680,\n",
      "          22.7972], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False, False,  True,\n",
      "         True,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([96.9279, 37.7164, 80.7429, 72.6499, 62.1758, 57.0467, 83.7060, 36.5634,\n",
      "        86.5961, 71.9260, 87.0444, 82.6665, 57.7454, 57.0462, 33.9556, 56.5375,\n",
      "        82.6552, 66.6534, 65.8945, 96.4546, 73.7836, 70.5280, 86.8362, 62.0649,\n",
      "        63.9583, 45.5422, 74.9522, 66.8148, 78.9687, 66.1242, 43.3264, 45.6081,\n",
      "        50.8724, 57.0576, 68.5216, 62.6340, 50.9933, 54.7940, 42.1645, 54.5177,\n",
      "        38.4652, 26.0620, 41.8264, 14.5373, 45.0715, 31.4335, 30.4584, 30.0532],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 74.4847,  73.4424,  87.1239,  57.5554,  79.4318,  72.9102,  42.7354,\n",
      "         35.9641, 115.7033,  44.0492,  55.3382,  29.0206, 144.5904,  29.9924,\n",
      "         62.4751,  80.1178, 128.5532,  38.1874,  69.3898,  50.0535, 147.8659,\n",
      "         60.2222,  54.4359,  57.3838,  57.2094,  38.4493,  43.5468,  36.9158,\n",
      "         33.0157,  41.7677,  38.2470,  63.4312,  78.6213,  59.9662,  57.7110,\n",
      "         91.6044,  25.2771], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([63.7385, 65.7188, 98.6700, 59.4733, 90.5938, 47.1907, 54.2487, 73.3771,\n",
      "        82.7869, 64.7390, 65.2685, 31.4621, 86.1187, 48.6166, 42.4722, 52.6719,\n",
      "        64.3567, 29.3560, 56.8121, 43.0760, 33.4660, 46.2486, 57.5651, 73.3675,\n",
      "        44.4252, 57.7475, 40.1952, 29.2019, 28.3932, 16.2619, 46.4728, 16.0558,\n",
      "        43.6684, 26.0874], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([34.1020, 24.9357,  9.9703, 18.1575, 13.8165,  2.4642,  3.3146],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 358.0900,  461.6448,  282.8669,  417.4195,  256.3937,  437.0417,\n",
      "         188.5653,  392.3374,  241.1853, 1243.5680,  204.3096,  330.5764,\n",
      "         430.6864,  195.0622,  957.9597, 1206.2886,  200.4677,  340.2171,\n",
      "         196.1873,  287.2912,  245.6480,  256.3141,  126.9025,   44.5639,\n",
      "         115.6942,  150.0646,   38.9694,  119.0506,   19.7409,   22.8413,\n",
      "          17.6307], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "         True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([43.2687, 52.0538, 26.3802, 43.9246, 16.2156], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([78.2154, 43.8134, 16.4508, 73.4211, 28.9875, 14.4180, 36.8552,  8.9096,\n",
      "        27.1266], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([79.1433, 55.2864, 78.1940, 48.8042, 48.9959, 44.1825, 66.4991, 99.6925,\n",
      "        66.9707, 66.6647, 51.5443, 65.0697, 38.8140, 50.0172, 67.5103, 41.5102,\n",
      "        47.8317, 65.4705, 52.3284, 64.4914, 27.5771, 55.1172], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([48.6831, 33.6064, 30.7273, 16.3807, 22.5529], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 220.7162,  251.4776,  320.8737,  346.2291,  392.5373,  392.5870,\n",
      "         494.2309,  276.0988,  608.7854,  332.3888, 1545.1946,  191.5388,\n",
      "         434.5937, 1082.9774,  239.5732,  161.4803,  143.3098,  102.5377],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([49.0455, 61.4000, 54.4818, 45.4383, 39.5515, 56.4174, 41.7253, 47.3591,\n",
      "        50.0817, 33.5700, 33.8264, 31.1977, 43.0567, 34.3114, 28.2350, 19.9414,\n",
      "        19.4854, 21.3530, 10.0821, 14.0865, 18.8600, 16.2498], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "        False, False], device='cuda:0')\n",
      "pruning 36 neurons.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "/home/tsuman/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1033: \n",
    "UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. \n",
    "This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
    "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
    "'''\n",
    "\n",
    "trainer.loop(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check if parameter in param_groupd\n",
    "c = 0\n",
    "for p in optimizer.param_groups[0]['params']:\n",
    "    print(p.shape)\n",
    "    c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.root_net.residual.fc1.shortcut.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_x, train_y in train_loader:\n",
    "    train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "    yout = dynet(train_x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for p in dynet.parameters():\n",
    "    print(p.shape)\n",
    "    for _p in optimizer.param_groups[0]['params']:\n",
    "        if _p is p:\n",
    "            print('Found')\n",
    "    print()\n",
    "    c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.training_func()\n",
    "# trainer.pruning_func()\n",
    "# trainer.maintainance_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    corrects = 0\n",
    "    dynet.eval()\n",
    "    for test_x, test_y in test_loader:\n",
    "        test_x  = test_x.to(device)\n",
    "        yout = dynet.forward(test_x)\n",
    "        outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "        correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "        corrects += correct\n",
    "    dynet.train()\n",
    "    acc = corrects/len(test_dataset)*100\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     corrects = 0\n",
    "#     dynet.train()\n",
    "#     for test_x, test_y in train_loader:\n",
    "#         test_x  = test_x.to(device)\n",
    "#         yout = dynet.forward(test_x)\n",
    "#         outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "#         correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "#         corrects += correct\n",
    "#     acc = corrects/len(train_dataset)*100\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr 66.908 -> 62.422 ## the adding neuron function is wrong.. not preserving the function.\n",
    "# te 71.77 -> 41.959999999999994\n",
    "\n",
    "# te -> 53.32, 53.32\n",
    "# 68.51 -> 68.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.adding_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.beta_del_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accs_all, label=\"train\")\n",
    "plt.plot(accs_test, label=\"test\")\n",
    "ymin, ymax = plt.gca().get_ylim()\n",
    "plt.text(0, 0.8*ymin+0.2*ymax, f\"Train-> max:{max(accs_all):.3f} end:{accs_all[-1]:.3f} \\nTest-> max:{max(accs_test):.3f} end:{accs_test[-1]:.3f}\")\n",
    "                    \n",
    "plt.legend()\n",
    "plt.savefig(f\"output/plots/{index}_{name}_cifar10_{exp_index}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.non_linearity.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_allocated(device=\"cuda:0\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
