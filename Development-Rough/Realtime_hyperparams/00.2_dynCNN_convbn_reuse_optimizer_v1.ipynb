{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "import time\n",
    "import sys, io\n",
    "\n",
    "import mylibrary.datasets as datasets\n",
    "import mylibrary.nnlib as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adam_custom\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.LongTensor([2,3])\n",
    "a += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes\n",
    "\n",
    "- Add BN after convolution directly.\n",
    "    - This helps keep weight norm uniform while changing the scaling parameter of BN\n",
    "    - This will help to make the weight gradient well behaved.\n",
    "    \n",
    "- Reuse Optimizer (Adam) for added or removed parameters\n",
    "    - This will (supposedly) remove unstable training\n",
    "    - Maybe we need to add different learning rate for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1]), tensor([2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hidden_neuron_number(i, o):\n",
    "    nh =  (max(i,o)*(min(i,o)**2))**(1/3)\n",
    "#     return max(nh, 1)\n",
    "    return nh\n",
    "\n",
    "class Shortcut_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=(3,3), stride=1):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self._kernel = kernel\n",
    "#         self._padding = list(((np.array(self._kernel)-1)/2).astype(int))\n",
    "        self._padding = [int((k-1)/2) for k in kernel]\n",
    "        self._stride = stride\n",
    "#         print(self._padding, self._kernel)\n",
    "\n",
    "#         self._kernel = [torch.tensor(k) for k in self._kernel]\n",
    "#         self._padding = [int(k) for k in self._padding]\n",
    "#         self._stride = torch.tensor(stride)\n",
    "        \n",
    "        self.conv = nn.Conv2d(input_dim, output_dim, self._kernel, stride=self._stride,\n",
    "                        padding=self._padding, bias=False)#\n",
    "#         self.weight = self.conv.weight\n",
    "        ## Shape = OutputDim, InputDim, Kernel0, Kernel1\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(self.conv.weight.data).copy_(self.conv.weight.data)\n",
    "        )\n",
    "        del self.conv\n",
    "        self.bn = nn.BatchNorm2d(output_dim)\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "        self.initial_freeze_bn = None\n",
    "        \n",
    "        self.add_parameters_to_optimizer()\n",
    "        return\n",
    "        \n",
    "    def add_parameters_to_optimizer(self):\n",
    "        ## internal optimizer\n",
    "#         print(list(self.parameters()))\n",
    "#         self.tree.optimizer.state[pp] = {'step':0, \"aa\":'hahaha'}\n",
    "\n",
    "# {'step': tensor([12, 12,  6,  6,  6]),\n",
    "#               'exp_avg': tensor([ 2.0893e-11,  7.7122e-10, -6.7105e-12, -5.0940e-10, -9.8008e-10]),\n",
    "#               'exp_avg_sq': tensor([2.3143e-19, 1.5871e-19, 3.0733e-20, 2.8796e-20, 4.1671e-20])}\n",
    "        \n",
    "        for p in self.parameters():\n",
    "#             self.tree.optimizer.state[p] = {}\n",
    "            self.tree.optimizer.param_groups[0]['params'].append(p)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.shape[1] > 0 and self.weight.shape[0] > 0:\n",
    "#             out_dim = self.weight.shape[0]\n",
    "#             return self.bn(self.conv(x))\n",
    "\n",
    "#             self.weight.data /= torch.norm(self.weight.data.reshape(out_dim, -1), dim=1).reshape(out_dim, 1, 1, 1)\n",
    "#             with torch.no_grad():\n",
    "#                 self.weight = self.weight/torch.norm(self.weight.reshape(out_dim, -1), dim=1).reshape(out_dim, 1, 1, 1)\n",
    "            \n",
    "            return self.bn(F.conv2d(x, self.weight, stride=self._stride, padding=self._padding))\n",
    "        ### output dim is 0\n",
    "        elif self.weight.shape[0] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###       #out_dim #inp_dim            #kernel\n",
    "            w = torch.zeros(1, 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return torch.zeros(o.shape[0], 0, o.shape[2], o.shape[3], dtype=x.dtype, device=x.device)\n",
    "        ### input dim is 0\n",
    "        elif x.shape[1] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###             #out_dim            #inp_dim            #kernel\n",
    "            w = torch.zeros(self.weight.shape[0], 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return o.data\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown shape of input {x.shape} or weight {self.weight.shape}\")\n",
    "\n",
    "#     def decay_std_ratio(self, factor):\n",
    "#         self.weight.data = self.weight.data - self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "        \n",
    "#     def decay_std_ratio_grad(self, factor):\n",
    "#         self.weight.grad = self.weight.grad + self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "    \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "#         self.initial_remove = torch.atan(self.weight.data[:, to_remove])\n",
    "\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.initial_freeze_bn = self.bn.weight.data[to_freeze], self.bn.bias.data[to_freeze]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    ## freeze output neuron's incoming weight \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        self.bn.weight.data[self.to_freeze] = self.initial_freeze_bn[0] \n",
    "        self.bn.bias.data[self.to_freeze] = self.initial_freeze_bn[1] \n",
    "        pass\n",
    "    \n",
    "    ## decay input neuron's outgoing weight \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "#         self.weight.data[:, self.to_remove] = torch.tan(self.initial_remove*self.tree.decay_factor)\n",
    "        pass\n",
    "     \n",
    "    ## remove output neuron \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "        \n",
    "        ### do the same thing to optimizer variables as well        \n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        self.weight.data = self.weight.data[remaining, :]\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        ops[self.weight][_var][remaining, :]\n",
    "        \n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        \n",
    "#         ## running_mean\n",
    "        _rm = self.bn.running_mean[remaining]\n",
    "        self.bn.running_mean = _rm\n",
    "        \n",
    "#         ## running_var\n",
    "        _rv = self.bn.running_var[remaining]\n",
    "        self.bn.running_var = _rv\n",
    "        \n",
    "#         ## weight\n",
    "        self.bn.weight.data = self.bn.weight.data[remaining]\n",
    "        self.bn.weight.grad = None\n",
    "        if len(ops[self.bn.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.bn.weight][_var] = \\\n",
    "                        ops[self.bn.weight][_var][remaining]\n",
    "\n",
    "        ## bias\n",
    "        self.bn.bias.data = self.bn.bias.data[remaining]\n",
    "        self.bn.bias.grad = None\n",
    "        if len(ops[self.bn.bias]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.bn.bias][_var] = \\\n",
    "                        ops[self.bn.bias][_var][remaining]\n",
    "        \n",
    "        self.bn.num_features = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    ## remove input neuron \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "#         print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "#         print(torch.count_nonzero(self.weight.data<1e-6))\n",
    "#         print(self.weight.data[:, self.to_remove])\n",
    "\n",
    "        ops = self.tree.optimizer.state\n",
    "\n",
    "        self.weight.data = self.weight.data[:, remaining]\n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        ops[self.weight][_var][:, remaining]\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        self.weight.data = torch.cat((self.weight.data, \\\n",
    "                                      torch.zeros(o, num, k0, k1, dtype=self.weight.data.dtype,\n",
    "                                      device=self.weight.data.device)), \n",
    "                                     dim=1)\n",
    "        self.weight.grad = None\n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        torch.cat((ops[self.weight][_var], \\\n",
    "                                  torch.zeros(o, num, k0, k1, dtype=ops[self.weight][_var].dtype,\n",
    "                                              device=ops[self.weight][_var].device)), \n",
    "                                  dim=1)\n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i) ### similar to Xavier init ?? !!\n",
    "#         stdv = torch.std(self.weight.data) ## if it does not work, revert it\n",
    "    \n",
    "        _new = torch.empty(num, i, k0, k1, dtype=self.weight.data.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        self.weight.data = torch.cat((self.weight.data, _new), dim=0)\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        torch.cat((ops[self.weight][_var], \\\n",
    "                                  torch.zeros(num, i, k0, k1, dtype=ops[self.weight][_var].dtype,\n",
    "                                              device=ops[self.weight][_var].device)), \n",
    "                                  dim=0)\n",
    "        \n",
    "        # print(self.weight.data.shape)\n",
    "                \n",
    "        ####https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d\n",
    "        ## running_mean\n",
    "        _rm = self.bn.running_mean\n",
    "        _rm = torch.cat((_rm, torch.zeros(num, dtype=_rm.dtype, device=_rm.device)))\n",
    "        self.bn.running_mean = _rm\n",
    "        \n",
    "        ## running_var\n",
    "        _rv = self.bn.running_var\n",
    "        _rv = torch.cat((_rv, torch.ones(num, dtype=_rv.dtype, device=_rv.device)))\n",
    "        self.bn.running_var = _rv\n",
    "        \n",
    "        ## weight\n",
    "        _w = self.bn.weight.data\n",
    "        _w = torch.cat((_w, torch.ones(num, dtype=_w.dtype, device=_w.device)))\n",
    "        self.bn.weight.data = _w\n",
    "        self.bn.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.bn.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.bn.weight][_var] = \\\n",
    "                        torch.cat((ops[self.bn.weight][_var], \\\n",
    "                                  torch.zeros(num, dtype=ops[self.bn.weight][_var].dtype,\n",
    "                                              device=ops[self.bn.weight][_var].device)), \n",
    "                                 )\n",
    "        \n",
    "        ## bias\n",
    "        _b = self.bn.bias.data\n",
    "        _b = torch.cat((_b, torch.zeros(num, dtype=_b.dtype, device=_b.device)))\n",
    "        self.bn.bias.data = _b\n",
    "        self.bn.bias.grad = None\n",
    "        \n",
    "        if len(ops[self.bn.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.bn.bias][_var] = \\\n",
    "                        torch.cat((ops[self.bn.bias][_var], \\\n",
    "                                  torch.zeros(num, dtype=ops[self.bn.bias][_var].dtype,\n",
    "                                              device=ops[self.bn.bias][_var].device)), \n",
    "                                 )\n",
    "        \n",
    "        self.bn.num_features += num\n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}S▚:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempTree():\n",
    "    def __init__(self):\n",
    "        self.optimizer = adam_custom.Adam([nn.Parameter(torch.Tensor(0))],\n",
    "                                          lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shortcut_Conv(\n",
       "  (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = TempTree()\n",
    "\n",
    "a = Shortcut_Conv(tree, 2, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Shortcut_Conv\n",
       "  (bn): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.jit.script(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict, {})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.optimizer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 0.2201,  0.0091,  0.2296],\n",
       "           [ 0.0235,  0.1418, -0.1405],\n",
       "           [-0.0120,  0.0592,  0.0264]],\n",
       " \n",
       "          [[-0.1595, -0.1986, -0.1108],\n",
       "           [ 0.0058, -0.0219, -0.2329],\n",
       "           [-0.2033,  0.1843, -0.1433]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.], requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.optimizer.param_groups[0]['params'] ##.append(a.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.optimizer.zero_grad()\n",
    "loss = a(torch.randn(3, 2, 4,4)).mean()\n",
    "# loss.backward(create_graph=False, retain_graph=False)\n",
    "loss.backward()\n",
    "tree.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.7140e-09,  8.9170e-09, -1.9542e-08],\n",
       "          [ 1.4346e-08, -3.3577e-09, -8.7224e-09],\n",
       "          [-4.7846e-09, -2.1579e-08, -2.2365e-08]],\n",
       "\n",
       "         [[-6.3537e-09,  2.2658e-08, -3.9814e-09],\n",
       "          [-1.6789e-08,  1.3521e-09,  1.3866e-08],\n",
       "          [-2.9326e-09, -1.9963e-08, -1.0544e-08]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = Shortcut_Conv(\"tree\", 2, 5).weight.data # O, I, k1,k2\n",
    "# # n = torch.norm(a.reshape(5, -1), dim=1, keepdim=True).unsqueeze(2).unsqueeze(2)\n",
    "# n = torch.norm(a.reshape(5, -1), dim=1).reshape(5, 1, 1, 1)\n",
    "# a_ = a/n\n",
    "# torch.norm(a_.reshape(5, -1), dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TempTree():\n",
    "#     def __init__(self, model):\n",
    "#         self.optimizer = adam_custom.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "tt = TempTree()\n",
    "a = Shortcut_Conv(tt, i, 5)\n",
    "# tt = TempTree(a)\n",
    "# a.tree = tt\n",
    "# tt.optimizer = adam_custom.Adam(a.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Optimizer.state_dict of Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.optimizer.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.optimizer.zero_grad()\n",
    "a(torch.randn(3, i, 4,4)).mean().backward()\n",
    "tt.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: torch.Size([5, 4, 3, 3])\n",
      "step : torch.Size([5, 4, 3, 3])\n",
      "exp_avg : torch.Size([5, 4, 3, 3])\n",
      "exp_avg_sq : torch.Size([5, 4, 3, 3])\n",
      "\n",
      "Key: torch.Size([5])\n",
      "step : torch.Size([5])\n",
      "exp_avg : torch.Size([5])\n",
      "exp_avg_sq : torch.Size([5])\n",
      "\n",
      "Key: torch.Size([5])\n",
      "step : torch.Size([5])\n",
      "exp_avg : torch.Size([5])\n",
      "exp_avg_sq : torch.Size([5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tt.optimizer.state[a.weight]['step']\n",
    "for k, v in tt.optimizer.state.items():\n",
    "    print('Key:', k.shape)\n",
    "    for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "        print(_var,':',v['step'].shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.bn.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.remove_freezed_connection([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.add_output_connection(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.add_input_connection(3)\n",
    "i += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.remove_decayed_connection([0, 1])\n",
    "i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.optimizer.zero_grad()\n",
    "loss = a(torch.randn(3, i, 4,4)).mean()\n",
    "loss.backward()\n",
    "# loss.backward(create_graph=False, retain_graph=False)\n",
    "tt.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 3, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {Parameter containing:\n",
       "             tensor([[[[-0.0449,  0.1063,  0.0438],\n",
       "                       [ 0.0684, -0.0727,  0.0406],\n",
       "                       [ 0.1473,  0.1080,  0.0405]],\n",
       "             \n",
       "                      [[-0.1131,  0.0843, -0.1516],\n",
       "                       [ 0.0568,  0.0131,  0.1132],\n",
       "                       [-0.0603,  0.0595,  0.1473]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0952, -0.0378, -0.1321],\n",
       "                       [-0.1496, -0.0841, -0.0753],\n",
       "                       [ 0.0664, -0.1169, -0.0547]],\n",
       "             \n",
       "                      [[ 0.0673, -0.0730,  0.1621],\n",
       "                       [ 0.0042,  0.1064, -0.0715],\n",
       "                       [ 0.1653,  0.0865,  0.1281]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.4383,  0.2027, -0.0720],\n",
       "                       [ 0.2633, -0.4294, -0.2708],\n",
       "                       [ 0.3354,  0.3368, -0.2368]],\n",
       "             \n",
       "                      [[-0.4609, -0.4815,  0.1385],\n",
       "                       [ 0.2723,  0.1207, -0.1545],\n",
       "                       [-0.3400, -0.0709, -0.4503]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.2076, -0.3196,  0.1189],\n",
       "                       [ 0.1957, -0.2267,  0.0942],\n",
       "                       [ 0.3049,  0.1719,  0.0645]],\n",
       "             \n",
       "                      [[-0.4854,  0.4348,  0.3764],\n",
       "                       [-0.3942,  0.3903, -0.4331],\n",
       "                       [ 0.2976,  0.3442,  0.0532]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.4901, -0.3879, -0.2957],\n",
       "                       [ 0.1339, -0.2546,  0.2456],\n",
       "                       [-0.4804, -0.4399,  0.3919]],\n",
       "             \n",
       "                      [[-0.3673,  0.4826,  0.4981],\n",
       "                       [-0.2446, -0.3649, -0.2618],\n",
       "                       [-0.1270,  0.4126, -0.2533]]]], requires_grad=True): {'step': tensor([[[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]],\n",
       "              \n",
       "              \n",
       "                      [[[1, 1, 1],\n",
       "                        [1, 1, 1],\n",
       "                        [1, 1, 1]],\n",
       "              \n",
       "                       [[1, 1, 1],\n",
       "                        [1, 1, 1],\n",
       "                        [1, 1, 1]]],\n",
       "              \n",
       "              \n",
       "                      [[[1, 1, 1],\n",
       "                        [1, 1, 1],\n",
       "                        [1, 1, 1]],\n",
       "              \n",
       "                       [[1, 1, 1],\n",
       "                        [1, 1, 1],\n",
       "                        [1, 1, 1]]],\n",
       "              \n",
       "              \n",
       "                      [[[1, 1, 1],\n",
       "                        [1, 1, 1],\n",
       "                        [1, 1, 1]],\n",
       "              \n",
       "                       [[1, 1, 1],\n",
       "                        [1, 1, 1],\n",
       "                        [1, 1, 1]]]]),\n",
       "              'exp_avg': tensor([[[[-1.5882e-10,  1.0437e-10, -7.0496e-11],\n",
       "                        [ 2.0946e-10,  4.3719e-11,  3.2628e-10],\n",
       "                        [ 8.2319e-10,  6.8091e-10,  6.4714e-10]],\n",
       "              \n",
       "                       [[-2.6500e-10,  9.2714e-10, -4.2296e-10],\n",
       "                        [ 1.9338e-10,  6.1665e-10,  9.0642e-10],\n",
       "                        [-2.8263e-10,  5.7490e-10,  9.7031e-10]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7633e-10, -3.6716e-10, -4.8323e-10],\n",
       "                        [-3.9281e-10, -4.5722e-10, -3.3568e-10],\n",
       "                        [ 2.4437e-10, -5.1343e-11,  2.6289e-11]],\n",
       "              \n",
       "                       [[-1.6241e-12, -3.7442e-10,  1.0241e-10],\n",
       "                        [-1.8571e-10,  1.9234e-10, -4.6376e-10],\n",
       "                        [ 4.1249e-10,  1.5446e-10,  4.2599e-10]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0464e-10, -1.0602e-10,  5.1671e-11],\n",
       "                        [ 3.2278e-11,  9.9565e-11,  2.1318e-11],\n",
       "                        [ 1.0413e-12,  1.0558e-11,  8.0744e-11]],\n",
       "              \n",
       "                       [[ 5.4855e-11,  1.5409e-10, -1.1688e-11],\n",
       "                        [-1.3928e-10, -4.4679e-11,  1.0046e-10],\n",
       "                        [ 6.9795e-11,  1.5058e-10,  6.1849e-11]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.8840e-11,  4.7085e-11, -3.4433e-11],\n",
       "                        [ 3.1099e-11, -1.2111e-11, -5.3778e-11],\n",
       "                        [-2.6452e-11, -1.0835e-10, -4.7662e-11]],\n",
       "              \n",
       "                       [[ 9.3345e-11, -1.1817e-10, -6.2406e-11],\n",
       "                        [ 9.6135e-11, -1.2555e-10,  5.7667e-11],\n",
       "                        [-5.8047e-11, -1.4798e-10,  1.6413e-12]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4563e-11, -3.3690e-11,  2.8757e-12],\n",
       "                        [-4.3191e-12, -1.2401e-11,  3.9565e-12],\n",
       "                        [-3.1688e-11, -1.4241e-11,  2.7616e-11]],\n",
       "              \n",
       "                       [[-3.8902e-11,  1.9080e-11,  2.2140e-11],\n",
       "                        [-4.3076e-11, -3.8169e-11, -1.3878e-11],\n",
       "                        [-1.6237e-11,  6.0929e-11, -3.7551e-12]]]]),\n",
       "              'exp_avg_sq': tensor([[[[1.9456e-21, 6.1252e-22, 2.9703e-22],\n",
       "                        [3.3117e-21, 1.1044e-21, 5.9387e-21],\n",
       "                        [4.6835e-20, 3.6175e-20, 2.7967e-20]],\n",
       "              \n",
       "                       [[4.6704e-21, 5.9399e-20, 9.9648e-21],\n",
       "                        [2.0750e-21, 5.4303e-20, 5.4857e-20],\n",
       "                        [6.8695e-21, 2.8833e-20, 7.2337e-20]]],\n",
       "              \n",
       "              \n",
       "                      [[[4.8525e-21, 8.3124e-21, 1.3250e-20],\n",
       "                        [1.0441e-20, 1.2794e-20, 1.5905e-20],\n",
       "                        [3.3396e-21, 4.6881e-21, 3.0342e-22]],\n",
       "              \n",
       "                       [[3.5048e-22, 9.1872e-21, 1.7741e-21],\n",
       "                        [3.2832e-21, 2.2891e-21, 1.2974e-20],\n",
       "                        [9.5397e-21, 5.3716e-21, 1.0029e-20]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.0949e-21, 1.1241e-21, 2.6699e-22],\n",
       "                        [1.0419e-22, 9.9132e-22, 4.5445e-23],\n",
       "                        [1.0844e-25, 1.1147e-23, 6.5195e-22]],\n",
       "              \n",
       "                       [[3.0091e-22, 2.3744e-21, 1.3660e-23],\n",
       "                        [1.9398e-21, 1.9962e-22, 1.0093e-21],\n",
       "                        [4.8713e-22, 2.2674e-21, 3.8253e-22]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.5085e-22, 2.2170e-22, 1.1857e-22],\n",
       "                        [9.6714e-23, 1.4668e-23, 2.8921e-22],\n",
       "                        [6.9971e-23, 1.1740e-21, 2.2717e-22]],\n",
       "              \n",
       "                       [[8.7133e-22, 1.3965e-21, 3.8946e-22],\n",
       "                        [9.2419e-22, 1.5764e-21, 3.3254e-22],\n",
       "                        [3.3695e-22, 2.1899e-21, 2.6940e-25]]],\n",
       "              \n",
       "              \n",
       "                      [[[6.0334e-23, 1.1350e-22, 8.2697e-25],\n",
       "                        [1.8655e-24, 1.5377e-23, 1.5654e-24],\n",
       "                        [1.0041e-22, 2.0280e-23, 7.6264e-23]],\n",
       "              \n",
       "                       [[1.5133e-22, 3.6406e-23, 4.9016e-23],\n",
       "                        [1.8555e-22, 1.4569e-22, 1.9259e-23],\n",
       "                        [2.6363e-23, 3.7123e-22, 1.4101e-24]]]])},\n",
       "             Parameter containing:\n",
       "             tensor([1.0001, 1.0001, 1.0000, 1.0000, 1.0000], requires_grad=True): {'step': tensor([2, 2, 1, 1, 1]),\n",
       "              'exp_avg': tensor([-9.0443e-10, -6.0048e-10,  3.4851e-10,  3.3725e-10, -1.5089e-10]),\n",
       "              'exp_avg_sq': tensor([4.6553e-20, 2.2319e-20, 1.2146e-20, 1.1374e-20, 2.2769e-21])},\n",
       "             Parameter containing:\n",
       "             tensor([-2.0000e-04, -2.0000e-04, -9.9999e-05, -9.9999e-05, -9.9999e-05],\n",
       "                    requires_grad=True): {'step': tensor([2, 2, 1, 1, 1]),\n",
       "              'exp_avg': tensor([0.0380, 0.0380, 0.0200, 0.0200, 0.0200]),\n",
       "              'exp_avg_sq': tensor([7.9960e-05, 7.9960e-05, 4.0000e-05, 4.0000e-05, 4.0000e-05])}})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.optimizer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.optimizer.zero_grad()\n",
    "a(torch.randn(3, i, 4,4)).mean().backward()\n",
    "tt.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0003, -0.0003, -0.0002, -0.0002, -0.0002], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.bn.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1.0001, 1.0001, 1.0000, 1.0000, 1.0000], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.bn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 3, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {Parameter containing:\n",
       "             tensor([[[[-0.0449,  0.1063,  0.0438],\n",
       "                       [ 0.0684, -0.0727,  0.0406],\n",
       "                       [ 0.1473,  0.1080,  0.0405]],\n",
       "             \n",
       "                      [[-0.1131,  0.0843, -0.1516],\n",
       "                       [ 0.0568,  0.0131,  0.1131],\n",
       "                       [-0.0603,  0.0595,  0.1473]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0952, -0.0378, -0.1321],\n",
       "                       [-0.1496, -0.0841, -0.0753],\n",
       "                       [ 0.0664, -0.1169, -0.0547]],\n",
       "             \n",
       "                      [[ 0.0673, -0.0730,  0.1621],\n",
       "                       [ 0.0042,  0.1064, -0.0715],\n",
       "                       [ 0.1653,  0.0865,  0.1281]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.4383,  0.2027, -0.0720],\n",
       "                       [ 0.2633, -0.4294, -0.2708],\n",
       "                       [ 0.3354,  0.3368, -0.2368]],\n",
       "             \n",
       "                      [[-0.4609, -0.4815,  0.1385],\n",
       "                       [ 0.2723,  0.1207, -0.1545],\n",
       "                       [-0.3400, -0.0709, -0.4504]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.2076, -0.3196,  0.1189],\n",
       "                       [ 0.1957, -0.2267,  0.0942],\n",
       "                       [ 0.3049,  0.1719,  0.0645]],\n",
       "             \n",
       "                      [[-0.4854,  0.4348,  0.3764],\n",
       "                       [-0.3942,  0.3903, -0.4331],\n",
       "                       [ 0.2976,  0.3442,  0.0532]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.4901, -0.3879, -0.2957],\n",
       "                       [ 0.1339, -0.2546,  0.2456],\n",
       "                       [-0.4804, -0.4399,  0.3919]],\n",
       "             \n",
       "                      [[-0.3673,  0.4826,  0.4981],\n",
       "                       [-0.2446, -0.3649, -0.2618],\n",
       "                       [-0.1270,  0.4126, -0.2533]]]], requires_grad=True): {'step': tensor([[[[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]],\n",
       "              \n",
       "                       [[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]]],\n",
       "              \n",
       "              \n",
       "                      [[[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]],\n",
       "              \n",
       "                       [[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]]]),\n",
       "              'exp_avg': tensor([[[[-1.5462e-10,  1.4446e-10, -4.8499e-11],\n",
       "                        [ 2.5355e-10, -6.3172e-11,  3.4549e-10],\n",
       "                        [ 7.3896e-10,  6.5892e-10,  5.9531e-10]],\n",
       "              \n",
       "                       [[-3.1283e-10,  9.1739e-10, -4.2931e-10],\n",
       "                        [ 1.8716e-10,  5.2210e-10,  8.7695e-10],\n",
       "                        [-2.4080e-10,  5.4785e-10,  9.2273e-10]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.5631e-10, -2.9921e-10, -3.3451e-10],\n",
       "                        [-4.1587e-11, -4.7133e-10, -8.1133e-11],\n",
       "                        [-4.2188e-11,  3.6928e-10, -3.7298e-11]],\n",
       "              \n",
       "                       [[-2.2427e-10, -9.8801e-11, -3.7137e-10],\n",
       "                        [ 1.3530e-10, -5.4371e-10, -2.6847e-10],\n",
       "                        [-2.4298e-11,  7.2533e-11,  1.7819e-10]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.9111e-11, -8.9471e-11,  3.0441e-11],\n",
       "                        [ 6.8397e-11,  5.7246e-11,  3.3519e-12],\n",
       "                        [ 4.8505e-12,  3.8939e-11,  4.8637e-11]],\n",
       "              \n",
       "                       [[ 1.6133e-11,  1.1629e-10,  7.5940e-12],\n",
       "                        [-1.1838e-10, -4.5653e-11,  7.9046e-11],\n",
       "                        [ 5.0130e-11,  1.4434e-10,  2.7131e-11]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.2944e-11,  2.5927e-11, -2.9577e-11],\n",
       "                        [ 1.1375e-10, -9.3942e-11, -7.8020e-11],\n",
       "                        [ 5.5163e-11, -1.4349e-10, -1.8399e-13]],\n",
       "              \n",
       "                       [[-7.0307e-11,  2.7181e-11,  1.8239e-12],\n",
       "                        [-5.7037e-12,  3.6717e-11, -2.1598e-11],\n",
       "                        [ 1.9900e-11, -3.1707e-11, -3.7189e-11]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.8910e-11,  3.0787e-11, -5.5707e-12],\n",
       "                        [-6.0844e-11,  1.0502e-10, -3.2709e-13],\n",
       "                        [ 4.5598e-12,  6.9366e-11, -6.4018e-11]],\n",
       "              \n",
       "                       [[ 1.6357e-10, -1.5521e-10, -1.8740e-11],\n",
       "                        [-1.0152e-11, -4.2218e-11,  4.5926e-11],\n",
       "                        [ 5.2145e-11, -4.4801e-11,  6.2105e-11]]]]),\n",
       "              'exp_avg_sq': tensor([[[[1.9573e-21, 8.6722e-22, 3.1907e-22],\n",
       "                        [3.7313e-21, 2.1543e-21, 6.2014e-21],\n",
       "                        [4.6789e-20, 3.6351e-20, 2.7955e-20]],\n",
       "              \n",
       "                       [[5.2182e-21, 6.0028e-20, 1.0192e-20],\n",
       "                        [2.0901e-21, 5.4357e-20, 5.5176e-20],\n",
       "                        [6.8811e-21, 2.8897e-20, 7.2509e-20]]],\n",
       "              \n",
       "              \n",
       "                      [[[4.8535e-21, 8.4017e-21, 1.4245e-20],\n",
       "                        [2.0161e-20, 1.3140e-20, 2.0772e-20],\n",
       "                        [1.0207e-20, 2.1947e-20, 6.7470e-22]],\n",
       "              \n",
       "                       [[5.3145e-21, 1.4851e-20, 2.3260e-20],\n",
       "                        [1.2427e-20, 5.3669e-20, 1.5179e-20],\n",
       "                        [2.5175e-20, 5.8082e-21, 1.4230e-20]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1165e-21, 1.1265e-21, 2.9252e-22],\n",
       "                        [2.5890e-22, 1.0951e-21, 7.0472e-23],\n",
       "                        [1.6397e-24, 9.7788e-23, 7.0906e-22]],\n",
       "              \n",
       "                       [[4.1108e-22, 2.4222e-21, 4.6454e-23],\n",
       "                        [1.9427e-21, 2.0238e-22, 1.0212e-21],\n",
       "                        [5.0274e-22, 2.2729e-21, 4.6356e-22]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.5708e-22, 2.4853e-22, 1.1865e-22],\n",
       "                        [8.3215e-22, 7.0426e-22, 3.7665e-22],\n",
       "                        [6.9352e-22, 1.3842e-21, 4.0937e-22]],\n",
       "              \n",
       "                       [[3.2518e-21, 3.1782e-21, 7.2535e-22],\n",
       "                        [1.7738e-21, 3.8163e-21, 8.7241e-22],\n",
       "                        [8.5707e-22, 3.2174e-21, 1.4977e-22]]],\n",
       "              \n",
       "              \n",
       "                      [[[5.6462e-22, 4.8681e-22, 7.4829e-24],\n",
       "                        [3.2627e-22, 1.3652e-21, 3.0754e-24],\n",
       "                        [2.0973e-22, 6.9565e-22, 8.6602e-22]],\n",
       "              \n",
       "                       [[4.0948e-21, 3.0080e-21, 1.9847e-22],\n",
       "                        [2.6726e-22, 1.5173e-22, 3.6047e-22],\n",
       "                        [4.7200e-22, 1.3636e-21, 4.3024e-22]]]])},\n",
       "             Parameter containing:\n",
       "             tensor([1.0001, 1.0001, 1.0000, 1.0000, 1.0000], requires_grad=True): {'step': tensor([3, 3, 2, 2, 2]),\n",
       "              'exp_avg': tensor([-8.7805e-10, -1.1521e-10,  2.1108e-10, -9.1060e-11,  3.0630e-10]),\n",
       "              'exp_avg_sq': tensor([4.6917e-20, 4.0379e-20, 1.3186e-20, 2.6932e-20, 2.1820e-20])},\n",
       "             Parameter containing:\n",
       "             tensor([-0.0003, -0.0003, -0.0002, -0.0002, -0.0002], requires_grad=True): {'step': tensor([3, 3, 2, 2, 2]),\n",
       "              'exp_avg': tensor([0.0542, 0.0542, 0.0380, 0.0380, 0.0380]),\n",
       "              'exp_avg_sq': tensor([1.1988e-04, 1.1988e-04, 7.9960e-05, 7.9960e-05, 7.9960e-05])}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.optimizer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = nn.Parameter(torch.zeros(10))\n",
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.optimizer.state[pp] = {'step':0, \"aa\":'hahaha'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {Parameter containing:\n",
       "             tensor([[[[-0.0449,  0.1063,  0.0438],\n",
       "                       [ 0.0684, -0.0727,  0.0406],\n",
       "                       [ 0.1473,  0.1080,  0.0405]],\n",
       "             \n",
       "                      [[-0.1131,  0.0843, -0.1516],\n",
       "                       [ 0.0568,  0.0131,  0.1131],\n",
       "                       [-0.0603,  0.0595,  0.1473]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0952, -0.0378, -0.1321],\n",
       "                       [-0.1496, -0.0841, -0.0753],\n",
       "                       [ 0.0664, -0.1169, -0.0547]],\n",
       "             \n",
       "                      [[ 0.0673, -0.0730,  0.1621],\n",
       "                       [ 0.0042,  0.1064, -0.0715],\n",
       "                       [ 0.1653,  0.0865,  0.1281]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.4383,  0.2027, -0.0720],\n",
       "                       [ 0.2633, -0.4294, -0.2708],\n",
       "                       [ 0.3354,  0.3368, -0.2368]],\n",
       "             \n",
       "                      [[-0.4609, -0.4815,  0.1385],\n",
       "                       [ 0.2723,  0.1207, -0.1545],\n",
       "                       [-0.3400, -0.0709, -0.4504]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.2076, -0.3196,  0.1189],\n",
       "                       [ 0.1957, -0.2267,  0.0942],\n",
       "                       [ 0.3049,  0.1719,  0.0645]],\n",
       "             \n",
       "                      [[-0.4854,  0.4348,  0.3764],\n",
       "                       [-0.3942,  0.3903, -0.4331],\n",
       "                       [ 0.2976,  0.3442,  0.0532]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.4901, -0.3879, -0.2957],\n",
       "                       [ 0.1339, -0.2546,  0.2456],\n",
       "                       [-0.4804, -0.4399,  0.3919]],\n",
       "             \n",
       "                      [[-0.3673,  0.4826,  0.4981],\n",
       "                       [-0.2446, -0.3649, -0.2618],\n",
       "                       [-0.1270,  0.4126, -0.2533]]]], requires_grad=True): {'step': tensor([[[[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]],\n",
       "              \n",
       "                       [[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]]],\n",
       "              \n",
       "              \n",
       "                      [[[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]],\n",
       "              \n",
       "                       [[3, 3, 3],\n",
       "                        [3, 3, 3],\n",
       "                        [3, 3, 3]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]],\n",
       "              \n",
       "              \n",
       "                      [[[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]],\n",
       "              \n",
       "                       [[2, 2, 2],\n",
       "                        [2, 2, 2],\n",
       "                        [2, 2, 2]]]]),\n",
       "              'exp_avg': tensor([[[[-1.5462e-10,  1.4446e-10, -4.8499e-11],\n",
       "                        [ 2.5355e-10, -6.3172e-11,  3.4549e-10],\n",
       "                        [ 7.3896e-10,  6.5892e-10,  5.9531e-10]],\n",
       "              \n",
       "                       [[-3.1283e-10,  9.1739e-10, -4.2931e-10],\n",
       "                        [ 1.8716e-10,  5.2210e-10,  8.7695e-10],\n",
       "                        [-2.4080e-10,  5.4785e-10,  9.2273e-10]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.5631e-10, -2.9921e-10, -3.3451e-10],\n",
       "                        [-4.1587e-11, -4.7133e-10, -8.1133e-11],\n",
       "                        [-4.2188e-11,  3.6928e-10, -3.7298e-11]],\n",
       "              \n",
       "                       [[-2.2427e-10, -9.8801e-11, -3.7137e-10],\n",
       "                        [ 1.3530e-10, -5.4371e-10, -2.6847e-10],\n",
       "                        [-2.4298e-11,  7.2533e-11,  1.7819e-10]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.9111e-11, -8.9471e-11,  3.0441e-11],\n",
       "                        [ 6.8397e-11,  5.7246e-11,  3.3519e-12],\n",
       "                        [ 4.8505e-12,  3.8939e-11,  4.8637e-11]],\n",
       "              \n",
       "                       [[ 1.6133e-11,  1.1629e-10,  7.5940e-12],\n",
       "                        [-1.1838e-10, -4.5653e-11,  7.9046e-11],\n",
       "                        [ 5.0130e-11,  1.4434e-10,  2.7131e-11]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.2944e-11,  2.5927e-11, -2.9577e-11],\n",
       "                        [ 1.1375e-10, -9.3942e-11, -7.8020e-11],\n",
       "                        [ 5.5163e-11, -1.4349e-10, -1.8399e-13]],\n",
       "              \n",
       "                       [[-7.0307e-11,  2.7181e-11,  1.8239e-12],\n",
       "                        [-5.7037e-12,  3.6717e-11, -2.1598e-11],\n",
       "                        [ 1.9900e-11, -3.1707e-11, -3.7189e-11]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.8910e-11,  3.0787e-11, -5.5707e-12],\n",
       "                        [-6.0844e-11,  1.0502e-10, -3.2709e-13],\n",
       "                        [ 4.5598e-12,  6.9366e-11, -6.4018e-11]],\n",
       "              \n",
       "                       [[ 1.6357e-10, -1.5521e-10, -1.8740e-11],\n",
       "                        [-1.0152e-11, -4.2218e-11,  4.5926e-11],\n",
       "                        [ 5.2145e-11, -4.4801e-11,  6.2105e-11]]]]),\n",
       "              'exp_avg_sq': tensor([[[[1.9573e-21, 8.6722e-22, 3.1907e-22],\n",
       "                        [3.7313e-21, 2.1543e-21, 6.2014e-21],\n",
       "                        [4.6789e-20, 3.6351e-20, 2.7955e-20]],\n",
       "              \n",
       "                       [[5.2182e-21, 6.0028e-20, 1.0192e-20],\n",
       "                        [2.0901e-21, 5.4357e-20, 5.5176e-20],\n",
       "                        [6.8811e-21, 2.8897e-20, 7.2509e-20]]],\n",
       "              \n",
       "              \n",
       "                      [[[4.8535e-21, 8.4017e-21, 1.4245e-20],\n",
       "                        [2.0161e-20, 1.3140e-20, 2.0772e-20],\n",
       "                        [1.0207e-20, 2.1947e-20, 6.7470e-22]],\n",
       "              \n",
       "                       [[5.3145e-21, 1.4851e-20, 2.3260e-20],\n",
       "                        [1.2427e-20, 5.3669e-20, 1.5179e-20],\n",
       "                        [2.5175e-20, 5.8082e-21, 1.4230e-20]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.1165e-21, 1.1265e-21, 2.9252e-22],\n",
       "                        [2.5890e-22, 1.0951e-21, 7.0472e-23],\n",
       "                        [1.6397e-24, 9.7788e-23, 7.0906e-22]],\n",
       "              \n",
       "                       [[4.1108e-22, 2.4222e-21, 4.6454e-23],\n",
       "                        [1.9427e-21, 2.0238e-22, 1.0212e-21],\n",
       "                        [5.0274e-22, 2.2729e-21, 4.6356e-22]]],\n",
       "              \n",
       "              \n",
       "                      [[[1.5708e-22, 2.4853e-22, 1.1865e-22],\n",
       "                        [8.3215e-22, 7.0426e-22, 3.7665e-22],\n",
       "                        [6.9352e-22, 1.3842e-21, 4.0937e-22]],\n",
       "              \n",
       "                       [[3.2518e-21, 3.1782e-21, 7.2535e-22],\n",
       "                        [1.7738e-21, 3.8163e-21, 8.7241e-22],\n",
       "                        [8.5707e-22, 3.2174e-21, 1.4977e-22]]],\n",
       "              \n",
       "              \n",
       "                      [[[5.6462e-22, 4.8681e-22, 7.4829e-24],\n",
       "                        [3.2627e-22, 1.3652e-21, 3.0754e-24],\n",
       "                        [2.0973e-22, 6.9565e-22, 8.6602e-22]],\n",
       "              \n",
       "                       [[4.0948e-21, 3.0080e-21, 1.9847e-22],\n",
       "                        [2.6726e-22, 1.5173e-22, 3.6047e-22],\n",
       "                        [4.7200e-22, 1.3636e-21, 4.3024e-22]]]])},\n",
       "             Parameter containing:\n",
       "             tensor([1.0001, 1.0001, 1.0000, 1.0000, 1.0000], requires_grad=True): {'step': tensor([3, 3, 2, 2, 2]),\n",
       "              'exp_avg': tensor([-8.7805e-10, -1.1521e-10,  2.1108e-10, -9.1060e-11,  3.0630e-10]),\n",
       "              'exp_avg_sq': tensor([4.6917e-20, 4.0379e-20, 1.3186e-20, 2.6932e-20, 2.1820e-20])},\n",
       "             Parameter containing:\n",
       "             tensor([-0.0003, -0.0003, -0.0002, -0.0002, -0.0002], requires_grad=True): {'step': tensor([3, 3, 2, 2, 2]),\n",
       "              'exp_avg': tensor([0.0542, 0.0542, 0.0380, 0.0380, 0.0380]),\n",
       "              'exp_avg_sq': tensor([1.1988e-04, 1.1988e-04, 7.9960e-05, 7.9960e-05, 7.9960e-05])},\n",
       "             Parameter containing:\n",
       "             tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True): {'step': 0,\n",
       "              'aa': 'hahaha'}})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.optimizer.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearity_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.actf = actf_obj\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actf(x)\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        pass\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=NonLinearity_Conv\n",
       "  (actf): RecursiveScriptModule(original_name=ReLU)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = NonLinearity_Conv(tree, 10)\n",
    "torch.jit.script(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearity(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.bias = nn.Parameter(torch.zeros(io_dim))\n",
    "        self.actf = actf_obj\n",
    "        \n",
    "        self.tree.optimizer.state[self.bias] = {}\n",
    "        tree.optimizer.param_groups[0]['params'].append(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actf(x+self.bias)\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        _b = torch.cat((self.bias.data, torch.zeros(num, dtype=self.bias.data.dtype,\n",
    "                                                    device=self.bias.data.device)))\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        _b = self.bias.data[remaining]\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=NonLinearity\n",
       "  (actf): RecursiveScriptModule(original_name=ReLU)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = NonLinearity(tree, 10)\n",
    "torch.jit.script(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, hidden_dim, output_dim, stride=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "#         self.stride = stride\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = HierarchicalResidual_Conv(self.tree, input_dim, hidden_dim, stride=stride, activation=activation) \n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = HierarchicalResidual_Conv(self.tree, hidden_dim, output_dim, activation=activation)\n",
    "        self.fc1.shortcut.bn.weight.data *= 0.        \n",
    "        self.fc1.shortcut.weight.data *= 0.1        \n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "            self.apnz = self.apnz/self.count\n",
    "#             self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "            self.significance = self.significance*(1-self.apnz**33) / 0.872 ## tried on desmos.\n",
    "\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "#         self.count = None\n",
    "\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "#             self.significance += z.pow(2).sum(dim=0)\n",
    "            self.significance += z.abs().sum(dim=0)\n",
    "#             self.significance += z.abs().pow(0.8).sum(dim=0)\n",
    "#             print(f\"SIG ACT:\\n{float(self.activations.abs().mean())}\")\n",
    "#             print(f\"GRAD Mean, Std:\\n{float(grad_output[0].data.abs().mean()), float(grad_output[0].data.std())}\")\n",
    "\n",
    "            if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "                self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "        #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "                self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below=None, above=None, mask=None):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(self.significance.numel(), dtype=torch.bool)\n",
    "        if below:\n",
    "            mask = torch.logical_or(mask,self.significance<=below)\n",
    "        if above:\n",
    "            mask = torch.logical_or(mask,self.significance>above)\n",
    "            \n",
    "        print(f\"Significance:\\n{self.significance}\\nPrune:\\n{mask}\")\n",
    "            \n",
    "        self.to_remove = torch.nonzero(mask).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "#         max_dim = np.ceil((self.tree.parent_dict[self].input_dim+\\\n",
    "#             self.tree.parent_dict[self].output_dim)/2)\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None: ## it is shortcut conv\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, stride=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.stride = 1\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, stride=self.stride).to(self.tree.device)\n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None ## this can be Residual Layer or None\n",
    "        ##### only one of shortcut or residual can be None at a time\n",
    "        self.forward = self.forward_shortcut\n",
    "        \n",
    "        self.std_ratio = 0. ## 0-> all variation due to shortcut, 1-> residual\n",
    "        self.target_std_ratio = 0. ##\n",
    "    \n",
    "    def forward_both(self, r):\n",
    "\n",
    "        s = self.shortcut(r)\n",
    "        r = self.residual(r)\n",
    "\n",
    "        if self.residual.hook is None: ### dont execute when computing significance\n",
    "            s_std = torch.std(s, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            r_std = torch.std(r, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            stdr = r_std/(s_std+r_std)\n",
    "\n",
    "            self.std_ratio = self.tree.beta_std_ratio*self.std_ratio + (1-self.tree.beta_std_ratio)*stdr.data\n",
    "            if r_std.min() > 1e-9:\n",
    "                ## recover for the fact that when decaying neurons, target ratio should also be reducing\n",
    "                if self.tree.total_decay_steps:\n",
    "                    i, o = self.shortcut.weight.shape[1],self.shortcut.weight.shape[0]\n",
    "                    if self.shortcut.to_remove is not None:\n",
    "                        i -= len(self.shortcut.to_remove)\n",
    "                    if self.shortcut.to_freeze is not None:\n",
    "                        o -= len(self.shortcut.to_freeze)\n",
    "                    h = self.residual.hidden_dim\n",
    "                    if self.residual.to_remove is not None:\n",
    "                        h -= len(self.residual.to_remove)\n",
    "                    \n",
    "#                     tr = h/np.ceil((i+o)/2 +1)\n",
    "                    tr = h/_get_hidden_neuron_number(i, o)\n",
    "                    self.compute_target_std_ratio(tr)\n",
    "                else:\n",
    "                    self.compute_target_std_ratio()\n",
    "                self.get_std_loss(stdr)\n",
    "        return s+r\n",
    "    \n",
    "    def forward_shortcut(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def forward_residual(self, x):\n",
    "        self.compute_target_std_ratio()\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def compute_target_std_ratio(self, tr = None):\n",
    "        if tr is None:\n",
    "#             tr = self.residual.hidden_dim/np.ceil((self.input_dim+self.output_dim)/2 +1)\n",
    "            tr = self.residual.hidden_dim/_get_hidden_neuron_number(self.input_dim, self.output_dim)\n",
    "#             tr = self.residual.hidden_dim/np.ceil(self.output_dim/2 +1)\n",
    "\n",
    "        tr = np.clip(tr, 0., 1.)\n",
    "        self.target_std_ratio = self.tree.beta_std_ratio*self.target_std_ratio +\\\n",
    "                                (1-self.tree.beta_std_ratio)*tr\n",
    "        pass        \n",
    "    \n",
    "    def get_std_loss(self, stdr):\n",
    "        del_std = self.target_std_ratio-stdr\n",
    "        del_std_loss = (del_std**2 + torch.abs(del_std)).mean()\n",
    "#         del_std_loss = (del_std**2).mean()\n",
    "        self.tree.std_loss += del_std_loss\n",
    "        return\n",
    "            \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_freezing_connection(to_freeze)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_decaying_connection(to_remove)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_freezed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.remove_freezed_connection(remaining)\n",
    "            if self.shortcut: self.std_ratio = self.std_ratio[:, remaining]\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_decayed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_input_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_output_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.add_output_connection(num)\n",
    "            # if torch.is_tensor(self.std_ratio):\n",
    "            if self.shortcut:\n",
    "                self.std_ratio = torch.cat((self.std_ratio, torch.zeros(1, num, device=self.tree.device)), dim=1)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        \n",
    "        if self.residual is None:\n",
    "            # print(f\"Adding {num} hidden units.. in new residual_layer\")\n",
    "            self.residual = Residual_Conv(self.tree, self.input_dim,\n",
    "                                          num, self.output_dim, stride=self.stride,\n",
    "                                          activation=self.activation).to(self.tree.device)\n",
    "            \n",
    "            self.tree.parent_dict[self.residual] = self\n",
    "            if self.shortcut is None:\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            else:\n",
    "                self.forward = self.forward_both\n",
    "                self.std_ratio = torch.zeros(1, self.output_dim, device=self.tree.device)\n",
    "                \n",
    "        else:\n",
    "            # print(f\"Adding {num} hidden units..\")\n",
    "            self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            if self.std_ratio.min()>0.98 and self.target_std_ratio>0.98:\n",
    "                del self.tree.parent_dict[self.shortcut]\n",
    "                del self.shortcut\n",
    "                self.shortcut = None\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            \n",
    "        elif self.target_std_ratio<0.95:\n",
    "            self.shortcut = Shortcut_Conv(self.tree, self.input_dim, self.output_dim, stride=self.stride)\n",
    "            self.shortcut.bn.weight.data *= 0.\n",
    "            self.shortcut.weight.data *= 0.1\n",
    "            self.forward = self.forward_both\n",
    "            \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.residual.hidden_dim < 1:\n",
    "            del self.tree.parent_dict[self.residual]\n",
    "            del self.residual\n",
    "            ### its parent (Residual_Conv) removes it from dynamic list if possible\n",
    "            self.residual = None\n",
    "            self.forward = self.forward_shortcut\n",
    "            self.std_ratio = 0.\n",
    "            return\n",
    "        \n",
    "#         max_dim = np.ceil((self.input_dim+self.output_dim)/2)\n",
    "        # max_dim = min((self.input_dim, self.output_dim))+1\n",
    "        max_dim = _get_hidden_neuron_number(self.input_dim, self.output_dim) + 1 \n",
    "        # print(\"MaxDIM\", max_dim, self.residual.hidden_dim)\n",
    "        if self.residual.hidden_dim > max_dim:\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc0)\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc1)\n",
    "            # print(\"Added\", self.residual)\n",
    "            \n",
    "        # self.residual.fc0.morph_network()\n",
    "        # self.residual.fc1.morph_network()\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        stdr = self.std_ratio\n",
    "        if torch.is_tensor(self.std_ratio):\n",
    "            stdr = self.std_ratio.min()\n",
    "            \n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{self.target_std_ratio}, s:{stdr}\")\n",
    "        if self.shortcut:\n",
    "            self.shortcut.print_network_debug(depth+1)\n",
    "        if self.residual:\n",
    "            self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        if self.residual is None:\n",
    "            return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            print(f\"{pre_string}╠════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}║    \")\n",
    "            print(f\"{pre_string}╠════╝\")\n",
    "        else:\n",
    "            print(f\"{pre_string}╚════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}     \")\n",
    "            print(f\"{pre_string}╔════╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Conv Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation, hidden_dim, post_activation=None):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "        self.post_activation = post_activation\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = hrnet0\n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = hrnet1\n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        if self.post_activation:\n",
    "            x = self.post_activation(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "            self.apnz = self.apnz/self.count\n",
    "#             self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "            self.significance = self.significance*(1-self.apnz**33) / 0.872 ## tried on desmos.\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "#         self.count = None\n",
    "\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "#             self.significance += z.pow(2).sum(dim=0)\n",
    "            self.significance += z.abs().sum(dim=0)\n",
    "#             self.significance += z.abs().pow(0.8).sum(dim=0)\n",
    "#             print(\"Current Significance \\n\", self.significance)\n",
    "#             print(f\"SIG ACT:\\n{float(self.activations.abs().mean())}\")\n",
    "#             print(f\"GRAD Mean, Std:\\n{float(grad_output[0].data.abs().mean()), float(grad_output[0].data.std())}\")\n",
    "\n",
    "            if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "                self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "        #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "                self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below=None, above=None, mask=None):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(self.significance.numel(), dtype=torch.bool)\n",
    "        if below:\n",
    "            mask = torch.logical_or(mask,self.significance<=below)\n",
    "        if above:\n",
    "            mask = torch.logical_or(mask,self.significance>above)\n",
    "            \n",
    "        print(f\"Significance:\\n{self.significance}\\nPrune:\\n{mask}\")\n",
    "        \n",
    "        self.to_remove = torch.nonzero(mask).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None:\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10)<0 \n",
    "b = torch.randn(10) > 0.5\n",
    "torch.nonzero(torch.logical_and(a,b), as_tuple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation=nn.ReLU(), post_activation=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = hrnet0.input_dim\n",
    "        self.output_dim = hrnet1.output_dim\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = None\n",
    "        self.residual = Residual_Conv_Connector(self.tree, hrnet0, hrnet1,\n",
    "                                                activation, hrnet0.output_dim, post_activation)\n",
    "        self.tree.parent_dict[self.residual] = self\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.residual.fc1.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.residual.fc1.add_output_connection(num)\n",
    "        \n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):  \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        print(f\"{pre_string}╚╗\")\n",
    "        self.residual.print_network(f\"{pre_string} \")\n",
    "        print(f\"{pre_string}╔╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut only Hierarchical Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        _wd = nn.Linear(input_dim, output_dim, bias=False).weight.data\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "        self.add_parameters_to_optimizer()\n",
    "        return\n",
    "        \n",
    "    def add_parameters_to_optimizer(self):\n",
    "        for p in self.parameters():\n",
    "#             self.tree.optimizer.state[p] = {}\n",
    "            self.tree.optimizer.param_groups[0]['params'].append(p)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## input_dim        ## output_dim\n",
    "        if x.shape[1] + self.weight.shape[1] > 0:\n",
    "            return x.matmul(self.weight.t())\n",
    "        else:\n",
    "            # print(x.shape, self.weight.shape)\n",
    "            # print(x.matmul(self.weight.t()))\n",
    "            if x.shape[1] + self.weight.shape[1] == 0:\n",
    "                return torch.zeros(x.shape[0], self.weight.shape[0], dtype=x.dtype, device=x.device)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        pass\n",
    "    \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "        pass\n",
    "            \n",
    "     \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "#         _w = self.weight.data[remaining, :]\n",
    "#         del self.weight\n",
    "#         self.weight = nn.Parameter(_w)\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        self.weight.data = self.weight.data[remaining, :]\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        ops[self.weight][_var][remaining, :]\n",
    "        \n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "#         _w = self.weight.data[:, remaining]\n",
    "#         del self.weight\n",
    "#         self.weight = nn.Parameter(_w)\n",
    "        ops = self.tree.optimizer.state\n",
    "\n",
    "        self.weight.data = self.weight.data[:, remaining]\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        ops[self.weight][_var][:, remaining]\n",
    "\n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        _w = torch.zeros(o, num, dtype=self.weight.data.dtype, device=self.weight.data.device)\n",
    "#         _w += torch.randn_like(_w)\n",
    "        _w = torch.cat((self.weight.data, _w), dim=1)\n",
    "        self.weight.data = _w\n",
    "        self.weight.grad = None\n",
    "                \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        torch.cat((ops[self.weight][_var], \\\n",
    "                                  torch.zeros(o, num, dtype=ops[self.weight][_var].dtype,\n",
    "                                              device=ops[self.weight][_var].device)), \n",
    "                                  dim=1)\n",
    "        \n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i)\n",
    "#         stdv = torch.std(self.weight.data)\n",
    "    \n",
    "        _new = torch.empty(num, i, dtype=self.weight.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        _w = torch.cat((self.weight.data, _new), dim=0)\n",
    "        self.weight.data = _w\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        torch.cat((ops[self.weight][_var], \\\n",
    "                                  torch.zeros(num, i, k0, k1, dtype=ops[self.weight][_var].dtype,\n",
    "                                              device=ops[self.weight][_var].device)), \n",
    "                                  dim=0)\n",
    "        \n",
    "        # print(self.weight.data.shape)        \n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}S:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n",
    "\n",
    "\n",
    "class HierarchicalResidual_Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=None, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        ## this can be Shortcut Layer or None\n",
    "        if kernel is None:\n",
    "            self.shortcut = Shortcut(tree, self.input_dim, self.output_dim) \n",
    "        else:\n",
    "            self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, kernel, stride) \n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.shortcut.start_freezing_connection(to_freeze)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.shortcut.start_decaying_connection(to_remove)\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.shortcut.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.shortcut.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.shortcut.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.shortcut.add_output_connection(num)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        print(\"Cannot Add Hidden neuron to Shortcut Only Layer\")\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        pass\n",
    "        \n",
    "    def morph_network(self):\n",
    "        pass\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.shortcut.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree and Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree_State():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.DYNAMIC_LIST = set() ## residual parent is added, to make code effecient.\n",
    "        ## the parents which is not intended to have residual connection should not be added.\n",
    "        self.beta_std_ratio = None\n",
    "        self.beta_del_neuron = None\n",
    "        self.device = 'cpu'\n",
    "    \n",
    "        self.parent_dict = {}\n",
    "    \n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual:set = None\n",
    "        self.freeze_connection_shortcut:set = None\n",
    "        self.decay_connection_shortcut:set = None\n",
    "\n",
    "        self.decay_rate_std = 0.001\n",
    "\n",
    "        self.add_to_remove_ratio = 2.\n",
    "        \n",
    "#         self.dummy_param = nn.Parameter(torch.Tensor([0]))\n",
    "#         self.optimizer = adam_custom.Adam([self.dummy_param])\n",
    "        self.optimizer = None\n",
    "    \n",
    "        pass\n",
    "    \n",
    "    def get_decay_factor(self):\n",
    "        ratio = self.current_decay_step/self.total_decay_steps\n",
    "#         self.decay_factor = np.exp(-2*ratio)*(1-ratio)\n",
    "        ratio = np.clip(ratio, 0, 1)\n",
    "        self.decay_factor = (1-ratio)**2\n",
    "#         self.decay_factor = (1-ratio)\n",
    "        pass\n",
    "    \n",
    "    def clear_decay_variables(self):\n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual = None\n",
    "        self.freeze_connection_shortcut = None\n",
    "        self.decay_connection_shortcut = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tree_State()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constructing Hierarchical Residual CNN (Resnet Inspired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutActivation(nn.Module):\n",
    "    \n",
    "    def __init__(self, p=0.1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout2d(p) ## ok to reuse dropout !! caution\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dropout(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, device, lr, input_dim = 1, hidden_dims = [8, 16, 32, 64], output_dim = 10, final_activation=None,\n",
    "                 num_stat=5, num_std=100, decay_rate_std=0.001):\n",
    "        super().__init__()\n",
    "        self.tree = Tree_State()\n",
    "        self.tree.beta_del_neuron = (num_stat-1)/num_stat\n",
    "        self.tree.beta_std_ratio = (num_std-1)/num_std\n",
    "        self.tree.decay_rate_std = decay_rate_std\n",
    "        self.tree.device = device\n",
    "        \n",
    "        \n",
    "        dummy_param = nn.Parameter(torch.Tensor([0]))\n",
    "        ############################################################\n",
    "        self.tree.optimizer = adam_custom.Adam([dummy_param], lr=lr, weight_decay=1e-5)\n",
    "        self.tree.optimizer.param_groups[0]['params'] = []\n",
    "        ############################################################\n",
    "        \n",
    "        \n",
    "        self.root_net = None\n",
    "        self._construct_root_net(input_dim, hidden_dims, output_dim)\n",
    "        \n",
    "#         self.tree.DYNAMIC_LIST.add(self.root_net)\n",
    "        self.tree.parent_dict[self.root_net] = None\n",
    "        \n",
    "        if final_activation is None:\n",
    "            final_activation = lambda x: x\n",
    "        self.non_linearity = NonLinearity(self.tree, output_dim, final_activation)\n",
    "        \n",
    "        self.neurons_added = 0\n",
    "\n",
    "        self._remove_below = None ## temporary variable\n",
    "        \n",
    "    def _construct_root_net(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        actf = DropoutActivation()\n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 8, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 8, 8, activation=actf)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 8, 16, stride=2, activation=actf)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2, activation=actf)\n",
    "        hrn3 = HierarchicalResidual_Conv(self.tree, 32, 32, stride=2, activation=actf)\n",
    "\n",
    "    \n",
    "        actf = lambda x: x\n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0, actf)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1, actf)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2, actf)\n",
    "        hrnR0123 = HierarchicalResidual_Connector(self.tree, hrnR012, hrn3, actf)\n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 32, 10)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR0123fc = HierarchicalResidual_Connector(self.tree, hrnR0123, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR0123fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "        morphables = [self.root_net, hrnR0123, hrnR012, hrnR01, hrnR0, hrn3, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def _construct_root_net2(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        \n",
    "        \n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 16, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 16, 16)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 32, 64, stride=2)\n",
    "\n",
    "        actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "    \n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1, actf)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2, actf)\n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 64, 10)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR012fc = HierarchicalResidual_Connector(self.tree, hrnR012, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR012fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "        morphables = [hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrnR012, hrnR01, hrnR0, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.non_linearity(self.root_net(x))\n",
    "\n",
    "    def add_neurons(self, num):\n",
    "        num_stat = int(num*0.7)\n",
    "        num_random = num - num_stat\n",
    "        \n",
    "        DL = list(self.tree.DYNAMIC_LIST)\n",
    "        if num_random>0:\n",
    "            rands = torch.randint(high=len(DL), size=(num_random,))\n",
    "            index, count = torch.unique(rands, sorted=False, return_counts=True)\n",
    "            for i, idx in enumerate(index):\n",
    "                DL[idx].add_hidden_neuron(int(count[i]))\n",
    "\n",
    "        if num_stat>0:\n",
    "            del_neurons = []\n",
    "            for hr in DL:\n",
    "                if hr.residual:\n",
    "                    del_neurons.append(hr.residual.del_neurons)#+1e-7)\n",
    "                else:\n",
    "                    del_neurons.append(0.)#1e-7) ## residual layer yet not created \n",
    "            \n",
    "            prob_stat = torch.tensor(del_neurons)\n",
    "            prob_stat = torch.log(torch.exp(prob_stat)+1.)\n",
    "            m = torch.distributions.multinomial.Multinomial(total_count=num_stat,\n",
    "                                                            probs= prob_stat)\n",
    "            count = m.sample()#.type(torch.long)\n",
    "            for i, hr in enumerate(DL):\n",
    "                if count[i] < 1: continue\n",
    "                hr.add_hidden_neuron(int(count[i]))\n",
    "        \n",
    "        self.neurons_added += num \n",
    "        pass\n",
    "\n",
    "    def identify_removable_neurons(self, num=None, threshold_min=0., threshold_max=1.):\n",
    "        \n",
    "        all_sig = []\n",
    "        self.all_sig_ = []\n",
    "        \n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                all_sig.append(hr.residual.significance)\n",
    "                \n",
    "        all_sigs = torch.cat(all_sig)\n",
    "        del all_sig\n",
    "        \n",
    "#         print(\"All_sigs\", all_sigs)\n",
    "        \n",
    "#         print(\"Normalization\", (all_sigs/all_sigs.sum()).sum())\n",
    "        \n",
    "        ### Normalizes such that importance 1 is average importance\n",
    "        normalizer = float(torch.sum(all_sigs))/len(all_sigs)\n",
    "        all_sig = all_sigs/normalizer\n",
    "\n",
    "        ### Normalizes to range [0, 1]\n",
    "#         max_sig = all_sigs.max()\n",
    "#         all_sig = all_sigs/(max_sig+1e-9)\n",
    "#         print(\"All_sig\", all_sig)\n",
    "#         print(\"Sig sum\", all_sig.sum())\n",
    "        print(f\"Significance Stat:\\nMin, Max: {float(all_sig.min()), float(all_sig.max())}\")\n",
    "        print(f\"Mean, Std: {float(all_sig.mean()), float(all_sig.std())}\")\n",
    "        all_sig = all_sig[all_sig<threshold_max]\n",
    "        if len(all_sig)<1: ## if all significance is above threshold max \n",
    "            return 0, None, all_sigs\n",
    "        all_sig = torch.sort(all_sig)[0] ### sorted significance scores\n",
    "        \n",
    "        self.all_sig_ = all_sig\n",
    "        \n",
    "        if not num:num = int(np.ceil(self.neurons_added/self.tree.add_to_remove_ratio))\n",
    "        ## reset the neurons_added number if decay is started\n",
    "\n",
    "        remove_below = threshold_min\n",
    "        if num>len(all_sig):\n",
    "            remove_below = float(all_sig[-1])\n",
    "        elif num>0:\n",
    "            remove_below = float(all_sig[num-1])\n",
    "        \n",
    "        ### sig < threshold_min is always removed; whatsoever\n",
    "        if remove_below < threshold_min:\n",
    "            remove_below = threshold_min\n",
    "            \n",
    "        print(\"remove_below\", remove_below, \"true:\", remove_below*normalizer)\n",
    "        remove_below *= normalizer\n",
    "#         remove_below *= max_sig\n",
    "#         print(\"remove_below\", remove_below)\n",
    "\n",
    "        self._remove_below = remove_below\n",
    "#         self._remove_above = remove_above*normalizer\n",
    "        self._remove_above = None\n",
    "\n",
    "        return remove_below, all_sigs\n",
    "\n",
    "    def decay_neuron_start(self, decay_steps=1000):\n",
    "        if self._remove_below is None: return 0\n",
    "        \n",
    "        self.neurons_added = 0 ## resetting this variable\n",
    "        \n",
    "        self.tree.total_decay_steps = decay_steps\n",
    "        self.tree.current_decay_step = 0\n",
    "        self.tree.remove_neuron_residual = set()\n",
    "        self.tree.freeze_connection_shortcut = set()\n",
    "        self.tree.decay_connection_shortcut = set()\n",
    "        \n",
    "        count_remove = 0\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                ### always prune 1 % of the neurons randomly. It might overlap with less significant neurons\n",
    "                mask = torch.bernoulli(torch.ones_like(hr.residual.significance)*0.05).type(torch.bool)\n",
    "                count_remove += hr.residual.identify_removable_neurons(below=self._remove_below,\n",
    "                                                                       above=self._remove_above,\n",
    "                                                                       mask = mask\n",
    "                                                                      )\n",
    "        if count_remove<1:\n",
    "            self.tree.clear_decay_variables()\n",
    "        return count_remove\n",
    "    \n",
    "    def decay_neuron_step(self):\n",
    "        if self.tree.total_decay_steps is None:\n",
    "            return 0\n",
    "        \n",
    "        self.tree.current_decay_step += 1\n",
    "        \n",
    "        if self.tree.current_decay_step < self.tree.total_decay_steps:\n",
    "            self.tree.get_decay_factor()\n",
    "            for sh in self.tree.decay_connection_shortcut:\n",
    "                sh.decay_connection_step()\n",
    "            for sh in self.tree.freeze_connection_shortcut:\n",
    "                sh.freeze_connection_step()\n",
    "            return 1\n",
    "        else:\n",
    "#             if self.tree.current_decay_step == self.tree.total_decay_steps:\n",
    "#                 for sh in self.tree.decay_connection_shortcut:\n",
    "#     #                 sh.decay_connection_step()\n",
    "#                     print(\"------------------\")\n",
    "#                     print(sh.weight.data.shape, \"removing decayed; \", sh.to_remove)\n",
    "#                     print(\"Small vals\", torch.count_nonzero(sh.weight.data<1e-6))\n",
    "#                     print(\"data\", sh.weight.data[:, sh.to_remove])\n",
    "#                     print(\"grads\", sh.weight.grad[:, sh.to_remove])\n",
    "#                     print(\"initial\", sh.initial_remove)\n",
    "#                     break\n",
    "\n",
    "            \n",
    "            \n",
    "#             for rs in self.tree.remove_neuron_residual:\n",
    "#                 rs.remove_decayed_neurons()\n",
    "                \n",
    "#             self.tree.clear_decay_variables()\n",
    "#             self.maintain_network()\n",
    "\n",
    "            ### need to decay and freeze all the time\n",
    "            for sh in self.tree.decay_connection_shortcut:\n",
    "                sh.decay_connection_step()\n",
    "            for sh in self.tree.freeze_connection_shortcut:\n",
    "                sh.freeze_connection_step()\n",
    "            return -1\n",
    "        \n",
    "    def remove_decayed_neurons(self):\n",
    "        for rs in self.tree.remove_neuron_residual:\n",
    "            rs.remove_decayed_neurons()\n",
    "                \n",
    "        self.tree.clear_decay_variables()\n",
    "        self.maintain_network()\n",
    "        return\n",
    "\n",
    "    def compute_del_neurons(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.compute_del_neurons()\n",
    "    \n",
    "    def maintain_network(self):\n",
    "        self.root_net.maintain_shortcut_connection()\n",
    "        self.root_net.morph_network()\n",
    "        \n",
    "    def start_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.start_computing_significance()\n",
    "\n",
    "    def finish_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.finish_computing_significance()\n",
    "            \n",
    "    def print_network_debug(self):\n",
    "        self.root_net.print_network_debug(0)\n",
    "        \n",
    "    def print_network(self):\n",
    "        print(self.root_net.input_dim)\n",
    "        self.root_net.print_network()\n",
    "        print(\"│\")\n",
    "        print(self.root_net.output_dim)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.binomial(1, 0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.bernoulli(torch.ones(10)*0.01).type(torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dycnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcifar-10-batches-py\u001b[0m/  \u001b[01;31mcifar-10-python.tar.gz\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "ls \"../../_Datasets/cifar10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = transforms.Compose([\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "cifar_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameters\n",
    "# learning_rate = 0.001\n",
    "learning_rate = 0.0009\n",
    "\n",
    "num_add_neuron = 50 #50#25#10\n",
    "num_decay_steps = int(len(train_loader)*2)#3\n",
    "\n",
    "remove_above = 12 #10\n",
    "threshold_max = 0.5\n",
    "threshold_min = 0.01\n",
    "\n",
    "train_epoch_min = 1 #1\n",
    "train_epoch_max = 10 #10 #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet = Dynamic_CNN(device, learning_rate).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = dynet.tree.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563, 313)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.add_to_remove_ratio = 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.optimizer.param_groups[0]['lr'] = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necessary for initializing variables in Adam Optimizer\n",
    "dynet(torch.randn(1,3,32,32).to(device)).mean().backward()\n",
    "dynet.tree.optimizer.step()\n",
    "\n",
    "dynet.tree.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([-0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009,\n",
       "         -0.0009, -0.0009], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-5.5703e-02, -1.8930e-01, -1.2059e-01],\n",
       "           [ 1.7856e-02,  1.4319e-01, -2.3043e-02],\n",
       "           [ 1.7780e-01, -1.2991e-01,  9.9326e-02]],\n",
       " \n",
       "          [[-4.1155e-02,  1.8590e-01, -6.7313e-02],\n",
       "           [ 1.2220e-01,  3.9127e-02, -1.1250e-01],\n",
       "           [-7.2557e-02,  1.9125e-01,  6.6179e-02]],\n",
       " \n",
       "          [[-1.8131e-01,  1.3526e-01,  1.2205e-01],\n",
       "           [ 1.4724e-01,  9.4187e-02, -9.7348e-03],\n",
       "           [ 6.0044e-02, -1.6911e-01,  1.6732e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.6294e-03,  8.4629e-04,  1.2711e-01],\n",
       "           [ 1.2078e-01,  6.4700e-02, -1.4869e-01],\n",
       "           [-3.1222e-02, -5.2589e-02, -1.7636e-02]],\n",
       " \n",
       "          [[ 3.4677e-02,  5.5599e-02, -4.6393e-02],\n",
       "           [-4.7956e-02,  1.5129e-01, -2.2086e-02],\n",
       "           [-1.3991e-01,  1.0761e-01,  1.4586e-01]],\n",
       " \n",
       "          [[-9.2866e-02,  7.2832e-02, -6.9745e-02],\n",
       "           [ 1.5640e-01,  6.2109e-02, -1.3392e-01],\n",
       "           [-1.2589e-01, -4.6108e-02,  1.6425e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 8.9153e-02, -4.8346e-02, -1.4552e-01],\n",
       "           [ 1.1294e-01,  1.4859e-01, -4.3243e-02],\n",
       "           [-3.7931e-02, -9.1050e-02,  1.4655e-01]],\n",
       " \n",
       "          [[ 1.8323e-01, -2.4805e-02, -1.6270e-01],\n",
       "           [ 6.6408e-02, -6.0292e-02,  6.1545e-02],\n",
       "           [-1.6321e-01, -1.6069e-02, -9.7403e-02]],\n",
       " \n",
       "          [[ 4.7428e-02,  2.0885e-02, -6.2822e-02],\n",
       "           [ 1.6861e-01,  8.4014e-02,  1.3429e-01],\n",
       "           [ 1.3844e-01,  1.9040e-01,  9.4029e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.8477e-02, -1.7285e-04, -3.1358e-02],\n",
       "           [-1.1659e-01, -2.1163e-02, -1.1040e-01],\n",
       "           [ 5.7792e-02,  1.2855e-01, -1.8485e-02]],\n",
       " \n",
       "          [[-4.6322e-02, -7.3238e-02,  8.3404e-02],\n",
       "           [-1.2309e-01, -1.6627e-01, -2.7592e-02],\n",
       "           [ 4.6386e-02, -1.7531e-01,  1.7540e-01]],\n",
       " \n",
       "          [[ 9.9185e-02,  3.3236e-02,  3.7494e-02],\n",
       "           [ 6.1158e-02, -1.6947e-02, -1.8616e-01],\n",
       "           [ 4.6673e-02,  1.5698e-01, -7.9005e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.1837e-01, -1.4000e-01, -3.3200e-03],\n",
       "           [-9.2440e-02,  1.8589e-01,  6.3253e-02],\n",
       "           [-1.0663e-01,  9.2424e-02,  5.1739e-02]],\n",
       " \n",
       "          [[-2.3630e-03, -9.2072e-03,  1.8170e-02],\n",
       "           [ 4.2063e-02,  8.5953e-02, -5.8541e-02],\n",
       "           [ 8.1687e-02, -8.4843e-02,  5.9178e-02]],\n",
       " \n",
       "          [[ 5.9736e-02,  7.8083e-02,  1.1295e-01],\n",
       "           [-1.7301e-01, -4.1670e-02,  1.7512e-01],\n",
       "           [ 3.0574e-02, -1.6459e-01, -1.8772e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.7431e-01, -8.0356e-02,  1.3006e-01],\n",
       "           [-1.7206e-01, -9.4230e-02,  9.8254e-02],\n",
       "           [-1.7612e-01, -1.3631e-01, -3.8322e-02]],\n",
       " \n",
       "          [[-1.9088e-01,  1.4425e-01, -1.4387e-01],\n",
       "           [ 1.2341e-01, -5.7118e-02,  3.7989e-02],\n",
       "           [-1.6740e-01, -3.3371e-02,  1.8572e-01]],\n",
       " \n",
       "          [[-2.9792e-02,  1.5768e-01, -3.9545e-02],\n",
       "           [-6.5627e-02,  1.1575e-01,  6.0459e-02],\n",
       "           [ 1.0293e-01,  1.1593e-01,  8.1366e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0220e-01,  2.1723e-02, -1.4390e-02],\n",
       "           [-1.5553e-01, -8.2993e-02, -6.6285e-02],\n",
       "           [-6.7716e-02, -5.7565e-02,  1.7704e-01]],\n",
       " \n",
       "          [[-1.4083e-01, -1.4394e-01,  1.7138e-01],\n",
       "           [ 6.2994e-02, -4.3541e-02, -5.0947e-02],\n",
       "           [ 7.2890e-02,  9.5993e-02,  1.1132e-01]],\n",
       " \n",
       "          [[-1.8661e-01, -1.7437e-01,  1.3776e-01],\n",
       "           [-9.0988e-02, -6.8625e-02, -1.6615e-02],\n",
       "           [-4.4022e-02, -6.9727e-02, -1.3337e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.1279e-01, -1.5291e-01, -8.4339e-02],\n",
       "           [-1.9075e-01, -1.4527e-01,  1.8494e-02],\n",
       "           [-1.6626e-01,  6.7979e-02,  3.7581e-02]],\n",
       " \n",
       "          [[-1.1558e-01, -1.7290e-01,  2.7687e-03],\n",
       "           [ 1.4515e-01,  9.6457e-02,  1.5122e-01],\n",
       "           [-1.4001e-01, -2.4721e-02,  1.7420e-01]],\n",
       " \n",
       "          [[-1.3206e-04,  1.2146e-01,  1.1441e-01],\n",
       "           [-6.5931e-02, -1.6496e-01, -5.5603e-02],\n",
       "           [ 1.2408e-01, -1.3413e-01,  1.2070e-01]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-5.4724e-07,  7.6908e-08, -2.4104e-07, -1.6286e-06, -2.2674e-06,\n",
       "         -6.9854e-07, -3.7948e-07,  8.3706e-07], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-1.1204e-01, -8.7083e-03, -9.5877e-02],\n",
       "           [ 3.1211e-02,  1.0616e-01,  4.7606e-02],\n",
       "           [ 7.9017e-02,  6.9394e-03, -1.0252e-01]],\n",
       " \n",
       "          [[-2.7647e-02, -1.2667e-03, -7.7603e-02],\n",
       "           [-2.1547e-02, -1.4778e-02, -5.9826e-02],\n",
       "           [-2.1059e-02,  3.9364e-03,  5.2409e-02]],\n",
       " \n",
       "          [[-6.2742e-02,  1.1027e-01,  6.7881e-02],\n",
       "           [ 2.9714e-02,  7.0672e-03, -2.2641e-03],\n",
       "           [-9.6225e-02, -6.6526e-02,  5.1314e-02]],\n",
       " \n",
       "          [[-3.6849e-03, -2.2615e-02, -4.0396e-02],\n",
       "           [-1.1675e-01,  6.0034e-02,  3.3405e-03],\n",
       "           [-3.2858e-02,  8.2775e-02,  6.7217e-02]],\n",
       " \n",
       "          [[-4.8862e-02, -1.1199e-01, -1.0224e-01],\n",
       "           [-8.0395e-02,  7.9611e-02, -8.1639e-02],\n",
       "           [-3.5312e-02, -3.6413e-02, -9.9292e-02]],\n",
       " \n",
       "          [[-1.0013e-01,  3.1484e-02, -6.9833e-02],\n",
       "           [ 7.9256e-02, -7.5665e-02,  5.3070e-02],\n",
       "           [-3.6440e-02,  4.0639e-02,  1.1125e-01]],\n",
       " \n",
       "          [[ 9.7275e-02,  2.1696e-02, -5.1008e-02],\n",
       "           [-3.5896e-02, -6.1242e-02,  8.3026e-02],\n",
       "           [ 2.7951e-02, -1.0980e-01,  1.4407e-03]],\n",
       " \n",
       "          [[-1.7412e-02, -9.1045e-02,  3.3372e-02],\n",
       "           [ 3.3294e-02, -9.2244e-02, -7.1671e-02],\n",
       "           [ 6.2237e-02,  9.3343e-02,  1.0248e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.4262e-02, -5.5827e-02, -8.6799e-03],\n",
       "           [-2.9752e-02, -8.0327e-03,  1.0054e-02],\n",
       "           [ 6.7571e-02,  9.3637e-02,  6.0055e-02]],\n",
       " \n",
       "          [[-1.0465e-01,  8.9079e-02,  4.4068e-03],\n",
       "           [ 1.1168e-01,  5.3959e-02, -1.0490e-01],\n",
       "           [-1.0673e-01,  1.1333e-01, -5.1636e-02]],\n",
       " \n",
       "          [[ 5.0618e-02,  1.1133e-01, -1.3613e-02],\n",
       "           [-9.9312e-02, -6.7640e-02,  3.7624e-02],\n",
       "           [-9.9570e-03, -1.9544e-02,  9.6853e-02]],\n",
       " \n",
       "          [[ 3.9988e-02,  8.0686e-02,  4.1634e-02],\n",
       "           [-8.8330e-02, -3.7574e-02,  1.0231e-01],\n",
       "           [ 6.1909e-02,  5.0839e-02, -8.1225e-02]],\n",
       " \n",
       "          [[-9.2668e-02, -2.3513e-02,  7.2364e-02],\n",
       "           [-2.2982e-02,  5.4584e-02, -7.4816e-02],\n",
       "           [-2.0781e-02,  8.1380e-02,  6.1109e-02]],\n",
       " \n",
       "          [[-5.5055e-02, -2.1303e-03, -7.0863e-02],\n",
       "           [ 4.4606e-02, -4.3191e-02,  1.1012e-01],\n",
       "           [ 3.2819e-02, -3.5415e-02,  1.0293e-02]],\n",
       " \n",
       "          [[ 6.7616e-02, -1.0545e-01, -3.3338e-02],\n",
       "           [-9.8349e-02,  2.5693e-03, -2.4482e-02],\n",
       "           [ 3.4979e-02,  3.7004e-02, -2.7554e-02]],\n",
       " \n",
       "          [[-7.1072e-02,  1.4042e-02,  4.9901e-02],\n",
       "           [-4.7128e-02,  5.6501e-02, -3.1151e-02],\n",
       "           [ 7.6442e-02,  1.0259e-01, -8.5512e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 4.4465e-02, -7.2087e-02, -2.6747e-02],\n",
       "           [-1.6917e-02, -1.0905e-01, -2.6073e-02],\n",
       "           [ 1.6281e-02,  8.3130e-02, -8.9424e-02]],\n",
       " \n",
       "          [[-8.3924e-02,  8.2892e-02,  1.1293e-01],\n",
       "           [-3.3177e-02, -1.1330e-01,  1.1440e-01],\n",
       "           [ 5.2956e-02,  9.6606e-02,  6.1532e-02]],\n",
       " \n",
       "          [[ 9.0560e-02, -4.4822e-02, -1.0569e-01],\n",
       "           [-7.9810e-02, -1.0703e-01, -8.4526e-02],\n",
       "           [ 1.1298e-03,  1.0660e-01, -1.1671e-01]],\n",
       " \n",
       "          [[-8.3851e-02, -3.7165e-02,  2.1225e-02],\n",
       "           [ 3.1691e-02, -7.9305e-02,  1.0605e-01],\n",
       "           [ 6.2138e-02, -2.6249e-02,  2.1649e-02]],\n",
       " \n",
       "          [[ 7.4596e-03, -1.6809e-02, -1.1597e-01],\n",
       "           [-3.6375e-02, -3.0000e-03,  1.5307e-02],\n",
       "           [ 9.4800e-02, -1.0783e-01,  2.6700e-03]],\n",
       " \n",
       "          [[ 7.0491e-02, -7.4283e-02, -2.1550e-02],\n",
       "           [ 1.0519e-01,  7.4596e-04,  2.9523e-02],\n",
       "           [ 8.6646e-02, -5.9483e-02,  4.1908e-03]],\n",
       " \n",
       "          [[-3.7324e-02, -8.3647e-02, -6.5151e-02],\n",
       "           [ 1.1025e-01,  3.5278e-02, -1.1336e-01],\n",
       "           [ 8.0002e-02, -5.5011e-02,  1.5078e-02]],\n",
       " \n",
       "          [[-9.6799e-02, -1.0770e-01,  9.1954e-02],\n",
       "           [-1.1190e-01,  9.3753e-02,  7.8819e-02],\n",
       "           [ 7.9659e-02, -7.0194e-02, -6.1916e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.7113e-02,  1.1386e-02, -6.5783e-02],\n",
       "           [ 2.9118e-02, -8.9332e-02,  1.5017e-02],\n",
       "           [-4.9888e-02, -9.0033e-02, -1.0234e-01]],\n",
       " \n",
       "          [[ 3.1489e-02,  6.1833e-02, -2.4286e-02],\n",
       "           [ 1.0610e-01,  7.8311e-02, -8.5046e-02],\n",
       "           [-4.2122e-02, -4.5390e-02, -9.6377e-02]],\n",
       " \n",
       "          [[ 9.6539e-02,  9.9606e-02, -1.0202e-01],\n",
       "           [-7.5750e-02, -5.0135e-02,  5.2602e-02],\n",
       "           [-5.5276e-02, -1.0262e-01, -2.2844e-02]],\n",
       " \n",
       "          [[ 6.4503e-02, -1.7541e-03,  7.5780e-02],\n",
       "           [-3.6450e-02, -4.2538e-02, -6.8106e-02],\n",
       "           [-1.0984e-01, -1.1161e-01, -4.8644e-02]],\n",
       " \n",
       "          [[ 6.9338e-02,  6.9271e-02,  8.3634e-02],\n",
       "           [-1.1427e-01, -8.2171e-02,  5.3822e-05],\n",
       "           [-5.4916e-02,  9.2989e-02,  8.9718e-02]],\n",
       " \n",
       "          [[ 6.4322e-02,  1.1107e-03,  3.6293e-02],\n",
       "           [-6.5285e-02, -1.7867e-02, -1.0609e-01],\n",
       "           [-4.2338e-02,  2.1777e-02, -4.8608e-02]],\n",
       " \n",
       "          [[ 8.7332e-02, -6.8450e-02,  9.9521e-02],\n",
       "           [-6.2570e-02,  4.0924e-03, -3.7535e-02],\n",
       "           [-7.3029e-02, -5.4864e-02,  3.4797e-02]],\n",
       " \n",
       "          [[ 5.9457e-03, -1.1542e-01, -8.6007e-02],\n",
       "           [ 5.8610e-02, -9.7701e-02,  6.1189e-02],\n",
       "           [ 4.4234e-02, -1.1357e-01, -5.6371e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.7427e-02,  3.0319e-02,  5.9533e-03],\n",
       "           [ 3.4840e-02, -3.7193e-02, -4.3857e-02],\n",
       "           [-7.0265e-02, -6.5320e-02, -2.5849e-02]],\n",
       " \n",
       "          [[ 5.0931e-03, -1.1098e-01, -7.8434e-02],\n",
       "           [-1.0322e-01, -1.0942e-01, -3.1720e-02],\n",
       "           [-5.0116e-02, -1.1369e-01, -3.3012e-02]],\n",
       " \n",
       "          [[ 9.2474e-02,  3.8578e-02,  8.0028e-02],\n",
       "           [-4.1832e-02, -1.1725e-02,  3.8776e-02],\n",
       "           [ 5.8156e-02,  1.5310e-02, -9.9664e-02]],\n",
       " \n",
       "          [[ 1.0196e-01, -2.2067e-02,  1.0031e-01],\n",
       "           [ 3.5484e-02, -9.3825e-02,  1.3292e-02],\n",
       "           [ 5.0543e-02, -2.6210e-03, -6.8574e-02]],\n",
       " \n",
       "          [[ 7.4940e-02,  8.3696e-02,  2.5095e-02],\n",
       "           [ 6.8669e-02, -3.7566e-02, -2.1460e-02],\n",
       "           [ 3.7734e-02, -5.3456e-02, -8.2738e-02]],\n",
       " \n",
       "          [[ 1.0753e-01,  7.5158e-02,  1.0068e-01],\n",
       "           [ 5.8314e-02,  2.1967e-02,  1.6198e-02],\n",
       "           [-6.7203e-02, -3.0378e-02,  9.4868e-03]],\n",
       " \n",
       "          [[ 2.7230e-02, -4.4423e-02, -5.1098e-02],\n",
       "           [ 5.9860e-02,  6.6437e-02, -8.1790e-02],\n",
       "           [-2.0532e-02, -9.3936e-02, -1.1095e-01]],\n",
       " \n",
       "          [[-1.1821e-02, -9.4017e-02,  4.8936e-02],\n",
       "           [-1.0155e-02,  5.6482e-02,  3.1082e-02],\n",
       "           [ 1.4605e-02, -2.9733e-02, -5.3402e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 4.0995e-02,  9.7524e-02, -9.9673e-02],\n",
       "           [ 6.8130e-02, -8.8644e-02, -9.6812e-02],\n",
       "           [-1.1039e-01, -8.7048e-02,  3.1276e-02]],\n",
       " \n",
       "          [[ 6.4818e-02, -4.2535e-02, -3.8181e-02],\n",
       "           [ 1.1434e-02,  1.9142e-02, -5.8794e-02],\n",
       "           [ 8.5606e-02,  2.1307e-02, -1.3207e-02]],\n",
       " \n",
       "          [[-1.0908e-01, -6.3910e-02, -1.0976e-01],\n",
       "           [ 4.6956e-02,  1.9721e-02,  1.0178e-01],\n",
       "           [-7.9414e-02,  6.6230e-02,  6.9752e-02]],\n",
       " \n",
       "          [[ 3.4678e-02, -8.8657e-03,  1.1414e-01],\n",
       "           [-1.1413e-01, -2.2530e-02, -8.6386e-02],\n",
       "           [ 1.3316e-02,  6.9726e-02, -9.7201e-02]],\n",
       " \n",
       "          [[ 1.0052e-01, -3.2168e-02, -9.8293e-02],\n",
       "           [ 3.4497e-02,  7.3934e-03,  9.4932e-02],\n",
       "           [ 6.2105e-02,  5.2318e-02,  6.7303e-02]],\n",
       " \n",
       "          [[-4.4967e-02,  8.8341e-02,  4.5762e-02],\n",
       "           [-8.0300e-02,  2.7251e-02, -3.5652e-02],\n",
       "           [-7.8153e-02, -1.2157e-02,  6.9326e-02]],\n",
       " \n",
       "          [[ 1.7938e-02,  7.0601e-02,  2.1974e-02],\n",
       "           [ 2.9613e-02,  1.3054e-02, -9.7282e-02],\n",
       "           [-1.1679e-01,  3.6296e-02, -4.7555e-02]],\n",
       " \n",
       "          [[-1.0453e-01, -1.0581e-01, -1.5969e-02],\n",
       "           [ 1.0772e-02,  6.4301e-02,  7.9499e-02],\n",
       "           [ 2.9531e-02, -2.9546e-02, -4.7459e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 4.9752e-02, -7.3081e-03,  3.2373e-02],\n",
       "           [-4.7106e-03, -5.1779e-02, -9.9985e-02],\n",
       "           [-6.6789e-02,  6.4428e-03,  6.3614e-03]],\n",
       " \n",
       "          [[ 9.2401e-02,  4.3053e-02, -3.1678e-02],\n",
       "           [-6.5732e-02,  9.5645e-02, -9.6327e-02],\n",
       "           [ 1.0088e-01, -1.3430e-02,  7.2236e-02]],\n",
       " \n",
       "          [[ 3.1151e-02, -2.0771e-03,  1.0930e-01],\n",
       "           [-4.1599e-02,  1.3183e-02,  1.4337e-03],\n",
       "           [-7.1621e-02, -1.0437e-01,  1.4319e-02]],\n",
       " \n",
       "          [[ 2.0051e-02, -7.0241e-02, -1.0355e-01],\n",
       "           [-1.1678e-01,  8.5280e-02,  1.1335e-01],\n",
       "           [ 2.9223e-02,  3.5493e-02, -3.7593e-02]],\n",
       " \n",
       "          [[ 4.4081e-02,  7.2522e-03, -6.9983e-02],\n",
       "           [-7.0121e-02,  2.3762e-02, -3.8279e-02],\n",
       "           [-8.1340e-02,  2.3944e-02,  5.1958e-02]],\n",
       " \n",
       "          [[ 1.1531e-01, -1.4706e-02,  7.7645e-02],\n",
       "           [ 3.9597e-02, -7.3726e-02,  5.2114e-02],\n",
       "           [-7.1826e-02, -1.0179e-01,  9.6568e-02]],\n",
       " \n",
       "          [[ 1.0253e-01, -1.0252e-01, -8.0612e-02],\n",
       "           [ 5.3928e-02, -5.9175e-03, -1.3993e-02],\n",
       "           [-8.6369e-02,  5.6161e-02,  1.0544e-01]],\n",
       " \n",
       "          [[ 3.9484e-03,  1.1043e-01,  1.0585e-01],\n",
       "           [-1.0136e-01,  5.7687e-02, -1.0653e-01],\n",
       "           [ 5.9775e-02, -7.2165e-02, -9.6223e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.6241e-02, -8.1856e-02, -3.7362e-02],\n",
       "           [ 2.9810e-02,  5.2440e-02, -7.9531e-03],\n",
       "           [ 1.1292e-01,  3.9550e-02, -2.4322e-02]],\n",
       " \n",
       "          [[-1.0676e-01, -9.1573e-02,  4.6585e-02],\n",
       "           [-7.1540e-02, -1.0945e-01, -9.4906e-02],\n",
       "           [ 1.8249e-02, -1.1438e-01, -2.7742e-02]],\n",
       " \n",
       "          [[-6.1363e-02, -5.0361e-02,  1.0191e-01],\n",
       "           [ 5.4346e-02, -3.6740e-02, -9.8894e-03],\n",
       "           [ 2.3241e-03,  4.5398e-02,  3.6972e-02]],\n",
       " \n",
       "          [[-5.9315e-02, -9.3393e-02, -5.3063e-02],\n",
       "           [ 6.9268e-02, -3.2603e-02, -2.7271e-02],\n",
       "           [ 3.5140e-02,  1.0841e-01,  4.3908e-02]],\n",
       " \n",
       "          [[ 3.9270e-02, -4.5543e-02, -9.6131e-02],\n",
       "           [ 1.0409e-01,  1.1317e-02, -1.1154e-01],\n",
       "           [ 7.4669e-02, -9.6542e-02,  8.6878e-02]],\n",
       " \n",
       "          [[ 1.0593e-01, -6.2832e-02, -8.5777e-02],\n",
       "           [ 9.3292e-02,  6.0841e-02,  4.2705e-02],\n",
       "           [-3.1276e-02, -1.0453e-01,  2.5033e-03]],\n",
       " \n",
       "          [[ 7.4915e-02,  7.3952e-02,  3.0586e-02],\n",
       "           [ 3.8279e-03,  5.8632e-02,  2.8540e-02],\n",
       "           [-9.0073e-02, -1.1049e-01,  1.0026e-01]],\n",
       " \n",
       "          [[-5.6125e-02, -4.6258e-02, -9.2218e-02],\n",
       "           [-6.7282e-02,  5.5424e-02,  5.2263e-02],\n",
       "           [ 2.8136e-02, -2.4876e-04,  6.3413e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-5.9647e-07,  2.4330e-06, -4.5162e-07,  4.8748e-07, -3.9361e-07,\n",
       "          5.2772e-07, -6.9997e-07,  1.2391e-06], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 1.0596e-01, -1.0975e-01, -6.7626e-02],\n",
       "           [ 6.5663e-02,  1.0368e-02, -6.6725e-02],\n",
       "           [ 1.1211e-01,  4.7146e-02, -7.0935e-02]],\n",
       " \n",
       "          [[-2.0753e-02, -5.1216e-02,  1.0655e-02],\n",
       "           [ 4.5260e-02, -1.0892e-01,  6.8115e-03],\n",
       "           [-1.5083e-02, -4.6793e-02,  8.8906e-02]],\n",
       " \n",
       "          [[ 3.4628e-02, -4.8004e-03,  3.4816e-02],\n",
       "           [-7.2462e-02, -4.1115e-02,  4.5502e-02],\n",
       "           [ 4.3811e-02,  3.8335e-02, -2.0451e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0073e-02, -1.1173e-01,  1.0189e-01],\n",
       "           [ 5.4502e-02, -7.7062e-02,  8.0561e-02],\n",
       "           [ 1.1590e-01,  4.9802e-02,  8.8252e-03]],\n",
       " \n",
       "          [[-7.1533e-02,  1.0322e-01, -9.2861e-02],\n",
       "           [ 2.9812e-02,  8.4678e-02,  4.6187e-02],\n",
       "           [ 1.0990e-01,  8.5500e-03, -3.9742e-02]],\n",
       " \n",
       "          [[-5.9567e-02,  8.6537e-02,  5.0265e-02],\n",
       "           [ 6.8574e-02, -6.5873e-03,  7.8880e-03],\n",
       "           [ 1.0865e-01,  5.2039e-03,  5.0599e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 1.1654e-01, -9.3164e-02, -8.8377e-04],\n",
       "           [ 8.0141e-02, -2.6843e-02,  8.5009e-02],\n",
       "           [ 6.6311e-02,  1.1513e-01, -8.2012e-02]],\n",
       " \n",
       "          [[-1.0365e-01, -2.8935e-02,  3.1974e-02],\n",
       "           [-5.9020e-02,  5.7771e-02,  4.8060e-02],\n",
       "           [-8.6415e-02,  8.2729e-02, -9.9609e-02]],\n",
       " \n",
       "          [[-2.6040e-02,  5.1873e-02,  5.8832e-02],\n",
       "           [-8.6570e-02,  2.4066e-02, -9.5450e-02],\n",
       "           [ 3.9675e-02,  8.3497e-02, -3.1599e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.3471e-02,  8.9466e-02,  5.9875e-02],\n",
       "           [-2.5478e-02, -2.9288e-02, -2.2079e-02],\n",
       "           [ 9.9045e-02,  5.0363e-02, -3.2930e-02]],\n",
       " \n",
       "          [[ 1.3561e-02,  1.1109e-01,  5.0832e-02],\n",
       "           [ 9.8663e-02, -1.1557e-01, -1.4724e-02],\n",
       "           [-5.0860e-02,  8.1908e-02,  5.2527e-02]],\n",
       " \n",
       "          [[-4.4433e-03,  5.6079e-04, -1.0445e-01],\n",
       "           [-2.4524e-03, -1.1076e-01, -3.5353e-02],\n",
       "           [-1.5659e-02,  1.0133e-01,  7.1413e-02]]],\n",
       " \n",
       " \n",
       "         [[[-7.9916e-02,  9.4997e-02,  6.3330e-02],\n",
       "           [ 2.5214e-02,  5.8911e-03, -1.0595e-01],\n",
       "           [-1.0115e-02,  5.7860e-02, -4.4751e-02]],\n",
       " \n",
       "          [[ 5.0578e-02,  3.6965e-02,  4.9432e-02],\n",
       "           [ 9.6686e-02,  8.9037e-02, -1.1252e-01],\n",
       "           [ 4.0072e-02, -4.7423e-02, -9.3374e-02]],\n",
       " \n",
       "          [[-3.7092e-02,  9.8984e-02, -8.5466e-02],\n",
       "           [-1.9025e-02, -7.6891e-02,  7.5766e-02],\n",
       "           [ 3.7416e-02, -7.3661e-02, -5.4462e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.5936e-02,  1.2329e-02,  6.4016e-02],\n",
       "           [ 9.7418e-02,  7.3226e-02,  5.6733e-02],\n",
       "           [-1.1685e-02, -7.3565e-02, -1.7415e-02]],\n",
       " \n",
       "          [[ 4.5692e-02, -4.8535e-02, -7.3836e-02],\n",
       "           [-9.9504e-02, -5.4173e-02, -6.6413e-02],\n",
       "           [ 1.0260e-01,  9.6400e-02, -4.4833e-02]],\n",
       " \n",
       "          [[-8.3892e-02, -1.0901e-02, -5.9537e-03],\n",
       "           [ 5.5924e-02, -4.3667e-02, -1.1505e-01],\n",
       "           [ 7.0446e-02,  5.3306e-02, -8.2058e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-1.5829e-02, -1.1814e-02,  3.4990e-02],\n",
       "           [-8.6993e-02,  5.9150e-02, -5.6401e-02],\n",
       "           [-9.7279e-02, -6.9699e-03, -7.5337e-02]],\n",
       " \n",
       "          [[ 2.9813e-02,  1.1549e-01, -4.6069e-02],\n",
       "           [-4.3739e-02,  1.7784e-02, -7.2684e-02],\n",
       "           [ 2.1939e-02,  1.0849e-01,  4.3122e-02]],\n",
       " \n",
       "          [[ 1.5324e-02, -1.0589e-01,  6.3949e-03],\n",
       "           [-5.8176e-02, -5.9067e-02,  5.1694e-02],\n",
       "           [ 8.1678e-02, -1.3615e-02,  1.1435e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 8.8207e-02,  1.4562e-02,  5.7979e-02],\n",
       "           [ 2.1414e-02,  7.6598e-02,  7.1055e-02],\n",
       "           [-3.3177e-02,  1.0537e-01,  3.9153e-02]],\n",
       " \n",
       "          [[ 7.6854e-02, -1.1022e-01, -3.8383e-02],\n",
       "           [-1.2481e-02,  6.9782e-03,  9.3605e-02],\n",
       "           [ 7.5039e-02, -4.2673e-02,  1.0677e-01]],\n",
       " \n",
       "          [[-2.0955e-02, -1.1383e-01, -7.4705e-03],\n",
       "           [ 1.3901e-02, -3.5887e-02,  6.3933e-02],\n",
       "           [-1.0142e-01,  1.1030e-01, -1.1376e-01]]],\n",
       " \n",
       " \n",
       "         [[[-8.1285e-02,  7.0261e-02, -3.6781e-02],\n",
       "           [-2.5183e-02, -6.9895e-02,  1.5650e-02],\n",
       "           [-9.9628e-02, -1.6389e-03, -6.6890e-02]],\n",
       " \n",
       "          [[ 5.6473e-02, -9.3429e-02,  7.4969e-02],\n",
       "           [ 7.9101e-03, -5.1197e-02,  7.1813e-05],\n",
       "           [-8.0214e-02,  9.1234e-02,  1.0307e-01]],\n",
       " \n",
       "          [[-2.9440e-02,  1.9557e-02, -3.8387e-02],\n",
       "           [-3.1686e-02,  5.2514e-02, -7.8262e-03],\n",
       "           [-9.1575e-02, -1.1179e-01, -6.6905e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 7.8368e-02, -4.7290e-02, -7.3153e-02],\n",
       "           [ 1.0338e-01,  1.2947e-02, -6.1828e-02],\n",
       "           [ 4.2105e-02,  5.9849e-02, -8.5272e-02]],\n",
       " \n",
       "          [[-3.9795e-03,  1.0408e-01,  4.1150e-02],\n",
       "           [-2.5273e-03,  4.0724e-02,  9.0541e-02],\n",
       "           [ 9.2784e-04,  8.1323e-02,  6.5463e-02]],\n",
       " \n",
       "          [[-5.8732e-02, -5.1107e-02, -3.1513e-02],\n",
       "           [ 7.7662e-02, -6.3788e-02,  3.4241e-02],\n",
       "           [-9.3838e-02,  1.8005e-02, -5.3472e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5299e-02,  8.2401e-03,  2.1083e-02],\n",
       "           [-3.8618e-02,  9.7016e-02, -3.8389e-02],\n",
       "           [-2.4069e-02, -6.5083e-03, -4.2386e-02]],\n",
       " \n",
       "          [[ 6.2663e-02, -9.0784e-02, -8.3011e-02],\n",
       "           [-1.3387e-02, -1.1652e-01,  4.7518e-02],\n",
       "           [-2.6113e-02, -6.9434e-02, -1.1569e-01]],\n",
       " \n",
       "          [[ 1.3033e-02, -6.6901e-02, -1.7800e-02],\n",
       "           [ 1.8789e-03, -1.8216e-02,  8.8044e-02],\n",
       "           [ 3.7556e-02,  4.6266e-02, -7.4150e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.2628e-02,  1.0918e-01,  1.0979e-01],\n",
       "           [ 1.0463e-01, -1.8819e-02, -7.4758e-02],\n",
       "           [-7.6780e-02,  2.5457e-02, -1.1930e-03]],\n",
       " \n",
       "          [[ 5.7548e-02,  4.0585e-02,  1.3014e-02],\n",
       "           [-4.6813e-02,  5.5623e-02, -4.4226e-02],\n",
       "           [ 8.2877e-02,  5.7495e-02, -2.7419e-02]],\n",
       " \n",
       "          [[-1.1593e-01, -3.2670e-03,  4.3557e-02],\n",
       "           [ 6.5554e-02,  4.5566e-02, -8.2169e-02],\n",
       "           [ 7.5876e-03, -2.6699e-02, -3.4675e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 5.8686e-07,  1.2452e-06, -8.1077e-07,  1.0770e-06,  1.7409e-06,\n",
       "          1.5199e-06,  8.0789e-07, -3.6236e-07, -2.8402e-07,  9.8090e-07,\n",
       "          3.4722e-08, -1.0380e-06,  1.8672e-06, -7.8568e-07,  3.5101e-07,\n",
       "         -8.4805e-07], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 6.1545e-02, -2.5755e-02, -3.0255e-02],\n",
       "           [ 6.5135e-02, -1.4185e-03, -6.3586e-02],\n",
       "           [-1.9883e-04, -8.2284e-02,  1.4199e-02]],\n",
       " \n",
       "          [[-2.1829e-02,  1.5695e-02,  3.0291e-02],\n",
       "           [ 5.3487e-02,  8.6514e-03, -2.5305e-05],\n",
       "           [-4.2518e-02,  7.6312e-02,  5.7079e-02]],\n",
       " \n",
       "          [[ 5.6845e-02,  6.3257e-03, -5.0063e-03],\n",
       "           [ 2.3441e-03, -5.4608e-02,  2.5415e-02],\n",
       "           [ 2.5078e-02,  6.4004e-02,  4.6130e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.9855e-02, -3.7643e-02,  5.1973e-03],\n",
       "           [-3.7177e-02,  8.1941e-02, -7.0355e-02],\n",
       "           [-3.4938e-02,  7.2912e-02,  5.9808e-02]],\n",
       " \n",
       "          [[ 3.9299e-02,  7.3678e-02, -6.9843e-02],\n",
       "           [ 6.7369e-04,  6.8069e-02, -2.6743e-02],\n",
       "           [-4.7060e-02, -2.7564e-02,  5.1519e-02]],\n",
       " \n",
       "          [[ 1.5899e-02,  4.9935e-02, -7.4445e-03],\n",
       "           [ 6.9981e-02,  3.4213e-02, -3.1852e-02],\n",
       "           [ 6.1247e-02,  3.0474e-02, -4.2813e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.0590e-02, -4.6881e-02, -4.4005e-02],\n",
       "           [-3.5301e-02, -5.8977e-02,  6.3820e-02],\n",
       "           [ 6.4758e-02,  7.2446e-02,  7.6219e-02]],\n",
       " \n",
       "          [[ 7.4843e-02, -4.6326e-02, -5.1652e-02],\n",
       "           [-6.8931e-02, -9.6670e-03,  7.2617e-02],\n",
       "           [-4.8979e-02, -2.9423e-02, -2.0687e-02]],\n",
       " \n",
       "          [[ 8.2399e-02,  3.5895e-02, -4.0375e-03],\n",
       "           [-1.4516e-02,  3.1052e-02,  5.6045e-02],\n",
       "           [ 5.6737e-02, -1.1004e-02,  3.1714e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.5732e-02, -2.2628e-03,  9.5411e-03],\n",
       "           [ 4.5326e-02, -7.5841e-02, -2.8063e-02],\n",
       "           [ 1.0002e-03, -7.7265e-02, -5.0481e-02]],\n",
       " \n",
       "          [[ 7.1998e-02,  5.3682e-02, -3.1301e-02],\n",
       "           [-6.8043e-02, -4.2934e-02,  3.6241e-02],\n",
       "           [-1.7740e-02,  3.9852e-02, -2.2435e-02]],\n",
       " \n",
       "          [[ 7.7409e-02, -6.6034e-02, -7.9840e-02],\n",
       "           [-5.2669e-02, -9.8033e-04, -7.7379e-02],\n",
       "           [ 1.3191e-02, -6.8430e-02,  5.3819e-02]]],\n",
       " \n",
       " \n",
       "         [[[-5.8276e-02,  1.8108e-02,  4.4398e-02],\n",
       "           [ 1.9902e-02,  6.3793e-02,  1.4575e-02],\n",
       "           [-4.1643e-02,  4.2388e-02,  8.5860e-03]],\n",
       " \n",
       "          [[-3.0130e-02, -5.6281e-02,  3.8607e-02],\n",
       "           [-1.8111e-02,  2.6771e-02, -3.2807e-02],\n",
       "           [ 5.4618e-02,  2.6179e-02,  2.7122e-02]],\n",
       " \n",
       "          [[-2.0599e-02, -5.9663e-02,  6.4338e-02],\n",
       "           [-7.2501e-02,  2.6952e-02, -5.3535e-02],\n",
       "           [-4.0827e-02, -1.8861e-02, -7.1223e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.0185e-02, -6.2509e-03, -4.4695e-02],\n",
       "           [-5.6713e-02, -4.0402e-02, -3.4646e-02],\n",
       "           [-3.5109e-02, -1.1761e-02,  3.1162e-02]],\n",
       " \n",
       "          [[-3.9473e-02, -1.1967e-02,  9.5971e-03],\n",
       "           [-6.6047e-02,  8.0939e-02,  4.6709e-02],\n",
       "           [-1.6333e-02, -6.4883e-02, -5.4153e-02]],\n",
       " \n",
       "          [[ 3.7342e-03, -2.7650e-02, -2.4019e-03],\n",
       "           [ 1.4065e-02, -6.8253e-02,  1.6427e-02],\n",
       "           [-3.9895e-02, -8.1606e-04,  4.1382e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 2.3269e-02, -9.7696e-03, -3.0570e-02],\n",
       "           [-7.2370e-03, -5.8600e-02, -3.0513e-02],\n",
       "           [ 2.7931e-02, -2.9153e-02,  7.9066e-02]],\n",
       " \n",
       "          [[-7.7981e-02, -6.9618e-02, -6.0775e-02],\n",
       "           [-6.8895e-02, -4.9319e-03, -1.4858e-02],\n",
       "           [-5.1013e-02, -2.0263e-02, -2.6223e-02]],\n",
       " \n",
       "          [[ 3.9540e-04,  6.2399e-02,  5.6095e-02],\n",
       "           [-2.3143e-02,  2.8216e-02,  4.6482e-02],\n",
       "           [ 2.0760e-02, -4.8600e-03, -6.8615e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.3467e-02, -4.9715e-02,  4.6443e-02],\n",
       "           [-6.6425e-02,  2.6898e-02,  7.2396e-02],\n",
       "           [-1.6380e-02,  4.8104e-02,  4.5025e-02]],\n",
       " \n",
       "          [[ 7.0954e-02,  2.0291e-03,  7.1266e-02],\n",
       "           [ 1.1057e-04,  1.9433e-02,  7.8180e-03],\n",
       "           [ 3.0439e-02, -4.9531e-02, -3.0125e-02]],\n",
       " \n",
       "          [[-2.7447e-02, -7.2128e-02,  3.3803e-02],\n",
       "           [ 3.8007e-02, -7.1991e-02,  5.2392e-02],\n",
       "           [ 4.1566e-02, -4.0306e-02,  1.6414e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.3178e-03, -1.6543e-03, -6.1757e-02],\n",
       "           [-7.2586e-02,  3.4263e-02, -7.9382e-02],\n",
       "           [ 6.7826e-02,  6.4592e-02, -2.7865e-02]],\n",
       " \n",
       "          [[-2.4082e-02,  5.7231e-02,  5.5941e-02],\n",
       "           [-6.9826e-02,  1.2477e-02, -1.7072e-02],\n",
       "           [ 9.7098e-03,  1.1594e-02,  4.6919e-02]],\n",
       " \n",
       "          [[ 2.9194e-02,  3.9365e-02, -1.3362e-02],\n",
       "           [ 2.4809e-02,  4.4264e-02, -1.1918e-02],\n",
       "           [ 6.3834e-02, -6.8513e-02,  3.1943e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.5733e-02,  6.8796e-02, -7.5403e-02],\n",
       "           [-2.5067e-02, -7.7378e-02,  1.0725e-02],\n",
       "           [ 3.0559e-03, -8.0738e-02,  1.8577e-03]],\n",
       " \n",
       "          [[-1.3031e-02, -2.3833e-03,  3.3129e-02],\n",
       "           [-5.1761e-02, -3.8394e-02,  1.7094e-02],\n",
       "           [-4.7187e-02,  1.5464e-04, -1.5795e-04]],\n",
       " \n",
       "          [[ 7.0528e-02, -1.0816e-02,  3.4486e-02],\n",
       "           [ 4.1999e-02,  7.4477e-02, -3.8414e-02],\n",
       "           [-7.4973e-03,  5.8760e-02,  3.1816e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.8688e-02,  3.5065e-02,  5.1718e-02],\n",
       "           [-6.8801e-02, -1.2409e-02,  6.7678e-02],\n",
       "           [-3.3908e-02,  3.9285e-02,  4.6076e-02]],\n",
       " \n",
       "          [[-5.8353e-02,  1.7291e-02,  7.9536e-02],\n",
       "           [-7.8129e-02, -4.8334e-02,  7.4241e-02],\n",
       "           [-8.0521e-02,  9.7051e-04,  8.1252e-02]],\n",
       " \n",
       "          [[-3.7044e-02, -7.7808e-03,  2.8930e-02],\n",
       "           [ 1.0509e-02,  5.9272e-03, -3.0357e-02],\n",
       "           [ 1.6576e-02, -6.2739e-02,  7.8021e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 8.1416e-02,  3.7485e-02,  3.5392e-03],\n",
       "           [-4.5124e-02, -3.6792e-02,  6.8156e-02],\n",
       "           [ 5.2442e-02,  1.3064e-02,  1.6670e-02]],\n",
       " \n",
       "          [[-7.3824e-02,  4.8923e-02,  2.1964e-02],\n",
       "           [-4.9955e-02,  2.4044e-02,  7.8671e-02],\n",
       "           [-2.0406e-02, -1.0696e-02,  2.9814e-02]],\n",
       " \n",
       "          [[ 2.5656e-02, -7.8388e-02, -4.4324e-02],\n",
       "           [-2.6371e-02, -2.0741e-02, -5.5637e-03],\n",
       "           [ 6.5592e-02,  1.3189e-03, -9.4741e-03]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-7.6767e-06, -1.6385e-05,  1.3541e-05,  2.1361e-05, -1.7681e-05,\n",
       "         -4.8198e-05,  5.1499e-05, -9.1552e-06, -3.4718e-06,  3.2657e-05,\n",
       "          3.8678e-06, -2.2812e-06,  1.6884e-05,  1.9567e-05, -1.2952e-05,\n",
       "         -1.0281e-05, -1.2125e-05, -2.4054e-05, -1.4744e-05,  3.5042e-05,\n",
       "          4.3283e-05, -1.7663e-05,  4.5216e-05,  8.0786e-06, -7.2738e-06,\n",
       "          2.6967e-06,  1.0066e-05,  1.6601e-05,  2.0003e-05, -1.1828e-06,\n",
       "          6.3208e-06, -1.3973e-05], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 3.1744e-02,  3.1336e-02, -5.5938e-02],\n",
       "           [-5.1533e-03, -1.8420e-02, -3.6304e-02],\n",
       "           [ 5.2676e-02,  9.4321e-03, -1.0797e-02]],\n",
       " \n",
       "          [[-2.1922e-02, -4.3320e-03,  3.3511e-02],\n",
       "           [ 4.9289e-02, -4.6558e-02,  5.5960e-02],\n",
       "           [-4.8238e-02,  1.5065e-02,  2.8974e-03]],\n",
       " \n",
       "          [[ 4.0667e-02, -5.7487e-02, -4.7816e-02],\n",
       "           [-4.8536e-03,  2.2438e-03,  1.6419e-02],\n",
       "           [-5.6072e-02,  5.0685e-02, -3.9631e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.1067e-02, -2.3050e-02,  4.3441e-02],\n",
       "           [-2.1422e-02, -4.6006e-02,  7.6456e-03],\n",
       "           [ 4.8565e-02,  3.9755e-02,  4.1196e-02]],\n",
       " \n",
       "          [[-3.7476e-02,  2.9933e-02,  3.2557e-03],\n",
       "           [-1.8384e-02, -4.1543e-02,  1.1644e-03],\n",
       "           [ 4.3525e-02,  8.1638e-03, -4.5551e-02]],\n",
       " \n",
       "          [[-5.4229e-02, -5.5574e-02,  2.9455e-02],\n",
       "           [-1.6550e-02,  1.6571e-02, -5.2363e-02],\n",
       "           [ 5.5803e-02,  4.1561e-02,  4.5703e-02]]],\n",
       " \n",
       " \n",
       "         [[[-2.0155e-02,  5.0757e-02, -9.7323e-03],\n",
       "           [-5.5364e-02,  5.4745e-02, -3.3077e-02],\n",
       "           [ 3.4361e-02, -1.2416e-02,  2.0255e-02]],\n",
       " \n",
       "          [[ 1.5667e-02, -5.1455e-02,  4.2655e-02],\n",
       "           [-1.2932e-02, -6.0655e-05, -1.8967e-02],\n",
       "           [ 2.7875e-02, -4.2182e-02,  4.7373e-02]],\n",
       " \n",
       "          [[-5.2472e-02,  3.2710e-02, -2.9960e-02],\n",
       "           [ 1.3809e-02, -3.9521e-02,  3.5218e-02],\n",
       "           [ 5.5890e-02,  4.7717e-02,  1.5513e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.8193e-03, -3.6968e-02,  5.6189e-02],\n",
       "           [ 1.4854e-02,  3.5683e-02, -2.9635e-03],\n",
       "           [-5.6170e-02,  2.9276e-02,  1.9187e-02]],\n",
       " \n",
       "          [[ 2.9704e-05,  3.5180e-03, -1.9931e-02],\n",
       "           [-2.4019e-02, -5.3890e-02, -4.6929e-02],\n",
       "           [ 2.9329e-02,  2.4773e-02,  2.3904e-02]],\n",
       " \n",
       "          [[-1.4687e-02, -4.8514e-02,  4.7355e-02],\n",
       "           [ 6.7067e-03, -5.7339e-02, -4.9660e-02],\n",
       "           [-4.5257e-02,  1.8382e-02,  4.1813e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.2118e-02,  1.1108e-02, -3.9488e-02],\n",
       "           [ 1.2113e-02, -4.6451e-02,  2.9029e-02],\n",
       "           [-4.4708e-02,  2.8630e-02, -2.4268e-02]],\n",
       " \n",
       "          [[ 3.0027e-02, -2.4410e-03, -5.6745e-02],\n",
       "           [ 5.3758e-02,  5.0228e-02, -6.6729e-03],\n",
       "           [-4.1804e-02,  2.7879e-02, -3.6789e-02]],\n",
       " \n",
       "          [[ 3.1481e-02, -2.5541e-02, -4.5401e-02],\n",
       "           [-1.1106e-03,  3.5548e-02, -1.4066e-02],\n",
       "           [-8.6258e-03,  3.3563e-02,  4.5753e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.2044e-03,  3.8417e-02,  1.3796e-02],\n",
       "           [ 5.0629e-02,  3.9409e-02,  7.1723e-03],\n",
       "           [-4.5453e-02,  1.4417e-02,  4.3204e-02]],\n",
       " \n",
       "          [[-4.7168e-02,  5.3939e-02, -3.1210e-02],\n",
       "           [-8.3225e-03,  2.9979e-02,  2.7252e-02],\n",
       "           [ 3.8383e-03, -5.4794e-02, -3.8269e-02]],\n",
       " \n",
       "          [[ 4.9931e-02, -1.2501e-02, -5.5178e-02],\n",
       "           [ 5.5145e-02, -4.7784e-02, -3.6517e-02],\n",
       "           [ 1.2032e-02, -1.3279e-02,  4.4421e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.1517e-02,  3.0642e-02,  2.3573e-02],\n",
       "           [ 1.7418e-02,  4.6410e-02,  5.3175e-02],\n",
       "           [ 1.7964e-02, -3.9221e-03,  4.6974e-02]],\n",
       " \n",
       "          [[-7.4388e-03, -3.7422e-03, -3.3172e-02],\n",
       "           [-2.9688e-02, -3.0788e-02,  3.1492e-02],\n",
       "           [ 2.4739e-02, -2.6563e-02,  5.6180e-02]],\n",
       " \n",
       "          [[ 8.5692e-03, -1.0164e-02, -1.2341e-02],\n",
       "           [-3.4171e-02, -2.1653e-03,  4.3661e-04],\n",
       "           [ 2.1370e-03,  4.1807e-02,  3.5781e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.7253e-02,  2.5401e-02,  2.4998e-02],\n",
       "           [-1.7863e-02, -5.6149e-02, -2.1577e-02],\n",
       "           [-1.8727e-02, -6.2238e-03,  5.4202e-02]],\n",
       " \n",
       "          [[ 2.5563e-02, -3.9188e-02,  3.4852e-02],\n",
       "           [-3.5927e-02, -4.3376e-02,  4.8089e-02],\n",
       "           [ 1.3890e-02,  8.3516e-04,  3.2387e-02]],\n",
       " \n",
       "          [[-1.4132e-02,  5.1420e-02,  5.5781e-02],\n",
       "           [ 4.2211e-02, -1.0197e-02, -1.4300e-02],\n",
       "           [ 5.4201e-02, -8.2894e-03,  5.0925e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.5463e-02,  4.9407e-02,  3.4347e-02],\n",
       "           [ 2.4408e-02, -4.3995e-02, -4.3833e-02],\n",
       "           [ 3.5274e-02,  4.5617e-02,  4.3064e-02]],\n",
       " \n",
       "          [[-1.3730e-03,  5.7486e-02, -8.3643e-04],\n",
       "           [ 5.7782e-02, -1.3993e-02, -4.0537e-03],\n",
       "           [ 7.2438e-03,  2.6414e-02,  3.7233e-02]],\n",
       " \n",
       "          [[ 1.6865e-02,  3.2087e-02, -2.2260e-02],\n",
       "           [ 4.0377e-02,  2.4609e-02,  2.5129e-02],\n",
       "           [-1.1770e-03,  2.6462e-02, -3.9013e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0476e-03, -2.6004e-02,  3.7730e-02],\n",
       "           [ 3.0523e-02, -3.8981e-02, -1.2943e-02],\n",
       "           [-4.2061e-02, -2.1544e-02,  1.9544e-02]],\n",
       " \n",
       "          [[ 3.0390e-02, -3.5637e-02,  2.0345e-02],\n",
       "           [ 4.6735e-02, -2.7419e-02, -4.3140e-03],\n",
       "           [ 2.1792e-02, -4.4930e-02,  3.3591e-02]],\n",
       " \n",
       "          [[ 3.3708e-02,  5.7520e-02, -5.3480e-02],\n",
       "           [ 7.1827e-05,  7.4651e-03, -3.3444e-02],\n",
       "           [-3.9308e-02,  4.8881e-02, -4.0356e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.5541e-02, -3.8651e-02,  8.3843e-04],\n",
       "           [ 4.6055e-02, -4.5120e-02, -3.1067e-02],\n",
       "           [-3.8748e-02,  5.2796e-02, -9.6967e-03]],\n",
       " \n",
       "          [[-2.6249e-02, -5.5250e-02, -2.1020e-02],\n",
       "           [ 2.4380e-02,  1.2085e-02, -4.7428e-02],\n",
       "           [-4.6271e-02, -1.4017e-02, -3.4384e-03]],\n",
       " \n",
       "          [[ 2.4946e-02, -2.0147e-02, -4.1027e-02],\n",
       "           [ 4.4302e-03,  5.1828e-02,  2.0739e-02],\n",
       "           [-4.0186e-02, -5.1957e-02, -1.5967e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 7.0424e-03,  8.6631e-03, -3.5196e-02],\n",
       "           [-3.7285e-02,  2.3237e-02,  1.6657e-02],\n",
       "           [-4.7231e-02, -1.4883e-02, -5.1617e-02]],\n",
       " \n",
       "          [[-1.5154e-02,  4.9572e-02,  4.3428e-02],\n",
       "           [ 7.6614e-04,  9.1983e-03,  1.9455e-02],\n",
       "           [-1.5877e-02, -4.1168e-02, -4.2357e-02]],\n",
       " \n",
       "          [[ 2.8154e-02,  1.6265e-02,  3.0679e-02],\n",
       "           [ 5.9688e-03,  3.0965e-02, -1.9785e-02],\n",
       "           [-2.8556e-02,  3.4626e-02,  3.7962e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0009,  0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009,\n",
       "         -0.0009,  0.0009, -0.0009, -0.0009,  0.0009, -0.0009, -0.0009,  0.0009,\n",
       "         -0.0009,  0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009,  0.0009,\n",
       "          0.0009, -0.0009,  0.0009,  0.0009,  0.0009, -0.0009, -0.0009, -0.0009],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-3.0711e-02, -1.3899e-01,  1.6985e-01,  2.2527e-03,  1.3149e-01,\n",
       "           1.6615e-01,  5.8196e-02, -1.0391e-01,  1.6657e-02, -1.0205e-01,\n",
       "           1.6057e-01,  1.2919e-02, -8.9279e-02,  8.0864e-02, -2.2494e-02,\n",
       "           1.5895e-01,  4.2922e-02, -1.3999e-01,  1.1208e-01, -1.0373e-01,\n",
       "          -3.4353e-02, -4.5973e-02,  1.5541e-02, -1.1932e-01, -1.6035e-01,\n",
       "           1.3740e-01, -3.2832e-02, -1.6209e-01, -1.3869e-01,  1.6593e-01,\n",
       "           1.1447e-02,  7.9548e-02],\n",
       "         [-1.1330e-01, -1.6943e-01,  1.6627e-01, -1.6539e-02, -8.5556e-02,\n",
       "           9.3202e-02,  1.0194e-01, -3.4526e-02,  1.2686e-01, -4.4639e-02,\n",
       "          -9.1295e-02,  1.6389e-01,  8.0881e-02,  1.3088e-01,  1.5748e-01,\n",
       "          -2.6114e-02, -1.5626e-01,  3.8963e-03,  6.1163e-03,  1.6429e-01,\n",
       "          -9.4605e-02,  1.2916e-01,  5.7506e-02, -1.4262e-01,  4.0048e-02,\n",
       "           1.7580e-01, -1.3731e-01,  1.5238e-01, -1.8653e-02, -1.0140e-01,\n",
       "           1.6931e-01, -9.6589e-02],\n",
       "         [ 1.7260e-01, -1.1723e-01,  1.5279e-01,  2.8800e-02,  2.1542e-03,\n",
       "           1.6420e-01,  1.1357e-01,  1.6957e-01,  1.2864e-01, -1.0627e-01,\n",
       "          -1.6331e-01, -7.1200e-03, -1.5838e-02,  1.6691e-01,  4.9412e-04,\n",
       "          -1.1760e-01,  1.2660e-01,  9.7868e-02, -5.9527e-02, -1.6849e-01,\n",
       "           8.6861e-02,  4.0142e-02,  7.3615e-03,  6.9747e-02,  1.6371e-01,\n",
       "          -1.0563e-01, -2.1516e-02, -1.4729e-01, -8.9833e-02,  6.4067e-02,\n",
       "          -6.6878e-02, -1.2586e-01],\n",
       "         [ 1.6851e-01, -8.3853e-02, -1.7394e-01,  1.4606e-01,  1.7006e-01,\n",
       "          -4.4924e-03,  1.5840e-01,  9.4216e-02,  7.9215e-02,  6.3516e-02,\n",
       "           7.4758e-02,  1.0716e-01, -5.7206e-02,  5.1078e-02,  3.5527e-02,\n",
       "          -8.9927e-03,  8.9860e-02, -7.9521e-03, -5.1184e-02,  1.1163e-01,\n",
       "          -3.5511e-02,  6.2530e-02, -6.6099e-02, -1.7789e-02, -1.7571e-01,\n",
       "           1.9416e-03, -1.2640e-01,  1.4322e-01, -1.4577e-01, -1.3440e-01,\n",
       "           1.3281e-01,  3.1660e-02],\n",
       "         [-1.5007e-01,  8.8802e-02,  1.7206e-01,  3.6690e-02, -1.3441e-01,\n",
       "          -9.7930e-02,  1.5504e-01, -4.6453e-02, -2.6889e-02,  1.4307e-01,\n",
       "          -1.1474e-01,  1.0538e-02,  1.4052e-01,  7.7940e-02,  9.6892e-02,\n",
       "           7.0762e-03,  1.7165e-01,  1.5900e-01, -7.6010e-02,  4.3836e-03,\n",
       "           9.8786e-02, -6.4007e-02,  5.4855e-02, -3.4510e-02, -1.4143e-01,\n",
       "          -9.4281e-02, -7.2634e-02,  1.3923e-02,  1.7501e-02, -6.6764e-03,\n",
       "           1.0911e-01,  9.4701e-02],\n",
       "         [-7.8749e-02, -1.0077e-01,  1.1290e-01,  1.6936e-01,  1.5559e-01,\n",
       "          -1.4035e-01, -8.8472e-03,  1.7413e-01, -6.3398e-02,  8.6687e-03,\n",
       "           9.5781e-02, -1.2141e-01, -4.9166e-02, -7.5782e-03, -1.4587e-01,\n",
       "           6.6293e-02,  6.3786e-02, -1.1102e-01,  1.7072e-01,  1.6437e-01,\n",
       "          -1.0556e-01,  1.3061e-01, -1.7802e-02,  1.4175e-01,  2.2295e-02,\n",
       "          -1.4278e-01, -1.3471e-01, -1.2136e-01,  1.7312e-01,  1.3875e-01,\n",
       "          -4.1411e-02, -1.4304e-01],\n",
       "         [ 1.5041e-01,  5.1614e-02, -1.1988e-01, -5.5419e-02,  8.2644e-02,\n",
       "           7.7389e-02, -1.0022e-01,  6.4703e-03,  2.3378e-02,  2.4285e-02,\n",
       "          -6.7714e-03,  6.1844e-02,  7.2616e-02,  1.0361e-01,  2.7448e-02,\n",
       "           1.5971e-01,  8.3857e-02,  1.3887e-01, -2.1324e-02,  1.1010e-01,\n",
       "          -7.3420e-02,  2.5021e-02,  5.4832e-02, -4.1287e-02,  9.4068e-02,\n",
       "          -1.3449e-01, -1.1493e-01,  5.3144e-02, -5.9686e-02, -1.3906e-02,\n",
       "          -1.4190e-02,  1.0950e-01],\n",
       "         [ 1.4487e-01, -6.1498e-02,  2.3649e-03,  3.3710e-02,  2.3101e-02,\n",
       "           1.0858e-01,  1.4257e-04,  3.2968e-02,  1.1589e-01,  9.5508e-02,\n",
       "           1.6099e-01,  3.9948e-02, -1.6291e-01,  7.6339e-02,  9.8943e-02,\n",
       "          -5.6420e-02, -1.2999e-01, -1.4362e-01, -1.5908e-01,  1.0650e-02,\n",
       "           8.5370e-02, -3.1825e-02, -1.3035e-01,  4.7297e-04, -1.0591e-01,\n",
       "           4.6073e-02,  1.2399e-01,  3.7303e-02,  2.3648e-02, -2.7876e-02,\n",
       "           1.2950e-01,  8.5019e-02],\n",
       "         [ 8.7826e-02, -6.6990e-02, -9.3978e-02, -4.6776e-02, -2.6795e-02,\n",
       "           1.2192e-01,  1.6248e-01,  6.2664e-02, -8.2346e-02, -9.6425e-02,\n",
       "           4.8198e-02, -9.7766e-02, -1.1962e-01,  2.0291e-03, -4.0373e-02,\n",
       "          -6.4097e-02,  1.4119e-01,  1.2480e-01,  1.4788e-01, -7.4847e-02,\n",
       "           6.8732e-02, -5.0356e-02,  1.0688e-01, -7.4759e-02, -8.8562e-02,\n",
       "           1.6897e-01, -1.6680e-01, -7.1811e-02,  4.9237e-02,  1.6840e-01,\n",
       "          -1.6552e-01,  3.3155e-03],\n",
       "         [ 1.5443e-01,  8.9688e-02,  1.6744e-01, -1.7581e-01, -1.3346e-01,\n",
       "           7.8006e-02,  3.8708e-02,  8.1507e-02, -1.1900e-01, -1.7189e-01,\n",
       "          -4.5377e-02,  1.5611e-01,  1.1569e-01, -9.0740e-02, -4.8263e-02,\n",
       "          -1.7559e-01,  1.0899e-01, -1.2556e-01, -1.3399e-02,  7.8219e-02,\n",
       "           7.1496e-03,  7.3182e-02, -4.7178e-02, -1.7371e-01,  1.5143e-01,\n",
       "           1.1145e-01, -1.2145e-01, -1.6120e-01, -5.6644e-02,  3.9727e-02,\n",
       "          -7.3665e-02,  4.5313e-02]], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dynet.tree.optimizer.state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[-5.5703e-02, -1.8930e-01, -1.2059e-01],\n",
       "           [ 1.7856e-02,  1.4319e-01, -2.3043e-02],\n",
       "           [ 1.7780e-01, -1.2991e-01,  9.9326e-02]],\n",
       " \n",
       "          [[-4.1155e-02,  1.8590e-01, -6.7313e-02],\n",
       "           [ 1.2220e-01,  3.9127e-02, -1.1250e-01],\n",
       "           [-7.2557e-02,  1.9125e-01,  6.6179e-02]],\n",
       " \n",
       "          [[-1.8131e-01,  1.3526e-01,  1.2205e-01],\n",
       "           [ 1.4724e-01,  9.4187e-02, -9.7348e-03],\n",
       "           [ 6.0044e-02, -1.6911e-01,  1.6732e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.6294e-03,  8.4629e-04,  1.2711e-01],\n",
       "           [ 1.2078e-01,  6.4700e-02, -1.4869e-01],\n",
       "           [-3.1222e-02, -5.2589e-02, -1.7636e-02]],\n",
       " \n",
       "          [[ 3.4677e-02,  5.5599e-02, -4.6393e-02],\n",
       "           [-4.7956e-02,  1.5129e-01, -2.2086e-02],\n",
       "           [-1.3991e-01,  1.0761e-01,  1.4586e-01]],\n",
       " \n",
       "          [[-9.2866e-02,  7.2832e-02, -6.9745e-02],\n",
       "           [ 1.5640e-01,  6.2109e-02, -1.3392e-01],\n",
       "           [-1.2589e-01, -4.6108e-02,  1.6425e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 8.9153e-02, -4.8346e-02, -1.4552e-01],\n",
       "           [ 1.1294e-01,  1.4859e-01, -4.3243e-02],\n",
       "           [-3.7931e-02, -9.1050e-02,  1.4655e-01]],\n",
       " \n",
       "          [[ 1.8323e-01, -2.4805e-02, -1.6270e-01],\n",
       "           [ 6.6408e-02, -6.0292e-02,  6.1545e-02],\n",
       "           [-1.6321e-01, -1.6069e-02, -9.7403e-02]],\n",
       " \n",
       "          [[ 4.7428e-02,  2.0885e-02, -6.2822e-02],\n",
       "           [ 1.6861e-01,  8.4014e-02,  1.3429e-01],\n",
       "           [ 1.3844e-01,  1.9040e-01,  9.4029e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.8477e-02, -1.7285e-04, -3.1358e-02],\n",
       "           [-1.1659e-01, -2.1163e-02, -1.1040e-01],\n",
       "           [ 5.7792e-02,  1.2855e-01, -1.8485e-02]],\n",
       " \n",
       "          [[-4.6322e-02, -7.3238e-02,  8.3404e-02],\n",
       "           [-1.2309e-01, -1.6627e-01, -2.7592e-02],\n",
       "           [ 4.6386e-02, -1.7531e-01,  1.7540e-01]],\n",
       " \n",
       "          [[ 9.9185e-02,  3.3236e-02,  3.7494e-02],\n",
       "           [ 6.1158e-02, -1.6947e-02, -1.8616e-01],\n",
       "           [ 4.6673e-02,  1.5698e-01, -7.9005e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.1837e-01, -1.4000e-01, -3.3200e-03],\n",
       "           [-9.2440e-02,  1.8589e-01,  6.3253e-02],\n",
       "           [-1.0663e-01,  9.2424e-02,  5.1739e-02]],\n",
       " \n",
       "          [[-2.3630e-03, -9.2072e-03,  1.8170e-02],\n",
       "           [ 4.2063e-02,  8.5953e-02, -5.8541e-02],\n",
       "           [ 8.1687e-02, -8.4843e-02,  5.9178e-02]],\n",
       " \n",
       "          [[ 5.9736e-02,  7.8083e-02,  1.1295e-01],\n",
       "           [-1.7301e-01, -4.1670e-02,  1.7512e-01],\n",
       "           [ 3.0574e-02, -1.6459e-01, -1.8772e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.7431e-01, -8.0356e-02,  1.3006e-01],\n",
       "           [-1.7206e-01, -9.4230e-02,  9.8254e-02],\n",
       "           [-1.7612e-01, -1.3631e-01, -3.8322e-02]],\n",
       " \n",
       "          [[-1.9088e-01,  1.4425e-01, -1.4387e-01],\n",
       "           [ 1.2341e-01, -5.7118e-02,  3.7989e-02],\n",
       "           [-1.6740e-01, -3.3371e-02,  1.8572e-01]],\n",
       " \n",
       "          [[-2.9792e-02,  1.5768e-01, -3.9545e-02],\n",
       "           [-6.5627e-02,  1.1575e-01,  6.0459e-02],\n",
       "           [ 1.0293e-01,  1.1593e-01,  8.1366e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0220e-01,  2.1723e-02, -1.4390e-02],\n",
       "           [-1.5553e-01, -8.2993e-02, -6.6285e-02],\n",
       "           [-6.7716e-02, -5.7565e-02,  1.7704e-01]],\n",
       " \n",
       "          [[-1.4083e-01, -1.4394e-01,  1.7138e-01],\n",
       "           [ 6.2994e-02, -4.3541e-02, -5.0947e-02],\n",
       "           [ 7.2890e-02,  9.5993e-02,  1.1132e-01]],\n",
       " \n",
       "          [[-1.8661e-01, -1.7437e-01,  1.3776e-01],\n",
       "           [-9.0988e-02, -6.8625e-02, -1.6615e-02],\n",
       "           [-4.4022e-02, -6.9727e-02, -1.3337e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.1279e-01, -1.5291e-01, -8.4339e-02],\n",
       "           [-1.9075e-01, -1.4527e-01,  1.8494e-02],\n",
       "           [-1.6626e-01,  6.7979e-02,  3.7581e-02]],\n",
       " \n",
       "          [[-1.1558e-01, -1.7290e-01,  2.7687e-03],\n",
       "           [ 1.4515e-01,  9.6457e-02,  1.5122e-01],\n",
       "           [-1.4001e-01, -2.4721e-02,  1.7420e-01]],\n",
       " \n",
       "          [[-1.3206e-04,  1.2146e-01,  1.1441e-01],\n",
       "           [-6.5931e-02, -1.6496e-01, -5.5603e-02],\n",
       "           [ 1.2408e-01, -1.3413e-01,  1.2070e-01]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-5.4724e-07,  7.6908e-08, -2.4104e-07, -1.6286e-06, -2.2674e-06,\n",
       "         -6.9854e-07, -3.7948e-07,  8.3706e-07], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-1.1204e-01, -8.7083e-03, -9.5877e-02],\n",
       "           [ 3.1211e-02,  1.0616e-01,  4.7606e-02],\n",
       "           [ 7.9017e-02,  6.9394e-03, -1.0252e-01]],\n",
       " \n",
       "          [[-2.7647e-02, -1.2667e-03, -7.7603e-02],\n",
       "           [-2.1547e-02, -1.4778e-02, -5.9826e-02],\n",
       "           [-2.1059e-02,  3.9364e-03,  5.2409e-02]],\n",
       " \n",
       "          [[-6.2742e-02,  1.1027e-01,  6.7881e-02],\n",
       "           [ 2.9714e-02,  7.0672e-03, -2.2641e-03],\n",
       "           [-9.6225e-02, -6.6526e-02,  5.1314e-02]],\n",
       " \n",
       "          [[-3.6849e-03, -2.2615e-02, -4.0396e-02],\n",
       "           [-1.1675e-01,  6.0034e-02,  3.3405e-03],\n",
       "           [-3.2858e-02,  8.2775e-02,  6.7217e-02]],\n",
       " \n",
       "          [[-4.8862e-02, -1.1199e-01, -1.0224e-01],\n",
       "           [-8.0395e-02,  7.9611e-02, -8.1639e-02],\n",
       "           [-3.5312e-02, -3.6413e-02, -9.9292e-02]],\n",
       " \n",
       "          [[-1.0013e-01,  3.1484e-02, -6.9833e-02],\n",
       "           [ 7.9256e-02, -7.5665e-02,  5.3070e-02],\n",
       "           [-3.6440e-02,  4.0639e-02,  1.1125e-01]],\n",
       " \n",
       "          [[ 9.7275e-02,  2.1696e-02, -5.1008e-02],\n",
       "           [-3.5896e-02, -6.1242e-02,  8.3026e-02],\n",
       "           [ 2.7951e-02, -1.0980e-01,  1.4407e-03]],\n",
       " \n",
       "          [[-1.7412e-02, -9.1045e-02,  3.3372e-02],\n",
       "           [ 3.3294e-02, -9.2244e-02, -7.1671e-02],\n",
       "           [ 6.2237e-02,  9.3343e-02,  1.0248e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.4262e-02, -5.5827e-02, -8.6799e-03],\n",
       "           [-2.9752e-02, -8.0327e-03,  1.0054e-02],\n",
       "           [ 6.7571e-02,  9.3637e-02,  6.0055e-02]],\n",
       " \n",
       "          [[-1.0465e-01,  8.9079e-02,  4.4068e-03],\n",
       "           [ 1.1168e-01,  5.3959e-02, -1.0490e-01],\n",
       "           [-1.0673e-01,  1.1333e-01, -5.1636e-02]],\n",
       " \n",
       "          [[ 5.0618e-02,  1.1133e-01, -1.3613e-02],\n",
       "           [-9.9312e-02, -6.7640e-02,  3.7624e-02],\n",
       "           [-9.9570e-03, -1.9544e-02,  9.6853e-02]],\n",
       " \n",
       "          [[ 3.9988e-02,  8.0686e-02,  4.1634e-02],\n",
       "           [-8.8330e-02, -3.7574e-02,  1.0231e-01],\n",
       "           [ 6.1909e-02,  5.0839e-02, -8.1225e-02]],\n",
       " \n",
       "          [[-9.2668e-02, -2.3513e-02,  7.2364e-02],\n",
       "           [-2.2982e-02,  5.4584e-02, -7.4816e-02],\n",
       "           [-2.0781e-02,  8.1380e-02,  6.1109e-02]],\n",
       " \n",
       "          [[-5.5055e-02, -2.1303e-03, -7.0863e-02],\n",
       "           [ 4.4606e-02, -4.3191e-02,  1.1012e-01],\n",
       "           [ 3.2819e-02, -3.5415e-02,  1.0293e-02]],\n",
       " \n",
       "          [[ 6.7616e-02, -1.0545e-01, -3.3338e-02],\n",
       "           [-9.8349e-02,  2.5693e-03, -2.4482e-02],\n",
       "           [ 3.4979e-02,  3.7004e-02, -2.7554e-02]],\n",
       " \n",
       "          [[-7.1072e-02,  1.4042e-02,  4.9901e-02],\n",
       "           [-4.7128e-02,  5.6501e-02, -3.1151e-02],\n",
       "           [ 7.6442e-02,  1.0259e-01, -8.5512e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 4.4465e-02, -7.2087e-02, -2.6747e-02],\n",
       "           [-1.6917e-02, -1.0905e-01, -2.6073e-02],\n",
       "           [ 1.6281e-02,  8.3130e-02, -8.9424e-02]],\n",
       " \n",
       "          [[-8.3924e-02,  8.2892e-02,  1.1293e-01],\n",
       "           [-3.3177e-02, -1.1330e-01,  1.1440e-01],\n",
       "           [ 5.2956e-02,  9.6606e-02,  6.1532e-02]],\n",
       " \n",
       "          [[ 9.0560e-02, -4.4822e-02, -1.0569e-01],\n",
       "           [-7.9810e-02, -1.0703e-01, -8.4526e-02],\n",
       "           [ 1.1298e-03,  1.0660e-01, -1.1671e-01]],\n",
       " \n",
       "          [[-8.3851e-02, -3.7165e-02,  2.1225e-02],\n",
       "           [ 3.1691e-02, -7.9305e-02,  1.0605e-01],\n",
       "           [ 6.2138e-02, -2.6249e-02,  2.1649e-02]],\n",
       " \n",
       "          [[ 7.4596e-03, -1.6809e-02, -1.1597e-01],\n",
       "           [-3.6375e-02, -3.0000e-03,  1.5307e-02],\n",
       "           [ 9.4800e-02, -1.0783e-01,  2.6700e-03]],\n",
       " \n",
       "          [[ 7.0491e-02, -7.4283e-02, -2.1550e-02],\n",
       "           [ 1.0519e-01,  7.4596e-04,  2.9523e-02],\n",
       "           [ 8.6646e-02, -5.9483e-02,  4.1908e-03]],\n",
       " \n",
       "          [[-3.7324e-02, -8.3647e-02, -6.5151e-02],\n",
       "           [ 1.1025e-01,  3.5278e-02, -1.1336e-01],\n",
       "           [ 8.0002e-02, -5.5011e-02,  1.5078e-02]],\n",
       " \n",
       "          [[-9.6799e-02, -1.0770e-01,  9.1954e-02],\n",
       "           [-1.1190e-01,  9.3753e-02,  7.8819e-02],\n",
       "           [ 7.9659e-02, -7.0194e-02, -6.1916e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.7113e-02,  1.1386e-02, -6.5783e-02],\n",
       "           [ 2.9118e-02, -8.9332e-02,  1.5017e-02],\n",
       "           [-4.9888e-02, -9.0033e-02, -1.0234e-01]],\n",
       " \n",
       "          [[ 3.1489e-02,  6.1833e-02, -2.4286e-02],\n",
       "           [ 1.0610e-01,  7.8311e-02, -8.5046e-02],\n",
       "           [-4.2122e-02, -4.5390e-02, -9.6377e-02]],\n",
       " \n",
       "          [[ 9.6539e-02,  9.9606e-02, -1.0202e-01],\n",
       "           [-7.5750e-02, -5.0135e-02,  5.2602e-02],\n",
       "           [-5.5276e-02, -1.0262e-01, -2.2844e-02]],\n",
       " \n",
       "          [[ 6.4503e-02, -1.7541e-03,  7.5780e-02],\n",
       "           [-3.6450e-02, -4.2538e-02, -6.8106e-02],\n",
       "           [-1.0984e-01, -1.1161e-01, -4.8644e-02]],\n",
       " \n",
       "          [[ 6.9338e-02,  6.9271e-02,  8.3634e-02],\n",
       "           [-1.1427e-01, -8.2171e-02,  5.3822e-05],\n",
       "           [-5.4916e-02,  9.2989e-02,  8.9718e-02]],\n",
       " \n",
       "          [[ 6.4322e-02,  1.1107e-03,  3.6293e-02],\n",
       "           [-6.5285e-02, -1.7867e-02, -1.0609e-01],\n",
       "           [-4.2338e-02,  2.1777e-02, -4.8608e-02]],\n",
       " \n",
       "          [[ 8.7332e-02, -6.8450e-02,  9.9521e-02],\n",
       "           [-6.2570e-02,  4.0924e-03, -3.7535e-02],\n",
       "           [-7.3029e-02, -5.4864e-02,  3.4797e-02]],\n",
       " \n",
       "          [[ 5.9457e-03, -1.1542e-01, -8.6007e-02],\n",
       "           [ 5.8610e-02, -9.7701e-02,  6.1189e-02],\n",
       "           [ 4.4234e-02, -1.1357e-01, -5.6371e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.7427e-02,  3.0319e-02,  5.9533e-03],\n",
       "           [ 3.4840e-02, -3.7193e-02, -4.3857e-02],\n",
       "           [-7.0265e-02, -6.5320e-02, -2.5849e-02]],\n",
       " \n",
       "          [[ 5.0931e-03, -1.1098e-01, -7.8434e-02],\n",
       "           [-1.0322e-01, -1.0942e-01, -3.1720e-02],\n",
       "           [-5.0116e-02, -1.1369e-01, -3.3012e-02]],\n",
       " \n",
       "          [[ 9.2474e-02,  3.8578e-02,  8.0028e-02],\n",
       "           [-4.1832e-02, -1.1725e-02,  3.8776e-02],\n",
       "           [ 5.8156e-02,  1.5310e-02, -9.9664e-02]],\n",
       " \n",
       "          [[ 1.0196e-01, -2.2067e-02,  1.0031e-01],\n",
       "           [ 3.5484e-02, -9.3825e-02,  1.3292e-02],\n",
       "           [ 5.0543e-02, -2.6210e-03, -6.8574e-02]],\n",
       " \n",
       "          [[ 7.4940e-02,  8.3696e-02,  2.5095e-02],\n",
       "           [ 6.8669e-02, -3.7566e-02, -2.1460e-02],\n",
       "           [ 3.7734e-02, -5.3456e-02, -8.2738e-02]],\n",
       " \n",
       "          [[ 1.0753e-01,  7.5158e-02,  1.0068e-01],\n",
       "           [ 5.8314e-02,  2.1967e-02,  1.6198e-02],\n",
       "           [-6.7203e-02, -3.0378e-02,  9.4868e-03]],\n",
       " \n",
       "          [[ 2.7230e-02, -4.4423e-02, -5.1098e-02],\n",
       "           [ 5.9860e-02,  6.6437e-02, -8.1790e-02],\n",
       "           [-2.0532e-02, -9.3936e-02, -1.1095e-01]],\n",
       " \n",
       "          [[-1.1821e-02, -9.4017e-02,  4.8936e-02],\n",
       "           [-1.0155e-02,  5.6482e-02,  3.1082e-02],\n",
       "           [ 1.4605e-02, -2.9733e-02, -5.3402e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 4.0995e-02,  9.7524e-02, -9.9673e-02],\n",
       "           [ 6.8130e-02, -8.8644e-02, -9.6812e-02],\n",
       "           [-1.1039e-01, -8.7048e-02,  3.1276e-02]],\n",
       " \n",
       "          [[ 6.4818e-02, -4.2535e-02, -3.8181e-02],\n",
       "           [ 1.1434e-02,  1.9142e-02, -5.8794e-02],\n",
       "           [ 8.5606e-02,  2.1307e-02, -1.3207e-02]],\n",
       " \n",
       "          [[-1.0908e-01, -6.3910e-02, -1.0976e-01],\n",
       "           [ 4.6956e-02,  1.9721e-02,  1.0178e-01],\n",
       "           [-7.9414e-02,  6.6230e-02,  6.9752e-02]],\n",
       " \n",
       "          [[ 3.4678e-02, -8.8657e-03,  1.1414e-01],\n",
       "           [-1.1413e-01, -2.2530e-02, -8.6386e-02],\n",
       "           [ 1.3316e-02,  6.9726e-02, -9.7201e-02]],\n",
       " \n",
       "          [[ 1.0052e-01, -3.2168e-02, -9.8293e-02],\n",
       "           [ 3.4497e-02,  7.3934e-03,  9.4932e-02],\n",
       "           [ 6.2105e-02,  5.2318e-02,  6.7303e-02]],\n",
       " \n",
       "          [[-4.4967e-02,  8.8341e-02,  4.5762e-02],\n",
       "           [-8.0300e-02,  2.7251e-02, -3.5652e-02],\n",
       "           [-7.8153e-02, -1.2157e-02,  6.9326e-02]],\n",
       " \n",
       "          [[ 1.7938e-02,  7.0601e-02,  2.1974e-02],\n",
       "           [ 2.9613e-02,  1.3054e-02, -9.7282e-02],\n",
       "           [-1.1679e-01,  3.6296e-02, -4.7555e-02]],\n",
       " \n",
       "          [[-1.0453e-01, -1.0581e-01, -1.5969e-02],\n",
       "           [ 1.0772e-02,  6.4301e-02,  7.9499e-02],\n",
       "           [ 2.9531e-02, -2.9546e-02, -4.7459e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 4.9752e-02, -7.3081e-03,  3.2373e-02],\n",
       "           [-4.7106e-03, -5.1779e-02, -9.9985e-02],\n",
       "           [-6.6789e-02,  6.4428e-03,  6.3614e-03]],\n",
       " \n",
       "          [[ 9.2401e-02,  4.3053e-02, -3.1678e-02],\n",
       "           [-6.5732e-02,  9.5645e-02, -9.6327e-02],\n",
       "           [ 1.0088e-01, -1.3430e-02,  7.2236e-02]],\n",
       " \n",
       "          [[ 3.1151e-02, -2.0771e-03,  1.0930e-01],\n",
       "           [-4.1599e-02,  1.3183e-02,  1.4337e-03],\n",
       "           [-7.1621e-02, -1.0437e-01,  1.4319e-02]],\n",
       " \n",
       "          [[ 2.0051e-02, -7.0241e-02, -1.0355e-01],\n",
       "           [-1.1678e-01,  8.5280e-02,  1.1335e-01],\n",
       "           [ 2.9223e-02,  3.5493e-02, -3.7593e-02]],\n",
       " \n",
       "          [[ 4.4081e-02,  7.2522e-03, -6.9983e-02],\n",
       "           [-7.0121e-02,  2.3762e-02, -3.8279e-02],\n",
       "           [-8.1340e-02,  2.3944e-02,  5.1958e-02]],\n",
       " \n",
       "          [[ 1.1531e-01, -1.4706e-02,  7.7645e-02],\n",
       "           [ 3.9597e-02, -7.3726e-02,  5.2114e-02],\n",
       "           [-7.1826e-02, -1.0179e-01,  9.6568e-02]],\n",
       " \n",
       "          [[ 1.0253e-01, -1.0252e-01, -8.0612e-02],\n",
       "           [ 5.3928e-02, -5.9175e-03, -1.3993e-02],\n",
       "           [-8.6369e-02,  5.6161e-02,  1.0544e-01]],\n",
       " \n",
       "          [[ 3.9484e-03,  1.1043e-01,  1.0585e-01],\n",
       "           [-1.0136e-01,  5.7687e-02, -1.0653e-01],\n",
       "           [ 5.9775e-02, -7.2165e-02, -9.6223e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.6241e-02, -8.1856e-02, -3.7362e-02],\n",
       "           [ 2.9810e-02,  5.2440e-02, -7.9531e-03],\n",
       "           [ 1.1292e-01,  3.9550e-02, -2.4322e-02]],\n",
       " \n",
       "          [[-1.0676e-01, -9.1573e-02,  4.6585e-02],\n",
       "           [-7.1540e-02, -1.0945e-01, -9.4906e-02],\n",
       "           [ 1.8249e-02, -1.1438e-01, -2.7742e-02]],\n",
       " \n",
       "          [[-6.1363e-02, -5.0361e-02,  1.0191e-01],\n",
       "           [ 5.4346e-02, -3.6740e-02, -9.8894e-03],\n",
       "           [ 2.3241e-03,  4.5398e-02,  3.6972e-02]],\n",
       " \n",
       "          [[-5.9315e-02, -9.3393e-02, -5.3063e-02],\n",
       "           [ 6.9268e-02, -3.2603e-02, -2.7271e-02],\n",
       "           [ 3.5140e-02,  1.0841e-01,  4.3908e-02]],\n",
       " \n",
       "          [[ 3.9270e-02, -4.5543e-02, -9.6131e-02],\n",
       "           [ 1.0409e-01,  1.1317e-02, -1.1154e-01],\n",
       "           [ 7.4669e-02, -9.6542e-02,  8.6878e-02]],\n",
       " \n",
       "          [[ 1.0593e-01, -6.2832e-02, -8.5777e-02],\n",
       "           [ 9.3292e-02,  6.0841e-02,  4.2705e-02],\n",
       "           [-3.1276e-02, -1.0453e-01,  2.5033e-03]],\n",
       " \n",
       "          [[ 7.4915e-02,  7.3952e-02,  3.0586e-02],\n",
       "           [ 3.8279e-03,  5.8632e-02,  2.8540e-02],\n",
       "           [-9.0073e-02, -1.1049e-01,  1.0026e-01]],\n",
       " \n",
       "          [[-5.6125e-02, -4.6258e-02, -9.2218e-02],\n",
       "           [-6.7282e-02,  5.5424e-02,  5.2263e-02],\n",
       "           [ 2.8136e-02, -2.4876e-04,  6.3413e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-5.9647e-07,  2.4330e-06, -4.5162e-07,  4.8748e-07, -3.9361e-07,\n",
       "          5.2772e-07, -6.9997e-07,  1.2391e-06], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 1.0596e-01, -1.0975e-01, -6.7626e-02],\n",
       "           [ 6.5663e-02,  1.0368e-02, -6.6725e-02],\n",
       "           [ 1.1211e-01,  4.7146e-02, -7.0935e-02]],\n",
       " \n",
       "          [[-2.0753e-02, -5.1216e-02,  1.0655e-02],\n",
       "           [ 4.5260e-02, -1.0892e-01,  6.8115e-03],\n",
       "           [-1.5083e-02, -4.6793e-02,  8.8906e-02]],\n",
       " \n",
       "          [[ 3.4628e-02, -4.8004e-03,  3.4816e-02],\n",
       "           [-7.2462e-02, -4.1115e-02,  4.5502e-02],\n",
       "           [ 4.3811e-02,  3.8335e-02, -2.0451e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0073e-02, -1.1173e-01,  1.0189e-01],\n",
       "           [ 5.4502e-02, -7.7062e-02,  8.0561e-02],\n",
       "           [ 1.1590e-01,  4.9802e-02,  8.8252e-03]],\n",
       " \n",
       "          [[-7.1533e-02,  1.0322e-01, -9.2861e-02],\n",
       "           [ 2.9812e-02,  8.4678e-02,  4.6187e-02],\n",
       "           [ 1.0990e-01,  8.5500e-03, -3.9742e-02]],\n",
       " \n",
       "          [[-5.9567e-02,  8.6537e-02,  5.0265e-02],\n",
       "           [ 6.8574e-02, -6.5873e-03,  7.8880e-03],\n",
       "           [ 1.0865e-01,  5.2039e-03,  5.0599e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 1.1654e-01, -9.3164e-02, -8.8377e-04],\n",
       "           [ 8.0141e-02, -2.6843e-02,  8.5009e-02],\n",
       "           [ 6.6311e-02,  1.1513e-01, -8.2012e-02]],\n",
       " \n",
       "          [[-1.0365e-01, -2.8935e-02,  3.1974e-02],\n",
       "           [-5.9020e-02,  5.7771e-02,  4.8060e-02],\n",
       "           [-8.6415e-02,  8.2729e-02, -9.9609e-02]],\n",
       " \n",
       "          [[-2.6040e-02,  5.1873e-02,  5.8832e-02],\n",
       "           [-8.6570e-02,  2.4066e-02, -9.5450e-02],\n",
       "           [ 3.9675e-02,  8.3497e-02, -3.1599e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.3471e-02,  8.9466e-02,  5.9875e-02],\n",
       "           [-2.5478e-02, -2.9288e-02, -2.2079e-02],\n",
       "           [ 9.9045e-02,  5.0363e-02, -3.2930e-02]],\n",
       " \n",
       "          [[ 1.3561e-02,  1.1109e-01,  5.0832e-02],\n",
       "           [ 9.8663e-02, -1.1557e-01, -1.4724e-02],\n",
       "           [-5.0860e-02,  8.1908e-02,  5.2527e-02]],\n",
       " \n",
       "          [[-4.4433e-03,  5.6079e-04, -1.0445e-01],\n",
       "           [-2.4524e-03, -1.1076e-01, -3.5353e-02],\n",
       "           [-1.5659e-02,  1.0133e-01,  7.1413e-02]]],\n",
       " \n",
       " \n",
       "         [[[-7.9916e-02,  9.4997e-02,  6.3330e-02],\n",
       "           [ 2.5214e-02,  5.8911e-03, -1.0595e-01],\n",
       "           [-1.0115e-02,  5.7860e-02, -4.4751e-02]],\n",
       " \n",
       "          [[ 5.0578e-02,  3.6965e-02,  4.9432e-02],\n",
       "           [ 9.6686e-02,  8.9037e-02, -1.1252e-01],\n",
       "           [ 4.0072e-02, -4.7423e-02, -9.3374e-02]],\n",
       " \n",
       "          [[-3.7092e-02,  9.8984e-02, -8.5466e-02],\n",
       "           [-1.9025e-02, -7.6891e-02,  7.5766e-02],\n",
       "           [ 3.7416e-02, -7.3661e-02, -5.4462e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.5936e-02,  1.2329e-02,  6.4016e-02],\n",
       "           [ 9.7418e-02,  7.3226e-02,  5.6733e-02],\n",
       "           [-1.1685e-02, -7.3565e-02, -1.7415e-02]],\n",
       " \n",
       "          [[ 4.5692e-02, -4.8535e-02, -7.3836e-02],\n",
       "           [-9.9504e-02, -5.4173e-02, -6.6413e-02],\n",
       "           [ 1.0260e-01,  9.6400e-02, -4.4833e-02]],\n",
       " \n",
       "          [[-8.3892e-02, -1.0901e-02, -5.9537e-03],\n",
       "           [ 5.5924e-02, -4.3667e-02, -1.1505e-01],\n",
       "           [ 7.0446e-02,  5.3306e-02, -8.2058e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-1.5829e-02, -1.1814e-02,  3.4990e-02],\n",
       "           [-8.6993e-02,  5.9150e-02, -5.6401e-02],\n",
       "           [-9.7279e-02, -6.9699e-03, -7.5337e-02]],\n",
       " \n",
       "          [[ 2.9813e-02,  1.1549e-01, -4.6069e-02],\n",
       "           [-4.3739e-02,  1.7784e-02, -7.2684e-02],\n",
       "           [ 2.1939e-02,  1.0849e-01,  4.3122e-02]],\n",
       " \n",
       "          [[ 1.5324e-02, -1.0589e-01,  6.3949e-03],\n",
       "           [-5.8176e-02, -5.9067e-02,  5.1694e-02],\n",
       "           [ 8.1678e-02, -1.3615e-02,  1.1435e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 8.8207e-02,  1.4562e-02,  5.7979e-02],\n",
       "           [ 2.1414e-02,  7.6598e-02,  7.1055e-02],\n",
       "           [-3.3177e-02,  1.0537e-01,  3.9153e-02]],\n",
       " \n",
       "          [[ 7.6854e-02, -1.1022e-01, -3.8383e-02],\n",
       "           [-1.2481e-02,  6.9782e-03,  9.3605e-02],\n",
       "           [ 7.5039e-02, -4.2673e-02,  1.0677e-01]],\n",
       " \n",
       "          [[-2.0955e-02, -1.1383e-01, -7.4705e-03],\n",
       "           [ 1.3901e-02, -3.5887e-02,  6.3933e-02],\n",
       "           [-1.0142e-01,  1.1030e-01, -1.1376e-01]]],\n",
       " \n",
       " \n",
       "         [[[-8.1285e-02,  7.0261e-02, -3.6781e-02],\n",
       "           [-2.5183e-02, -6.9895e-02,  1.5650e-02],\n",
       "           [-9.9628e-02, -1.6389e-03, -6.6890e-02]],\n",
       " \n",
       "          [[ 5.6473e-02, -9.3429e-02,  7.4969e-02],\n",
       "           [ 7.9101e-03, -5.1197e-02,  7.1813e-05],\n",
       "           [-8.0214e-02,  9.1234e-02,  1.0307e-01]],\n",
       " \n",
       "          [[-2.9440e-02,  1.9557e-02, -3.8387e-02],\n",
       "           [-3.1686e-02,  5.2514e-02, -7.8262e-03],\n",
       "           [-9.1575e-02, -1.1179e-01, -6.6905e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 7.8368e-02, -4.7290e-02, -7.3153e-02],\n",
       "           [ 1.0338e-01,  1.2947e-02, -6.1828e-02],\n",
       "           [ 4.2105e-02,  5.9849e-02, -8.5272e-02]],\n",
       " \n",
       "          [[-3.9795e-03,  1.0408e-01,  4.1150e-02],\n",
       "           [-2.5273e-03,  4.0724e-02,  9.0541e-02],\n",
       "           [ 9.2784e-04,  8.1323e-02,  6.5463e-02]],\n",
       " \n",
       "          [[-5.8732e-02, -5.1107e-02, -3.1513e-02],\n",
       "           [ 7.7662e-02, -6.3788e-02,  3.4241e-02],\n",
       "           [-9.3838e-02,  1.8005e-02, -5.3472e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5299e-02,  8.2401e-03,  2.1083e-02],\n",
       "           [-3.8618e-02,  9.7016e-02, -3.8389e-02],\n",
       "           [-2.4069e-02, -6.5083e-03, -4.2386e-02]],\n",
       " \n",
       "          [[ 6.2663e-02, -9.0784e-02, -8.3011e-02],\n",
       "           [-1.3387e-02, -1.1652e-01,  4.7518e-02],\n",
       "           [-2.6113e-02, -6.9434e-02, -1.1569e-01]],\n",
       " \n",
       "          [[ 1.3033e-02, -6.6901e-02, -1.7800e-02],\n",
       "           [ 1.8789e-03, -1.8216e-02,  8.8044e-02],\n",
       "           [ 3.7556e-02,  4.6266e-02, -7.4150e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.2628e-02,  1.0918e-01,  1.0979e-01],\n",
       "           [ 1.0463e-01, -1.8819e-02, -7.4758e-02],\n",
       "           [-7.6780e-02,  2.5457e-02, -1.1930e-03]],\n",
       " \n",
       "          [[ 5.7548e-02,  4.0585e-02,  1.3014e-02],\n",
       "           [-4.6813e-02,  5.5623e-02, -4.4226e-02],\n",
       "           [ 8.2877e-02,  5.7495e-02, -2.7419e-02]],\n",
       " \n",
       "          [[-1.1593e-01, -3.2670e-03,  4.3557e-02],\n",
       "           [ 6.5554e-02,  4.5566e-02, -8.2169e-02],\n",
       "           [ 7.5876e-03, -2.6699e-02, -3.4675e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 5.8686e-07,  1.2452e-06, -8.1077e-07,  1.0770e-06,  1.7409e-06,\n",
       "          1.5199e-06,  8.0789e-07, -3.6236e-07, -2.8402e-07,  9.8090e-07,\n",
       "          3.4722e-08, -1.0380e-06,  1.8672e-06, -7.8568e-07,  3.5101e-07,\n",
       "         -8.4805e-07], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 6.1545e-02, -2.5755e-02, -3.0255e-02],\n",
       "           [ 6.5135e-02, -1.4185e-03, -6.3586e-02],\n",
       "           [-1.9883e-04, -8.2284e-02,  1.4199e-02]],\n",
       " \n",
       "          [[-2.1829e-02,  1.5695e-02,  3.0291e-02],\n",
       "           [ 5.3487e-02,  8.6514e-03, -2.5305e-05],\n",
       "           [-4.2518e-02,  7.6312e-02,  5.7079e-02]],\n",
       " \n",
       "          [[ 5.6845e-02,  6.3257e-03, -5.0063e-03],\n",
       "           [ 2.3441e-03, -5.4608e-02,  2.5415e-02],\n",
       "           [ 2.5078e-02,  6.4004e-02,  4.6130e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.9855e-02, -3.7643e-02,  5.1973e-03],\n",
       "           [-3.7177e-02,  8.1941e-02, -7.0355e-02],\n",
       "           [-3.4938e-02,  7.2912e-02,  5.9808e-02]],\n",
       " \n",
       "          [[ 3.9299e-02,  7.3678e-02, -6.9843e-02],\n",
       "           [ 6.7369e-04,  6.8069e-02, -2.6743e-02],\n",
       "           [-4.7060e-02, -2.7564e-02,  5.1519e-02]],\n",
       " \n",
       "          [[ 1.5899e-02,  4.9935e-02, -7.4445e-03],\n",
       "           [ 6.9981e-02,  3.4213e-02, -3.1852e-02],\n",
       "           [ 6.1247e-02,  3.0474e-02, -4.2813e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.0590e-02, -4.6881e-02, -4.4005e-02],\n",
       "           [-3.5301e-02, -5.8977e-02,  6.3820e-02],\n",
       "           [ 6.4758e-02,  7.2446e-02,  7.6219e-02]],\n",
       " \n",
       "          [[ 7.4843e-02, -4.6326e-02, -5.1652e-02],\n",
       "           [-6.8931e-02, -9.6670e-03,  7.2617e-02],\n",
       "           [-4.8979e-02, -2.9423e-02, -2.0687e-02]],\n",
       " \n",
       "          [[ 8.2399e-02,  3.5895e-02, -4.0375e-03],\n",
       "           [-1.4516e-02,  3.1052e-02,  5.6045e-02],\n",
       "           [ 5.6737e-02, -1.1004e-02,  3.1714e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.5732e-02, -2.2628e-03,  9.5411e-03],\n",
       "           [ 4.5326e-02, -7.5841e-02, -2.8063e-02],\n",
       "           [ 1.0002e-03, -7.7265e-02, -5.0481e-02]],\n",
       " \n",
       "          [[ 7.1998e-02,  5.3682e-02, -3.1301e-02],\n",
       "           [-6.8043e-02, -4.2934e-02,  3.6241e-02],\n",
       "           [-1.7740e-02,  3.9852e-02, -2.2435e-02]],\n",
       " \n",
       "          [[ 7.7409e-02, -6.6034e-02, -7.9840e-02],\n",
       "           [-5.2669e-02, -9.8033e-04, -7.7379e-02],\n",
       "           [ 1.3191e-02, -6.8430e-02,  5.3819e-02]]],\n",
       " \n",
       " \n",
       "         [[[-5.8276e-02,  1.8108e-02,  4.4398e-02],\n",
       "           [ 1.9902e-02,  6.3793e-02,  1.4575e-02],\n",
       "           [-4.1643e-02,  4.2388e-02,  8.5860e-03]],\n",
       " \n",
       "          [[-3.0130e-02, -5.6281e-02,  3.8607e-02],\n",
       "           [-1.8111e-02,  2.6771e-02, -3.2807e-02],\n",
       "           [ 5.4618e-02,  2.6179e-02,  2.7122e-02]],\n",
       " \n",
       "          [[-2.0599e-02, -5.9663e-02,  6.4338e-02],\n",
       "           [-7.2501e-02,  2.6952e-02, -5.3535e-02],\n",
       "           [-4.0827e-02, -1.8861e-02, -7.1223e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.0185e-02, -6.2509e-03, -4.4695e-02],\n",
       "           [-5.6713e-02, -4.0402e-02, -3.4646e-02],\n",
       "           [-3.5109e-02, -1.1761e-02,  3.1162e-02]],\n",
       " \n",
       "          [[-3.9473e-02, -1.1967e-02,  9.5971e-03],\n",
       "           [-6.6047e-02,  8.0939e-02,  4.6709e-02],\n",
       "           [-1.6333e-02, -6.4883e-02, -5.4153e-02]],\n",
       " \n",
       "          [[ 3.7342e-03, -2.7650e-02, -2.4019e-03],\n",
       "           [ 1.4065e-02, -6.8253e-02,  1.6427e-02],\n",
       "           [-3.9895e-02, -8.1606e-04,  4.1382e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 2.3269e-02, -9.7696e-03, -3.0570e-02],\n",
       "           [-7.2370e-03, -5.8600e-02, -3.0513e-02],\n",
       "           [ 2.7931e-02, -2.9153e-02,  7.9066e-02]],\n",
       " \n",
       "          [[-7.7981e-02, -6.9618e-02, -6.0775e-02],\n",
       "           [-6.8895e-02, -4.9319e-03, -1.4858e-02],\n",
       "           [-5.1013e-02, -2.0263e-02, -2.6223e-02]],\n",
       " \n",
       "          [[ 3.9540e-04,  6.2399e-02,  5.6095e-02],\n",
       "           [-2.3143e-02,  2.8216e-02,  4.6482e-02],\n",
       "           [ 2.0760e-02, -4.8600e-03, -6.8615e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.3467e-02, -4.9715e-02,  4.6443e-02],\n",
       "           [-6.6425e-02,  2.6898e-02,  7.2396e-02],\n",
       "           [-1.6380e-02,  4.8104e-02,  4.5025e-02]],\n",
       " \n",
       "          [[ 7.0954e-02,  2.0291e-03,  7.1266e-02],\n",
       "           [ 1.1057e-04,  1.9433e-02,  7.8180e-03],\n",
       "           [ 3.0439e-02, -4.9531e-02, -3.0125e-02]],\n",
       " \n",
       "          [[-2.7447e-02, -7.2128e-02,  3.3803e-02],\n",
       "           [ 3.8007e-02, -7.1991e-02,  5.2392e-02],\n",
       "           [ 4.1566e-02, -4.0306e-02,  1.6414e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.3178e-03, -1.6543e-03, -6.1757e-02],\n",
       "           [-7.2586e-02,  3.4263e-02, -7.9382e-02],\n",
       "           [ 6.7826e-02,  6.4592e-02, -2.7865e-02]],\n",
       " \n",
       "          [[-2.4082e-02,  5.7231e-02,  5.5941e-02],\n",
       "           [-6.9826e-02,  1.2477e-02, -1.7072e-02],\n",
       "           [ 9.7098e-03,  1.1594e-02,  4.6919e-02]],\n",
       " \n",
       "          [[ 2.9194e-02,  3.9365e-02, -1.3362e-02],\n",
       "           [ 2.4809e-02,  4.4264e-02, -1.1918e-02],\n",
       "           [ 6.3834e-02, -6.8513e-02,  3.1943e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.5733e-02,  6.8796e-02, -7.5403e-02],\n",
       "           [-2.5067e-02, -7.7378e-02,  1.0725e-02],\n",
       "           [ 3.0559e-03, -8.0738e-02,  1.8577e-03]],\n",
       " \n",
       "          [[-1.3031e-02, -2.3833e-03,  3.3129e-02],\n",
       "           [-5.1761e-02, -3.8394e-02,  1.7094e-02],\n",
       "           [-4.7187e-02,  1.5464e-04, -1.5795e-04]],\n",
       " \n",
       "          [[ 7.0528e-02, -1.0816e-02,  3.4486e-02],\n",
       "           [ 4.1999e-02,  7.4477e-02, -3.8414e-02],\n",
       "           [-7.4973e-03,  5.8760e-02,  3.1816e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.8688e-02,  3.5065e-02,  5.1718e-02],\n",
       "           [-6.8801e-02, -1.2409e-02,  6.7678e-02],\n",
       "           [-3.3908e-02,  3.9285e-02,  4.6076e-02]],\n",
       " \n",
       "          [[-5.8353e-02,  1.7291e-02,  7.9536e-02],\n",
       "           [-7.8129e-02, -4.8334e-02,  7.4241e-02],\n",
       "           [-8.0521e-02,  9.7051e-04,  8.1252e-02]],\n",
       " \n",
       "          [[-3.7044e-02, -7.7808e-03,  2.8930e-02],\n",
       "           [ 1.0509e-02,  5.9272e-03, -3.0357e-02],\n",
       "           [ 1.6576e-02, -6.2739e-02,  7.8021e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 8.1416e-02,  3.7485e-02,  3.5392e-03],\n",
       "           [-4.5124e-02, -3.6792e-02,  6.8156e-02],\n",
       "           [ 5.2442e-02,  1.3064e-02,  1.6670e-02]],\n",
       " \n",
       "          [[-7.3824e-02,  4.8923e-02,  2.1964e-02],\n",
       "           [-4.9955e-02,  2.4044e-02,  7.8671e-02],\n",
       "           [-2.0406e-02, -1.0696e-02,  2.9814e-02]],\n",
       " \n",
       "          [[ 2.5656e-02, -7.8388e-02, -4.4324e-02],\n",
       "           [-2.6371e-02, -2.0741e-02, -5.5637e-03],\n",
       "           [ 6.5592e-02,  1.3189e-03, -9.4741e-03]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-7.6767e-06, -1.6385e-05,  1.3541e-05,  2.1361e-05, -1.7681e-05,\n",
       "         -4.8198e-05,  5.1499e-05, -9.1552e-06, -3.4718e-06,  3.2657e-05,\n",
       "          3.8678e-06, -2.2812e-06,  1.6884e-05,  1.9567e-05, -1.2952e-05,\n",
       "         -1.0281e-05, -1.2125e-05, -2.4054e-05, -1.4744e-05,  3.5042e-05,\n",
       "          4.3283e-05, -1.7663e-05,  4.5216e-05,  8.0786e-06, -7.2738e-06,\n",
       "          2.6967e-06,  1.0066e-05,  1.6601e-05,  2.0003e-05, -1.1828e-06,\n",
       "          6.3208e-06, -1.3973e-05], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 3.1744e-02,  3.1336e-02, -5.5938e-02],\n",
       "           [-5.1533e-03, -1.8420e-02, -3.6304e-02],\n",
       "           [ 5.2676e-02,  9.4321e-03, -1.0797e-02]],\n",
       " \n",
       "          [[-2.1922e-02, -4.3320e-03,  3.3511e-02],\n",
       "           [ 4.9289e-02, -4.6558e-02,  5.5960e-02],\n",
       "           [-4.8238e-02,  1.5065e-02,  2.8974e-03]],\n",
       " \n",
       "          [[ 4.0667e-02, -5.7487e-02, -4.7816e-02],\n",
       "           [-4.8536e-03,  2.2438e-03,  1.6419e-02],\n",
       "           [-5.6072e-02,  5.0685e-02, -3.9631e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.1067e-02, -2.3050e-02,  4.3441e-02],\n",
       "           [-2.1422e-02, -4.6006e-02,  7.6456e-03],\n",
       "           [ 4.8565e-02,  3.9755e-02,  4.1196e-02]],\n",
       " \n",
       "          [[-3.7476e-02,  2.9933e-02,  3.2557e-03],\n",
       "           [-1.8384e-02, -4.1543e-02,  1.1644e-03],\n",
       "           [ 4.3525e-02,  8.1638e-03, -4.5551e-02]],\n",
       " \n",
       "          [[-5.4229e-02, -5.5574e-02,  2.9455e-02],\n",
       "           [-1.6550e-02,  1.6571e-02, -5.2363e-02],\n",
       "           [ 5.5803e-02,  4.1561e-02,  4.5703e-02]]],\n",
       " \n",
       " \n",
       "         [[[-2.0155e-02,  5.0757e-02, -9.7323e-03],\n",
       "           [-5.5364e-02,  5.4745e-02, -3.3077e-02],\n",
       "           [ 3.4361e-02, -1.2416e-02,  2.0255e-02]],\n",
       " \n",
       "          [[ 1.5667e-02, -5.1455e-02,  4.2655e-02],\n",
       "           [-1.2932e-02, -6.0655e-05, -1.8967e-02],\n",
       "           [ 2.7875e-02, -4.2182e-02,  4.7373e-02]],\n",
       " \n",
       "          [[-5.2472e-02,  3.2710e-02, -2.9960e-02],\n",
       "           [ 1.3809e-02, -3.9521e-02,  3.5218e-02],\n",
       "           [ 5.5890e-02,  4.7717e-02,  1.5513e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.8193e-03, -3.6968e-02,  5.6189e-02],\n",
       "           [ 1.4854e-02,  3.5683e-02, -2.9635e-03],\n",
       "           [-5.6170e-02,  2.9276e-02,  1.9187e-02]],\n",
       " \n",
       "          [[ 2.9704e-05,  3.5180e-03, -1.9931e-02],\n",
       "           [-2.4019e-02, -5.3890e-02, -4.6929e-02],\n",
       "           [ 2.9329e-02,  2.4773e-02,  2.3904e-02]],\n",
       " \n",
       "          [[-1.4687e-02, -4.8514e-02,  4.7355e-02],\n",
       "           [ 6.7067e-03, -5.7339e-02, -4.9660e-02],\n",
       "           [-4.5257e-02,  1.8382e-02,  4.1813e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.2118e-02,  1.1108e-02, -3.9488e-02],\n",
       "           [ 1.2113e-02, -4.6451e-02,  2.9029e-02],\n",
       "           [-4.4708e-02,  2.8630e-02, -2.4268e-02]],\n",
       " \n",
       "          [[ 3.0027e-02, -2.4410e-03, -5.6745e-02],\n",
       "           [ 5.3758e-02,  5.0228e-02, -6.6729e-03],\n",
       "           [-4.1804e-02,  2.7879e-02, -3.6789e-02]],\n",
       " \n",
       "          [[ 3.1481e-02, -2.5541e-02, -4.5401e-02],\n",
       "           [-1.1106e-03,  3.5548e-02, -1.4066e-02],\n",
       "           [-8.6258e-03,  3.3563e-02,  4.5753e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.2044e-03,  3.8417e-02,  1.3796e-02],\n",
       "           [ 5.0629e-02,  3.9409e-02,  7.1723e-03],\n",
       "           [-4.5453e-02,  1.4417e-02,  4.3204e-02]],\n",
       " \n",
       "          [[-4.7168e-02,  5.3939e-02, -3.1210e-02],\n",
       "           [-8.3225e-03,  2.9979e-02,  2.7252e-02],\n",
       "           [ 3.8383e-03, -5.4794e-02, -3.8269e-02]],\n",
       " \n",
       "          [[ 4.9931e-02, -1.2501e-02, -5.5178e-02],\n",
       "           [ 5.5145e-02, -4.7784e-02, -3.6517e-02],\n",
       "           [ 1.2032e-02, -1.3279e-02,  4.4421e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.1517e-02,  3.0642e-02,  2.3573e-02],\n",
       "           [ 1.7418e-02,  4.6410e-02,  5.3175e-02],\n",
       "           [ 1.7964e-02, -3.9221e-03,  4.6974e-02]],\n",
       " \n",
       "          [[-7.4388e-03, -3.7422e-03, -3.3172e-02],\n",
       "           [-2.9688e-02, -3.0788e-02,  3.1492e-02],\n",
       "           [ 2.4739e-02, -2.6563e-02,  5.6180e-02]],\n",
       " \n",
       "          [[ 8.5692e-03, -1.0164e-02, -1.2341e-02],\n",
       "           [-3.4171e-02, -2.1653e-03,  4.3661e-04],\n",
       "           [ 2.1370e-03,  4.1807e-02,  3.5781e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.7253e-02,  2.5401e-02,  2.4998e-02],\n",
       "           [-1.7863e-02, -5.6149e-02, -2.1577e-02],\n",
       "           [-1.8727e-02, -6.2238e-03,  5.4202e-02]],\n",
       " \n",
       "          [[ 2.5563e-02, -3.9188e-02,  3.4852e-02],\n",
       "           [-3.5927e-02, -4.3376e-02,  4.8089e-02],\n",
       "           [ 1.3890e-02,  8.3516e-04,  3.2387e-02]],\n",
       " \n",
       "          [[-1.4132e-02,  5.1420e-02,  5.5781e-02],\n",
       "           [ 4.2211e-02, -1.0197e-02, -1.4300e-02],\n",
       "           [ 5.4201e-02, -8.2894e-03,  5.0925e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.5463e-02,  4.9407e-02,  3.4347e-02],\n",
       "           [ 2.4408e-02, -4.3995e-02, -4.3833e-02],\n",
       "           [ 3.5274e-02,  4.5617e-02,  4.3064e-02]],\n",
       " \n",
       "          [[-1.3730e-03,  5.7486e-02, -8.3643e-04],\n",
       "           [ 5.7782e-02, -1.3993e-02, -4.0537e-03],\n",
       "           [ 7.2438e-03,  2.6414e-02,  3.7233e-02]],\n",
       " \n",
       "          [[ 1.6865e-02,  3.2087e-02, -2.2260e-02],\n",
       "           [ 4.0377e-02,  2.4609e-02,  2.5129e-02],\n",
       "           [-1.1770e-03,  2.6462e-02, -3.9013e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0476e-03, -2.6004e-02,  3.7730e-02],\n",
       "           [ 3.0523e-02, -3.8981e-02, -1.2943e-02],\n",
       "           [-4.2061e-02, -2.1544e-02,  1.9544e-02]],\n",
       " \n",
       "          [[ 3.0390e-02, -3.5637e-02,  2.0345e-02],\n",
       "           [ 4.6735e-02, -2.7419e-02, -4.3140e-03],\n",
       "           [ 2.1792e-02, -4.4930e-02,  3.3591e-02]],\n",
       " \n",
       "          [[ 3.3708e-02,  5.7520e-02, -5.3480e-02],\n",
       "           [ 7.1827e-05,  7.4651e-03, -3.3444e-02],\n",
       "           [-3.9308e-02,  4.8881e-02, -4.0356e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.5541e-02, -3.8651e-02,  8.3843e-04],\n",
       "           [ 4.6055e-02, -4.5120e-02, -3.1067e-02],\n",
       "           [-3.8748e-02,  5.2796e-02, -9.6967e-03]],\n",
       " \n",
       "          [[-2.6249e-02, -5.5250e-02, -2.1020e-02],\n",
       "           [ 2.4380e-02,  1.2085e-02, -4.7428e-02],\n",
       "           [-4.6271e-02, -1.4017e-02, -3.4384e-03]],\n",
       " \n",
       "          [[ 2.4946e-02, -2.0147e-02, -4.1027e-02],\n",
       "           [ 4.4302e-03,  5.1828e-02,  2.0739e-02],\n",
       "           [-4.0186e-02, -5.1957e-02, -1.5967e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 7.0424e-03,  8.6631e-03, -3.5196e-02],\n",
       "           [-3.7285e-02,  2.3237e-02,  1.6657e-02],\n",
       "           [-4.7231e-02, -1.4883e-02, -5.1617e-02]],\n",
       " \n",
       "          [[-1.5154e-02,  4.9572e-02,  4.3428e-02],\n",
       "           [ 7.6614e-04,  9.1983e-03,  1.9455e-02],\n",
       "           [-1.5877e-02, -4.1168e-02, -4.2357e-02]],\n",
       " \n",
       "          [[ 2.8154e-02,  1.6265e-02,  3.0679e-02],\n",
       "           [ 5.9688e-03,  3.0965e-02, -1.9785e-02],\n",
       "           [-2.8556e-02,  3.4626e-02,  3.7962e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0009,  0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009,\n",
       "         -0.0009,  0.0009, -0.0009, -0.0009,  0.0009, -0.0009, -0.0009,  0.0009,\n",
       "         -0.0009,  0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009,  0.0009,\n",
       "          0.0009, -0.0009,  0.0009,  0.0009,  0.0009, -0.0009, -0.0009, -0.0009],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-3.0711e-02, -1.3899e-01,  1.6985e-01,  2.2527e-03,  1.3149e-01,\n",
       "           1.6615e-01,  5.8196e-02, -1.0391e-01,  1.6657e-02, -1.0205e-01,\n",
       "           1.6057e-01,  1.2919e-02, -8.9279e-02,  8.0864e-02, -2.2494e-02,\n",
       "           1.5895e-01,  4.2922e-02, -1.3999e-01,  1.1208e-01, -1.0373e-01,\n",
       "          -3.4353e-02, -4.5973e-02,  1.5541e-02, -1.1932e-01, -1.6035e-01,\n",
       "           1.3740e-01, -3.2832e-02, -1.6209e-01, -1.3869e-01,  1.6593e-01,\n",
       "           1.1447e-02,  7.9548e-02],\n",
       "         [-1.1330e-01, -1.6943e-01,  1.6627e-01, -1.6539e-02, -8.5556e-02,\n",
       "           9.3202e-02,  1.0194e-01, -3.4526e-02,  1.2686e-01, -4.4639e-02,\n",
       "          -9.1295e-02,  1.6389e-01,  8.0881e-02,  1.3088e-01,  1.5748e-01,\n",
       "          -2.6114e-02, -1.5626e-01,  3.8963e-03,  6.1163e-03,  1.6429e-01,\n",
       "          -9.4605e-02,  1.2916e-01,  5.7506e-02, -1.4262e-01,  4.0048e-02,\n",
       "           1.7580e-01, -1.3731e-01,  1.5238e-01, -1.8653e-02, -1.0140e-01,\n",
       "           1.6931e-01, -9.6589e-02],\n",
       "         [ 1.7260e-01, -1.1723e-01,  1.5279e-01,  2.8800e-02,  2.1542e-03,\n",
       "           1.6420e-01,  1.1357e-01,  1.6957e-01,  1.2864e-01, -1.0627e-01,\n",
       "          -1.6331e-01, -7.1200e-03, -1.5838e-02,  1.6691e-01,  4.9412e-04,\n",
       "          -1.1760e-01,  1.2660e-01,  9.7868e-02, -5.9527e-02, -1.6849e-01,\n",
       "           8.6861e-02,  4.0142e-02,  7.3615e-03,  6.9747e-02,  1.6371e-01,\n",
       "          -1.0563e-01, -2.1516e-02, -1.4729e-01, -8.9833e-02,  6.4067e-02,\n",
       "          -6.6878e-02, -1.2586e-01],\n",
       "         [ 1.6851e-01, -8.3853e-02, -1.7394e-01,  1.4606e-01,  1.7006e-01,\n",
       "          -4.4924e-03,  1.5840e-01,  9.4216e-02,  7.9215e-02,  6.3516e-02,\n",
       "           7.4758e-02,  1.0716e-01, -5.7206e-02,  5.1078e-02,  3.5527e-02,\n",
       "          -8.9927e-03,  8.9860e-02, -7.9521e-03, -5.1184e-02,  1.1163e-01,\n",
       "          -3.5511e-02,  6.2530e-02, -6.6099e-02, -1.7789e-02, -1.7571e-01,\n",
       "           1.9416e-03, -1.2640e-01,  1.4322e-01, -1.4577e-01, -1.3440e-01,\n",
       "           1.3281e-01,  3.1660e-02],\n",
       "         [-1.5007e-01,  8.8802e-02,  1.7206e-01,  3.6690e-02, -1.3441e-01,\n",
       "          -9.7930e-02,  1.5504e-01, -4.6453e-02, -2.6889e-02,  1.4307e-01,\n",
       "          -1.1474e-01,  1.0538e-02,  1.4052e-01,  7.7940e-02,  9.6892e-02,\n",
       "           7.0762e-03,  1.7165e-01,  1.5900e-01, -7.6010e-02,  4.3836e-03,\n",
       "           9.8786e-02, -6.4007e-02,  5.4855e-02, -3.4510e-02, -1.4143e-01,\n",
       "          -9.4281e-02, -7.2634e-02,  1.3923e-02,  1.7501e-02, -6.6764e-03,\n",
       "           1.0911e-01,  9.4701e-02],\n",
       "         [-7.8749e-02, -1.0077e-01,  1.1290e-01,  1.6936e-01,  1.5559e-01,\n",
       "          -1.4035e-01, -8.8472e-03,  1.7413e-01, -6.3398e-02,  8.6687e-03,\n",
       "           9.5781e-02, -1.2141e-01, -4.9166e-02, -7.5782e-03, -1.4587e-01,\n",
       "           6.6293e-02,  6.3786e-02, -1.1102e-01,  1.7072e-01,  1.6437e-01,\n",
       "          -1.0556e-01,  1.3061e-01, -1.7802e-02,  1.4175e-01,  2.2295e-02,\n",
       "          -1.4278e-01, -1.3471e-01, -1.2136e-01,  1.7312e-01,  1.3875e-01,\n",
       "          -4.1411e-02, -1.4304e-01],\n",
       "         [ 1.5041e-01,  5.1614e-02, -1.1988e-01, -5.5419e-02,  8.2644e-02,\n",
       "           7.7389e-02, -1.0022e-01,  6.4703e-03,  2.3378e-02,  2.4285e-02,\n",
       "          -6.7714e-03,  6.1844e-02,  7.2616e-02,  1.0361e-01,  2.7448e-02,\n",
       "           1.5971e-01,  8.3857e-02,  1.3887e-01, -2.1324e-02,  1.1010e-01,\n",
       "          -7.3420e-02,  2.5021e-02,  5.4832e-02, -4.1287e-02,  9.4068e-02,\n",
       "          -1.3449e-01, -1.1493e-01,  5.3144e-02, -5.9686e-02, -1.3906e-02,\n",
       "          -1.4190e-02,  1.0950e-01],\n",
       "         [ 1.4487e-01, -6.1498e-02,  2.3649e-03,  3.3710e-02,  2.3101e-02,\n",
       "           1.0858e-01,  1.4257e-04,  3.2968e-02,  1.1589e-01,  9.5508e-02,\n",
       "           1.6099e-01,  3.9948e-02, -1.6291e-01,  7.6339e-02,  9.8943e-02,\n",
       "          -5.6420e-02, -1.2999e-01, -1.4362e-01, -1.5908e-01,  1.0650e-02,\n",
       "           8.5370e-02, -3.1825e-02, -1.3035e-01,  4.7297e-04, -1.0591e-01,\n",
       "           4.6073e-02,  1.2399e-01,  3.7303e-02,  2.3648e-02, -2.7876e-02,\n",
       "           1.2950e-01,  8.5019e-02],\n",
       "         [ 8.7826e-02, -6.6990e-02, -9.3978e-02, -4.6776e-02, -2.6795e-02,\n",
       "           1.2192e-01,  1.6248e-01,  6.2664e-02, -8.2346e-02, -9.6425e-02,\n",
       "           4.8198e-02, -9.7766e-02, -1.1962e-01,  2.0291e-03, -4.0373e-02,\n",
       "          -6.4097e-02,  1.4119e-01,  1.2480e-01,  1.4788e-01, -7.4847e-02,\n",
       "           6.8732e-02, -5.0356e-02,  1.0688e-01, -7.4759e-02, -8.8562e-02,\n",
       "           1.6897e-01, -1.6680e-01, -7.1811e-02,  4.9237e-02,  1.6840e-01,\n",
       "          -1.6552e-01,  3.3155e-03],\n",
       "         [ 1.5443e-01,  8.9688e-02,  1.6744e-01, -1.7581e-01, -1.3346e-01,\n",
       "           7.8006e-02,  3.8708e-02,  8.1507e-02, -1.1900e-01, -1.7189e-01,\n",
       "          -4.5377e-02,  1.5611e-01,  1.1569e-01, -9.0740e-02, -4.8263e-02,\n",
       "          -1.7559e-01,  1.0899e-01, -1.2556e-01, -1.3399e-02,  7.8219e-02,\n",
       "           7.1496e-03,  7.3182e-02, -4.7178e-02, -1.7371e-01,  1.5143e-01,\n",
       "           1.1145e-01, -1.2145e-01, -1.6120e-01, -5.6644e-02,  3.9727e-02,\n",
       "          -7.3665e-02,  4.5313e-02]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009,\n",
       "         -0.0009, -0.0009], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynet.tree.optimizer.param_groups[0]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[-5.5703e-02, -1.8930e-01, -1.2059e-01],\n",
       "           [ 1.7856e-02,  1.4319e-01, -2.3043e-02],\n",
       "           [ 1.7780e-01, -1.2991e-01,  9.9326e-02]],\n",
       " \n",
       "          [[-4.1155e-02,  1.8590e-01, -6.7313e-02],\n",
       "           [ 1.2220e-01,  3.9127e-02, -1.1250e-01],\n",
       "           [-7.2557e-02,  1.9125e-01,  6.6179e-02]],\n",
       " \n",
       "          [[-1.8131e-01,  1.3526e-01,  1.2205e-01],\n",
       "           [ 1.4724e-01,  9.4187e-02, -9.7348e-03],\n",
       "           [ 6.0044e-02, -1.6911e-01,  1.6732e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 2.6294e-03,  8.4629e-04,  1.2711e-01],\n",
       "           [ 1.2078e-01,  6.4700e-02, -1.4869e-01],\n",
       "           [-3.1222e-02, -5.2589e-02, -1.7636e-02]],\n",
       " \n",
       "          [[ 3.4677e-02,  5.5599e-02, -4.6393e-02],\n",
       "           [-4.7956e-02,  1.5129e-01, -2.2086e-02],\n",
       "           [-1.3991e-01,  1.0761e-01,  1.4586e-01]],\n",
       " \n",
       "          [[-9.2866e-02,  7.2832e-02, -6.9745e-02],\n",
       "           [ 1.5640e-01,  6.2109e-02, -1.3392e-01],\n",
       "           [-1.2589e-01, -4.6108e-02,  1.6425e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 8.9153e-02, -4.8346e-02, -1.4552e-01],\n",
       "           [ 1.1294e-01,  1.4859e-01, -4.3243e-02],\n",
       "           [-3.7931e-02, -9.1050e-02,  1.4655e-01]],\n",
       " \n",
       "          [[ 1.8323e-01, -2.4805e-02, -1.6270e-01],\n",
       "           [ 6.6408e-02, -6.0292e-02,  6.1545e-02],\n",
       "           [-1.6321e-01, -1.6069e-02, -9.7403e-02]],\n",
       " \n",
       "          [[ 4.7428e-02,  2.0885e-02, -6.2822e-02],\n",
       "           [ 1.6861e-01,  8.4014e-02,  1.3429e-01],\n",
       "           [ 1.3844e-01,  1.9040e-01,  9.4029e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.8477e-02, -1.7285e-04, -3.1358e-02],\n",
       "           [-1.1659e-01, -2.1163e-02, -1.1040e-01],\n",
       "           [ 5.7792e-02,  1.2855e-01, -1.8485e-02]],\n",
       " \n",
       "          [[-4.6322e-02, -7.3238e-02,  8.3404e-02],\n",
       "           [-1.2309e-01, -1.6627e-01, -2.7592e-02],\n",
       "           [ 4.6386e-02, -1.7531e-01,  1.7540e-01]],\n",
       " \n",
       "          [[ 9.9185e-02,  3.3236e-02,  3.7494e-02],\n",
       "           [ 6.1158e-02, -1.6947e-02, -1.8616e-01],\n",
       "           [ 4.6673e-02,  1.5698e-01, -7.9005e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.1837e-01, -1.4000e-01, -3.3200e-03],\n",
       "           [-9.2440e-02,  1.8589e-01,  6.3253e-02],\n",
       "           [-1.0663e-01,  9.2424e-02,  5.1739e-02]],\n",
       " \n",
       "          [[-2.3630e-03, -9.2072e-03,  1.8170e-02],\n",
       "           [ 4.2063e-02,  8.5953e-02, -5.8541e-02],\n",
       "           [ 8.1687e-02, -8.4843e-02,  5.9178e-02]],\n",
       " \n",
       "          [[ 5.9736e-02,  7.8083e-02,  1.1295e-01],\n",
       "           [-1.7301e-01, -4.1670e-02,  1.7512e-01],\n",
       "           [ 3.0574e-02, -1.6459e-01, -1.8772e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.7431e-01, -8.0356e-02,  1.3006e-01],\n",
       "           [-1.7206e-01, -9.4230e-02,  9.8254e-02],\n",
       "           [-1.7612e-01, -1.3631e-01, -3.8322e-02]],\n",
       " \n",
       "          [[-1.9088e-01,  1.4425e-01, -1.4387e-01],\n",
       "           [ 1.2341e-01, -5.7118e-02,  3.7989e-02],\n",
       "           [-1.6740e-01, -3.3371e-02,  1.8572e-01]],\n",
       " \n",
       "          [[-2.9792e-02,  1.5768e-01, -3.9545e-02],\n",
       "           [-6.5627e-02,  1.1575e-01,  6.0459e-02],\n",
       "           [ 1.0293e-01,  1.1593e-01,  8.1366e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.0220e-01,  2.1723e-02, -1.4390e-02],\n",
       "           [-1.5553e-01, -8.2993e-02, -6.6285e-02],\n",
       "           [-6.7716e-02, -5.7565e-02,  1.7704e-01]],\n",
       " \n",
       "          [[-1.4083e-01, -1.4394e-01,  1.7138e-01],\n",
       "           [ 6.2994e-02, -4.3541e-02, -5.0947e-02],\n",
       "           [ 7.2890e-02,  9.5993e-02,  1.1132e-01]],\n",
       " \n",
       "          [[-1.8661e-01, -1.7437e-01,  1.3776e-01],\n",
       "           [-9.0988e-02, -6.8625e-02, -1.6615e-02],\n",
       "           [-4.4022e-02, -6.9727e-02, -1.3337e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.1279e-01, -1.5291e-01, -8.4339e-02],\n",
       "           [-1.9075e-01, -1.4527e-01,  1.8494e-02],\n",
       "           [-1.6626e-01,  6.7979e-02,  3.7581e-02]],\n",
       " \n",
       "          [[-1.1558e-01, -1.7290e-01,  2.7687e-03],\n",
       "           [ 1.4515e-01,  9.6457e-02,  1.5122e-01],\n",
       "           [-1.4001e-01, -2.4721e-02,  1.7420e-01]],\n",
       " \n",
       "          [[-1.3206e-04,  1.2146e-01,  1.1441e-01],\n",
       "           [-6.5931e-02, -1.6496e-01, -5.5603e-02],\n",
       "           [ 1.2408e-01, -1.3413e-01,  1.2070e-01]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-5.4724e-07,  7.6908e-08, -2.4104e-07, -1.6286e-06, -2.2674e-06,\n",
       "         -6.9854e-07, -3.7948e-07,  8.3706e-07], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[-1.1204e-01, -8.7083e-03, -9.5877e-02],\n",
       "           [ 3.1211e-02,  1.0616e-01,  4.7606e-02],\n",
       "           [ 7.9017e-02,  6.9394e-03, -1.0252e-01]],\n",
       " \n",
       "          [[-2.7647e-02, -1.2667e-03, -7.7603e-02],\n",
       "           [-2.1547e-02, -1.4778e-02, -5.9826e-02],\n",
       "           [-2.1059e-02,  3.9364e-03,  5.2409e-02]],\n",
       " \n",
       "          [[-6.2742e-02,  1.1027e-01,  6.7881e-02],\n",
       "           [ 2.9714e-02,  7.0672e-03, -2.2641e-03],\n",
       "           [-9.6225e-02, -6.6526e-02,  5.1314e-02]],\n",
       " \n",
       "          [[-3.6849e-03, -2.2615e-02, -4.0396e-02],\n",
       "           [-1.1675e-01,  6.0034e-02,  3.3405e-03],\n",
       "           [-3.2858e-02,  8.2775e-02,  6.7217e-02]],\n",
       " \n",
       "          [[-4.8862e-02, -1.1199e-01, -1.0224e-01],\n",
       "           [-8.0395e-02,  7.9611e-02, -8.1639e-02],\n",
       "           [-3.5312e-02, -3.6413e-02, -9.9292e-02]],\n",
       " \n",
       "          [[-1.0013e-01,  3.1484e-02, -6.9833e-02],\n",
       "           [ 7.9256e-02, -7.5665e-02,  5.3070e-02],\n",
       "           [-3.6440e-02,  4.0639e-02,  1.1125e-01]],\n",
       " \n",
       "          [[ 9.7275e-02,  2.1696e-02, -5.1008e-02],\n",
       "           [-3.5896e-02, -6.1242e-02,  8.3026e-02],\n",
       "           [ 2.7951e-02, -1.0980e-01,  1.4407e-03]],\n",
       " \n",
       "          [[-1.7412e-02, -9.1045e-02,  3.3372e-02],\n",
       "           [ 3.3294e-02, -9.2244e-02, -7.1671e-02],\n",
       "           [ 6.2237e-02,  9.3343e-02,  1.0248e-01]]],\n",
       " \n",
       " \n",
       "         [[[-2.4262e-02, -5.5827e-02, -8.6799e-03],\n",
       "           [-2.9752e-02, -8.0327e-03,  1.0054e-02],\n",
       "           [ 6.7571e-02,  9.3637e-02,  6.0055e-02]],\n",
       " \n",
       "          [[-1.0465e-01,  8.9079e-02,  4.4068e-03],\n",
       "           [ 1.1168e-01,  5.3959e-02, -1.0490e-01],\n",
       "           [-1.0673e-01,  1.1333e-01, -5.1636e-02]],\n",
       " \n",
       "          [[ 5.0618e-02,  1.1133e-01, -1.3613e-02],\n",
       "           [-9.9312e-02, -6.7640e-02,  3.7624e-02],\n",
       "           [-9.9570e-03, -1.9544e-02,  9.6853e-02]],\n",
       " \n",
       "          [[ 3.9988e-02,  8.0686e-02,  4.1634e-02],\n",
       "           [-8.8330e-02, -3.7574e-02,  1.0231e-01],\n",
       "           [ 6.1909e-02,  5.0839e-02, -8.1225e-02]],\n",
       " \n",
       "          [[-9.2668e-02, -2.3513e-02,  7.2364e-02],\n",
       "           [-2.2982e-02,  5.4584e-02, -7.4816e-02],\n",
       "           [-2.0781e-02,  8.1380e-02,  6.1109e-02]],\n",
       " \n",
       "          [[-5.5055e-02, -2.1303e-03, -7.0863e-02],\n",
       "           [ 4.4606e-02, -4.3191e-02,  1.1012e-01],\n",
       "           [ 3.2819e-02, -3.5415e-02,  1.0293e-02]],\n",
       " \n",
       "          [[ 6.7616e-02, -1.0545e-01, -3.3338e-02],\n",
       "           [-9.8349e-02,  2.5693e-03, -2.4482e-02],\n",
       "           [ 3.4979e-02,  3.7004e-02, -2.7554e-02]],\n",
       " \n",
       "          [[-7.1072e-02,  1.4042e-02,  4.9901e-02],\n",
       "           [-4.7128e-02,  5.6501e-02, -3.1151e-02],\n",
       "           [ 7.6442e-02,  1.0259e-01, -8.5512e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 4.4465e-02, -7.2087e-02, -2.6747e-02],\n",
       "           [-1.6917e-02, -1.0905e-01, -2.6073e-02],\n",
       "           [ 1.6281e-02,  8.3130e-02, -8.9424e-02]],\n",
       " \n",
       "          [[-8.3924e-02,  8.2892e-02,  1.1293e-01],\n",
       "           [-3.3177e-02, -1.1330e-01,  1.1440e-01],\n",
       "           [ 5.2956e-02,  9.6606e-02,  6.1532e-02]],\n",
       " \n",
       "          [[ 9.0560e-02, -4.4822e-02, -1.0569e-01],\n",
       "           [-7.9810e-02, -1.0703e-01, -8.4526e-02],\n",
       "           [ 1.1298e-03,  1.0660e-01, -1.1671e-01]],\n",
       " \n",
       "          [[-8.3851e-02, -3.7165e-02,  2.1225e-02],\n",
       "           [ 3.1691e-02, -7.9305e-02,  1.0605e-01],\n",
       "           [ 6.2138e-02, -2.6249e-02,  2.1649e-02]],\n",
       " \n",
       "          [[ 7.4596e-03, -1.6809e-02, -1.1597e-01],\n",
       "           [-3.6375e-02, -3.0000e-03,  1.5307e-02],\n",
       "           [ 9.4800e-02, -1.0783e-01,  2.6700e-03]],\n",
       " \n",
       "          [[ 7.0491e-02, -7.4283e-02, -2.1550e-02],\n",
       "           [ 1.0519e-01,  7.4596e-04,  2.9523e-02],\n",
       "           [ 8.6646e-02, -5.9483e-02,  4.1908e-03]],\n",
       " \n",
       "          [[-3.7324e-02, -8.3647e-02, -6.5151e-02],\n",
       "           [ 1.1025e-01,  3.5278e-02, -1.1336e-01],\n",
       "           [ 8.0002e-02, -5.5011e-02,  1.5078e-02]],\n",
       " \n",
       "          [[-9.6799e-02, -1.0770e-01,  9.1954e-02],\n",
       "           [-1.1190e-01,  9.3753e-02,  7.8819e-02],\n",
       "           [ 7.9659e-02, -7.0194e-02, -6.1916e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.7113e-02,  1.1386e-02, -6.5783e-02],\n",
       "           [ 2.9118e-02, -8.9332e-02,  1.5017e-02],\n",
       "           [-4.9888e-02, -9.0033e-02, -1.0234e-01]],\n",
       " \n",
       "          [[ 3.1489e-02,  6.1833e-02, -2.4286e-02],\n",
       "           [ 1.0610e-01,  7.8311e-02, -8.5046e-02],\n",
       "           [-4.2122e-02, -4.5390e-02, -9.6377e-02]],\n",
       " \n",
       "          [[ 9.6539e-02,  9.9606e-02, -1.0202e-01],\n",
       "           [-7.5750e-02, -5.0135e-02,  5.2602e-02],\n",
       "           [-5.5276e-02, -1.0262e-01, -2.2844e-02]],\n",
       " \n",
       "          [[ 6.4503e-02, -1.7541e-03,  7.5780e-02],\n",
       "           [-3.6450e-02, -4.2538e-02, -6.8106e-02],\n",
       "           [-1.0984e-01, -1.1161e-01, -4.8644e-02]],\n",
       " \n",
       "          [[ 6.9338e-02,  6.9271e-02,  8.3634e-02],\n",
       "           [-1.1427e-01, -8.2171e-02,  5.3822e-05],\n",
       "           [-5.4916e-02,  9.2989e-02,  8.9718e-02]],\n",
       " \n",
       "          [[ 6.4322e-02,  1.1107e-03,  3.6293e-02],\n",
       "           [-6.5285e-02, -1.7867e-02, -1.0609e-01],\n",
       "           [-4.2338e-02,  2.1777e-02, -4.8608e-02]],\n",
       " \n",
       "          [[ 8.7332e-02, -6.8450e-02,  9.9521e-02],\n",
       "           [-6.2570e-02,  4.0924e-03, -3.7535e-02],\n",
       "           [-7.3029e-02, -5.4864e-02,  3.4797e-02]],\n",
       " \n",
       "          [[ 5.9457e-03, -1.1542e-01, -8.6007e-02],\n",
       "           [ 5.8610e-02, -9.7701e-02,  6.1189e-02],\n",
       "           [ 4.4234e-02, -1.1357e-01, -5.6371e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.7427e-02,  3.0319e-02,  5.9533e-03],\n",
       "           [ 3.4840e-02, -3.7193e-02, -4.3857e-02],\n",
       "           [-7.0265e-02, -6.5320e-02, -2.5849e-02]],\n",
       " \n",
       "          [[ 5.0931e-03, -1.1098e-01, -7.8434e-02],\n",
       "           [-1.0322e-01, -1.0942e-01, -3.1720e-02],\n",
       "           [-5.0116e-02, -1.1369e-01, -3.3012e-02]],\n",
       " \n",
       "          [[ 9.2474e-02,  3.8578e-02,  8.0028e-02],\n",
       "           [-4.1832e-02, -1.1725e-02,  3.8776e-02],\n",
       "           [ 5.8156e-02,  1.5310e-02, -9.9664e-02]],\n",
       " \n",
       "          [[ 1.0196e-01, -2.2067e-02,  1.0031e-01],\n",
       "           [ 3.5484e-02, -9.3825e-02,  1.3292e-02],\n",
       "           [ 5.0543e-02, -2.6210e-03, -6.8574e-02]],\n",
       " \n",
       "          [[ 7.4940e-02,  8.3696e-02,  2.5095e-02],\n",
       "           [ 6.8669e-02, -3.7566e-02, -2.1460e-02],\n",
       "           [ 3.7734e-02, -5.3456e-02, -8.2738e-02]],\n",
       " \n",
       "          [[ 1.0753e-01,  7.5158e-02,  1.0068e-01],\n",
       "           [ 5.8314e-02,  2.1967e-02,  1.6198e-02],\n",
       "           [-6.7203e-02, -3.0378e-02,  9.4868e-03]],\n",
       " \n",
       "          [[ 2.7230e-02, -4.4423e-02, -5.1098e-02],\n",
       "           [ 5.9860e-02,  6.6437e-02, -8.1790e-02],\n",
       "           [-2.0532e-02, -9.3936e-02, -1.1095e-01]],\n",
       " \n",
       "          [[-1.1821e-02, -9.4017e-02,  4.8936e-02],\n",
       "           [-1.0155e-02,  5.6482e-02,  3.1082e-02],\n",
       "           [ 1.4605e-02, -2.9733e-02, -5.3402e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 4.0995e-02,  9.7524e-02, -9.9673e-02],\n",
       "           [ 6.8130e-02, -8.8644e-02, -9.6812e-02],\n",
       "           [-1.1039e-01, -8.7048e-02,  3.1276e-02]],\n",
       " \n",
       "          [[ 6.4818e-02, -4.2535e-02, -3.8181e-02],\n",
       "           [ 1.1434e-02,  1.9142e-02, -5.8794e-02],\n",
       "           [ 8.5606e-02,  2.1307e-02, -1.3207e-02]],\n",
       " \n",
       "          [[-1.0908e-01, -6.3910e-02, -1.0976e-01],\n",
       "           [ 4.6956e-02,  1.9721e-02,  1.0178e-01],\n",
       "           [-7.9414e-02,  6.6230e-02,  6.9752e-02]],\n",
       " \n",
       "          [[ 3.4678e-02, -8.8657e-03,  1.1414e-01],\n",
       "           [-1.1413e-01, -2.2530e-02, -8.6386e-02],\n",
       "           [ 1.3316e-02,  6.9726e-02, -9.7201e-02]],\n",
       " \n",
       "          [[ 1.0052e-01, -3.2168e-02, -9.8293e-02],\n",
       "           [ 3.4497e-02,  7.3934e-03,  9.4932e-02],\n",
       "           [ 6.2105e-02,  5.2318e-02,  6.7303e-02]],\n",
       " \n",
       "          [[-4.4967e-02,  8.8341e-02,  4.5762e-02],\n",
       "           [-8.0300e-02,  2.7251e-02, -3.5652e-02],\n",
       "           [-7.8153e-02, -1.2157e-02,  6.9326e-02]],\n",
       " \n",
       "          [[ 1.7938e-02,  7.0601e-02,  2.1974e-02],\n",
       "           [ 2.9613e-02,  1.3054e-02, -9.7282e-02],\n",
       "           [-1.1679e-01,  3.6296e-02, -4.7555e-02]],\n",
       " \n",
       "          [[-1.0453e-01, -1.0581e-01, -1.5969e-02],\n",
       "           [ 1.0772e-02,  6.4301e-02,  7.9499e-02],\n",
       "           [ 2.9531e-02, -2.9546e-02, -4.7459e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 4.9752e-02, -7.3081e-03,  3.2373e-02],\n",
       "           [-4.7106e-03, -5.1779e-02, -9.9985e-02],\n",
       "           [-6.6789e-02,  6.4428e-03,  6.3614e-03]],\n",
       " \n",
       "          [[ 9.2401e-02,  4.3053e-02, -3.1678e-02],\n",
       "           [-6.5732e-02,  9.5645e-02, -9.6327e-02],\n",
       "           [ 1.0088e-01, -1.3430e-02,  7.2236e-02]],\n",
       " \n",
       "          [[ 3.1151e-02, -2.0771e-03,  1.0930e-01],\n",
       "           [-4.1599e-02,  1.3183e-02,  1.4337e-03],\n",
       "           [-7.1621e-02, -1.0437e-01,  1.4319e-02]],\n",
       " \n",
       "          [[ 2.0051e-02, -7.0241e-02, -1.0355e-01],\n",
       "           [-1.1678e-01,  8.5280e-02,  1.1335e-01],\n",
       "           [ 2.9223e-02,  3.5493e-02, -3.7593e-02]],\n",
       " \n",
       "          [[ 4.4081e-02,  7.2522e-03, -6.9983e-02],\n",
       "           [-7.0121e-02,  2.3762e-02, -3.8279e-02],\n",
       "           [-8.1340e-02,  2.3944e-02,  5.1958e-02]],\n",
       " \n",
       "          [[ 1.1531e-01, -1.4706e-02,  7.7645e-02],\n",
       "           [ 3.9597e-02, -7.3726e-02,  5.2114e-02],\n",
       "           [-7.1826e-02, -1.0179e-01,  9.6568e-02]],\n",
       " \n",
       "          [[ 1.0253e-01, -1.0252e-01, -8.0612e-02],\n",
       "           [ 5.3928e-02, -5.9175e-03, -1.3993e-02],\n",
       "           [-8.6369e-02,  5.6161e-02,  1.0544e-01]],\n",
       " \n",
       "          [[ 3.9484e-03,  1.1043e-01,  1.0585e-01],\n",
       "           [-1.0136e-01,  5.7687e-02, -1.0653e-01],\n",
       "           [ 5.9775e-02, -7.2165e-02, -9.6223e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.6241e-02, -8.1856e-02, -3.7362e-02],\n",
       "           [ 2.9810e-02,  5.2440e-02, -7.9531e-03],\n",
       "           [ 1.1292e-01,  3.9550e-02, -2.4322e-02]],\n",
       " \n",
       "          [[-1.0676e-01, -9.1573e-02,  4.6585e-02],\n",
       "           [-7.1540e-02, -1.0945e-01, -9.4906e-02],\n",
       "           [ 1.8249e-02, -1.1438e-01, -2.7742e-02]],\n",
       " \n",
       "          [[-6.1363e-02, -5.0361e-02,  1.0191e-01],\n",
       "           [ 5.4346e-02, -3.6740e-02, -9.8894e-03],\n",
       "           [ 2.3241e-03,  4.5398e-02,  3.6972e-02]],\n",
       " \n",
       "          [[-5.9315e-02, -9.3393e-02, -5.3063e-02],\n",
       "           [ 6.9268e-02, -3.2603e-02, -2.7271e-02],\n",
       "           [ 3.5140e-02,  1.0841e-01,  4.3908e-02]],\n",
       " \n",
       "          [[ 3.9270e-02, -4.5543e-02, -9.6131e-02],\n",
       "           [ 1.0409e-01,  1.1317e-02, -1.1154e-01],\n",
       "           [ 7.4669e-02, -9.6542e-02,  8.6878e-02]],\n",
       " \n",
       "          [[ 1.0593e-01, -6.2832e-02, -8.5777e-02],\n",
       "           [ 9.3292e-02,  6.0841e-02,  4.2705e-02],\n",
       "           [-3.1276e-02, -1.0453e-01,  2.5033e-03]],\n",
       " \n",
       "          [[ 7.4915e-02,  7.3952e-02,  3.0586e-02],\n",
       "           [ 3.8279e-03,  5.8632e-02,  2.8540e-02],\n",
       "           [-9.0073e-02, -1.1049e-01,  1.0026e-01]],\n",
       " \n",
       "          [[-5.6125e-02, -4.6258e-02, -9.2218e-02],\n",
       "           [-6.7282e-02,  5.5424e-02,  5.2263e-02],\n",
       "           [ 2.8136e-02, -2.4876e-04,  6.3413e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-5.9647e-07,  2.4330e-06, -4.5162e-07,  4.8748e-07, -3.9361e-07,\n",
       "          5.2772e-07, -6.9997e-07,  1.2391e-06], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 1.0596e-01, -1.0975e-01, -6.7626e-02],\n",
       "           [ 6.5663e-02,  1.0368e-02, -6.6725e-02],\n",
       "           [ 1.1211e-01,  4.7146e-02, -7.0935e-02]],\n",
       " \n",
       "          [[-2.0753e-02, -5.1216e-02,  1.0655e-02],\n",
       "           [ 4.5260e-02, -1.0892e-01,  6.8115e-03],\n",
       "           [-1.5083e-02, -4.6793e-02,  8.8906e-02]],\n",
       " \n",
       "          [[ 3.4628e-02, -4.8004e-03,  3.4816e-02],\n",
       "           [-7.2462e-02, -4.1115e-02,  4.5502e-02],\n",
       "           [ 4.3811e-02,  3.8335e-02, -2.0451e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0073e-02, -1.1173e-01,  1.0189e-01],\n",
       "           [ 5.4502e-02, -7.7062e-02,  8.0561e-02],\n",
       "           [ 1.1590e-01,  4.9802e-02,  8.8252e-03]],\n",
       " \n",
       "          [[-7.1533e-02,  1.0322e-01, -9.2861e-02],\n",
       "           [ 2.9812e-02,  8.4678e-02,  4.6187e-02],\n",
       "           [ 1.0990e-01,  8.5500e-03, -3.9742e-02]],\n",
       " \n",
       "          [[-5.9567e-02,  8.6537e-02,  5.0265e-02],\n",
       "           [ 6.8574e-02, -6.5873e-03,  7.8880e-03],\n",
       "           [ 1.0865e-01,  5.2039e-03,  5.0599e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 1.1654e-01, -9.3164e-02, -8.8377e-04],\n",
       "           [ 8.0141e-02, -2.6843e-02,  8.5009e-02],\n",
       "           [ 6.6311e-02,  1.1513e-01, -8.2012e-02]],\n",
       " \n",
       "          [[-1.0365e-01, -2.8935e-02,  3.1974e-02],\n",
       "           [-5.9020e-02,  5.7771e-02,  4.8060e-02],\n",
       "           [-8.6415e-02,  8.2729e-02, -9.9609e-02]],\n",
       " \n",
       "          [[-2.6040e-02,  5.1873e-02,  5.8832e-02],\n",
       "           [-8.6570e-02,  2.4066e-02, -9.5450e-02],\n",
       "           [ 3.9675e-02,  8.3497e-02, -3.1599e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.3471e-02,  8.9466e-02,  5.9875e-02],\n",
       "           [-2.5478e-02, -2.9288e-02, -2.2079e-02],\n",
       "           [ 9.9045e-02,  5.0363e-02, -3.2930e-02]],\n",
       " \n",
       "          [[ 1.3561e-02,  1.1109e-01,  5.0832e-02],\n",
       "           [ 9.8663e-02, -1.1557e-01, -1.4724e-02],\n",
       "           [-5.0860e-02,  8.1908e-02,  5.2527e-02]],\n",
       " \n",
       "          [[-4.4433e-03,  5.6079e-04, -1.0445e-01],\n",
       "           [-2.4524e-03, -1.1076e-01, -3.5353e-02],\n",
       "           [-1.5659e-02,  1.0133e-01,  7.1413e-02]]],\n",
       " \n",
       " \n",
       "         [[[-7.9916e-02,  9.4997e-02,  6.3330e-02],\n",
       "           [ 2.5214e-02,  5.8911e-03, -1.0595e-01],\n",
       "           [-1.0115e-02,  5.7860e-02, -4.4751e-02]],\n",
       " \n",
       "          [[ 5.0578e-02,  3.6965e-02,  4.9432e-02],\n",
       "           [ 9.6686e-02,  8.9037e-02, -1.1252e-01],\n",
       "           [ 4.0072e-02, -4.7423e-02, -9.3374e-02]],\n",
       " \n",
       "          [[-3.7092e-02,  9.8984e-02, -8.5466e-02],\n",
       "           [-1.9025e-02, -7.6891e-02,  7.5766e-02],\n",
       "           [ 3.7416e-02, -7.3661e-02, -5.4462e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.5936e-02,  1.2329e-02,  6.4016e-02],\n",
       "           [ 9.7418e-02,  7.3226e-02,  5.6733e-02],\n",
       "           [-1.1685e-02, -7.3565e-02, -1.7415e-02]],\n",
       " \n",
       "          [[ 4.5692e-02, -4.8535e-02, -7.3836e-02],\n",
       "           [-9.9504e-02, -5.4173e-02, -6.6413e-02],\n",
       "           [ 1.0260e-01,  9.6400e-02, -4.4833e-02]],\n",
       " \n",
       "          [[-8.3892e-02, -1.0901e-02, -5.9537e-03],\n",
       "           [ 5.5924e-02, -4.3667e-02, -1.1505e-01],\n",
       "           [ 7.0446e-02,  5.3306e-02, -8.2058e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-1.5829e-02, -1.1814e-02,  3.4990e-02],\n",
       "           [-8.6993e-02,  5.9150e-02, -5.6401e-02],\n",
       "           [-9.7279e-02, -6.9699e-03, -7.5337e-02]],\n",
       " \n",
       "          [[ 2.9813e-02,  1.1549e-01, -4.6069e-02],\n",
       "           [-4.3739e-02,  1.7784e-02, -7.2684e-02],\n",
       "           [ 2.1939e-02,  1.0849e-01,  4.3122e-02]],\n",
       " \n",
       "          [[ 1.5324e-02, -1.0589e-01,  6.3949e-03],\n",
       "           [-5.8176e-02, -5.9067e-02,  5.1694e-02],\n",
       "           [ 8.1678e-02, -1.3615e-02,  1.1435e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 8.8207e-02,  1.4562e-02,  5.7979e-02],\n",
       "           [ 2.1414e-02,  7.6598e-02,  7.1055e-02],\n",
       "           [-3.3177e-02,  1.0537e-01,  3.9153e-02]],\n",
       " \n",
       "          [[ 7.6854e-02, -1.1022e-01, -3.8383e-02],\n",
       "           [-1.2481e-02,  6.9782e-03,  9.3605e-02],\n",
       "           [ 7.5039e-02, -4.2673e-02,  1.0677e-01]],\n",
       " \n",
       "          [[-2.0955e-02, -1.1383e-01, -7.4705e-03],\n",
       "           [ 1.3901e-02, -3.5887e-02,  6.3933e-02],\n",
       "           [-1.0142e-01,  1.1030e-01, -1.1376e-01]]],\n",
       " \n",
       " \n",
       "         [[[-8.1285e-02,  7.0261e-02, -3.6781e-02],\n",
       "           [-2.5183e-02, -6.9895e-02,  1.5650e-02],\n",
       "           [-9.9628e-02, -1.6389e-03, -6.6890e-02]],\n",
       " \n",
       "          [[ 5.6473e-02, -9.3429e-02,  7.4969e-02],\n",
       "           [ 7.9101e-03, -5.1197e-02,  7.1813e-05],\n",
       "           [-8.0214e-02,  9.1234e-02,  1.0307e-01]],\n",
       " \n",
       "          [[-2.9440e-02,  1.9557e-02, -3.8387e-02],\n",
       "           [-3.1686e-02,  5.2514e-02, -7.8262e-03],\n",
       "           [-9.1575e-02, -1.1179e-01, -6.6905e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 7.8368e-02, -4.7290e-02, -7.3153e-02],\n",
       "           [ 1.0338e-01,  1.2947e-02, -6.1828e-02],\n",
       "           [ 4.2105e-02,  5.9849e-02, -8.5272e-02]],\n",
       " \n",
       "          [[-3.9795e-03,  1.0408e-01,  4.1150e-02],\n",
       "           [-2.5273e-03,  4.0724e-02,  9.0541e-02],\n",
       "           [ 9.2784e-04,  8.1323e-02,  6.5463e-02]],\n",
       " \n",
       "          [[-5.8732e-02, -5.1107e-02, -3.1513e-02],\n",
       "           [ 7.7662e-02, -6.3788e-02,  3.4241e-02],\n",
       "           [-9.3838e-02,  1.8005e-02, -5.3472e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5299e-02,  8.2401e-03,  2.1083e-02],\n",
       "           [-3.8618e-02,  9.7016e-02, -3.8389e-02],\n",
       "           [-2.4069e-02, -6.5083e-03, -4.2386e-02]],\n",
       " \n",
       "          [[ 6.2663e-02, -9.0784e-02, -8.3011e-02],\n",
       "           [-1.3387e-02, -1.1652e-01,  4.7518e-02],\n",
       "           [-2.6113e-02, -6.9434e-02, -1.1569e-01]],\n",
       " \n",
       "          [[ 1.3033e-02, -6.6901e-02, -1.7800e-02],\n",
       "           [ 1.8789e-03, -1.8216e-02,  8.8044e-02],\n",
       "           [ 3.7556e-02,  4.6266e-02, -7.4150e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.2628e-02,  1.0918e-01,  1.0979e-01],\n",
       "           [ 1.0463e-01, -1.8819e-02, -7.4758e-02],\n",
       "           [-7.6780e-02,  2.5457e-02, -1.1930e-03]],\n",
       " \n",
       "          [[ 5.7548e-02,  4.0585e-02,  1.3014e-02],\n",
       "           [-4.6813e-02,  5.5623e-02, -4.4226e-02],\n",
       "           [ 8.2877e-02,  5.7495e-02, -2.7419e-02]],\n",
       " \n",
       "          [[-1.1593e-01, -3.2670e-03,  4.3557e-02],\n",
       "           [ 6.5554e-02,  4.5566e-02, -8.2169e-02],\n",
       "           [ 7.5876e-03, -2.6699e-02, -3.4675e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 5.8686e-07,  1.2452e-06, -8.1077e-07,  1.0770e-06,  1.7409e-06,\n",
       "          1.5199e-06,  8.0789e-07, -3.6236e-07, -2.8402e-07,  9.8090e-07,\n",
       "          3.4722e-08, -1.0380e-06,  1.8672e-06, -7.8568e-07,  3.5101e-07,\n",
       "         -8.4805e-07], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 6.1545e-02, -2.5755e-02, -3.0255e-02],\n",
       "           [ 6.5135e-02, -1.4185e-03, -6.3586e-02],\n",
       "           [-1.9883e-04, -8.2284e-02,  1.4199e-02]],\n",
       " \n",
       "          [[-2.1829e-02,  1.5695e-02,  3.0291e-02],\n",
       "           [ 5.3487e-02,  8.6514e-03, -2.5305e-05],\n",
       "           [-4.2518e-02,  7.6312e-02,  5.7079e-02]],\n",
       " \n",
       "          [[ 5.6845e-02,  6.3257e-03, -5.0063e-03],\n",
       "           [ 2.3441e-03, -5.4608e-02,  2.5415e-02],\n",
       "           [ 2.5078e-02,  6.4004e-02,  4.6130e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.9855e-02, -3.7643e-02,  5.1973e-03],\n",
       "           [-3.7177e-02,  8.1941e-02, -7.0355e-02],\n",
       "           [-3.4938e-02,  7.2912e-02,  5.9808e-02]],\n",
       " \n",
       "          [[ 3.9299e-02,  7.3678e-02, -6.9843e-02],\n",
       "           [ 6.7369e-04,  6.8069e-02, -2.6743e-02],\n",
       "           [-4.7060e-02, -2.7564e-02,  5.1519e-02]],\n",
       " \n",
       "          [[ 1.5899e-02,  4.9935e-02, -7.4445e-03],\n",
       "           [ 6.9981e-02,  3.4213e-02, -3.1852e-02],\n",
       "           [ 6.1247e-02,  3.0474e-02, -4.2813e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.0590e-02, -4.6881e-02, -4.4005e-02],\n",
       "           [-3.5301e-02, -5.8977e-02,  6.3820e-02],\n",
       "           [ 6.4758e-02,  7.2446e-02,  7.6219e-02]],\n",
       " \n",
       "          [[ 7.4843e-02, -4.6326e-02, -5.1652e-02],\n",
       "           [-6.8931e-02, -9.6670e-03,  7.2617e-02],\n",
       "           [-4.8979e-02, -2.9423e-02, -2.0687e-02]],\n",
       " \n",
       "          [[ 8.2399e-02,  3.5895e-02, -4.0375e-03],\n",
       "           [-1.4516e-02,  3.1052e-02,  5.6045e-02],\n",
       "           [ 5.6737e-02, -1.1004e-02,  3.1714e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.5732e-02, -2.2628e-03,  9.5411e-03],\n",
       "           [ 4.5326e-02, -7.5841e-02, -2.8063e-02],\n",
       "           [ 1.0002e-03, -7.7265e-02, -5.0481e-02]],\n",
       " \n",
       "          [[ 7.1998e-02,  5.3682e-02, -3.1301e-02],\n",
       "           [-6.8043e-02, -4.2934e-02,  3.6241e-02],\n",
       "           [-1.7740e-02,  3.9852e-02, -2.2435e-02]],\n",
       " \n",
       "          [[ 7.7409e-02, -6.6034e-02, -7.9840e-02],\n",
       "           [-5.2669e-02, -9.8033e-04, -7.7379e-02],\n",
       "           [ 1.3191e-02, -6.8430e-02,  5.3819e-02]]],\n",
       " \n",
       " \n",
       "         [[[-5.8276e-02,  1.8108e-02,  4.4398e-02],\n",
       "           [ 1.9902e-02,  6.3793e-02,  1.4575e-02],\n",
       "           [-4.1643e-02,  4.2388e-02,  8.5860e-03]],\n",
       " \n",
       "          [[-3.0130e-02, -5.6281e-02,  3.8607e-02],\n",
       "           [-1.8111e-02,  2.6771e-02, -3.2807e-02],\n",
       "           [ 5.4618e-02,  2.6179e-02,  2.7122e-02]],\n",
       " \n",
       "          [[-2.0599e-02, -5.9663e-02,  6.4338e-02],\n",
       "           [-7.2501e-02,  2.6952e-02, -5.3535e-02],\n",
       "           [-4.0827e-02, -1.8861e-02, -7.1223e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.0185e-02, -6.2509e-03, -4.4695e-02],\n",
       "           [-5.6713e-02, -4.0402e-02, -3.4646e-02],\n",
       "           [-3.5109e-02, -1.1761e-02,  3.1162e-02]],\n",
       " \n",
       "          [[-3.9473e-02, -1.1967e-02,  9.5971e-03],\n",
       "           [-6.6047e-02,  8.0939e-02,  4.6709e-02],\n",
       "           [-1.6333e-02, -6.4883e-02, -5.4153e-02]],\n",
       " \n",
       "          [[ 3.7342e-03, -2.7650e-02, -2.4019e-03],\n",
       "           [ 1.4065e-02, -6.8253e-02,  1.6427e-02],\n",
       "           [-3.9895e-02, -8.1606e-04,  4.1382e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 2.3269e-02, -9.7696e-03, -3.0570e-02],\n",
       "           [-7.2370e-03, -5.8600e-02, -3.0513e-02],\n",
       "           [ 2.7931e-02, -2.9153e-02,  7.9066e-02]],\n",
       " \n",
       "          [[-7.7981e-02, -6.9618e-02, -6.0775e-02],\n",
       "           [-6.8895e-02, -4.9319e-03, -1.4858e-02],\n",
       "           [-5.1013e-02, -2.0263e-02, -2.6223e-02]],\n",
       " \n",
       "          [[ 3.9540e-04,  6.2399e-02,  5.6095e-02],\n",
       "           [-2.3143e-02,  2.8216e-02,  4.6482e-02],\n",
       "           [ 2.0760e-02, -4.8600e-03, -6.8615e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.3467e-02, -4.9715e-02,  4.6443e-02],\n",
       "           [-6.6425e-02,  2.6898e-02,  7.2396e-02],\n",
       "           [-1.6380e-02,  4.8104e-02,  4.5025e-02]],\n",
       " \n",
       "          [[ 7.0954e-02,  2.0291e-03,  7.1266e-02],\n",
       "           [ 1.1057e-04,  1.9433e-02,  7.8180e-03],\n",
       "           [ 3.0439e-02, -4.9531e-02, -3.0125e-02]],\n",
       " \n",
       "          [[-2.7447e-02, -7.2128e-02,  3.3803e-02],\n",
       "           [ 3.8007e-02, -7.1991e-02,  5.2392e-02],\n",
       "           [ 4.1566e-02, -4.0306e-02,  1.6414e-02]]],\n",
       " \n",
       " \n",
       "         [[[-8.3178e-03, -1.6543e-03, -6.1757e-02],\n",
       "           [-7.2586e-02,  3.4263e-02, -7.9382e-02],\n",
       "           [ 6.7826e-02,  6.4592e-02, -2.7865e-02]],\n",
       " \n",
       "          [[-2.4082e-02,  5.7231e-02,  5.5941e-02],\n",
       "           [-6.9826e-02,  1.2477e-02, -1.7072e-02],\n",
       "           [ 9.7098e-03,  1.1594e-02,  4.6919e-02]],\n",
       " \n",
       "          [[ 2.9194e-02,  3.9365e-02, -1.3362e-02],\n",
       "           [ 2.4809e-02,  4.4264e-02, -1.1918e-02],\n",
       "           [ 6.3834e-02, -6.8513e-02,  3.1943e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.5733e-02,  6.8796e-02, -7.5403e-02],\n",
       "           [-2.5067e-02, -7.7378e-02,  1.0725e-02],\n",
       "           [ 3.0559e-03, -8.0738e-02,  1.8577e-03]],\n",
       " \n",
       "          [[-1.3031e-02, -2.3833e-03,  3.3129e-02],\n",
       "           [-5.1761e-02, -3.8394e-02,  1.7094e-02],\n",
       "           [-4.7187e-02,  1.5464e-04, -1.5795e-04]],\n",
       " \n",
       "          [[ 7.0528e-02, -1.0816e-02,  3.4486e-02],\n",
       "           [ 4.1999e-02,  7.4477e-02, -3.8414e-02],\n",
       "           [-7.4973e-03,  5.8760e-02,  3.1816e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.8688e-02,  3.5065e-02,  5.1718e-02],\n",
       "           [-6.8801e-02, -1.2409e-02,  6.7678e-02],\n",
       "           [-3.3908e-02,  3.9285e-02,  4.6076e-02]],\n",
       " \n",
       "          [[-5.8353e-02,  1.7291e-02,  7.9536e-02],\n",
       "           [-7.8129e-02, -4.8334e-02,  7.4241e-02],\n",
       "           [-8.0521e-02,  9.7051e-04,  8.1252e-02]],\n",
       " \n",
       "          [[-3.7044e-02, -7.7808e-03,  2.8930e-02],\n",
       "           [ 1.0509e-02,  5.9272e-03, -3.0357e-02],\n",
       "           [ 1.6576e-02, -6.2739e-02,  7.8021e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 8.1416e-02,  3.7485e-02,  3.5392e-03],\n",
       "           [-4.5124e-02, -3.6792e-02,  6.8156e-02],\n",
       "           [ 5.2442e-02,  1.3064e-02,  1.6670e-02]],\n",
       " \n",
       "          [[-7.3824e-02,  4.8923e-02,  2.1964e-02],\n",
       "           [-4.9955e-02,  2.4044e-02,  7.8671e-02],\n",
       "           [-2.0406e-02, -1.0696e-02,  2.9814e-02]],\n",
       " \n",
       "          [[ 2.5656e-02, -7.8388e-02, -4.4324e-02],\n",
       "           [-2.6371e-02, -2.0741e-02, -5.5637e-03],\n",
       "           [ 6.5592e-02,  1.3189e-03, -9.4741e-03]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-7.6767e-06, -1.6385e-05,  1.3541e-05,  2.1361e-05, -1.7681e-05,\n",
       "         -4.8198e-05,  5.1499e-05, -9.1552e-06, -3.4718e-06,  3.2657e-05,\n",
       "          3.8678e-06, -2.2812e-06,  1.6884e-05,  1.9567e-05, -1.2952e-05,\n",
       "         -1.0281e-05, -1.2125e-05, -2.4054e-05, -1.4744e-05,  3.5042e-05,\n",
       "          4.3283e-05, -1.7663e-05,  4.5216e-05,  8.0786e-06, -7.2738e-06,\n",
       "          2.6967e-06,  1.0066e-05,  1.6601e-05,  2.0003e-05, -1.1828e-06,\n",
       "          6.3208e-06, -1.3973e-05], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[[[ 3.1744e-02,  3.1336e-02, -5.5938e-02],\n",
       "           [-5.1533e-03, -1.8420e-02, -3.6304e-02],\n",
       "           [ 5.2676e-02,  9.4321e-03, -1.0797e-02]],\n",
       " \n",
       "          [[-2.1922e-02, -4.3320e-03,  3.3511e-02],\n",
       "           [ 4.9289e-02, -4.6558e-02,  5.5960e-02],\n",
       "           [-4.8238e-02,  1.5065e-02,  2.8974e-03]],\n",
       " \n",
       "          [[ 4.0667e-02, -5.7487e-02, -4.7816e-02],\n",
       "           [-4.8536e-03,  2.2438e-03,  1.6419e-02],\n",
       "           [-5.6072e-02,  5.0685e-02, -3.9631e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-5.1067e-02, -2.3050e-02,  4.3441e-02],\n",
       "           [-2.1422e-02, -4.6006e-02,  7.6456e-03],\n",
       "           [ 4.8565e-02,  3.9755e-02,  4.1196e-02]],\n",
       " \n",
       "          [[-3.7476e-02,  2.9933e-02,  3.2557e-03],\n",
       "           [-1.8384e-02, -4.1543e-02,  1.1644e-03],\n",
       "           [ 4.3525e-02,  8.1638e-03, -4.5551e-02]],\n",
       " \n",
       "          [[-5.4229e-02, -5.5574e-02,  2.9455e-02],\n",
       "           [-1.6550e-02,  1.6571e-02, -5.2363e-02],\n",
       "           [ 5.5803e-02,  4.1561e-02,  4.5703e-02]]],\n",
       " \n",
       " \n",
       "         [[[-2.0155e-02,  5.0757e-02, -9.7323e-03],\n",
       "           [-5.5364e-02,  5.4745e-02, -3.3077e-02],\n",
       "           [ 3.4361e-02, -1.2416e-02,  2.0255e-02]],\n",
       " \n",
       "          [[ 1.5667e-02, -5.1455e-02,  4.2655e-02],\n",
       "           [-1.2932e-02, -6.0655e-05, -1.8967e-02],\n",
       "           [ 2.7875e-02, -4.2182e-02,  4.7373e-02]],\n",
       " \n",
       "          [[-5.2472e-02,  3.2710e-02, -2.9960e-02],\n",
       "           [ 1.3809e-02, -3.9521e-02,  3.5218e-02],\n",
       "           [ 5.5890e-02,  4.7717e-02,  1.5513e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.8193e-03, -3.6968e-02,  5.6189e-02],\n",
       "           [ 1.4854e-02,  3.5683e-02, -2.9635e-03],\n",
       "           [-5.6170e-02,  2.9276e-02,  1.9187e-02]],\n",
       " \n",
       "          [[ 2.9704e-05,  3.5180e-03, -1.9931e-02],\n",
       "           [-2.4019e-02, -5.3890e-02, -4.6929e-02],\n",
       "           [ 2.9329e-02,  2.4773e-02,  2.3904e-02]],\n",
       " \n",
       "          [[-1.4687e-02, -4.8514e-02,  4.7355e-02],\n",
       "           [ 6.7067e-03, -5.7339e-02, -4.9660e-02],\n",
       "           [-4.5257e-02,  1.8382e-02,  4.1813e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.2118e-02,  1.1108e-02, -3.9488e-02],\n",
       "           [ 1.2113e-02, -4.6451e-02,  2.9029e-02],\n",
       "           [-4.4708e-02,  2.8630e-02, -2.4268e-02]],\n",
       " \n",
       "          [[ 3.0027e-02, -2.4410e-03, -5.6745e-02],\n",
       "           [ 5.3758e-02,  5.0228e-02, -6.6729e-03],\n",
       "           [-4.1804e-02,  2.7879e-02, -3.6789e-02]],\n",
       " \n",
       "          [[ 3.1481e-02, -2.5541e-02, -4.5401e-02],\n",
       "           [-1.1106e-03,  3.5548e-02, -1.4066e-02],\n",
       "           [-8.6258e-03,  3.3563e-02,  4.5753e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.2044e-03,  3.8417e-02,  1.3796e-02],\n",
       "           [ 5.0629e-02,  3.9409e-02,  7.1723e-03],\n",
       "           [-4.5453e-02,  1.4417e-02,  4.3204e-02]],\n",
       " \n",
       "          [[-4.7168e-02,  5.3939e-02, -3.1210e-02],\n",
       "           [-8.3225e-03,  2.9979e-02,  2.7252e-02],\n",
       "           [ 3.8383e-03, -5.4794e-02, -3.8269e-02]],\n",
       " \n",
       "          [[ 4.9931e-02, -1.2501e-02, -5.5178e-02],\n",
       "           [ 5.5145e-02, -4.7784e-02, -3.6517e-02],\n",
       "           [ 1.2032e-02, -1.3279e-02,  4.4421e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.1517e-02,  3.0642e-02,  2.3573e-02],\n",
       "           [ 1.7418e-02,  4.6410e-02,  5.3175e-02],\n",
       "           [ 1.7964e-02, -3.9221e-03,  4.6974e-02]],\n",
       " \n",
       "          [[-7.4388e-03, -3.7422e-03, -3.3172e-02],\n",
       "           [-2.9688e-02, -3.0788e-02,  3.1492e-02],\n",
       "           [ 2.4739e-02, -2.6563e-02,  5.6180e-02]],\n",
       " \n",
       "          [[ 8.5692e-03, -1.0164e-02, -1.2341e-02],\n",
       "           [-3.4171e-02, -2.1653e-03,  4.3661e-04],\n",
       "           [ 2.1370e-03,  4.1807e-02,  3.5781e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-3.7253e-02,  2.5401e-02,  2.4998e-02],\n",
       "           [-1.7863e-02, -5.6149e-02, -2.1577e-02],\n",
       "           [-1.8727e-02, -6.2238e-03,  5.4202e-02]],\n",
       " \n",
       "          [[ 2.5563e-02, -3.9188e-02,  3.4852e-02],\n",
       "           [-3.5927e-02, -4.3376e-02,  4.8089e-02],\n",
       "           [ 1.3890e-02,  8.3516e-04,  3.2387e-02]],\n",
       " \n",
       "          [[-1.4132e-02,  5.1420e-02,  5.5781e-02],\n",
       "           [ 4.2211e-02, -1.0197e-02, -1.4300e-02],\n",
       "           [ 5.4201e-02, -8.2894e-03,  5.0925e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.5463e-02,  4.9407e-02,  3.4347e-02],\n",
       "           [ 2.4408e-02, -4.3995e-02, -4.3833e-02],\n",
       "           [ 3.5274e-02,  4.5617e-02,  4.3064e-02]],\n",
       " \n",
       "          [[-1.3730e-03,  5.7486e-02, -8.3643e-04],\n",
       "           [ 5.7782e-02, -1.3993e-02, -4.0537e-03],\n",
       "           [ 7.2438e-03,  2.6414e-02,  3.7233e-02]],\n",
       " \n",
       "          [[ 1.6865e-02,  3.2087e-02, -2.2260e-02],\n",
       "           [ 4.0377e-02,  2.4609e-02,  2.5129e-02],\n",
       "           [-1.1770e-03,  2.6462e-02, -3.9013e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.0476e-03, -2.6004e-02,  3.7730e-02],\n",
       "           [ 3.0523e-02, -3.8981e-02, -1.2943e-02],\n",
       "           [-4.2061e-02, -2.1544e-02,  1.9544e-02]],\n",
       " \n",
       "          [[ 3.0390e-02, -3.5637e-02,  2.0345e-02],\n",
       "           [ 4.6735e-02, -2.7419e-02, -4.3140e-03],\n",
       "           [ 2.1792e-02, -4.4930e-02,  3.3591e-02]],\n",
       " \n",
       "          [[ 3.3708e-02,  5.7520e-02, -5.3480e-02],\n",
       "           [ 7.1827e-05,  7.4651e-03, -3.3444e-02],\n",
       "           [-3.9308e-02,  4.8881e-02, -4.0356e-02]]],\n",
       " \n",
       " \n",
       "         [[[-3.5541e-02, -3.8651e-02,  8.3843e-04],\n",
       "           [ 4.6055e-02, -4.5120e-02, -3.1067e-02],\n",
       "           [-3.8748e-02,  5.2796e-02, -9.6967e-03]],\n",
       " \n",
       "          [[-2.6249e-02, -5.5250e-02, -2.1020e-02],\n",
       "           [ 2.4380e-02,  1.2085e-02, -4.7428e-02],\n",
       "           [-4.6271e-02, -1.4017e-02, -3.4384e-03]],\n",
       " \n",
       "          [[ 2.4946e-02, -2.0147e-02, -4.1027e-02],\n",
       "           [ 4.4302e-03,  5.1828e-02,  2.0739e-02],\n",
       "           [-4.0186e-02, -5.1957e-02, -1.5967e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 7.0424e-03,  8.6631e-03, -3.5196e-02],\n",
       "           [-3.7285e-02,  2.3237e-02,  1.6657e-02],\n",
       "           [-4.7231e-02, -1.4883e-02, -5.1617e-02]],\n",
       " \n",
       "          [[-1.5154e-02,  4.9572e-02,  4.3428e-02],\n",
       "           [ 7.6614e-04,  9.1983e-03,  1.9455e-02],\n",
       "           [-1.5877e-02, -4.1168e-02, -4.2357e-02]],\n",
       " \n",
       "          [[ 2.8154e-02,  1.6265e-02,  3.0679e-02],\n",
       "           [ 5.9688e-03,  3.0965e-02, -1.9785e-02],\n",
       "           [-2.8556e-02,  3.4626e-02,  3.7962e-02]]]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991,\n",
       "         0.9991, 0.9991, 0.9991, 0.9991, 0.9991], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0009,  0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009,\n",
       "         -0.0009,  0.0009, -0.0009, -0.0009,  0.0009, -0.0009, -0.0009,  0.0009,\n",
       "         -0.0009,  0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009,  0.0009,\n",
       "          0.0009, -0.0009,  0.0009,  0.0009,  0.0009, -0.0009, -0.0009, -0.0009],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-3.0711e-02, -1.3899e-01,  1.6985e-01,  2.2527e-03,  1.3149e-01,\n",
       "           1.6615e-01,  5.8196e-02, -1.0391e-01,  1.6657e-02, -1.0205e-01,\n",
       "           1.6057e-01,  1.2919e-02, -8.9279e-02,  8.0864e-02, -2.2494e-02,\n",
       "           1.5895e-01,  4.2922e-02, -1.3999e-01,  1.1208e-01, -1.0373e-01,\n",
       "          -3.4353e-02, -4.5973e-02,  1.5541e-02, -1.1932e-01, -1.6035e-01,\n",
       "           1.3740e-01, -3.2832e-02, -1.6209e-01, -1.3869e-01,  1.6593e-01,\n",
       "           1.1447e-02,  7.9548e-02],\n",
       "         [-1.1330e-01, -1.6943e-01,  1.6627e-01, -1.6539e-02, -8.5556e-02,\n",
       "           9.3202e-02,  1.0194e-01, -3.4526e-02,  1.2686e-01, -4.4639e-02,\n",
       "          -9.1295e-02,  1.6389e-01,  8.0881e-02,  1.3088e-01,  1.5748e-01,\n",
       "          -2.6114e-02, -1.5626e-01,  3.8963e-03,  6.1163e-03,  1.6429e-01,\n",
       "          -9.4605e-02,  1.2916e-01,  5.7506e-02, -1.4262e-01,  4.0048e-02,\n",
       "           1.7580e-01, -1.3731e-01,  1.5238e-01, -1.8653e-02, -1.0140e-01,\n",
       "           1.6931e-01, -9.6589e-02],\n",
       "         [ 1.7260e-01, -1.1723e-01,  1.5279e-01,  2.8800e-02,  2.1542e-03,\n",
       "           1.6420e-01,  1.1357e-01,  1.6957e-01,  1.2864e-01, -1.0627e-01,\n",
       "          -1.6331e-01, -7.1200e-03, -1.5838e-02,  1.6691e-01,  4.9412e-04,\n",
       "          -1.1760e-01,  1.2660e-01,  9.7868e-02, -5.9527e-02, -1.6849e-01,\n",
       "           8.6861e-02,  4.0142e-02,  7.3615e-03,  6.9747e-02,  1.6371e-01,\n",
       "          -1.0563e-01, -2.1516e-02, -1.4729e-01, -8.9833e-02,  6.4067e-02,\n",
       "          -6.6878e-02, -1.2586e-01],\n",
       "         [ 1.6851e-01, -8.3853e-02, -1.7394e-01,  1.4606e-01,  1.7006e-01,\n",
       "          -4.4924e-03,  1.5840e-01,  9.4216e-02,  7.9215e-02,  6.3516e-02,\n",
       "           7.4758e-02,  1.0716e-01, -5.7206e-02,  5.1078e-02,  3.5527e-02,\n",
       "          -8.9927e-03,  8.9860e-02, -7.9521e-03, -5.1184e-02,  1.1163e-01,\n",
       "          -3.5511e-02,  6.2530e-02, -6.6099e-02, -1.7789e-02, -1.7571e-01,\n",
       "           1.9416e-03, -1.2640e-01,  1.4322e-01, -1.4577e-01, -1.3440e-01,\n",
       "           1.3281e-01,  3.1660e-02],\n",
       "         [-1.5007e-01,  8.8802e-02,  1.7206e-01,  3.6690e-02, -1.3441e-01,\n",
       "          -9.7930e-02,  1.5504e-01, -4.6453e-02, -2.6889e-02,  1.4307e-01,\n",
       "          -1.1474e-01,  1.0538e-02,  1.4052e-01,  7.7940e-02,  9.6892e-02,\n",
       "           7.0762e-03,  1.7165e-01,  1.5900e-01, -7.6010e-02,  4.3836e-03,\n",
       "           9.8786e-02, -6.4007e-02,  5.4855e-02, -3.4510e-02, -1.4143e-01,\n",
       "          -9.4281e-02, -7.2634e-02,  1.3923e-02,  1.7501e-02, -6.6764e-03,\n",
       "           1.0911e-01,  9.4701e-02],\n",
       "         [-7.8749e-02, -1.0077e-01,  1.1290e-01,  1.6936e-01,  1.5559e-01,\n",
       "          -1.4035e-01, -8.8472e-03,  1.7413e-01, -6.3398e-02,  8.6687e-03,\n",
       "           9.5781e-02, -1.2141e-01, -4.9166e-02, -7.5782e-03, -1.4587e-01,\n",
       "           6.6293e-02,  6.3786e-02, -1.1102e-01,  1.7072e-01,  1.6437e-01,\n",
       "          -1.0556e-01,  1.3061e-01, -1.7802e-02,  1.4175e-01,  2.2295e-02,\n",
       "          -1.4278e-01, -1.3471e-01, -1.2136e-01,  1.7312e-01,  1.3875e-01,\n",
       "          -4.1411e-02, -1.4304e-01],\n",
       "         [ 1.5041e-01,  5.1614e-02, -1.1988e-01, -5.5419e-02,  8.2644e-02,\n",
       "           7.7389e-02, -1.0022e-01,  6.4703e-03,  2.3378e-02,  2.4285e-02,\n",
       "          -6.7714e-03,  6.1844e-02,  7.2616e-02,  1.0361e-01,  2.7448e-02,\n",
       "           1.5971e-01,  8.3857e-02,  1.3887e-01, -2.1324e-02,  1.1010e-01,\n",
       "          -7.3420e-02,  2.5021e-02,  5.4832e-02, -4.1287e-02,  9.4068e-02,\n",
       "          -1.3449e-01, -1.1493e-01,  5.3144e-02, -5.9686e-02, -1.3906e-02,\n",
       "          -1.4190e-02,  1.0950e-01],\n",
       "         [ 1.4487e-01, -6.1498e-02,  2.3649e-03,  3.3710e-02,  2.3101e-02,\n",
       "           1.0858e-01,  1.4257e-04,  3.2968e-02,  1.1589e-01,  9.5508e-02,\n",
       "           1.6099e-01,  3.9948e-02, -1.6291e-01,  7.6339e-02,  9.8943e-02,\n",
       "          -5.6420e-02, -1.2999e-01, -1.4362e-01, -1.5908e-01,  1.0650e-02,\n",
       "           8.5370e-02, -3.1825e-02, -1.3035e-01,  4.7297e-04, -1.0591e-01,\n",
       "           4.6073e-02,  1.2399e-01,  3.7303e-02,  2.3648e-02, -2.7876e-02,\n",
       "           1.2950e-01,  8.5019e-02],\n",
       "         [ 8.7826e-02, -6.6990e-02, -9.3978e-02, -4.6776e-02, -2.6795e-02,\n",
       "           1.2192e-01,  1.6248e-01,  6.2664e-02, -8.2346e-02, -9.6425e-02,\n",
       "           4.8198e-02, -9.7766e-02, -1.1962e-01,  2.0291e-03, -4.0373e-02,\n",
       "          -6.4097e-02,  1.4119e-01,  1.2480e-01,  1.4788e-01, -7.4847e-02,\n",
       "           6.8732e-02, -5.0356e-02,  1.0688e-01, -7.4759e-02, -8.8562e-02,\n",
       "           1.6897e-01, -1.6680e-01, -7.1811e-02,  4.9237e-02,  1.6840e-01,\n",
       "          -1.6552e-01,  3.3155e-03],\n",
       "         [ 1.5443e-01,  8.9688e-02,  1.6744e-01, -1.7581e-01, -1.3346e-01,\n",
       "           7.8006e-02,  3.8708e-02,  8.1507e-02, -1.1900e-01, -1.7189e-01,\n",
       "          -4.5377e-02,  1.5611e-01,  1.1569e-01, -9.0740e-02, -4.8263e-02,\n",
       "          -1.7559e-01,  1.0899e-01, -1.2556e-01, -1.3399e-02,  7.8219e-02,\n",
       "           7.1496e-03,  7.3182e-02, -4.7178e-02, -1.7371e-01,  1.5143e-01,\n",
       "           1.1145e-01, -1.2145e-01, -1.6120e-01, -5.6644e-02,  3.9727e-02,\n",
       "          -7.3665e-02,  4.5313e-02]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009, -0.0009,\n",
       "         -0.0009, -0.0009], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynet.tree.optimizer.param_groups[0]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
       " 'exp_avg': tensor([[-3.1602e-08, -1.3986e-07,  1.7073e-07,  2.9826e-09,  1.3238e-07,\n",
       "           1.6701e-07,  5.9100e-08, -1.0482e-07,  1.7527e-08, -1.0294e-07,\n",
       "           1.6145e-07,  1.3702e-08, -9.0188e-08,  8.1716e-08, -2.3358e-08,\n",
       "           1.5981e-07,  4.3848e-08, -1.4088e-07,  1.1295e-07, -1.0461e-07,\n",
       "          -3.5173e-08, -4.6892e-08,  1.6426e-08, -1.2022e-07, -1.6115e-07,\n",
       "           1.3834e-07, -3.3678e-08, -1.6298e-07, -1.3963e-07,  1.6679e-07,\n",
       "           1.2307e-08,  8.0465e-08],\n",
       "         [-1.1421e-07, -1.7030e-07,  1.6716e-07, -1.7334e-08, -8.6455e-08,\n",
       "           9.4055e-08,  1.0285e-07, -3.5416e-08,  1.2777e-07, -4.5520e-08,\n",
       "          -9.2204e-08,  1.6473e-07,  8.1752e-08,  1.3174e-07,  1.5837e-07,\n",
       "          -2.7019e-08, -1.5711e-07,  4.6367e-09,  6.8835e-09,  1.6520e-07,\n",
       "          -9.5440e-08,  1.3001e-07,  5.8429e-08, -1.4352e-07,  4.1019e-08,\n",
       "           1.7674e-07, -1.3818e-07,  1.5328e-07, -1.9547e-08, -1.0233e-07,\n",
       "           1.7024e-07, -9.7452e-08],\n",
       "         [ 1.7348e-07, -1.1809e-07,  1.5368e-07,  2.9727e-08,  2.8086e-09,\n",
       "           1.6506e-07,  1.1448e-07,  1.7045e-07,  1.2955e-07, -1.0716e-07,\n",
       "          -1.6422e-07, -7.9757e-09, -1.6706e-08,  1.6776e-07,  9.2710e-10,\n",
       "          -1.1853e-07,  1.2754e-07,  9.8759e-08, -6.0431e-08, -1.6937e-07,\n",
       "           8.7807e-08,  4.0984e-08,  8.2009e-09,  7.0635e-08,  1.6470e-07,\n",
       "          -1.0648e-07, -2.2350e-08, -1.4818e-07, -9.0760e-08,  6.4916e-08,\n",
       "          -6.7737e-08, -1.2672e-07],\n",
       "         [ 1.6938e-07, -8.4715e-08, -1.7485e-07,  1.4701e-07,  1.7095e-07,\n",
       "          -5.2865e-09,  1.5932e-07,  9.5093e-08,  8.0123e-08,  6.4402e-08,\n",
       "           7.5628e-08,  1.0800e-07, -5.8109e-08,  5.1924e-08,  3.6403e-08,\n",
       "          -9.8470e-09,  9.0796e-08, -8.7599e-09, -5.2086e-08,  1.1254e-07,\n",
       "          -3.6331e-08,  6.3379e-08, -6.6949e-08, -1.8643e-08, -1.7651e-07,\n",
       "           2.6410e-09, -1.2727e-07,  1.4412e-07, -1.4670e-07, -1.3533e-07,\n",
       "           1.3373e-07,  3.2561e-08],\n",
       "         [-1.5098e-07,  8.9720e-08,  1.7295e-07,  3.7623e-08, -1.3532e-07,\n",
       "          -9.8858e-08,  1.5595e-07, -4.7348e-08, -2.7739e-08,  1.4397e-07,\n",
       "          -1.1565e-07,  1.1309e-08,  1.4139e-07,  7.8791e-08,  9.7783e-08,\n",
       "           7.8371e-09,  1.7259e-07,  1.5989e-07, -7.6917e-08,  5.1512e-09,\n",
       "           9.9733e-08, -6.4930e-08,  5.5776e-08, -3.5385e-08, -1.4224e-07,\n",
       "          -9.5125e-08, -7.3494e-08,  1.4771e-08,  1.8317e-08, -7.5079e-09,\n",
       "           1.1003e-07,  9.5619e-08],\n",
       "         [-7.9656e-08, -1.0163e-07,  1.1379e-07,  1.7031e-07,  1.5648e-07,\n",
       "          -1.4128e-07, -9.6440e-09,  1.7501e-07, -6.4265e-08,  9.4829e-09,\n",
       "           9.6653e-08, -1.2235e-07, -5.0067e-08, -8.4200e-09, -1.4676e-07,\n",
       "           6.7143e-08,  6.4719e-08, -1.1191e-07,  1.7159e-07,  1.6528e-07,\n",
       "          -1.0639e-07,  1.3147e-07, -1.8619e-08,  1.4264e-07,  2.3251e-08,\n",
       "          -1.4363e-07, -1.3558e-07, -1.2225e-07,  1.7398e-07,  1.3960e-07,\n",
       "          -4.2263e-08, -1.4391e-07],\n",
       "         [ 1.5128e-07,  5.2525e-08, -1.2078e-07, -5.6247e-08,  8.3524e-08,\n",
       "           7.8241e-08, -1.0109e-07,  7.2473e-09,  2.4261e-08,  2.5151e-08,\n",
       "          -7.5852e-09,  6.2674e-08,  7.3485e-08,  1.0446e-07,  2.8317e-08,\n",
       "           1.6057e-07,  8.4793e-08,  1.3976e-07, -2.2204e-08,  1.1101e-07,\n",
       "          -7.4252e-08,  2.5850e-08,  5.5753e-08, -4.2166e-08,  9.5052e-08,\n",
       "          -1.3533e-07, -1.1579e-07,  5.4032e-08, -6.0608e-08, -1.4786e-08,\n",
       "          -1.5006e-08,  1.1042e-07],\n",
       "         [ 1.4574e-07, -6.2356e-08,  3.0324e-09,  3.4641e-08,  2.3956e-08,\n",
       "           1.0944e-07,  4.3327e-10,  3.3828e-08,  1.1680e-07,  9.6399e-08,\n",
       "           1.6187e-07,  4.0771e-08, -1.6383e-07,  7.7191e-08,  9.9834e-08,\n",
       "          -5.7342e-08, -1.3084e-07, -1.4451e-07, -1.6000e-07,  1.1492e-08,\n",
       "           8.6316e-08, -3.2736e-08, -1.3120e-07,  8.9906e-10, -1.0671e-07,\n",
       "           4.7001e-08,  1.2491e-07,  3.8184e-08,  2.4476e-08, -2.8783e-08,\n",
       "           1.3042e-07,  8.5936e-08],\n",
       "         [ 8.8698e-08, -6.7849e-08, -9.4878e-08, -4.7602e-08, -2.7673e-08,\n",
       "           1.2278e-07,  1.6339e-07,  6.3537e-08, -8.3217e-08, -9.7316e-08,\n",
       "           4.9061e-08, -9.8713e-08, -1.2054e-07,  2.6450e-09, -4.1252e-08,\n",
       "          -6.5021e-08,  1.4213e-07,  1.2569e-07,  1.4875e-07, -7.5721e-08,\n",
       "           6.9675e-08, -5.1276e-08,  1.0781e-07, -7.5647e-08, -8.9359e-08,\n",
       "           1.6991e-07, -1.6767e-07, -7.2694e-08,  5.0082e-08,  1.6926e-07,\n",
       "          -1.6638e-07,  4.0658e-09],\n",
       "         [ 1.5530e-07,  9.0606e-08,  1.6833e-07, -1.7665e-07, -1.3436e-07,\n",
       "           7.8858e-08,  3.9604e-08,  8.2383e-08, -1.1987e-07, -1.7279e-07,\n",
       "          -4.6277e-08,  1.5695e-07,  1.1656e-07, -9.1667e-08, -4.9145e-08,\n",
       "          -1.7652e-07,  1.0993e-07, -1.2645e-07, -1.4259e-08,  7.9122e-08,\n",
       "           8.0056e-09,  7.4033e-08, -4.8022e-08, -1.7460e-07,  1.5242e-07,\n",
       "           1.1239e-07, -1.2232e-07, -1.6209e-07, -5.7565e-08,  4.0568e-08,\n",
       "          -7.4526e-08,  4.6221e-08]], device='cuda:0'),\n",
       " 'exp_avg_sq': tensor([[9.9867e-17, 1.9560e-15, 2.9149e-15, 8.8958e-19, 1.7523e-15, 2.7893e-15,\n",
       "          3.4928e-16, 1.0987e-15, 3.0720e-17, 1.0597e-15, 2.6065e-15, 1.8774e-17,\n",
       "          8.1338e-16, 6.6775e-16, 5.4557e-17, 2.5538e-15, 1.9227e-16, 1.9848e-15,\n",
       "          1.2758e-15, 1.0943e-15, 1.2371e-16, 2.1988e-16, 2.6982e-17, 1.4452e-15,\n",
       "          2.5970e-15, 1.9137e-15, 1.1342e-16, 2.6563e-15, 1.9495e-15, 2.7818e-15,\n",
       "          1.5146e-17, 6.4746e-16],\n",
       "         [1.3044e-15, 2.9001e-15, 2.7941e-15, 3.0046e-17, 7.4745e-16, 8.8463e-16,\n",
       "          1.0579e-15, 1.2543e-16, 1.6325e-15, 2.0720e-16, 8.5016e-16, 2.7135e-15,\n",
       "          6.6833e-16, 1.7354e-15, 2.5081e-15, 7.3001e-17, 2.4684e-15, 2.1499e-18,\n",
       "          4.7383e-18, 2.7291e-15, 9.1088e-16, 1.6903e-15, 3.4139e-16, 2.0597e-15,\n",
       "          1.6826e-16, 3.1236e-15, 1.9093e-15, 2.3495e-15, 3.8207e-17, 1.0472e-15,\n",
       "          2.8980e-15, 9.4969e-16],\n",
       "         [3.0096e-15, 1.3946e-15, 2.3617e-15, 8.8367e-17, 7.8880e-19, 2.7244e-15,\n",
       "          1.3106e-15, 2.9053e-15, 1.6784e-15, 1.1483e-15, 2.6969e-15, 6.3611e-18,\n",
       "          2.7908e-17, 2.8145e-15, 8.5952e-20, 1.4050e-15, 1.6267e-15, 9.7533e-16,\n",
       "          3.6519e-16, 2.8688e-15, 7.7100e-16, 1.6797e-16, 6.7255e-18, 4.9893e-16,\n",
       "          2.7126e-15, 1.1338e-15, 4.9951e-17, 2.1956e-15, 8.2374e-16, 4.2141e-16,\n",
       "          4.5883e-16, 1.6058e-15],\n",
       "         [2.8691e-15, 7.1766e-16, 3.0571e-15, 2.1612e-15, 2.9223e-15, 2.7947e-18,\n",
       "          2.5382e-15, 9.0426e-16, 6.4197e-16, 4.1476e-16, 5.7195e-16, 1.1664e-15,\n",
       "          3.3767e-16, 2.6961e-16, 1.3252e-16, 9.6962e-18, 8.2440e-16, 7.6736e-18,\n",
       "          2.7129e-16, 1.2664e-15, 1.3199e-16, 4.0169e-16, 4.4821e-16, 3.4756e-17,\n",
       "          3.1155e-15, 6.9751e-19, 1.6197e-15, 2.0771e-15, 2.1522e-15, 1.8314e-15,\n",
       "          1.7883e-15, 1.0602e-16],\n",
       "         [2.2796e-15, 8.0498e-16, 2.9910e-15, 1.4155e-16, 1.8310e-15, 9.7729e-16,\n",
       "          2.4320e-15, 2.2418e-16, 7.6946e-17, 2.0727e-15, 1.3375e-15, 1.2789e-17,\n",
       "          1.9992e-15, 6.2080e-16, 9.5615e-16, 6.1421e-18, 2.9788e-15, 2.5565e-15,\n",
       "          5.9162e-16, 2.6535e-18, 9.9468e-16, 4.2159e-16, 3.1110e-16, 1.2521e-16,\n",
       "          2.0231e-15, 9.0488e-16, 5.4014e-16, 2.1818e-17, 3.3551e-17, 5.6368e-18,\n",
       "          1.2106e-15, 9.1431e-16],\n",
       "         [6.3451e-16, 1.0329e-15, 1.2948e-15, 2.9006e-15, 2.4485e-15, 1.9960e-15,\n",
       "          9.3007e-18, 3.0630e-15, 4.1300e-16, 8.9925e-18, 9.3419e-16, 1.4971e-15,\n",
       "          2.5067e-16, 7.0896e-18, 2.1540e-15, 4.5082e-16, 4.1885e-16, 1.2524e-15,\n",
       "          2.9445e-15, 2.7317e-15, 1.1320e-15, 1.7284e-15, 3.4667e-17, 2.0346e-15,\n",
       "          5.4063e-17, 2.0629e-15, 1.8381e-15, 1.4945e-15, 3.0267e-15, 1.9489e-15,\n",
       "          1.7861e-16, 2.0709e-15],\n",
       "         [2.2886e-15, 2.7589e-16, 1.4588e-15, 3.1637e-16, 6.9762e-16, 6.1216e-16,\n",
       "          1.0220e-15, 5.2523e-18, 5.8858e-17, 6.3258e-17, 5.7536e-18, 3.9280e-16,\n",
       "          5.4001e-16, 1.0912e-15, 8.0185e-17, 2.5783e-15, 7.1899e-16, 1.9533e-15,\n",
       "          4.9302e-17, 1.2323e-15, 5.5134e-16, 6.6825e-17, 3.1084e-16, 1.7780e-16,\n",
       "          9.0348e-16, 1.8315e-15, 1.3407e-15, 2.9195e-16, 3.6734e-16, 2.1863e-17,\n",
       "          2.2518e-17, 1.2194e-15],\n",
       "         [2.1240e-15, 3.8883e-16, 9.1956e-19, 1.2000e-16, 5.7389e-17, 1.1976e-15,\n",
       "          1.8772e-20, 1.1443e-16, 1.3642e-15, 9.2927e-16, 2.6201e-15, 1.6623e-16,\n",
       "          2.6840e-15, 5.9584e-16, 9.9669e-16, 3.2881e-16, 1.7118e-15, 2.0883e-15,\n",
       "          2.5599e-15, 1.3207e-17, 7.4504e-16, 1.0716e-16, 1.7214e-15, 8.0830e-20,\n",
       "          1.1386e-15, 2.2091e-16, 1.5603e-15, 1.4580e-16, 5.9907e-17, 8.2845e-17,\n",
       "          1.7009e-15, 7.3851e-16],\n",
       "         [7.8673e-16, 4.6035e-16, 9.0017e-16, 2.2659e-16, 7.6578e-17, 1.5075e-15,\n",
       "          2.6696e-15, 4.0369e-16, 6.9250e-16, 9.4703e-16, 2.4070e-16, 9.7442e-16,\n",
       "          1.4529e-15, 6.9960e-19, 1.7017e-16, 4.2277e-16, 2.0202e-15, 1.5798e-15,\n",
       "          2.2127e-15, 5.7337e-16, 4.8547e-16, 2.6292e-16, 1.1622e-15, 5.7225e-16,\n",
       "          7.9851e-16, 2.8869e-15, 2.8112e-15, 5.2844e-16, 2.5083e-16, 2.8648e-15,\n",
       "          2.7684e-15, 1.6531e-18],\n",
       "         [2.4119e-15, 8.2095e-16, 2.8334e-15, 3.1206e-15, 1.8053e-15, 6.2186e-16,\n",
       "          1.5685e-16, 6.7869e-16, 1.4369e-15, 2.9855e-15, 2.1416e-16, 2.4632e-15,\n",
       "          1.3586e-15, 8.4029e-16, 2.4153e-16, 3.1158e-15, 1.2085e-15, 1.5990e-15,\n",
       "          2.0332e-17, 6.2602e-16, 6.4090e-18, 5.4809e-16, 2.3061e-16, 3.0487e-15,\n",
       "          2.3231e-15, 1.2631e-15, 1.4961e-15, 2.6273e-15, 3.3138e-16, 1.6458e-16,\n",
       "          5.5541e-16, 2.1364e-16]], device='cuda:0')}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynet.tree.optimizer.state[dynet.root_net.residual.fc1.shortcut.weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.root_net.residual.fc1.shortcut.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3126"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decay_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = '00.2'\n",
    "name = 'dynCNN_reuse_optim_v0'\n",
    "exp_index = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = {\n",
    "    'learning_rate':learning_rate,\n",
    "    'num_add_neuron':num_add_neuron,\n",
    "    'num_decay_steps':num_decay_steps,\n",
    "    'remove_above':remove_above,\n",
    "    'threshold_max':threshold_max,\n",
    "    'threshold_min':threshold_min,\n",
    "    'train_epoch_min':train_epoch_min,\n",
    "    'train_epoch_max':train_epoch_max,\n",
    "    'add_to_remove_ratio':dynet.tree.add_to_remove_ratio,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_json = f'hyperparameters/{index}_hyp_exp_{exp_index}.json'\n",
    "with open(hyp_json, 'w') as fp:\n",
    "    json.dump(hyp, fp, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoTrainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_func = None\n",
    "        self.adding_func = None\n",
    "        self.pruning_func = None\n",
    "        self.maintainance_func = None\n",
    "        self.extra_func = None\n",
    "        \n",
    "        self.log_func = None\n",
    "        \n",
    "    def loop(self, count = 15):\n",
    "        cb = count\n",
    "        for i in range(count):\n",
    "            if i>-0.1:\n",
    "                self.adding_func()\n",
    "            else:\n",
    "#                 global optimizer, warmup\n",
    "                dynet.print_network()    \n",
    "                \n",
    "                reset_optimizer()\n",
    "#                 optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#                 optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#                 warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader))\n",
    "            \n",
    "            \n",
    "            self.training_func()\n",
    "\n",
    "            self.log_func(i)\n",
    "            if self.extra_func:\n",
    "                self.extra_func()\n",
    "            \n",
    "            if i>-0.1:\n",
    "                self.pruning_func()\n",
    "            self.maintainance_func()\n",
    "            \n",
    "            self.log_func(i)\n",
    "            if self.extra_func:\n",
    "                self.extra_func()\n",
    "            \n",
    "            print(f\"=====================\")\n",
    "            print(f\"===LOOPS FINISHED :{i} ===\")\n",
    "            print(f\"Pausing for 2 second to give user time to STOP PROCESS\")\n",
    "            time.sleep(2)\n",
    "        self.training_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when to stop training functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeff(num_iter, coeff0, coeff1, coeff2, coeff_opt, loss_list):\n",
    "    if len(loss_list)<10: return np.array([0]), np.array([0]), float(coeff0.data.cpu()[0])\n",
    "    \n",
    "    _t = torch.tensor(loss_list)\n",
    "    _t = (_t - _t[-1])/(_t[0]-_t.min()) ## normalize to make first point at 1 and last at 0 \n",
    "    _t = torch.clamp(_t, -1.1, 1.1)\n",
    "    _x = torch.linspace(0, 1, steps=len(_t))\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        coeff_opt.zero_grad()\n",
    "        _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "        _loss = ((_y - _t)**2).mean()\n",
    "        _loss.backward()\n",
    "        coeff_opt.step()\n",
    "\n",
    "        coeff0.data = torch.clamp(coeff0.data, -20., 20.)\n",
    "        coeff1.data = torch.clamp(coeff1.data, 0.7, 2.)\n",
    "        coeff2.data = torch.clamp(coeff2.data, -0.2,0.1)\n",
    "        \n",
    "    if torch.isnan(coeff0.data[0]):\n",
    "        coeff0.data[0] = 0.\n",
    "        coeff1.data[0] = 0.\n",
    "        coeff2.data[0] = 1. ## this gives signal\n",
    "        \n",
    "    _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "    return _x.numpy(), _t.numpy(), _y.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global variables\n",
    "optimizer = None\n",
    "warmup = None\n",
    "coeff_opt = None\n",
    "\n",
    "loss_all = []\n",
    "accs_all = []\n",
    "accs_test = []\n",
    "events_all = []\n",
    "\n",
    "## for adam optimizer = \n",
    "# learning_rate *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reset_optimizer():\n",
    "#     global optimizer, warmup\n",
    "# #     optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "#     optimizer = adam_custom.Adam(dynet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# #     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "#     warmup = WarmupLR_Polynomial(optimizer, 0.5, len(train_loader), power=2)\n",
    "# #     warmup = WarmupLR_Polynomial(optimizer, 10/len(train_loader), len(train_loader))\n",
    "# #     get_bn_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_optimizer():\n",
    "    global dynet, warmup, optimizer\n",
    "\n",
    "    ## there are no param groups, but consider there are len=1\n",
    "    pg = dynet.tree.optimizer.param_groups\n",
    "    for i in range(len(pg)):\n",
    "        pg[i]['lr'] = learning_rate\n",
    "    \n",
    "        \n",
    "#     optimizer = dynet.tree.optimizer   \n",
    "#     dynet.tree.optimizer = adam_custom.Adam(dynet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    warmup = WarmupLR_Polynomial(dynet.tree.optimizer, 0.5, len(train_loader), power=2)\n",
    "    \n",
    "#     copy_optimizer()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def copy_optimizer():\n",
    "#     global dynet\n",
    "#     old_optim = dynet.tree.optimizer\n",
    "#     new_optim = adam_custom.Adam(dynet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "#     found=False\n",
    "#     for p in dynet.parameters():\n",
    "#         for _p in old_optim.param_groups[0]['params']:\n",
    "#             if _p is p:\n",
    "#                 found = True\n",
    "#                 new_optim.state[p] = old_optim.state[p]\n",
    "#     if not found:\n",
    "#         raise ValueError(\"Parameter could not be found\")\n",
    "    \n",
    "#     dynet.tree.optimizer = new_optim\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = dynet.tree.optimizer\n",
    "\n",
    "# copy_optimizer(optimizer)\n",
    "\n",
    "# dynet.tree.optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupLR_Polynomial():\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_epoch, num_batch_in_epoch, power=5):\n",
    "        self.warmup_epoch = warmup_epoch\n",
    "        self.optimizer = optimizer\n",
    "        self.num_batch = num_batch_in_epoch\n",
    "        self.steps = 0\n",
    "        self.power = power\n",
    "        self.backup_lr = []\n",
    "        for group in self.optimizer.param_groups:\n",
    "            self.backup_lr.append(float(group['lr']))\n",
    "        \n",
    "    def step(self):\n",
    "        self.steps += 1\n",
    "        steps = self.steps/self.num_batch\n",
    "        \n",
    "        factor = 1\n",
    "        warming = False\n",
    "        if steps<self.warmup_epoch:\n",
    "            factor = (steps/self.warmup_epoch)**self.power\n",
    "            warming = True\n",
    "            \n",
    "        for group, bkp_lr in zip(self.optimizer.param_groups, self.backup_lr):\n",
    "            group['lr'] = bkp_lr*factor\n",
    "        \n",
    "        return warming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neurons_func():\n",
    "    global optimizer, warmup, added, events_all\n",
    "    \n",
    "    ######################################33\n",
    "    ################# CHECK IF ADDING NEURONS CHANGES ACCURACY #####################\n",
    "#     with torch.no_grad():\n",
    "#         corrects = 0\n",
    "#         for test_x, test_y in train_loader:\n",
    "#             test_x  = test_x.to(device)\n",
    "#             yout = dynet.forward(test_x)\n",
    "#             outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "#             correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "#             corrects += correct\n",
    "#         accs_all.append(corrects/len(train_dataset)*100)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         corrects = 0\n",
    "#         dynet.eval()\n",
    "#         for test_x, test_y in test_loader:\n",
    "#             test_x  = test_x.to(device)\n",
    "#             yout = dynet.forward(test_x)\n",
    "#             outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "#             correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "#             corrects += correct\n",
    "#         dynet.train()\n",
    "#         accs_test.append(corrects/len(test_dataset)*100)\n",
    "    ######################################33\n",
    "    \n",
    "    \n",
    "    ### number of neurons\n",
    "    count = 0\n",
    "    for hr in dynet.tree.DYNAMIC_LIST:\n",
    "        if hr.residual:\n",
    "            count += hr.residual.hidden_dim\n",
    "    ## add more neurons relatively (+x%)\n",
    "    adding = num_add_neuron+int(0.07*count)\n",
    "    dynet.add_neurons(adding)\n",
    "    print(f\"Adding {adding} Neurons\")\n",
    "    added = adding\n",
    "    dynet.print_network()    \n",
    "    \n",
    "    reset_optimizer()\n",
    "#     optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#     warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader), power=1)\n",
    "    \n",
    "            \n",
    "    ######################################33\n",
    "    if len(accs_all)>0:\n",
    "        \n",
    "#         accs_all.append(accs_all[-1])\n",
    "        with torch.no_grad():\n",
    "            corrects = 0\n",
    "            for test_x, test_y in train_loader:\n",
    "                test_x  = test_x.to(device)\n",
    "                yout = dynet.forward(test_x)\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                corrects += correct\n",
    "            accs_all.append(corrects/len(train_dataset)*100)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            corrects = 0\n",
    "            dynet.eval()\n",
    "            for test_x, test_y in test_loader:\n",
    "                test_x  = test_x.to(device)\n",
    "                yout = dynet.forward(test_x)\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                corrects += correct\n",
    "            dynet.train()\n",
    "            accs_test.append(corrects/len(test_dataset)*100)\n",
    "    ######################################33\n",
    "    \n",
    "    events_all.append((len(accs_all), \"neurons added\"))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_children(module):\n",
    "    child = list(module.children())\n",
    "    if len(child) == 0:\n",
    "        return [module]\n",
    "    children = []\n",
    "    for ch in child:\n",
    "        grand_ch = get_children(ch)\n",
    "        children+=grand_ch\n",
    "    return children\n",
    "\n",
    "bn_params = []\n",
    "def get_bn_params():\n",
    "    global dynet, bn_params\n",
    "    bn_params = []\n",
    "    for module in get_children(dynet):\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            bn_params.append(module.weight)\n",
    "            bn_params.append(module.bias)\n",
    "            \n",
    "def clip_bn_weight_grads(val=0.05):\n",
    "    global bn_params\n",
    "    for bnp in bn_params:\n",
    "        bnp.grad = torch.clamp(bnp.grad, -val, val)\n",
    "        \n",
    "def get_bn_params_grads(val=0.05):\n",
    "    global bn_params\n",
    "    for bnp in bn_params:\n",
    "        if bnp.grad.abs().max() > val:\n",
    "            print(\"Batch Norm receiving high gradients!!\")\n",
    "            print(bnp.grad)\n",
    "            print()\n",
    "            \n",
    "def decay_bn_params(val=5e-5):\n",
    "    global bn_params\n",
    "    for bnp in bn_params:\n",
    "        bnp.data -= torch.sign(bnp.data)*val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_step(xx, yy):\n",
    "#     global dynet\n",
    "    \n",
    "#     yout = dynet(xx)\n",
    "#     loss = criterion(yout, yy) #+ dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "\n",
    "#     dynet.tree.optimizer.zero_grad(set_to_none = True)\n",
    "    \n",
    "#     loss.backward(create_graph=False, retain_graph=False)\n",
    "#     clip_bn_weight_grads()\n",
    "\n",
    "#     dynet.tree.optimizer.step()\n",
    "# #     dynet.zero_grad(True)\n",
    "    \n",
    "#     return yout, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_network_func():\n",
    "    global optimizer, warmup, loss_all, accs_all\n",
    "    \n",
    "    coeff0 = torch.zeros(1, requires_grad=True)\n",
    "    coeff1 = torch.zeros(1, requires_grad=True)\n",
    "    coeff2 = torch.zeros(1, requires_grad=True)\n",
    "    coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "    loss_list = []\n",
    "    prev_loss = None\n",
    "    beta_loss = (1000-1)/1000\n",
    "    loss_ = []\n",
    "    optimizer = dynet.tree.optimizer\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    breakall=False\n",
    "    \n",
    "    steps_ = -1\n",
    "    for epoch in range(train_epoch_max):\n",
    "        \n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "        for train_x, train_y in train_loader:\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "            steps_ += 1\n",
    "            \n",
    "#             dynet.decay_neuron_step()\n",
    "            dynet.tree.std_loss = 0.    \n",
    "\n",
    "            yout = dynet(train_x)\n",
    "            loss = criterion(yout, train_y) #+ dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "                    \n",
    "#             dynet.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=False)\n",
    "            \n",
    "            clip_bn_weight_grads()\n",
    "            optimizer.step()\n",
    "#             yout, loss = train_step(train_x, train_y)\n",
    "            \n",
    "            warmup.step()\n",
    "            \n",
    "            if steps_>100:\n",
    "                prev_loss = (1-beta_loss)*float(loss)+beta_loss*prev_loss\n",
    "                loss_list.append(prev_loss)\n",
    "            elif steps_ == 100:\n",
    "                loss_.append(float(loss))\n",
    "                prev_loss = np.mean(loss_)\n",
    "                loss_ = []\n",
    "            else:\n",
    "                loss_.append(float(loss))\n",
    "            \n",
    "            \n",
    "#             decay_bn_params()\n",
    "            \n",
    "            outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "            targets = train_y.data.cpu().numpy()\n",
    "\n",
    "            correct = (outputs == targets).sum()\n",
    "            train_acc += correct\n",
    "            train_count += len(outputs)\n",
    "\n",
    "            if steps_%100 == 0 and steps_>0:\n",
    "                if len(loss_list)>0:\n",
    "                    max_indx = np.argmax(loss_list)\n",
    "                    loss_list = loss_list[max_indx:]\n",
    "    #                 loss_all.append(float(loss))\n",
    "                \n",
    "                _x, _t, _y = update_coeff(50, coeff0, coeff1, coeff2, coeff_opt, loss_list)\n",
    "                _c = float(coeff0.data.cpu()[0])\n",
    "    #             if coeff2.data[0] > 0.5: ## this is a signal to reset optimizer\n",
    "                coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "                _info = f'ES: {epoch}:{steps_}, coeff:{_c:.3f}/{-5}, \\nLoss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "\n",
    "                ax.clear()\n",
    "                if len(_x)>0:\n",
    "                    ax.plot(_x, _t, c='c')\n",
    "                    ax.plot(_x, _y, c='m')\n",
    "                xmin, xmax = ax.get_xlim()\n",
    "                ymin, ymax = ax.get_ylim()\n",
    "                ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                ax2.clear()\n",
    "                if len(accs_all)>0:\n",
    "                    acc_tr = accs_all\n",
    "                    acc_te = accs_test\n",
    "                    if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                    if len(acc_te)>20: acc_te = acc_te[-20:]\n",
    "                    ax2.plot(acc_tr, marker='.', label=\"train\")\n",
    "                    ax2.plot(acc_te, marker='.', label=\"test\")\n",
    "                    ax2.legend(loc=\"lower right\")\n",
    "                    \n",
    "                    ymin, ymax = ax2.get_ylim()\n",
    "                    ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                    ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_te):.3f} end{acc_te[-1]:.3f}\")\n",
    "\n",
    "                \n",
    "                fig.canvas.draw()\n",
    "                plt.savefig(f\"./output/logs/_{index}_temp_train_plot.png\")\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                if _c < -5 and epoch>train_epoch_min: \n",
    "                    breakall=True\n",
    "                    break\n",
    "                    \n",
    "        if not breakall:\n",
    "            accs_all.append(train_acc/train_count*100.)\n",
    "            with torch.no_grad():\n",
    "                corrects = 0\n",
    "                dynet.eval()\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x  = test_x.to(device)\n",
    "                    yout = dynet.forward(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                    correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                    corrects += correct\n",
    "                dynet.train()\n",
    "                accs_test.append(corrects/len(test_dataset)*100)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_func():\n",
    "    global optimizer, warmup\n",
    "    reset_optimizer()\n",
    "#     optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#     warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader), power=0.5)\n",
    "    \n",
    "    optimizer = dynet.tree.optimizer\n",
    "    \n",
    "    \n",
    "    print(f\"Computing Network Siginificance\")\n",
    "    \n",
    "    dynet.eval()\n",
    "    dynet.start_computing_significance()\n",
    "\n",
    "    for train_x, train_y in train_loader:\n",
    "        train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "        dynet.tree.std_loss = 0.    \n",
    "        yout = dynet(train_x)\n",
    "#         yout.backward(gradient=torch.ones_like(yout))\n",
    "        loss = criterion(yout, train_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=False)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    dynet.finish_computing_significance()\n",
    "    \n",
    "    dynet.identify_removable_neurons(num=None,\n",
    "                                 threshold_min = threshold_min,\n",
    "                                 threshold_max = threshold_max)\n",
    "    num_remove = dynet.decay_neuron_start(decay_steps=num_decay_steps)\n",
    "    \n",
    "    dynet.train()\n",
    "    \n",
    "    if num_remove > 0:\n",
    "#     if num_remove < 0:\n",
    "        decayed = False\n",
    "        print(f\"pruning {num_remove} neurons.\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,4))\n",
    "        ax = fig.add_subplot(121)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        \n",
    "        loss_list = []\n",
    "        steps_ = -1\n",
    "        breakall=False\n",
    "\n",
    "        for epoch in range(train_epoch_max+int(np.ceil(num_decay_steps/len(train_loader)))):\n",
    "            loss_ = []\n",
    "            train_acc = 0\n",
    "            train_count = 0\n",
    "            \n",
    "            for train_x, train_y in train_loader:\n",
    "                train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "                steps_ += 1\n",
    "                \n",
    "#                 with torch.no_grad():\n",
    "                ret = dynet.decay_neuron_step()\n",
    "                dynet.tree.std_loss = 0.    \n",
    "        \n",
    "                if ret == -1 and not decayed:\n",
    "                    events_all.append((len(accs_all), \"neurons decayed\"))\n",
    "                    decayed = True\n",
    "                \n",
    "#                     copy_optimizer()\n",
    "#                     breakall = True\n",
    "#                     break\n",
    "\n",
    "                yout = dynet(train_x)\n",
    "                loss = criterion(yout, train_y) #+ dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "                \n",
    "                optimizer.zero_grad() ##set_to_none = True\n",
    "                loss.backward(retain_graph=False)\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss = float(loss)\n",
    "#                 yout, loss = train_step(train_x, train_y)\n",
    "                                \n",
    "                warmup.step()\n",
    "#                 decay_bn_params()\n",
    "                loss_.append(float(loss))\n",
    "                \n",
    "\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                targets = train_y.data.cpu().numpy()\n",
    "                correct = (outputs == targets).sum()\n",
    "                train_acc += correct\n",
    "                train_count += len(outputs)\n",
    "\n",
    "#                 dynet.decay_neuron_step()\n",
    "                \n",
    "                if steps_%50 == 0 and steps_>0:\n",
    "                    loss = np.mean(loss_)\n",
    "                    loss_ = []\n",
    "                    loss_list.append(loss)\n",
    "                \n",
    "                if steps_%100 == 0 and steps_>0:\n",
    "                    \n",
    "                    _info = f'ES: {epoch}:{steps_}, Loss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "#                     print(_info)\n",
    "                    ax.clear()\n",
    "                    out = (yout.data.cpu().numpy()>0.5).astype(int)\n",
    "                    ax.plot(loss_list)\n",
    "                    \n",
    "                    xmin, xmax = ax.get_xlim()\n",
    "                    ymin, ymax = ax.get_ylim()\n",
    "                    ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                    ax2.clear()\n",
    "                    if len(accs_all)>0:\n",
    "                        acc_tr = accs_all\n",
    "                        acc_te = accs_test\n",
    "                        if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                        if len(acc_te)>20: acc_te = acc_te[-20:]\n",
    "                        ax2.plot(acc_tr, marker='.', label=\"train\")\n",
    "                        ax2.plot(acc_te, marker='.', label=\"test\")\n",
    "                        ax2.legend(loc=\"lower right\")\n",
    "\n",
    "                        ymin, ymax = ax2.get_ylim()\n",
    "                        ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                        ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_te):.3f} end{acc_te[-1]:.3f}\")\n",
    "\n",
    "                    \n",
    "                    fig.canvas.draw()\n",
    "                    plt.savefig(f\"./output/logs/_{index}_temp_prune_plot.png\")\n",
    "#                     plt.pause(0.01)\n",
    "#                     print(\"\\n\")\n",
    "                    \n",
    "#                 if steps_>num_decay_steps+int(num_decay_steps/2): breakall=True\n",
    "#                 if steps_>(num_decay_steps+int(len(train_loader)*2.05)): breakall=True\n",
    "#                 if breakall: break\n",
    "\n",
    "#             if steps_>=(num_decay_steps):\n",
    "            if epoch >= (num_decay_steps/len(train_loader))+1.99:\n",
    "                breakall = True\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                corrects = 0\n",
    "                ret = dynet.decay_neuron_step()\n",
    "                dynet.eval()\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x  = test_x.to(device)\n",
    "                    yout = dynet.forward(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                    correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                    corrects += correct\n",
    "                dynet.train()\n",
    "                accs_test.append(corrects/len(test_dataset)*100)        \n",
    "\n",
    "            accs_all.append(train_acc/train_count*100.)\n",
    "\n",
    "#             if not breakall:\n",
    "#                 accs_all.append(train_acc/train_count*100.)\n",
    "#             else:\n",
    "#                 accs_all.append(accs_all[-1])\n",
    "#                 break\n",
    "            if breakall: break\n",
    "\n",
    "        plt.close()\n",
    "    \n",
    "    dynet.remove_decayed_neurons()\n",
    "    events_all.append((len(accs_all), \"neurons pruned\"))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1563"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintain_network():\n",
    "    dynet.compute_del_neurons()\n",
    "    dynet.maintain_network()\n",
    "    dynet.print_network()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network_stat(loop_indx):\n",
    "    stdout = sys.stdout\n",
    "    s = io.StringIO(newline=\"\")\n",
    "    sys.stdout = s\n",
    "    dynet.print_network()\n",
    "    sys.stdout = stdout\n",
    "    s.seek(0)\n",
    "    # prints = s.read()\n",
    "    architecture = s.getvalue()\n",
    "    s.close()\n",
    "    \n",
    "    ### number of neurons\n",
    "    count = 0\n",
    "    for hr in dynet.tree.DYNAMIC_LIST:\n",
    "        if hr.residual:\n",
    "            count += hr.residual.hidden_dim\n",
    "    \n",
    "    with open(f\"output/logs/{index}_{name}_log_{exp_index}.txt\", \"a+\") as f:\n",
    "        ### Print the configuration at top.\n",
    "#         if loop_indx == 0:\n",
    "        \n",
    "        if loop_indx >= 0:\n",
    "    \n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "            \n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%B %d, %Y @ %H:%M:%S\")\n",
    "            f.write(f\"DateTime: {dt_string}\")\n",
    "            \n",
    "            f.write(f\"num_add_neuron :{num_add_neuron}\\n add_to_remove_ratio :{dynet.tree.add_to_remove_ratio}\\n\")\n",
    "            f.write(f\"learning_rate :{learning_rate}\\n num_decay_steps :{num_decay_steps}\\n\")\n",
    "            f.write(f\"threshold_max :{threshold_max}\\n threshold_min :{threshold_min}\\n\")\n",
    "            f.write(f\"train_epoch_min :{train_epoch_min}\\n threshold_max :{train_epoch_max}\\n\")\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "        \n",
    "        f.write(f\"####################| Loop:{loop_indx} | Epoch: {len(accs_all)} \\n\")\n",
    "        num_params = sum(p.numel() for p in dynet.parameters())\n",
    "        num_trainable = sum(p.numel() for p in dynet.parameters() if p.requires_grad)\n",
    "        f.write(f\"| Dynamic Neurons:{count} | Total Parameters: {num_params} | Trainable Parameters: {num_trainable}\\n\")\n",
    "        f.write(f\"| Train Acc:{accs_all[-1]:.3f} | Test Acc: {accs_test[-1]:.3f}\\n\")\n",
    "        f.write(architecture)\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyperparameters_from_json():\n",
    "    global learning_rate,num_add_neuron,num_decay_steps,\\\n",
    "            remove_above,threshold_max,threshold_min,train_epoch_min,train_epoch_max,\\\n",
    "            dynet\n",
    "    with open(hyp_json, 'r') as fp:\n",
    "        hyps = json.load(fp)\n",
    "        learning_rate = hyps['learning_rate']\n",
    "        num_add_neuron = hyps['num_add_neuron']\n",
    "        num_decay_steps = hyps['num_decay_steps']\n",
    "        threshold_max = hyps['threshold_max']\n",
    "        threshold_min = hyps['threshold_min']\n",
    "        train_epoch_min = hyps['train_epoch_min']\n",
    "        train_epoch_max = hyps['train_epoch_max']\n",
    "        dynet.tree.add_to_remove_ratio = hyps['add_to_remove_ratio']\n",
    "        \n",
    "        \n",
    "    ############### Modifying to automate some meta/hyper-parameters\n",
    "    epochs = len(accs_all)\n",
    "    ###\n",
    "    if epochs<100:\n",
    "        learning_rate = 0.0009\n",
    "        num_add_neuron = 50\n",
    "        train_epoch_max = 8\n",
    "        dynet.tree.add_to_remove_ratio = 2.0\n",
    "    elif epochs<200:\n",
    "        learning_rate = 0.0005\n",
    "        train_epoch_max = 10\n",
    "        num_add_neuron = 80\n",
    "        dynet.tree.add_to_remove_ratio = 2.5\n",
    "    elif epochs<400:\n",
    "        learning_rate = 0.0003\n",
    "        train_epoch_max = 15\n",
    "        num_add_neuron = 80\n",
    "        dynet.tree.add_to_remove_ratio = 2.5\n",
    "    elif epochs<600:\n",
    "        learning_rate = 0.0003\n",
    "        train_epoch_max = 15\n",
    "        num_add_neuron = 40\n",
    "        dynet.tree.add_to_remove_ratio = 0.8\n",
    "    else:\n",
    "        learning_rate = 0.00012\n",
    "        train_epoch_max = 15\n",
    "        num_add_neuron = 50\n",
    "        dynet.tree.add_to_remove_ratio = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accs_save():\n",
    "    plt.plot(accs_all, label=\"train\")\n",
    "    plt.plot(accs_test, label=\"test\")\n",
    "    ymin, ymax = plt.gca().get_ylim()\n",
    "    plt.text(0, 0.8*ymin+0.2*ymax, f\"Train-> max:{max(accs_all):.3f} end:{accs_all[-1]:.3f} \\nTest-> max:{max(accs_test):.3f} end:{accs_test[-1]:.3f}\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"output/plots/{index}_{name}_cifar10_{exp_index}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    with open(f\"output/plots/{index}_{name}_cifar10_{exp_index}_event_dict.json\", 'w') as f:\n",
    "        d = {\n",
    "            \"train_accs\":accs_all,\n",
    "            \"test_accs\":accs_test,\n",
    "            \"event_dict\":events_all,\n",
    "        }\n",
    "        json.dump(d, f, indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_hyperparameters_from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_func():\n",
    "    load_hyperparameters_from_json()\n",
    "    plot_accs_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set all functions and begin automated loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AutoTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.adding_func = add_neurons_func\n",
    "trainer.training_func = training_network_func\n",
    "trainer.pruning_func = pruning_func\n",
    "trainer.maintainance_func = maintain_network\n",
    "trainer.log_func = save_network_stat\n",
    "trainer.extra_func = extra_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_neurons_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     8\n",
      "    ╔╝\n",
      "    8\n",
      "   ╔╝\n",
      "   16\n",
      "  ╔╝\n",
      "  32\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 56 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     13\n",
      "     ╠════╗\n",
      "     ║    7\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    14\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   19\n",
      "   ╠════╗\n",
      "   ║    6\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  40\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 38\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsuman/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significance Stat:\n",
      "Min, Max: (0.16641122102737427, 6.752259254455566)\n",
      "Mean, Std: (1.0, 1.0594067573547363)\n",
      "remove_below 0.2817663252353668 true: 77.6032048315836\n",
      "Significance:\n",
      "tensor([139.7092,  78.5225, 146.4444, 126.9177, 166.5993, 117.5065,  77.6032,\n",
      "        208.1071,  97.2072,  76.9220,  88.9756, 105.9960, 112.1148, 109.6911,\n",
      "         75.6391, 243.9059, 154.1853, 275.7532, 154.5835,  69.8834, 169.3827,\n",
      "        266.7863, 142.5314, 184.3219, 149.3132, 196.6468, 174.3133, 251.7649,\n",
      "        217.5310, 255.3817, 175.2290, 132.6508, 316.0193, 115.2871, 365.4428,\n",
      "        162.8930, 409.3323, 243.0872, 181.9184, 133.7161], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False,  True, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 263.0821,  509.7804,  373.1624,  673.6918,  300.6801, 1017.4932,\n",
      "         639.7733,  366.9876,  288.4758,  496.8020,  264.1484,  165.4806,\n",
      "         665.9021,  433.4626,  926.2075,  492.7930,  545.8220,  822.3985,\n",
      "         680.1591], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([266.7469, 574.7502, 369.0728, 295.9633, 710.0670, 208.6339, 323.2026,\n",
      "        210.1402, 518.2194, 274.4895, 467.7490, 255.9547, 608.9619],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([102.7794, 157.7888, 116.3991, 317.8369, 130.5798, 128.3897, 137.4179],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([120.9378, 177.6434, 259.2833, 211.5869, 120.9423, 216.5197],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([89.6083, 88.1622, 81.7087, 81.7541, 73.9286, 79.5426, 67.8523, 73.1556,\n",
      "        49.7312, 78.0795, 78.5417, 58.1890, 79.8810, 69.8364, 58.1077, 67.2987,\n",
      "        96.2232, 93.9225, 80.6738, 70.3535, 67.8169, 60.7429, 65.8740, 66.8742,\n",
      "        91.2308, 75.2958, 57.5307, 66.6938, 64.3666, 55.9891, 54.2159, 64.3408,\n",
      "        45.8325, 73.7950, 97.5061, 96.1194, 70.4510, 53.2854], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False,  True,  True,  True, False,\n",
      "        False,  True, False,  True,  True,  True, False, False, False,  True,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([193.2091, 101.8803, 191.4686, 122.5428, 166.5420, 100.7240, 245.6242,\n",
      "        142.7946], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([192.6973, 308.4509, 138.7090, 165.0948, 274.9473, 127.9300, 676.8734],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 500.0891,  767.5093,  387.8481,  301.8241,  362.5121, 1059.7726,\n",
      "         826.9534,  671.1933, 1859.6863,  586.8898, 1426.2400, 1404.0333,\n",
      "        1079.2555,  657.3875], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False], device='cuda:0')\n",
      "pruning 30 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     13\n",
      "     ╠════╗\n",
      "     ║    7\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    14\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   17\n",
      "   ╠════╗\n",
      "   ║    6\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  36\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 14\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :0 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 58 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     21\n",
      "     ╠════╗\n",
      "     ║    13\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    16\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   22\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  45\n",
      "  ╠════╗\n",
      "  ║    14\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 17\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.03033342771232128, 7.2029547691345215)\n",
      "Mean, Std: (1.0, 1.003751516342163)\n",
      "remove_below 0.29734331369400024 true: 82.8871330415623\n",
      "Significance:\n",
      "tensor([ 98.9176, 100.8989, 166.2415, 188.1431, 225.5226, 161.8425, 133.4063,\n",
      "        194.5469, 136.4014, 145.0103, 196.5384, 206.8258, 200.1147, 307.4318,\n",
      "        287.6760, 255.6574, 137.2099, 262.5057, 327.1085, 165.4840, 286.5688,\n",
      "        262.0384, 413.0577, 270.1519, 258.7846, 241.2288, 308.0195, 264.5497,\n",
      "        290.9403, 241.0936, 416.4839, 189.8030, 671.4038, 739.0385, 297.7193,\n",
      "        149.4579,  25.0045,  38.6026,  52.6508,  75.3375,  96.8715,  96.8303,\n",
      "         63.7296, 100.9817, 141.6638], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "        False, False,  True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 570.3546,  542.6804,  827.2765,  377.2289,  814.8990,  469.1762,\n",
      "         515.5483,  617.9302,  447.0084,  275.8634,  507.5215,  395.1652,\n",
      "         731.2137,  868.5521,  910.2862, 1004.8550,  817.0959,  115.3337,\n",
      "          96.7669,  211.5419,  119.4801,  345.4902], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 221.7077,  908.9411,  486.0338,  342.4338, 1167.2057,  146.5567,\n",
      "         598.7129,  504.3757,  685.7790,  199.4768,  288.3638,  448.5222,\n",
      "         345.1615,   38.2188,   44.0348,   10.4584,   53.7329,    8.4557,\n",
      "          22.3929,  213.3456,    9.9955], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True,  True,  True, False,\n",
      "         True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([131.2598, 122.3441, 108.0965, 168.7799,  77.2448, 105.0706, 119.5544,\n",
      "         78.9726, 127.1033,  76.9877, 101.0648,  82.3687,  81.7037,  86.9361,\n",
      "         61.0889,  81.9405], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False,  True, False,  True,\n",
      "        False,  True,  True, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([183.5182, 256.2485, 324.2244, 207.5253, 111.6891, 213.5792, 125.2781,\n",
      "        148.2728,  88.6311,  94.4258], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([178.8174, 173.0824, 193.8398, 185.5382, 211.9627, 182.5826, 131.8870,\n",
      "        151.1249, 200.2632, 172.3825, 185.6557, 193.9742, 237.3153, 207.1476,\n",
      "         19.0334,  10.9911,  10.4960], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([173.3848,  88.5735, 139.7486, 148.8241, 147.7482, 149.0678, 107.0117,\n",
      "        122.9518,  95.6703, 144.3855,  62.2176,  74.7259, 102.4430, 157.3872],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([178.6948, 161.5486,  95.4343, 143.6815, 189.7841, 119.3822, 636.7609,\n",
      "        153.1179, 141.7291, 119.4855, 142.3935,  94.4156,  71.4971],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 644.4985,  717.2737,  725.1126,  397.0281,  315.8106,  781.6030,\n",
      "         719.8501,  604.6971, 2007.8887,  723.9006, 1079.7599, 1119.1257,\n",
      "         843.6884,  429.3716,   82.8871,  275.2747,   77.1053,   67.6279,\n",
      "         109.6004,   96.9053,   95.2649,   75.6183], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False,  True,  True, False, False,\n",
      "        False,  True], device='cuda:0')\n",
      "pruning 38 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     13\n",
      "     ╠════╗\n",
      "     ║    12\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    9\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   21\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  38\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :1 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 59 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    20\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    29\n",
      "    ╠════╗\n",
      "    ║    15\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   30\n",
      "   ╠════╗\n",
      "   ║    18\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  45\n",
      "  ╠════╗\n",
      "  ║    13\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 16\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (1.5451640138053335e-05, 5.827098369598389)\n",
      "Mean, Std: (0.9999998807907104, 1.077609896659851)\n",
      "remove_below 0.2207653522491455 true: 57.50076777581589\n",
      "Significance:\n",
      "tensor([174.6776, 154.2250, 186.9427, 173.2181, 202.7738, 212.9036, 187.6741,\n",
      "         84.2506, 119.3712, 174.4283, 164.8681, 188.2834, 295.2954, 196.8896,\n",
      "        296.3642, 228.4985, 252.7103, 282.9783, 194.7563, 186.2057, 214.2686,\n",
      "        240.3032, 236.4351, 184.8939, 329.6869, 209.4154, 314.9164, 215.6808,\n",
      "        211.2610, 252.0744, 985.8813, 838.0761, 249.0921, 497.8273, 143.9435,\n",
      "        175.6962, 209.8308,  96.9792,  27.3108,  50.3095,  68.3271,  33.8581,\n",
      "         61.2950,  43.7504,  38.1361], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "        False,  True, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 669.9291,  520.2650,  554.3026,  721.3919,  489.2835,  464.2095,\n",
      "         937.3860,  605.7110,  324.0297,  725.9837,  358.8712,  596.9053,\n",
      "         912.3696,  995.9557,  721.2018, 1163.1466,  160.7198,  225.7230,\n",
      "         302.8728,  203.3817,  346.3789,   43.3816,   47.7709,   47.4754,\n",
      "          73.6098,   36.7626,   57.5008,   61.3605,   24.4437,   52.5394],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True,  True,  True, False,  True, False, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 399.2577,  980.0615,  446.7760, 1264.9879,  189.3523,  938.6813,\n",
      "         674.3587,  564.4720,  234.3383,  272.5661,  560.3812,  466.0391,\n",
      "         259.0303,   17.1741,    7.0572], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 94.1870,  82.5181,  79.9479, 115.5662,  86.2034, 131.1780, 133.4770,\n",
      "        125.0202,  94.7914,  61.2114,  53.8955,  58.8180,  71.8912,  93.8099,\n",
      "         52.6433], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
      "         True, False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([160.4134, 263.7226, 261.2925, 114.4295, 121.6845, 238.7206, 196.9756,\n",
      "        109.5350, 135.4740, 132.4997,  70.9607,  85.7865, 126.2384, 104.4118,\n",
      "         95.2127, 103.9747,  89.7629,  93.5717], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1.7293e+02, 1.7749e+02, 2.0547e+02, 2.0686e+02, 2.2964e+02, 1.8370e+02,\n",
      "        1.4663e+02, 1.7143e+02, 2.3000e+02, 1.8467e+02, 1.7655e+02, 1.8473e+02,\n",
      "        2.3825e+02, 1.7618e-02, 2.8734e+00, 4.0246e-03], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([119.8718, 113.9296, 134.3124, 170.0063, 165.8744, 128.2829, 190.8777,\n",
      "        166.1088, 115.2962,  93.7454,  64.0516,  55.8092,  98.7585],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
      "        False,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([163.2467, 136.9456,  96.4318, 155.3196, 159.7269, 100.8653, 429.5649,\n",
      "        118.8522, 120.1647,  94.2262, 146.7439, 100.0092,  69.4769,  75.3548,\n",
      "         84.5817,  85.6416,  79.5644,  56.3973,  53.3085,  83.3700],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 610.8434,  736.5316,  692.1505,  486.2905,  308.9342, 1040.2131,\n",
      "         653.3601, 1517.7321, 1029.0226,  992.6383, 1298.8292,  752.0880,\n",
      "         483.7805,  215.9473,  350.5510,  279.9973,  154.7601,  227.2636,\n",
      "          62.0448,  122.2730,   54.4528,   64.3409,   37.9208,   36.9184,\n",
      "          41.5074,   21.7909,   41.0069,   32.0211,   23.6907],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "pruning 32 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     13\n",
      "     ╠════╗\n",
      "     ║    18\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    21\n",
      "    ╠════╗\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   24\n",
      "   ╠════╗\n",
      "   ║    18\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  39\n",
      "  ╠════╗\n",
      "  ║    11\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :2 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 61 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     17\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ║    27\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    29\n",
      "    ╠════╗\n",
      "    ║    17\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   28\n",
      "   ╠════╗\n",
      "   ║    27\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  42\n",
      "  ╠════╗\n",
      "  ║    16\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 15\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.0019033995922654867, 4.922299385070801)\n",
      "Mean, Std: (0.9999998211860657, 1.0788553953170776)\n",
      "remove_below 0.18675756454467773 true: 43.88410410979034\n",
      "Significance:\n",
      "tensor([ 172.0122,  208.2973,  272.6711,  192.3156,  260.1399,  229.2814,\n",
      "         183.4342,  145.6218,  139.6656,  186.0518,  253.0692,  146.2432,\n",
      "         315.0713,  170.4330,  339.9742,  172.1347,  311.4263,  340.7307,\n",
      "         158.0598,  301.0158,  255.1251,  250.5840,  218.6364,  214.9586,\n",
      "         312.5474,  380.6529,  234.5137,  215.0668,  380.3917, 1059.1542,\n",
      "         838.8031,  214.3564,  579.1448,  118.4440,  271.5644,  215.9273,\n",
      "         159.7933,   99.9982,  294.7823,   40.0131,   34.4922,   34.1059],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 638.8192,  420.3815,  599.1994,  845.2074,  576.4666,  339.3762,\n",
      "         782.6221,  540.2901,  320.9385,  686.4503,  278.8121,  549.3417,\n",
      "         879.3112, 1005.6395,  726.6726, 1095.3774,  279.8889,  244.3977,\n",
      "         333.1939,  204.6533,  322.0682,  256.6986,  189.2101,  125.9434,\n",
      "          29.4478,   27.1398,   56.4344,   45.4056], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([5.1416e+02, 1.1566e+03, 3.4782e+02, 1.0989e+03, 2.1007e+02, 9.8509e+02,\n",
      "        4.5611e+02, 4.6789e+02, 2.3232e+02, 2.7583e+02, 6.0209e+02, 4.3322e+02,\n",
      "        2.7947e+02, 5.9349e-01, 7.3874e+00, 2.9792e+00, 2.7874e+00],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([101.1600, 102.7843, 110.8993, 100.7391, 103.1338, 115.3355, 122.5457,\n",
      "         95.9774,  86.7249,  93.7888,  58.5866,  45.1455,  42.2655,  62.3376,\n",
      "         67.8832,  48.7492,  37.2636], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False,  True,  True, False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([128.6382, 177.3408, 264.0754,  93.5017,  94.6758, 198.3144,  85.0764,\n",
      "        175.4226, 131.1644,  82.7524,  63.5483,  85.8373, 134.1091, 104.1298,\n",
      "        113.9373,  92.8674,  84.5542,  54.8338,  58.4594,  67.1497,  70.4300,\n",
      "         34.8525,  55.1522,  42.9321,  67.0002,  50.3239,  80.6986],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False,  True, False,  True, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([199.9005, 175.8304, 228.3767, 206.2514, 212.8371, 209.6795, 150.5852,\n",
      "        176.6450, 217.7158, 191.7739, 181.4603, 200.9403, 219.1128,   0.4473,\n",
      "          0.5458], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([130.9272, 170.1727,  79.3608,  96.1595,  92.6525, 136.0393, 103.2590,\n",
      "         63.8393, 105.4607, 100.0014, 164.8119, 113.0474,  65.4677, 104.4286,\n",
      "         63.7163, 101.2009], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([143.7245, 134.0816,  93.9710, 163.8482, 133.5296,  83.9239, 250.4025,\n",
      "        158.9185, 105.1521, 115.3372, 106.8670, 115.1864, 125.7653,  78.2459,\n",
      "         61.6187,  83.8201, 110.2489,  84.6078,  65.1105,  44.0862,  60.5144,\n",
      "         53.4972,  70.6314,  71.0892,  41.1175,  59.6367,  47.3384],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([39.9286, 47.0619, 43.8841, 37.8789, 36.7509], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([46.1760, 23.4008, 55.3671, 39.7367, 37.4518, 48.0835, 29.1362],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True,  True,  True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 729.0522,  561.0031,  706.1997,  785.8986,  409.2408, 1066.2898,\n",
      "         553.7123,  968.5531,  874.0332,  859.4456, 1049.0967,  497.1720,\n",
      "         631.1840,  271.5520,  489.0097,  361.0538,  274.1186,  373.2597,\n",
      "          85.9616,  189.6772,  159.2846,   44.6997,   33.5210,   29.0785,\n",
      "          27.3387,   22.2951,   32.8840,   32.1365,   38.1275],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "pruning 42 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    1\n",
      "     ║    ╠════╝\n",
      "     ║    26\n",
      "     ║    ╠════╗\n",
      "     ║    ║    2\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   26\n",
      "   ╠════╗\n",
      "   ║    24\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  39\n",
      "  ╠════╗\n",
      "  ║    12\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 11\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :3 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 63 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     14\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ║    38\n",
      "     ║    ╠════╗\n",
      "     ║    ║    8\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    30\n",
      "    ╠════╗\n",
      "    ║    19\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   27\n",
      "   ╠════╗\n",
      "   ║    39\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  42\n",
      "  ╠════╗\n",
      "  ║    16\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.012658882886171341, 5.540092945098877)\n",
      "Mean, Std: (1.0000001192092896, 1.1087205410003662)\n",
      "remove_below 0.20708401501178741 true: 41.17131614367537\n",
      "Significance:\n",
      "tensor([ 140.9348,  182.4553,  260.3898,  169.6575,  190.7487,  205.6818,\n",
      "         111.0636,  130.6698,  130.9704,  230.9932,  205.8147,  142.7068,\n",
      "         249.1700,  134.6311,  386.4310,  181.3848,  187.2635,  221.6683,\n",
      "         213.7525,  194.0428,  187.1766,  151.6426,  172.8533,  176.4460,\n",
      "         253.3270,  235.9522,  193.6618,  184.8154,  418.0944, 1009.2859,\n",
      "         616.3833,  183.0773,  681.7610,  174.9556,  275.4799,  289.1066,\n",
      "         170.1953,   72.1748,  310.6842,   60.9865,   12.8386,   45.5565],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False, False, False, False,\n",
      "         True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([674.5481, 482.4871, 486.2623, 711.0152, 441.5952, 263.4861, 455.1184,\n",
      "        524.7146, 232.4188, 653.6906, 188.6629, 417.2702, 708.1655, 783.1245,\n",
      "        623.5594, 863.4592, 273.6781, 210.5554, 339.9390, 193.7338, 347.8808,\n",
      "        164.3436, 175.3879, 136.0815, 119.3291, 187.6482,  39.7149],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 573.0695,  965.3112,  429.3676,  998.7864,  221.5374, 1101.4510,\n",
      "         443.0765,  235.5407,  274.7942,  580.0255,  436.3827,  364.2763,\n",
      "           3.9031,    3.5007], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 68.6096,  88.7879, 116.5567,  80.4558, 121.1194,  95.3454, 102.4772,\n",
      "         63.4923,  65.3461,  67.0301,  54.7634,  79.9508,  39.4115,  49.0677,\n",
      "         55.1376,  58.9527,  34.1423,  51.2796,  60.5555], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False,  True, False,  True, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([114.8148, 105.8083, 223.3071, 118.4666,  94.9610,  98.1258,  63.2833,\n",
      "        119.9102,  96.8525,  82.6408,  68.4528,  57.2237, 114.5611,  85.7207,\n",
      "         59.7765,  66.6376,  62.7367,  84.6651,  78.0537,  94.4930,  84.9824,\n",
      "         60.0037,  55.7678,  83.0981,  46.9686,  30.1857,  88.7722,  54.7479,\n",
      "         63.3131,  66.3654,  76.2383,  38.6588, 110.5371,  51.7286,  32.9498,\n",
      "         52.3275,  31.2762,  65.2214,  37.2158], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "         True,  True, False, False,  True, False,  True, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([194.8918, 183.4338, 189.5240, 224.2162, 243.4505, 247.9022, 184.2599,\n",
      "        204.0339, 190.7325, 184.8481, 194.1838,   2.5168,   2.8423],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 89.7173, 147.5602, 111.9073,  94.4061, 122.6497,  63.9348, 125.3906,\n",
      "         43.1105,  90.4615,  86.2070,  90.9654,  68.7203, 111.0825,  78.5377,\n",
      "        106.4617,  86.7627], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([121.3443, 105.7975,  86.7962, 162.0394, 125.2330,  77.5503, 176.1262,\n",
      "        115.1023,  98.5928,  89.8270,  82.5013,  95.6152, 129.3225,  84.0553,\n",
      "         84.1545,  72.1856,  83.2139,  80.2843,  73.3972,  44.2641,  82.3950,\n",
      "         56.4896,  77.2774,  65.7426,  87.4044,  40.5692,  44.2063,  43.6355,\n",
      "         16.9363,  40.6921,  41.0969,  50.6851,  73.0242,  63.1847,  32.3490,\n",
      "         51.0159,  35.8747,  39.4836], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False,  True,  True,\n",
      "         True, False, False, False,  True, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([92.9945, 67.2643, 65.1546, 59.6127, 41.1713], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([67.1584, 41.7629, 18.6290, 36.9028, 24.1556, 31.3887, 20.8367, 29.4689],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 680.3095,  747.5441,  594.3006,  581.3900,  468.2997, 1091.2629,\n",
      "         465.9034,  742.1891,  743.6863,  728.9028,  935.3322,  429.0441,\n",
      "         502.8723,  234.1500,  373.0142,  259.6237,  351.1327,  321.9243,\n",
      "         128.2308,  257.4908,  143.8807,  105.0551,   22.8607,   57.5707,\n",
      "          25.4200,   19.9022,   20.4605,   43.0795,   56.1762,   31.6595],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False,  True,  True,  True, False, False,  True],\n",
      "       device='cuda:0')\n",
      "pruning 38 neurons.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "/home/tsuman/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1033: \n",
    "UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. \n",
    "This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
    "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
    "'''\n",
    "\n",
    "trainer.loop(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check if parameter in param_groupd\n",
    "c = 0\n",
    "for p in optimizer.param_groups[0]['params']:\n",
    "    print(p.shape)\n",
    "    c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.root_net.residual.fc1.shortcut.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_x, train_y in train_loader:\n",
    "    train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "    yout = dynet(train_x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for p in dynet.parameters():\n",
    "    print(p.shape)\n",
    "    for _p in optimizer.param_groups[0]['params']:\n",
    "        if _p is p:\n",
    "            print('Found')\n",
    "    print()\n",
    "    c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.training_func()\n",
    "# trainer.pruning_func()\n",
    "# trainer.maintainance_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    corrects = 0\n",
    "    dynet.eval()\n",
    "    for test_x, test_y in test_loader:\n",
    "        test_x  = test_x.to(device)\n",
    "        yout = dynet.forward(test_x)\n",
    "        outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "        correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "        corrects += correct\n",
    "    dynet.train()\n",
    "    acc = corrects/len(test_dataset)*100\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     corrects = 0\n",
    "#     dynet.train()\n",
    "#     for test_x, test_y in train_loader:\n",
    "#         test_x  = test_x.to(device)\n",
    "#         yout = dynet.forward(test_x)\n",
    "#         outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "#         correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "#         corrects += correct\n",
    "#     acc = corrects/len(train_dataset)*100\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr 66.908 -> 62.422 ## the adding neuron function is wrong.. not preserving the function.\n",
    "# te 71.77 -> 41.959999999999994\n",
    "\n",
    "# te -> 53.32, 53.32\n",
    "# 68.51 -> 68.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.adding_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.beta_del_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accs_all, label=\"train\")\n",
    "plt.plot(accs_test, label=\"test\")\n",
    "ymin, ymax = plt.gca().get_ylim()\n",
    "plt.text(0, 0.8*ymin+0.2*ymax, f\"Train-> max:{max(accs_all):.3f} end:{accs_all[-1]:.3f} \\nTest-> max:{max(accs_test):.3f} end:{accs_test[-1]:.3f}\")\n",
    "                    \n",
    "plt.legend()\n",
    "plt.savefig(f\"output/plots/{index}_{name}_cifar10_{exp_index}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.non_linearity.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_allocated(device=\"cuda:0\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
