{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "import time\n",
    "import sys, io\n",
    "\n",
    "import mylibrary.datasets as datasets\n",
    "import mylibrary.nnlib as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adam_custom\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.LongTensor([2,3])\n",
    "a += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes\n",
    "\n",
    "- Add BN after convolution directly.\n",
    "    - This helps keep weight norm uniform while changing the scaling parameter of BN\n",
    "    - This will help to make the weight gradient well behaved.\n",
    "    \n",
    "- Reuse Optimizer (Adam) for added or removed parameters\n",
    "    - This will (supposedly) remove unstable training\n",
    "    - Maybe we need to add different learning rate for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hidden_neuron_number(i, o):\n",
    "    nh =  (max(i,o)*(min(i,o)**2))**(1/3)\n",
    "#     return max(nh, 1)\n",
    "    return nh\n",
    "\n",
    "class Shortcut_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=(3,3), stride=1):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self._kernel = np.array(kernel, dtype=int)\n",
    "        self._padding = tuple(((self._kernel-1)/2).astype(int))\n",
    "        self._stride = stride\n",
    "        _wd = nn.Conv2d(input_dim, output_dim, self._kernel, stride=self._stride,\n",
    "                        padding=self._padding, bias=False).weight.data\n",
    "        ## Shape = OutputDim, InputDim, Kernel0, Kernel1\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "        del _wd\n",
    "        self.bn = nn.BatchNorm2d(output_dim)\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "        self.initial_freeze_bn = None\n",
    "        \n",
    "        self.add_parameters_to_optimizer()\n",
    "        return\n",
    "        \n",
    "    def add_parameters_to_optimizer(self):\n",
    "        ## internal optimizer\n",
    "#         print(list(self.parameters()))\n",
    "#         self.tree.optimizer.state[pp] = {'step':0, \"aa\":'hahaha'}\n",
    "\n",
    "# {'step': tensor([12, 12,  6,  6,  6]),\n",
    "#               'exp_avg': tensor([ 2.0893e-11,  7.7122e-10, -6.7105e-12, -5.0940e-10, -9.8008e-10]),\n",
    "#               'exp_avg_sq': tensor([2.3143e-19, 1.5871e-19, 3.0733e-20, 2.8796e-20, 4.1671e-20])}\n",
    "        \n",
    "        for p in self.parameters():\n",
    "#             self.tree.optimizer.state[p] = {}\n",
    "            self.tree.optimizer.param_groups[0]['params'].append(p)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.shape[1] > 0 and self.weight.shape[0] > 0:\n",
    "            out_dim = self.weight.shape[0]\n",
    "            self.weight.data /= torch.norm(self.weight.data.reshape(out_dim, -1), dim=1).reshape(out_dim, 1, 1, 1)\n",
    "            \n",
    "            return self.bn(F.conv2d(x, self.weight, stride=self._stride, padding=self._padding))\n",
    "        ### output dim is 0\n",
    "        elif self.weight.shape[0] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###       #out_dim #inp_dim            #kernel\n",
    "            w = torch.zeros(1, 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return torch.zeros(o.shape[0], 0, o.shape[2], o.shape[3], dtype=x.dtype, device=x.device)\n",
    "        ### input dim is 0\n",
    "        elif x.shape[1] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###             #out_dim            #inp_dim            #kernel\n",
    "            w = torch.zeros(self.weight.shape[0], 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return o.data\n",
    "        else:\n",
    "            raise(f\"Unknown shape of input {x.shape} or weight {self.weight.shape}\")\n",
    "\n",
    "#     def decay_std_ratio(self, factor):\n",
    "#         self.weight.data = self.weight.data - self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "        \n",
    "#     def decay_std_ratio_grad(self, factor):\n",
    "#         self.weight.grad = self.weight.grad + self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "    \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "#         self.initial_remove = torch.atan(self.weight.data[:, to_remove])\n",
    "\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.initial_freeze_bn = self.bn.weight.data[to_freeze], self.bn.bias.data[to_freeze]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    ## freeze output neuron's incoming weight \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        self.bn.weight.data[self.to_freeze] = self.initial_freeze_bn[0] \n",
    "        self.bn.bias.data[self.to_freeze] = self.initial_freeze_bn[1] \n",
    "        pass\n",
    "    \n",
    "    ## decay input neuron's outgoing weight \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "#         self.weight.data[:, self.to_remove] = torch.tan(self.initial_remove*self.tree.decay_factor)\n",
    "        pass\n",
    "     \n",
    "    ## remove output neuron \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "        \n",
    "        ### do the same thing to optimizer variables as well        \n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        self.weight.data = self.weight.data[remaining, :]\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        ops[self.weight][_var][remaining, :]\n",
    "        \n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        \n",
    "#         ## running_mean\n",
    "        _rm = self.bn.running_mean[remaining]\n",
    "        self.bn.running_mean = _rm\n",
    "        \n",
    "#         ## running_var\n",
    "        _rv = self.bn.running_var[remaining]\n",
    "        self.bn.running_var = _rv\n",
    "        \n",
    "#         ## weight\n",
    "        self.bn.weight.data = self.bn.weight.data[remaining]\n",
    "        self.bn.weight.grad = None\n",
    "        if len(ops[self.bn.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.bn.weight][_var] = \\\n",
    "                        ops[self.bn.weight][_var][remaining]\n",
    "\n",
    "        ## bias\n",
    "        self.bn.bias.data = self.bn.bias.data[remaining]\n",
    "        self.bn.bias.grad = None\n",
    "        if len(ops[self.bn.bias]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.bn.bias][_var] = \\\n",
    "                        ops[self.bn.bias][_var][remaining]\n",
    "        \n",
    "        self.bn.num_features = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    ## remove input neuron \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "#         print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "#         print(torch.count_nonzero(self.weight.data<1e-6))\n",
    "#         print(self.weight.data[:, self.to_remove])\n",
    "\n",
    "        ops = self.tree.optimizer.state\n",
    "\n",
    "        self.weight.data = self.weight.data[:, remaining]\n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        ops[self.weight][_var][:, remaining]\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        self.weight.data = torch.cat((self.weight.data, \\\n",
    "                                      torch.zeros(o, num, k0, k1, dtype=self.weight.data.dtype,\n",
    "                                      device=self.weight.data.device)), \n",
    "                                     dim=1)\n",
    "        self.weight.grad = None\n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        torch.cat((ops[self.weight][_var], \\\n",
    "                                  torch.zeros(o, num, k0, k1, dtype=ops[self.weight][_var].dtype,\n",
    "                                              device=ops[self.weight][_var].device)), \n",
    "                                  dim=1)\n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i) ### similar to Xavier init ?? !!\n",
    "#         stdv = torch.std(self.weight.data) ## if it does not work, revert it\n",
    "    \n",
    "        _new = torch.empty(num, i, k0, k1, dtype=self.weight.data.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        self.weight.data = torch.cat((self.weight.data, _new), dim=0)\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        torch.cat((ops[self.weight][_var], \\\n",
    "                                  torch.zeros(num, i, k0, k1, dtype=ops[self.weight][_var].dtype,\n",
    "                                              device=ops[self.weight][_var].device)), \n",
    "                                  dim=0)\n",
    "        \n",
    "        # print(self.weight.data.shape)\n",
    "                \n",
    "        ####https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d\n",
    "        ## running_mean\n",
    "        _rm = self.bn.running_mean\n",
    "        _rm = torch.cat((_rm, torch.zeros(num, dtype=_rm.dtype, device=_rm.device)))\n",
    "        self.bn.running_mean = _rm\n",
    "        \n",
    "        ## running_var\n",
    "        _rv = self.bn.running_var\n",
    "        _rv = torch.cat((_rv, torch.ones(num, dtype=_rv.dtype, device=_rv.device)))\n",
    "        self.bn.running_var = _rv\n",
    "        \n",
    "        ## weight\n",
    "        _w = self.bn.weight.data\n",
    "        _w = torch.cat((_w, torch.ones(num, dtype=_w.dtype, device=_w.device)))\n",
    "        self.bn.weight.data = _w\n",
    "        self.bn.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.bn.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.bn.weight][_var] = \\\n",
    "                        torch.cat((ops[self.bn.weight][_var], \\\n",
    "                                  torch.zeros(num, dtype=ops[self.bn.weight][_var].dtype,\n",
    "                                              device=ops[self.bn.weight][_var].device)), \n",
    "                                 )\n",
    "        \n",
    "        ## bias\n",
    "        _b = self.bn.bias.data\n",
    "        _b = torch.cat((_b, torch.zeros(num, dtype=_b.dtype, device=_b.device)))\n",
    "        self.bn.bias.data = _b\n",
    "        self.bn.bias.grad = None\n",
    "        \n",
    "        if len(ops[self.bn.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.bn.bias][_var] = \\\n",
    "                        torch.cat((ops[self.bn.bias][_var], \\\n",
    "                                  torch.zeros(num, dtype=ops[self.bn.bias][_var].dtype,\n",
    "                                              device=ops[self.bn.bias][_var].device)), \n",
    "                                 )\n",
    "        \n",
    "        self.bn.num_features += num\n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}S▚:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TempTree():\n",
    "    def __init__(self):\n",
    "        self.optimizer = adam_custom.Adam([nn.Parameter(torch.Tensor(0))],\n",
    "                                          lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Shortcut_Conv(\n",
       "  (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = TempTree()\n",
    "\n",
    "a = Shortcut_Conv(tree, 2, 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearity_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.actf = actf_obj\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actf(x)\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        pass\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearity(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.bias = nn.Parameter(torch.zeros(io_dim))\n",
    "        self.actf = actf_obj\n",
    "        \n",
    "        self.tree.optimizer.state[self.bias] = {}\n",
    "        tree.optimizer.param_groups[0]['params'].append(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actf(x+self.bias)\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        _b = torch.cat((self.bias.data, torch.zeros(num, dtype=self.bias.data.dtype,\n",
    "                                                    device=self.bias.data.device)))\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        _b = self.bias.data[remaining]\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, hidden_dim, output_dim, stride=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "#         self.stride = stride\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = HierarchicalResidual_Conv(self.tree, input_dim, hidden_dim, stride=stride, activation=activation) \n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = HierarchicalResidual_Conv(self.tree, hidden_dim, output_dim, activation=activation)\n",
    "        self.fc1.shortcut.bn.weight.data *= 0.        \n",
    "        self.fc1.shortcut.weight.data *= 0.1        \n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "            self.apnz = self.apnz/self.count\n",
    "#             self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "            self.significance = self.significance*(1-self.apnz**33) / 0.872 ## tried on desmos.\n",
    "\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "#         self.count = None\n",
    "\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "#             self.significance += z.pow(2).sum(dim=0)\n",
    "            self.significance += z.abs().sum(dim=0)\n",
    "#             self.significance += z.abs().pow(0.8).sum(dim=0)\n",
    "#             print(f\"SIG ACT:\\n{float(self.activations.abs().mean())}\")\n",
    "#             print(f\"GRAD Mean, Std:\\n{float(grad_output[0].data.abs().mean()), float(grad_output[0].data.std())}\")\n",
    "\n",
    "            if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "                self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "        #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "                self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below=None, above=None, mask=None):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(self.significance.numel(), dtype=torch.bool)\n",
    "        if below:\n",
    "            mask = torch.logical_or(mask,self.significance<=below)\n",
    "        if above:\n",
    "            mask = torch.logical_or(mask,self.significance>above)\n",
    "            \n",
    "        print(f\"Significance:\\n{self.significance}\\nPrune:\\n{mask}\")\n",
    "            \n",
    "        self.to_remove = torch.nonzero(mask).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "#         max_dim = np.ceil((self.tree.parent_dict[self].input_dim+\\\n",
    "#             self.tree.parent_dict[self].output_dim)/2)\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None: ## it is shortcut conv\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, stride=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.stride = 1\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, stride=self.stride).to(self.tree.device)\n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None ## this can be Residual Layer or None\n",
    "        ##### only one of shortcut or residual can be None at a time\n",
    "        self.forward = self.forward_shortcut\n",
    "        \n",
    "        self.std_ratio = 0. ## 0-> all variation due to shortcut, 1-> residual\n",
    "        self.target_std_ratio = 0. ##\n",
    "    \n",
    "    def forward_both(self, r):\n",
    "\n",
    "        s = self.shortcut(r)\n",
    "        r = self.residual(r)\n",
    "\n",
    "        if self.residual.hook is None: ### dont execute when computing significance\n",
    "            s_std = torch.std(s, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            r_std = torch.std(r, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            stdr = r_std/(s_std+r_std)\n",
    "\n",
    "            self.std_ratio = self.tree.beta_std_ratio*self.std_ratio + (1-self.tree.beta_std_ratio)*stdr.data\n",
    "            if r_std.min() > 1e-9:\n",
    "                ## recover for the fact that when decaying neurons, target ratio should also be reducing\n",
    "                if self.tree.total_decay_steps:\n",
    "                    i, o = self.shortcut.weight.shape[1],self.shortcut.weight.shape[0]\n",
    "                    if self.shortcut.to_remove is not None:\n",
    "                        i -= len(self.shortcut.to_remove)\n",
    "                    if self.shortcut.to_freeze is not None:\n",
    "                        o -= len(self.shortcut.to_freeze)\n",
    "                    h = self.residual.hidden_dim\n",
    "                    if self.residual.to_remove is not None:\n",
    "                        h -= len(self.residual.to_remove)\n",
    "                    \n",
    "#                     tr = h/np.ceil((i+o)/2 +1)\n",
    "                    tr = h/_get_hidden_neuron_number(i, o)\n",
    "                    self.compute_target_std_ratio(tr)\n",
    "                else:\n",
    "                    self.compute_target_std_ratio()\n",
    "                self.get_std_loss(stdr)\n",
    "        return s+r\n",
    "    \n",
    "    def forward_shortcut(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def forward_residual(self, x):\n",
    "        self.compute_target_std_ratio()\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def compute_target_std_ratio(self, tr = None):\n",
    "        if tr is None:\n",
    "#             tr = self.residual.hidden_dim/np.ceil((self.input_dim+self.output_dim)/2 +1)\n",
    "            tr = self.residual.hidden_dim/_get_hidden_neuron_number(self.input_dim, self.output_dim)\n",
    "#             tr = self.residual.hidden_dim/np.ceil(self.output_dim/2 +1)\n",
    "\n",
    "        tr = np.clip(tr, 0., 1.)\n",
    "        self.target_std_ratio = self.tree.beta_std_ratio*self.target_std_ratio +\\\n",
    "                                (1-self.tree.beta_std_ratio)*tr\n",
    "        pass        \n",
    "    \n",
    "    def get_std_loss(self, stdr):\n",
    "        del_std = self.target_std_ratio-stdr\n",
    "        del_std_loss = (del_std**2 + torch.abs(del_std)).mean()\n",
    "#         del_std_loss = (del_std**2).mean()\n",
    "        self.tree.std_loss += del_std_loss\n",
    "        return\n",
    "            \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_freezing_connection(to_freeze)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_decaying_connection(to_remove)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_freezed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.remove_freezed_connection(remaining)\n",
    "            if self.shortcut: self.std_ratio = self.std_ratio[:, remaining]\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_decayed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_input_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_output_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.add_output_connection(num)\n",
    "            # if torch.is_tensor(self.std_ratio):\n",
    "            if self.shortcut:\n",
    "                self.std_ratio = torch.cat((self.std_ratio, torch.zeros(1, num, device=self.tree.device)), dim=1)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        \n",
    "        if self.residual is None:\n",
    "            # print(f\"Adding {num} hidden units.. in new residual_layer\")\n",
    "            self.residual = Residual_Conv(self.tree, self.input_dim,\n",
    "                                          num, self.output_dim, stride=self.stride,\n",
    "                                          activation=self.activation).to(self.tree.device)\n",
    "            \n",
    "            self.tree.parent_dict[self.residual] = self\n",
    "            if self.shortcut is None:\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            else:\n",
    "                self.forward = self.forward_both\n",
    "                self.std_ratio = torch.zeros(1, self.output_dim, device=self.tree.device)\n",
    "                \n",
    "        else:\n",
    "            # print(f\"Adding {num} hidden units..\")\n",
    "            self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            if self.std_ratio.min()>0.98 and self.target_std_ratio>0.98:\n",
    "                del self.tree.parent_dict[self.shortcut]\n",
    "                del self.shortcut\n",
    "                self.shortcut = None\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            \n",
    "        elif self.target_std_ratio<0.95:\n",
    "            self.shortcut = Shortcut_Conv(self.tree, self.input_dim, self.output_dim, stride=self.stride)\n",
    "            self.shortcut.bn.weight.data *= 0.\n",
    "            self.shortcut.weight.data *= 0.1\n",
    "            self.forward = self.forward_both\n",
    "            \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.residual.hidden_dim < 1:\n",
    "            del self.tree.parent_dict[self.residual]\n",
    "            del self.residual\n",
    "            ### its parent (Residual_Conv) removes it from dynamic list if possible\n",
    "            self.residual = None\n",
    "            self.forward = self.forward_shortcut\n",
    "            self.std_ratio = 0.\n",
    "            return\n",
    "        \n",
    "#         max_dim = np.ceil((self.input_dim+self.output_dim)/2)\n",
    "        # max_dim = min((self.input_dim, self.output_dim))+1\n",
    "        max_dim = _get_hidden_neuron_number(self.input_dim, self.output_dim) + 1 \n",
    "        # print(\"MaxDIM\", max_dim, self.residual.hidden_dim)\n",
    "        if self.residual.hidden_dim > max_dim:\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc0)\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc1)\n",
    "            # print(\"Added\", self.residual)\n",
    "            \n",
    "        # self.residual.fc0.morph_network()\n",
    "        # self.residual.fc1.morph_network()\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        stdr = self.std_ratio\n",
    "        if torch.is_tensor(self.std_ratio):\n",
    "            stdr = self.std_ratio.min()\n",
    "            \n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{self.target_std_ratio}, s:{stdr}\")\n",
    "        if self.shortcut:\n",
    "            self.shortcut.print_network_debug(depth+1)\n",
    "        if self.residual:\n",
    "            self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        if self.residual is None:\n",
    "            return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            print(f\"{pre_string}╠════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}║    \")\n",
    "            print(f\"{pre_string}╠════╝\")\n",
    "        else:\n",
    "            print(f\"{pre_string}╚════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}     \")\n",
    "            print(f\"{pre_string}╔════╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Conv Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation, hidden_dim, post_activation=None):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "        self.post_activation = post_activation\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = hrnet0\n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = hrnet1\n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        if self.post_activation:\n",
    "            x = self.post_activation(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "            self.apnz = self.apnz/self.count\n",
    "#             self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "            self.significance = self.significance*(1-self.apnz**33) / 0.872 ## tried on desmos.\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "#         self.count = None\n",
    "\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "#             self.significance += z.pow(2).sum(dim=0)\n",
    "            self.significance += z.abs().sum(dim=0)\n",
    "#             self.significance += z.abs().pow(0.8).sum(dim=0)\n",
    "#             print(\"Current Significance \\n\", self.significance)\n",
    "#             print(f\"SIG ACT:\\n{float(self.activations.abs().mean())}\")\n",
    "#             print(f\"GRAD Mean, Std:\\n{float(grad_output[0].data.abs().mean()), float(grad_output[0].data.std())}\")\n",
    "\n",
    "            if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "                self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "        #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "                self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below=None, above=None, mask=None):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(self.significance.numel(), dtype=torch.bool)\n",
    "        if below:\n",
    "            mask = torch.logical_or(mask,self.significance<=below)\n",
    "        if above:\n",
    "            mask = torch.logical_or(mask,self.significance>above)\n",
    "            \n",
    "        print(f\"Significance:\\n{self.significance}\\nPrune:\\n{mask}\")\n",
    "        \n",
    "        self.to_remove = torch.nonzero(mask).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None:\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10)<0 \n",
    "b = torch.randn(10) > 0.5\n",
    "torch.nonzero(torch.logical_and(a,b), as_tuple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation=nn.ReLU(), post_activation=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = hrnet0.input_dim\n",
    "        self.output_dim = hrnet1.output_dim\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = None\n",
    "        self.residual = Residual_Conv_Connector(self.tree, hrnet0, hrnet1,\n",
    "                                                activation, hrnet0.output_dim, post_activation)\n",
    "        self.tree.parent_dict[self.residual] = self\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.residual.fc1.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.residual.fc1.add_output_connection(num)\n",
    "        \n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):  \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        print(f\"{pre_string}╚╗\")\n",
    "        self.residual.print_network(f\"{pre_string} \")\n",
    "        print(f\"{pre_string}╔╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut only Hierarchical Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        _wd = nn.Linear(input_dim, output_dim, bias=False).weight.data\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "        self.add_parameters_to_optimizer()\n",
    "        return\n",
    "        \n",
    "    def add_parameters_to_optimizer(self):\n",
    "        for p in self.parameters():\n",
    "#             self.tree.optimizer.state[p] = {}\n",
    "            self.tree.optimizer.param_groups[0]['params'].append(p)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## input_dim        ## output_dim\n",
    "        if x.shape[1] + self.weight.shape[1] > 0:\n",
    "            return x.matmul(self.weight.t())\n",
    "        else:\n",
    "            # print(x.shape, self.weight.shape)\n",
    "            # print(x.matmul(self.weight.t()))\n",
    "            if x.shape[1] + self.weight.shape[1] == 0:\n",
    "                return torch.zeros(x.shape[0], self.weight.shape[0], dtype=x.dtype, device=x.device)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        pass\n",
    "    \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "        pass\n",
    "            \n",
    "     \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "#         _w = self.weight.data[remaining, :]\n",
    "#         del self.weight\n",
    "#         self.weight = nn.Parameter(_w)\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        self.weight.data = self.weight.data[remaining, :]\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        ops[self.weight][_var][remaining, :]\n",
    "        \n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "#         _w = self.weight.data[:, remaining]\n",
    "#         del self.weight\n",
    "#         self.weight = nn.Parameter(_w)\n",
    "        ops = self.tree.optimizer.state\n",
    "\n",
    "        self.weight.data = self.weight.data[:, remaining]\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        ops[self.weight][_var][:, remaining]\n",
    "\n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        _w = torch.zeros(o, num, dtype=self.weight.data.dtype, device=self.weight.data.device)\n",
    "#         _w += torch.randn_like(_w)\n",
    "        _w = torch.cat((self.weight.data, _w), dim=1)\n",
    "        self.weight.data = _w\n",
    "        self.weight.grad = None\n",
    "                \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        torch.cat((ops[self.weight][_var], \\\n",
    "                                  torch.zeros(o, num, dtype=ops[self.weight][_var].dtype,\n",
    "                                              device=ops[self.weight][_var].device)), \n",
    "                                  dim=1)\n",
    "        \n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        ops = self.tree.optimizer.state\n",
    "        \n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i)\n",
    "#         stdv = torch.std(self.weight.data)\n",
    "    \n",
    "        _new = torch.empty(num, i, dtype=self.weight.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        _w = torch.cat((self.weight.data, _new), dim=0)\n",
    "        self.weight.data = _w\n",
    "        self.weight.grad = None\n",
    "        \n",
    "        \n",
    "        if len(ops[self.weight]) > 0:\n",
    "            for _var in ['step', 'exp_avg', 'exp_avg_sq']:\n",
    "                ops[self.weight][_var] = \\\n",
    "                        torch.cat((ops[self.weight][_var], \\\n",
    "                                  torch.zeros(num, i, k0, k1, dtype=ops[self.weight][_var].dtype,\n",
    "                                              device=ops[self.weight][_var].device)), \n",
    "                                  dim=0)\n",
    "        \n",
    "        # print(self.weight.data.shape)        \n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}S:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n",
    "\n",
    "\n",
    "class HierarchicalResidual_Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=None, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        ## this can be Shortcut Layer or None\n",
    "        if kernel is None:\n",
    "            self.shortcut = Shortcut(tree, self.input_dim, self.output_dim) \n",
    "        else:\n",
    "            self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, kernel, stride) \n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.shortcut.start_freezing_connection(to_freeze)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.shortcut.start_decaying_connection(to_remove)\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.shortcut.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.shortcut.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.shortcut.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.shortcut.add_output_connection(num)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        print(\"Cannot Add Hidden neuron to Shortcut Only Layer\")\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        pass\n",
    "        \n",
    "    def morph_network(self):\n",
    "        pass\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.shortcut.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree and Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree_State():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.DYNAMIC_LIST = set() ## residual parent is added, to make code effecient.\n",
    "        ## the parents which is not intended to have residual connection should not be added.\n",
    "        self.beta_std_ratio = None\n",
    "        self.beta_del_neuron = None\n",
    "        self.device = 'cpu'\n",
    "    \n",
    "        self.parent_dict = {}\n",
    "    \n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual:set = None\n",
    "        self.freeze_connection_shortcut:set = None\n",
    "        self.decay_connection_shortcut:set = None\n",
    "\n",
    "        self.decay_rate_std = 0.001\n",
    "\n",
    "        self.add_to_remove_ratio = 2.\n",
    "        \n",
    "#         self.dummy_param = nn.Parameter(torch.Tensor([0]))\n",
    "#         self.optimizer = adam_custom.Adam([self.dummy_param])\n",
    "        self.optimizer = None\n",
    "    \n",
    "        pass\n",
    "    \n",
    "    def get_decay_factor(self):\n",
    "        ratio = self.current_decay_step/self.total_decay_steps\n",
    "#         self.decay_factor = np.exp(-2*ratio)*(1-ratio)\n",
    "        ratio = np.clip(ratio, 0, 1)\n",
    "        self.decay_factor = (1-ratio)**2\n",
    "#         self.decay_factor = (1-ratio)\n",
    "        pass\n",
    "    \n",
    "    def clear_decay_variables(self):\n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual = None\n",
    "        self.freeze_connection_shortcut = None\n",
    "        self.decay_connection_shortcut = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tree_State()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constructing Hierarchical Residual CNN (Resnet Inspired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutActivation(nn.Module):\n",
    "    \n",
    "    def __init__(self, p=0.1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout2d(p) ## ok to reuse dropout !! caution\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dropout(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, device, lr, input_dim = 1, hidden_dims = [8, 16, 32, 64], output_dim = 10, final_activation=None,\n",
    "                 num_stat=5, num_std=100, decay_rate_std=0.001):\n",
    "        super().__init__()\n",
    "        self.tree = Tree_State()\n",
    "        self.tree.beta_del_neuron = (num_stat-1)/num_stat\n",
    "        self.tree.beta_std_ratio = (num_std-1)/num_std\n",
    "        self.tree.decay_rate_std = decay_rate_std\n",
    "        self.tree.device = device\n",
    "        \n",
    "        \n",
    "        dummy_param = nn.Parameter(torch.Tensor([0]))\n",
    "        ############################################################\n",
    "        self.tree.optimizer = adam_custom.Adam([dummy_param], lr=lr, weight_decay=1e-5)\n",
    "        self.tree.optimizer.param_groups[0]['params'] = []\n",
    "        ############################################################\n",
    "        \n",
    "        \n",
    "        self.root_net = None\n",
    "        self._construct_root_net(input_dim, hidden_dims, output_dim)\n",
    "#         self._construct_root_net2(input_dim, hidden_dims, output_dim)\n",
    "        \n",
    "#         self.tree.DYNAMIC_LIST.add(self.root_net)\n",
    "        self.tree.parent_dict[self.root_net] = None\n",
    "        \n",
    "        if final_activation is None:\n",
    "            final_activation = lambda x: x\n",
    "        self.non_linearity = NonLinearity(self.tree, output_dim, final_activation)\n",
    "        \n",
    "        self.neurons_added = 0\n",
    "\n",
    "        self._remove_below = None ## temporary variable\n",
    "        \n",
    "    def _construct_root_net(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        actf = DropoutActivation()\n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 16, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 16, 16, activation=actf)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2, activation=actf)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 32, 64, stride=2, activation=actf)\n",
    "        hrn3 = HierarchicalResidual_Conv(self.tree, 64, 128, stride=2, activation=actf)\n",
    "\n",
    "    \n",
    "        actf = lambda x: x\n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0, actf)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1, actf)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2, actf)\n",
    "        hrnR0123 = HierarchicalResidual_Connector(self.tree, hrnR012, hrn3, actf)\n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 128, output_dim)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "        actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR0123fc = HierarchicalResidual_Connector(self.tree, hrnR0123, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR0123fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "        morphables = [self.root_net, hrnR0123, hrnR012, hrnR01, hrnR0, hrn3, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def _construct_root_net2(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        actf = DropoutActivation()\n",
    "        \n",
    "#         hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 16, kernel=(3,3), stride=1)\n",
    "#         hrn0 = HierarchicalResidual_Conv(self.tree, 16, 16)\n",
    "#         hrn1 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2)\n",
    "#         hrn2 = HierarchicalResidual_Conv(self.tree, 32, 64, stride=2)\n",
    "\n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 32, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 32, 32)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 32, 64, stride=2)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 64, 128, stride=2)\n",
    "\n",
    "\n",
    "#         actf = lambda x: x ## don't use connector with activation\n",
    "#         actf = nn.ReLU() ## use connector with activation\n",
    "    \n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0, actf)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1, actf)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2, actf)\n",
    "        \n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 128, output_dim)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR012fc = HierarchicalResidual_Connector(self.tree, hrnR012, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR012fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "#         morphables = [hrn2, hrn1, hrn0]\n",
    "        morphables = [self.root_net, hrnR012, hrnR01, hrnR0, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.non_linearity(self.root_net(x))\n",
    "\n",
    "    def add_neurons(self, num):\n",
    "        num_stat = int(num*0.7)\n",
    "        num_random = num - num_stat\n",
    "        \n",
    "        DL = list(self.tree.DYNAMIC_LIST)\n",
    "        if num_random>0:\n",
    "            rands = torch.randint(high=len(DL), size=(num_random,))\n",
    "            index, count = torch.unique(rands, sorted=False, return_counts=True)\n",
    "            for i, idx in enumerate(index):\n",
    "                DL[idx].add_hidden_neuron(int(count[i]))\n",
    "\n",
    "        if num_stat>0:\n",
    "            del_neurons = []\n",
    "            for hr in DL:\n",
    "                if hr.residual:\n",
    "                    del_neurons.append(hr.residual.del_neurons)#+1e-7)\n",
    "                else:\n",
    "                    del_neurons.append(0.)#1e-7) ## residual layer yet not created \n",
    "            \n",
    "            prob_stat = torch.tensor(del_neurons)\n",
    "            prob_stat = torch.log(torch.exp(prob_stat)+1.)\n",
    "            m = torch.distributions.multinomial.Multinomial(total_count=num_stat,\n",
    "                                                            probs= prob_stat)\n",
    "            count = m.sample()#.type(torch.long)\n",
    "            for i, hr in enumerate(DL):\n",
    "                if count[i] < 1: continue\n",
    "                hr.add_hidden_neuron(int(count[i]))\n",
    "        \n",
    "        self.neurons_added += num \n",
    "        pass\n",
    "\n",
    "    def identify_removable_neurons(self, num=None, threshold_min=0., threshold_max=1.):\n",
    "        \n",
    "        all_sig = []\n",
    "        self.all_sig_ = []\n",
    "        \n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                all_sig.append(hr.residual.significance)\n",
    "                \n",
    "        all_sigs = torch.cat(all_sig)\n",
    "        del all_sig\n",
    "        \n",
    "#         print(\"All_sigs\", all_sigs)\n",
    "        \n",
    "#         print(\"Normalization\", (all_sigs/all_sigs.sum()).sum())\n",
    "        \n",
    "        ### Normalizes such that importance 1 is average importance\n",
    "        normalizer = float(torch.sum(all_sigs))/len(all_sigs)\n",
    "        all_sig = all_sigs/normalizer\n",
    "\n",
    "        ### Normalizes to range [0, 1]\n",
    "#         max_sig = all_sigs.max()\n",
    "#         all_sig = all_sigs/(max_sig+1e-9)\n",
    "#         print(\"All_sig\", all_sig)\n",
    "#         print(\"Sig sum\", all_sig.sum())\n",
    "        print(f\"Significance Stat:\\nMin, Max: {float(all_sig.min()), float(all_sig.max())}\")\n",
    "        print(f\"Mean, Std: {float(all_sig.mean()), float(all_sig.std())}\")\n",
    "        all_sig = all_sig[all_sig<threshold_max]\n",
    "        if len(all_sig)<1: ## if all significance is above threshold max \n",
    "            return 0, None, all_sigs\n",
    "        all_sig = torch.sort(all_sig)[0] ### sorted significance scores\n",
    "        \n",
    "        self.all_sig_ = all_sig\n",
    "        \n",
    "        if not num:num = int(np.ceil(self.neurons_added/self.tree.add_to_remove_ratio))\n",
    "        ## reset the neurons_added number if decay is started\n",
    "\n",
    "        remove_below = threshold_min\n",
    "        if num>len(all_sig):\n",
    "            remove_below = float(all_sig[-1])\n",
    "        elif num>0:\n",
    "            remove_below = float(all_sig[num-1])\n",
    "        \n",
    "        ### sig < threshold_min is always removed; whatsoever\n",
    "        if remove_below < threshold_min:\n",
    "            remove_below = threshold_min\n",
    "            \n",
    "        print(\"remove_below\", remove_below, \"true:\", remove_below*normalizer)\n",
    "        remove_below *= normalizer\n",
    "#         remove_below *= max_sig\n",
    "#         print(\"remove_below\", remove_below)\n",
    "\n",
    "        self._remove_below = remove_below\n",
    "#         self._remove_above = remove_above*normalizer\n",
    "        self._remove_above = None\n",
    "\n",
    "        return remove_below, all_sigs\n",
    "\n",
    "    def decay_neuron_start(self, decay_steps=1000):\n",
    "        if self._remove_below is None: return 0\n",
    "        \n",
    "        self.neurons_added = 0 ## resetting this variable\n",
    "        \n",
    "        self.tree.total_decay_steps = decay_steps\n",
    "        self.tree.current_decay_step = 0\n",
    "        self.tree.remove_neuron_residual = set()\n",
    "        self.tree.freeze_connection_shortcut = set()\n",
    "        self.tree.decay_connection_shortcut = set()\n",
    "        \n",
    "        count_remove = 0\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                ### always prune 1 % of the neurons randomly. It might overlap with less significant neurons\n",
    "                mask = torch.bernoulli(torch.ones_like(hr.residual.significance)*0.05).type(torch.bool)\n",
    "                count_remove += hr.residual.identify_removable_neurons(below=self._remove_below,\n",
    "                                                                       above=self._remove_above,\n",
    "                                                                       mask = mask\n",
    "                                                                      )\n",
    "        if count_remove<1:\n",
    "            self.tree.clear_decay_variables()\n",
    "        return count_remove\n",
    "    \n",
    "    def decay_neuron_step(self):\n",
    "        if self.tree.total_decay_steps is None:\n",
    "            return 0\n",
    "        \n",
    "        self.tree.current_decay_step += 1\n",
    "        \n",
    "        if self.tree.current_decay_step < self.tree.total_decay_steps:\n",
    "            self.tree.get_decay_factor()\n",
    "            for sh in self.tree.decay_connection_shortcut:\n",
    "                sh.decay_connection_step()\n",
    "            for sh in self.tree.freeze_connection_shortcut:\n",
    "                sh.freeze_connection_step()\n",
    "            return 1\n",
    "        else:\n",
    "#             if self.tree.current_decay_step == self.tree.total_decay_steps:\n",
    "#                 for sh in self.tree.decay_connection_shortcut:\n",
    "#     #                 sh.decay_connection_step()\n",
    "#                     print(\"------------------\")\n",
    "#                     print(sh.weight.data.shape, \"removing decayed; \", sh.to_remove)\n",
    "#                     print(\"Small vals\", torch.count_nonzero(sh.weight.data<1e-6))\n",
    "#                     print(\"data\", sh.weight.data[:, sh.to_remove])\n",
    "#                     print(\"grads\", sh.weight.grad[:, sh.to_remove])\n",
    "#                     print(\"initial\", sh.initial_remove)\n",
    "#                     break\n",
    "\n",
    "            \n",
    "            \n",
    "#             for rs in self.tree.remove_neuron_residual:\n",
    "#                 rs.remove_decayed_neurons()\n",
    "                \n",
    "#             self.tree.clear_decay_variables()\n",
    "#             self.maintain_network()\n",
    "\n",
    "            ### need to decay and freeze all the time\n",
    "            for sh in self.tree.decay_connection_shortcut:\n",
    "                sh.decay_connection_step()\n",
    "            for sh in self.tree.freeze_connection_shortcut:\n",
    "                sh.freeze_connection_step()\n",
    "            return -1\n",
    "        \n",
    "    def remove_decayed_neurons(self):\n",
    "        for rs in self.tree.remove_neuron_residual:\n",
    "            rs.remove_decayed_neurons()\n",
    "                \n",
    "        self.tree.clear_decay_variables()\n",
    "        self.maintain_network()\n",
    "        return\n",
    "\n",
    "    def compute_del_neurons(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.compute_del_neurons()\n",
    "    \n",
    "    def maintain_network(self):\n",
    "        self.root_net.maintain_shortcut_connection()\n",
    "        self.root_net.morph_network()\n",
    "        \n",
    "    def start_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.start_computing_significance()\n",
    "\n",
    "    def finish_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.finish_computing_significance()\n",
    "            \n",
    "    def print_network_debug(self):\n",
    "        self.root_net.print_network_debug(0)\n",
    "        \n",
    "    def print_network(self):\n",
    "        print(self.root_net.input_dim)\n",
    "        self.root_net.print_network()\n",
    "        print(\"│\")\n",
    "        print(self.root_net.output_dim)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dycnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_train = transforms.Compose([\n",
    "#     transforms.RandomCrop(size=32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "#         std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "#     ),\n",
    "# ])\n",
    "\n",
    "# cifar_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(\n",
    "#         mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "#         std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "#     ),\n",
    "# ])\n",
    "\n",
    "# train_dataset = datasets.CIFAR10(root=\"../../_Datasets/cifar10/\", train=True, download=False, transform=cifar_train)\n",
    "# test_dataset = datasets.CIFAR10(root=\"../../_Datasets/cifar10/\", train=False, download=False, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train = transforms.Compose([\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4865, 0.4409],\n",
    "        std=[0.2009, 0.1984, 0.2023],\n",
    "    ),\n",
    "])\n",
    "\n",
    "cifar_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5071, 0.4865, 0.4409],\n",
    "        std=[0.2009, 0.1984, 0.2023],\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root=\"../../_Datasets/cifar100/\", train=True, download=False, transform=cifar_train)\n",
    "test_dataset = datasets.CIFAR100(root=\"../../_Datasets/cifar100/\", train=False, download=False, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameters\n",
    "learning_rate = 0.0003\n",
    "# learning_rate = 0.00003\n",
    "\n",
    "num_add_neuron = 50 #50#25#10\n",
    "num_decay_steps = int(len(train_loader)*2)#3\n",
    "\n",
    "remove_above = 12 #10\n",
    "threshold_max = 0.5\n",
    "threshold_min = 0.01\n",
    "\n",
    "train_epoch_min = 1 #1\n",
    "train_epoch_max = 15 #10 #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decay_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet = Dynamic_CNN(device, learning_rate, output_dim=100).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = dynet.tree.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391, 79)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.add_to_remove_ratio = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.optimizer.param_groups[0]['lr'] = learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = '00.3'\n",
    "name = 'dynCNN_reuse_optim_v1_c100'\n",
    "exp_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = {\n",
    "    'learning_rate':learning_rate,\n",
    "    'num_add_neuron':num_add_neuron,\n",
    "    'num_decay_steps':num_decay_steps,\n",
    "    'remove_above':remove_above,\n",
    "    'threshold_max':threshold_max,\n",
    "    'threshold_min':threshold_min,\n",
    "    'train_epoch_min':train_epoch_min,\n",
    "    'train_epoch_max':train_epoch_max,\n",
    "    'add_to_remove_ratio':dynet.tree.add_to_remove_ratio,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_json = f'hyperparameters/{index}_hyp_exp_{exp_index}.json'\n",
    "with open(hyp_json, 'w') as fp:\n",
    "    json.dump(hyp, fp, indent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoTrainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_func = None\n",
    "        self.adding_func = None\n",
    "        self.pruning_func = None\n",
    "        self.maintainance_func = None\n",
    "        self.extra_func = None\n",
    "        \n",
    "        self.log_func = None\n",
    "        \n",
    "    def loop(self, count = 15):\n",
    "        cb = count\n",
    "        for i in range(count):\n",
    "            if i>-0.1:\n",
    "                self.adding_func()\n",
    "            else:\n",
    "#                 global optimizer, warmup\n",
    "                dynet.print_network()    \n",
    "                \n",
    "                reset_optimizer()\n",
    "#                 optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#                 optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#                 warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader))\n",
    "            \n",
    "            \n",
    "            self.training_func()\n",
    "\n",
    "            self.log_func(i)\n",
    "            if self.extra_func:\n",
    "                self.extra_func()\n",
    "            \n",
    "            if i>-0.1:\n",
    "                self.pruning_func()\n",
    "            self.maintainance_func()\n",
    "            \n",
    "            self.log_func(i)\n",
    "            if self.extra_func:\n",
    "                self.extra_func()\n",
    "            \n",
    "            print(f\"=====================\")\n",
    "            print(f\"===LOOPS FINISHED :{i} ===\")\n",
    "            print(f\"Pausing for 2 second to give user time to STOP PROCESS\")\n",
    "            time.sleep(2)\n",
    "        self.training_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when to stop training functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeff(num_iter, coeff0, coeff1, coeff2, coeff_opt, loss_list):\n",
    "    if len(loss_list)<10: return np.array([0]), np.array([0]), float(coeff0.data.cpu()[0])\n",
    "    \n",
    "    _t = torch.tensor(loss_list)\n",
    "    _t = (_t - _t[-1])/(_t[0]-_t.min()) ## normalize to make first point at 1 and last at 0 \n",
    "    _t = torch.clamp(_t, -1.1, 1.1)\n",
    "    _x = torch.linspace(0, 1, steps=len(_t))\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        coeff_opt.zero_grad()\n",
    "        _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "        _loss = ((_y - _t)**2).mean()\n",
    "        _loss.backward()\n",
    "        coeff_opt.step()\n",
    "\n",
    "        coeff0.data = torch.clamp(coeff0.data, -20., 20.)\n",
    "        coeff1.data = torch.clamp(coeff1.data, 0.7, 2.)\n",
    "        coeff2.data = torch.clamp(coeff2.data, -0.2,0.1)\n",
    "        \n",
    "    if torch.isnan(coeff0.data[0]):\n",
    "        coeff0.data[0] = 0.\n",
    "        coeff1.data[0] = 0.\n",
    "        coeff2.data[0] = 1. ## this gives signal\n",
    "        \n",
    "    _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "    return _x.numpy(), _t.numpy(), _y.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global variables\n",
    "optimizer = None\n",
    "warmup = None\n",
    "coeff_opt = None\n",
    "\n",
    "loss_all = []\n",
    "accs_all = []\n",
    "accs_test = []\n",
    "events_all = []\n",
    "\n",
    "## for adam optimizer = \n",
    "# learning_rate *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reset_optimizer():\n",
    "#     global optimizer, warmup\n",
    "# #     optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "#     optimizer = adam_custom.Adam(dynet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# #     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "#     warmup = WarmupLR_Polynomial(optimizer, 0.5, len(train_loader), power=2)\n",
    "# #     warmup = WarmupLR_Polynomial(optimizer, 10/len(train_loader), len(train_loader))\n",
    "# #     get_bn_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_optimizer():\n",
    "    global dynet, warmup, optimizer\n",
    "    \n",
    "#     optimizer = dynet.tree.optimizer   \n",
    "#     dynet.tree.optimizer = adam_custom.Adam(dynet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "    ## there are no param groups, but consider there are len=1\n",
    "    pg = dynet.tree.optimizer.param_groups\n",
    "    for i in range(len(pg)):\n",
    "        pg[i]['lr'] = learning_rate\n",
    "        \n",
    "    warmup = WarmupLR_Polynomial(dynet.tree.optimizer, 2, len(train_loader), power=2)\n",
    "#     copy_optimizer()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reset_optimizer():\n",
    "#     global dynet, warmup, optimizer\n",
    "    \n",
    "#     def get_cosine_lr(lr, epoch, tmax):\n",
    "#         return (np.cos(epoch/tmax*np.pi)*0.5+0.5)*lr\n",
    "    \n",
    "#     ## there are no param groups, but consider there are len=1\n",
    "#     pg = dynet.tree.optimizer.param_groups\n",
    "#     for i in range(len(pg)):\n",
    "#         pg[i]['lr'] = get_cosine_lr(learning_rate, len(accs_all), tmax=400)\n",
    "        \n",
    "#     warmup = WarmupLR_Polynomial(dynet.tree.optimizer, 0.0, len(train_loader), power=2)\n",
    "    \n",
    "    \n",
    "# #     copy_optimizer()\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def copy_optimizer():\n",
    "#     global dynet\n",
    "#     old_optim = dynet.tree.optimizer\n",
    "#     new_optim = adam_custom.Adam(dynet.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "#     found=False\n",
    "#     for p in dynet.parameters():\n",
    "#         for _p in old_optim.param_groups[0]['params']:\n",
    "#             if _p is p:\n",
    "#                 found = True\n",
    "#                 new_optim.state[p] = old_optim.state[p]\n",
    "#     if not found:\n",
    "#         raise ValueError(\"Parameter could not be found\")\n",
    "    \n",
    "#     dynet.tree.optimizer = new_optim\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = dynet.tree.optimizer\n",
    "\n",
    "# copy_optimizer(optimizer)\n",
    "\n",
    "# dynet.tree.optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupLR_Polynomial():\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_epoch, num_batch_in_epoch, power=5):\n",
    "        self.warmup_epoch = warmup_epoch\n",
    "        self.optimizer = optimizer\n",
    "        self.num_batch = num_batch_in_epoch\n",
    "        self.steps = 0\n",
    "        self.power = power\n",
    "        self.backup_lr = []\n",
    "        for group in self.optimizer.param_groups:\n",
    "            self.backup_lr.append(float(group['lr']))\n",
    "        \n",
    "    def step(self):\n",
    "        self.steps += 1\n",
    "        steps = self.steps/self.num_batch\n",
    "        \n",
    "        factor = 1\n",
    "        warming = False\n",
    "        if steps<self.warmup_epoch:\n",
    "            factor = (steps/self.warmup_epoch)**self.power\n",
    "            warming = True\n",
    "            \n",
    "        for group, bkp_lr in zip(self.optimizer.param_groups, self.backup_lr):\n",
    "            group['lr'] = bkp_lr*factor\n",
    "        \n",
    "        return warming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neurons_func():\n",
    "    global optimizer, warmup, added, events_all\n",
    "    \n",
    "    ######################################33\n",
    "    ################# CHECK IF ADDING NEURONS CHANGES ACCURACY #####################\n",
    "#     with torch.no_grad():\n",
    "#         corrects = 0\n",
    "#         for test_x, test_y in train_loader:\n",
    "#             test_x  = test_x.to(device)\n",
    "#             yout = dynet.forward(test_x)\n",
    "#             outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "#             correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "#             corrects += correct\n",
    "#         accs_all.append(corrects/len(train_dataset)*100)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         corrects = 0\n",
    "#         dynet.eval()\n",
    "#         for test_x, test_y in test_loader:\n",
    "#             test_x  = test_x.to(device)\n",
    "#             yout = dynet.forward(test_x)\n",
    "#             outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "#             correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "#             corrects += correct\n",
    "#         dynet.train()\n",
    "#         accs_test.append(corrects/len(test_dataset)*100)\n",
    "    ######################################33\n",
    "    \n",
    "    \n",
    "    ### number of neurons\n",
    "    count = 0\n",
    "    for hr in dynet.tree.DYNAMIC_LIST:\n",
    "        if hr.residual:\n",
    "            count += hr.residual.hidden_dim\n",
    "    ## add more neurons relatively (+x%)\n",
    "    adding = num_add_neuron+int(0.07*count)\n",
    "    dynet.add_neurons(adding)\n",
    "    print(f\"Adding {adding} Neurons\")\n",
    "    added = adding\n",
    "    dynet.print_network()    \n",
    "    \n",
    "    reset_optimizer()\n",
    "#     optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#     warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader), power=1)\n",
    "    \n",
    "            \n",
    "    ######################################33\n",
    "    if len(accs_all)>0:\n",
    "        \n",
    "#         accs_all.append(accs_all[-1])\n",
    "        with torch.no_grad():\n",
    "            corrects = 0\n",
    "            for test_x, test_y in train_loader:\n",
    "                test_x  = test_x.to(device)\n",
    "                yout = dynet.forward(test_x)\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                corrects += correct\n",
    "            accs_all.append(corrects/len(train_dataset)*100)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            corrects = 0\n",
    "            dynet.eval()\n",
    "            for test_x, test_y in test_loader:\n",
    "                test_x  = test_x.to(device)\n",
    "                yout = dynet.forward(test_x)\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                corrects += correct\n",
    "            dynet.train()\n",
    "            accs_test.append(corrects/len(test_dataset)*100)\n",
    "    ######################################33\n",
    "    \n",
    "    events_all.append((len(accs_all), \"neurons added\"))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_children(module):\n",
    "    child = list(module.children())\n",
    "    if len(child) == 0:\n",
    "        return [module]\n",
    "    children = []\n",
    "    for ch in child:\n",
    "        grand_ch = get_children(ch)\n",
    "        children+=grand_ch\n",
    "    return children\n",
    "\n",
    "bn_params = []\n",
    "def get_bn_params():\n",
    "    global dynet, bn_params\n",
    "    bn_params = []\n",
    "    for module in get_children(dynet):\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            bn_params.append(module.weight)\n",
    "            bn_params.append(module.bias)\n",
    "            \n",
    "def clip_bn_weight_grads(val=0.05):\n",
    "    global bn_params\n",
    "    for bnp in bn_params:\n",
    "        bnp.grad = torch.clamp(bnp.grad, -val, val)\n",
    "        \n",
    "def get_bn_params_grads(val=0.05):\n",
    "    global bn_params\n",
    "    for bnp in bn_params:\n",
    "        if bnp.grad.abs().max() > val:\n",
    "            print(\"Batch Norm receiving high gradients!!\")\n",
    "            print(bnp.grad)\n",
    "            print()\n",
    "            \n",
    "def decay_bn_params(val=5e-5):\n",
    "    global bn_params\n",
    "    for bnp in bn_params:\n",
    "        bnp.data -= torch.sign(bnp.data)*val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_step(xx, yy):\n",
    "#     global dynet\n",
    "    \n",
    "#     yout = dynet(xx)\n",
    "#     loss = criterion(yout, yy) #+ dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "\n",
    "#     dynet.tree.optimizer.zero_grad(set_to_none = True)\n",
    "    \n",
    "#     loss.backward(create_graph=False, retain_graph=False)\n",
    "#     clip_bn_weight_grads()\n",
    "\n",
    "#     dynet.tree.optimizer.step()\n",
    "# #     dynet.zero_grad(True)\n",
    "    \n",
    "#     return yout, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_network_func():\n",
    "    global optimizer, warmup, loss_all, accs_all\n",
    "    \n",
    "    coeff0 = torch.zeros(1, requires_grad=True)\n",
    "    coeff1 = torch.zeros(1, requires_grad=True)\n",
    "    coeff2 = torch.zeros(1, requires_grad=True)\n",
    "    coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "    loss_list = []\n",
    "    prev_loss = None\n",
    "    beta_loss = (1000-1)/1000\n",
    "    loss_ = []\n",
    "    optimizer = dynet.tree.optimizer\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    breakall=False\n",
    "    \n",
    "    steps_ = -1\n",
    "    for epoch in range(train_epoch_max):\n",
    "        \n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "        for train_x, train_y in train_loader:\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "            steps_ += 1\n",
    "            \n",
    "#             dynet.decay_neuron_step()\n",
    "            dynet.tree.std_loss = 0.    \n",
    "\n",
    "            yout = dynet(train_x)\n",
    "            loss = criterion(yout, train_y) #+ dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "                    \n",
    "#             dynet.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=False)\n",
    "            \n",
    "            clip_bn_weight_grads()\n",
    "            optimizer.step()\n",
    "#             yout, loss = train_step(train_x, train_y)\n",
    "            \n",
    "            warmup.step()\n",
    "            \n",
    "            if steps_>100:\n",
    "                prev_loss = (1-beta_loss)*float(loss)+beta_loss*prev_loss\n",
    "                loss_list.append(prev_loss)\n",
    "            elif steps_ == 100:\n",
    "                loss_.append(float(loss))\n",
    "                prev_loss = np.mean(loss_)\n",
    "                loss_ = []\n",
    "            else:\n",
    "                loss_.append(float(loss))\n",
    "            \n",
    "            \n",
    "#             decay_bn_params()\n",
    "            \n",
    "            outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "            targets = train_y.data.cpu().numpy()\n",
    "\n",
    "            correct = (outputs == targets).sum()\n",
    "            train_acc += correct\n",
    "            train_count += len(outputs)\n",
    "\n",
    "            if steps_%100 == 0 and steps_>0:\n",
    "                if len(loss_list)>0:\n",
    "                    max_indx = np.argmax(loss_list)\n",
    "                    loss_list = loss_list[max_indx:]\n",
    "    #                 loss_all.append(float(loss))\n",
    "                \n",
    "                _x, _t, _y = update_coeff(50, coeff0, coeff1, coeff2, coeff_opt, loss_list)\n",
    "                _c = float(coeff0.data.cpu()[0])\n",
    "    #             if coeff2.data[0] > 0.5: ## this is a signal to reset optimizer\n",
    "                coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "                _info = f'ES: {epoch}:{steps_}, coeff:{_c:.3f}/{-5}, \\nLoss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "\n",
    "                ax.clear()\n",
    "                if len(_x)>0:\n",
    "                    ax.plot(_x, _t, c='c')\n",
    "                    ax.plot(_x, _y, c='m')\n",
    "                xmin, xmax = ax.get_xlim()\n",
    "                ymin, ymax = ax.get_ylim()\n",
    "                ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                ax2.clear()\n",
    "                if len(accs_all)>0:\n",
    "                    acc_tr = accs_all\n",
    "                    acc_te = accs_test\n",
    "                    if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                    if len(acc_te)>20: acc_te = acc_te[-20:]\n",
    "                    ax2.plot(acc_tr, marker='.', label=\"train\")\n",
    "                    ax2.plot(acc_te, marker='.', label=\"test\")\n",
    "                    ax2.legend(loc=\"lower right\")\n",
    "                    \n",
    "                    ymin, ymax = ax2.get_ylim()\n",
    "                    ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                    ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_te):.3f} end{acc_te[-1]:.3f}\")\n",
    "\n",
    "                \n",
    "                fig.canvas.draw()\n",
    "                plt.savefig(f\"./output/logs/_{index}_temp_train_plot.png\")\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                if _c < -5 and epoch>train_epoch_min: \n",
    "                    breakall=True\n",
    "                    break\n",
    "                    \n",
    "        if not breakall:\n",
    "            accs_all.append(train_acc/train_count*100.)\n",
    "            with torch.no_grad():\n",
    "                corrects = 0\n",
    "                dynet.eval()\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x  = test_x.to(device)\n",
    "                    yout = dynet.forward(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                    correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                    corrects += correct\n",
    "                dynet.train()\n",
    "                accs_test.append(corrects/len(test_dataset)*100)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_func():\n",
    "    global optimizer, warmup\n",
    "    reset_optimizer()\n",
    "#     optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#     warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader), power=0.5)\n",
    "    \n",
    "    optimizer = dynet.tree.optimizer\n",
    "    \n",
    "    \n",
    "    print(f\"Computing Network Siginificance\")\n",
    "    \n",
    "    dynet.eval()\n",
    "    dynet.start_computing_significance()\n",
    "\n",
    "    for train_x, train_y in train_loader:\n",
    "        train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "        dynet.tree.std_loss = 0.    \n",
    "        yout = dynet(train_x)\n",
    "#         yout.backward(gradient=torch.ones_like(yout))\n",
    "        loss = criterion(yout, train_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=False)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    dynet.finish_computing_significance()\n",
    "    \n",
    "    dynet.identify_removable_neurons(num=None,\n",
    "                                 threshold_min = threshold_min,\n",
    "                                 threshold_max = threshold_max)\n",
    "    num_remove = dynet.decay_neuron_start(decay_steps=num_decay_steps)\n",
    "    \n",
    "    dynet.train()\n",
    "    \n",
    "    if num_remove > 0:\n",
    "#     if num_remove < 0:\n",
    "        decayed = False\n",
    "        print(f\"pruning {num_remove} neurons.\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,4))\n",
    "        ax = fig.add_subplot(121)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        \n",
    "        loss_list = []\n",
    "        steps_ = -1\n",
    "        breakall=False\n",
    "\n",
    "        for epoch in range(train_epoch_max+int(np.ceil(num_decay_steps/len(train_loader)))):\n",
    "            loss_ = []\n",
    "            train_acc = 0\n",
    "            train_count = 0\n",
    "            \n",
    "            for train_x, train_y in train_loader:\n",
    "                train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "                steps_ += 1\n",
    "                \n",
    "#                 with torch.no_grad():\n",
    "                ret = dynet.decay_neuron_step()\n",
    "                dynet.tree.std_loss = 0.    \n",
    "        \n",
    "                if ret == -1 and not decayed:\n",
    "                    events_all.append((len(accs_all), \"neurons decayed\"))\n",
    "                    decayed = True\n",
    "                \n",
    "#                     copy_optimizer()\n",
    "#                     breakall = True\n",
    "#                     break\n",
    "\n",
    "                yout = dynet(train_x)\n",
    "                loss = criterion(yout, train_y) #+ dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "                \n",
    "                optimizer.zero_grad() ##set_to_none = True\n",
    "                loss.backward(retain_graph=False)\n",
    "                clip_bn_weight_grads()\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                loss = float(loss)\n",
    "#                 yout, loss = train_step(train_x, train_y)\n",
    "                                \n",
    "                warmup.step()\n",
    "#                 decay_bn_params()\n",
    "                loss_.append(float(loss))\n",
    "                \n",
    "\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                targets = train_y.data.cpu().numpy()\n",
    "                correct = (outputs == targets).sum()\n",
    "                train_acc += correct\n",
    "                train_count += len(outputs)\n",
    "\n",
    "#                 dynet.decay_neuron_step()\n",
    "                \n",
    "                if steps_%50 == 0 and steps_>0:\n",
    "                    loss = np.mean(loss_)\n",
    "                    loss_ = []\n",
    "                    loss_list.append(loss)\n",
    "                \n",
    "                if steps_%100 == 0 and steps_>0:\n",
    "                    \n",
    "                    _info = f'ES: {epoch}:{steps_}, Loss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "#                     print(_info)\n",
    "                    ax.clear()\n",
    "                    out = (yout.data.cpu().numpy()>0.5).astype(int)\n",
    "                    ax.plot(loss_list)\n",
    "                    \n",
    "                    xmin, xmax = ax.get_xlim()\n",
    "                    ymin, ymax = ax.get_ylim()\n",
    "                    ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                    ax2.clear()\n",
    "                    if len(accs_all)>0:\n",
    "                        acc_tr = accs_all\n",
    "                        acc_te = accs_test\n",
    "                        if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                        if len(acc_te)>20: acc_te = acc_te[-20:]\n",
    "                        ax2.plot(acc_tr, marker='.', label=\"train\")\n",
    "                        ax2.plot(acc_te, marker='.', label=\"test\")\n",
    "                        ax2.legend(loc=\"lower right\")\n",
    "\n",
    "                        ymin, ymax = ax2.get_ylim()\n",
    "                        ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                        ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_te):.3f} end{acc_te[-1]:.3f}\")\n",
    "\n",
    "                    \n",
    "                    fig.canvas.draw()\n",
    "                    plt.savefig(f\"./output/logs/_{index}_temp_prune_plot.png\")\n",
    "#                     plt.pause(0.01)\n",
    "#                     print(\"\\n\")\n",
    "                    \n",
    "#                 if steps_>num_decay_steps+int(num_decay_steps/2): breakall=True\n",
    "#                 if steps_>(num_decay_steps+int(len(train_loader)*2.05)): breakall=True\n",
    "#                 if breakall: break\n",
    "\n",
    "#             if steps_>=(num_decay_steps):\n",
    "            if epoch >= (num_decay_steps/len(train_loader))+1.99:\n",
    "                breakall = True\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                corrects = 0\n",
    "                ret = dynet.decay_neuron_step()\n",
    "                dynet.eval()\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x  = test_x.to(device)\n",
    "                    yout = dynet.forward(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                    correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                    corrects += correct\n",
    "                dynet.train()\n",
    "                accs_test.append(corrects/len(test_dataset)*100)        \n",
    "\n",
    "            accs_all.append(train_acc/train_count*100.)\n",
    "\n",
    "#             if not breakall:\n",
    "#                 accs_all.append(train_acc/train_count*100.)\n",
    "#             else:\n",
    "#                 accs_all.append(accs_all[-1])\n",
    "#                 break\n",
    "            if breakall: break\n",
    "\n",
    "        plt.close()\n",
    "    \n",
    "    dynet.remove_decayed_neurons()\n",
    "    events_all.append((len(accs_all), \"neurons pruned\"))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintain_network():\n",
    "    dynet.compute_del_neurons()\n",
    "    dynet.maintain_network()\n",
    "    dynet.print_network()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network_stat(loop_indx):\n",
    "    stdout = sys.stdout\n",
    "    s = io.StringIO(newline=\"\")\n",
    "    sys.stdout = s\n",
    "    dynet.print_network()\n",
    "    sys.stdout = stdout\n",
    "    s.seek(0)\n",
    "    # prints = s.read()\n",
    "    architecture = s.getvalue()\n",
    "    s.close()\n",
    "    \n",
    "    ### number of neurons\n",
    "    count = 0\n",
    "    for hr in dynet.tree.DYNAMIC_LIST:\n",
    "        if hr.residual:\n",
    "            count += hr.residual.hidden_dim\n",
    "    \n",
    "    with open(f\"output/logs/{index}_{name}_log_{exp_index}.txt\", \"a+\") as f:\n",
    "        ### Print the configuration at top.\n",
    "#         if loop_indx == 0:\n",
    "        \n",
    "        if loop_indx >= 0:\n",
    "    \n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "            \n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%B %d, %Y @ %H:%M:%S\")\n",
    "            f.write(f\"DateTime: {dt_string}\")\n",
    "            \n",
    "            f.write(f\"num_add_neuron :{num_add_neuron}\\n add_to_remove_ratio :{dynet.tree.add_to_remove_ratio}\\n\")\n",
    "            f.write(f\"learning_rate :{learning_rate}\\n num_decay_steps :{num_decay_steps}\\n\")\n",
    "            f.write(f\"threshold_max :{threshold_max}\\n threshold_min :{threshold_min}\\n\")\n",
    "            f.write(f\"train_epoch_min :{train_epoch_min}\\n threshold_max :{train_epoch_max}\\n\")\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "        \n",
    "        f.write(f\"####################| Loop:{loop_indx} | Epoch: {len(accs_all)} \\n\")\n",
    "        num_params = sum(p.numel() for p in dynet.parameters())\n",
    "        num_trainable = sum(p.numel() for p in dynet.parameters() if p.requires_grad)\n",
    "        f.write(f\"| Dynamic Neurons:{count} | Total Parameters: {num_params} | Trainable Parameters: {num_trainable}\\n\")\n",
    "        f.write(f\"| Train Acc:{accs_all[-1]:.3f} | Test Acc: {accs_test[-1]:.3f}\\n\")\n",
    "        f.write(architecture)\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyperparameters_from_json():\n",
    "    global learning_rate,num_add_neuron,num_decay_steps,\\\n",
    "            remove_above,threshold_max,threshold_min,train_epoch_min,train_epoch_max,\\\n",
    "            dynet\n",
    "    with open(hyp_json, 'r') as fp:\n",
    "        hyps = json.load(fp)\n",
    "        learning_rate = hyps['learning_rate']\n",
    "        num_add_neuron = hyps['num_add_neuron']\n",
    "        num_decay_steps = hyps['num_decay_steps']\n",
    "        threshold_max = hyps['threshold_max']\n",
    "        threshold_min = hyps['threshold_min']\n",
    "        train_epoch_min = hyps['train_epoch_min']\n",
    "        train_epoch_max = hyps['train_epoch_max']\n",
    "        dynet.tree.add_to_remove_ratio = hyps['add_to_remove_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accs_save():\n",
    "    plt.plot(accs_all, label=\"train\")\n",
    "    plt.plot(accs_test, label=\"test\")\n",
    "    ymin, ymax = plt.gca().get_ylim()\n",
    "    plt.text(0, 0.8*ymin+0.2*ymax, f\"Train-> max:{max(accs_all):.3f} end:{accs_all[-1]:.3f} \\nTest-> max:{max(accs_test):.3f} end:{accs_test[-1]:.3f}\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"output/plots/{index}_{name}_cifar100_{exp_index}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    with open(f\"output/plots/{index}_{name}_cifar100_{exp_index}_event_dict.json\", 'w') as f:\n",
    "        d = {\n",
    "            \"train_accs\":accs_all,\n",
    "            \"test_accs\":accs_test,\n",
    "            \"event_dict\":events_all,\n",
    "        }\n",
    "        json.dump(d, f, indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_func():\n",
    "    load_hyperparameters_from_json()\n",
    "    plot_accs_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set all functions and begin automated loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AutoTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.adding_func = add_neurons_func\n",
    "trainer.training_func = training_network_func\n",
    "trainer.pruning_func = pruning_func\n",
    "trainer.maintainance_func = maintain_network\n",
    "trainer.log_func = save_network_stat\n",
    "trainer.extra_func = extra_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_neurons_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     16\n",
      "    ╔╝\n",
      "    16\n",
      "   ╔╝\n",
      "   32\n",
      "  ╔╝\n",
      "  64\n",
      " ╔╝\n",
      " 128\n",
      "╔╝\n",
      "│\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 67 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    6\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    21\n",
      "    ╠════╗\n",
      "    ║    8\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   40\n",
      "   ╠════╗\n",
      "   ║    7\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  72\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 133\n",
      "╔╝\n",
      "│\n",
      "100\n",
      "Computing Network Siginificance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsuman/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significance Stat:\n",
      "Min, Max: (0.22380323708057404, 4.426582336425781)\n",
      "Mean, Std: (1.0, 0.9323133230209351)\n",
      "remove_below 0.2608362138271332 true: 10.014231497516471\n",
      "Significance:\n",
      "tensor([39.7486, 59.2894, 44.9194, 58.8616, 68.1051, 63.1673, 53.3258],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 94.9595, 104.6909,  65.5081, 108.3864, 119.2151,  66.5272,  61.8384,\n",
      "        100.5653,  80.3782, 112.0496,  74.8113,  74.6065, 110.4939,  91.7815,\n",
      "        115.2610,  87.4883,  70.4332,  32.1204,  93.7566, 136.4382, 132.8173,\n",
      "         52.2519,  52.3999,  63.3256,  48.1672,  46.2661,  59.3031,  24.6271],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([13.6113, 14.2317, 10.2093, 10.8445, 11.8770, 10.3046, 11.5789, 11.6281,\n",
      "        12.5997, 12.3325,  9.0740, 10.0030, 11.1248, 11.1276, 13.3998, 13.4096,\n",
      "        11.8483, 15.2818, 13.2194, 10.0402, 13.7009,  9.9485,  9.4535,  9.4044,\n",
      "         9.7362, 12.0654, 18.7595, 12.6467, 11.7031, 11.1511, 10.8873,  9.7037,\n",
      "        12.9261, 12.7154, 12.4924, 10.9343, 12.4584, 13.1900, 10.2133, 10.9113,\n",
      "        12.1734, 10.7046,  9.1425, 10.0142, 12.8748, 10.8313, 13.7720, 11.2759,\n",
      "         8.8404, 12.5130, 10.7994, 10.5574, 11.2483,  9.9376, 11.6921, 15.3151,\n",
      "         9.6189, 10.7436, 10.4156, 12.7339, 13.2637, 10.9407, 11.4078, 11.8140,\n",
      "        12.0646,  9.9412, 12.5159, 10.7406, 13.2588, 10.0388, 12.1992, 11.4185,\n",
      "         9.6200, 12.6555, 12.0096, 10.8757, 11.2321, 11.1616,  9.0777,  9.2061,\n",
      "        10.1485, 10.4755, 13.1433, 10.0739, 12.3572,  9.4046, 10.0479,  9.7960,\n",
      "        10.1354, 10.7066, 11.1782,  9.4168, 10.6900, 10.6066,  9.7074, 11.1794,\n",
      "        12.1227, 12.1484, 11.9174, 16.0506,  9.7563, 11.4840, 11.4109, 10.1739,\n",
      "        13.1577, 13.2893, 10.9286, 12.0361, 10.2398,  9.4628,  9.5636, 10.3570,\n",
      "        11.1451, 10.1262, 12.9991, 10.0458,  9.4660, 11.7103,  8.5924, 15.6870,\n",
      "         9.8270, 10.4474, 11.8094, 11.2144,  9.5580, 10.1995, 11.2218, 10.8820,\n",
      "        18.9382, 19.9844, 19.3106, 20.7447, 15.3885], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True, False,  True, False, False, False, False,  True, False,\n",
      "        False,  True,  True,  True,  True, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True, False, False, False, False,  True, False,\n",
      "        False, False, False,  True, False, False,  True, False, False, False,\n",
      "        False, False,  True, False, False,  True, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False, False,  True,  True,\n",
      "        False, False, False, False, False,  True, False,  True, False, False,\n",
      "        False,  True, False, False,  True, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False,  True,\n",
      "         True, False, False, False, False, False,  True, False,  True,  True,\n",
      "         True, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([42.7816, 58.7914, 53.4204, 59.4718, 44.7902, 61.6190, 63.8563, 79.7214],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 31.8154, 165.5954,  65.5911, 119.2132,  26.3548,  89.7352],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([120.5784,  64.7174,  98.0210, 169.9489, 145.0067,  96.4492,  90.7817,\n",
      "        130.9243,  88.8266,  72.0451, 156.8179,  79.0174, 147.6525,  82.4457,\n",
      "         82.5785,  65.2355,  59.6313, 122.5749, 137.6108, 146.8884, 161.5013],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 37.8758,  88.3342,  80.2693,  41.9066,  44.1797,  91.0208,  41.3863,\n",
      "         50.5549,  64.3968,  42.5692,  16.2907,  96.8931,  90.7220,  37.3498,\n",
      "         30.8033,  51.1821,  73.6554, 130.2720,  65.1262,  40.6836,  31.8397,\n",
      "         49.4215,  42.8471,  49.6024,  42.2350,  46.9913,  41.2483,  15.9888,\n",
      "         45.1865,  27.6617,  87.7552,  30.5433,  63.4573,  49.7835, 137.5758,\n",
      "         90.6913,  60.7748,  70.8787,  47.4232,  74.2970], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([15.7882, 23.5176, 40.9466, 40.2273, 34.4552, 19.8958, 21.1935, 21.5463,\n",
      "        23.0419, 61.5468, 21.4257, 33.8125, 18.4350, 20.1629, 34.7346, 18.6010,\n",
      "        15.8363, 39.5279, 16.1611, 42.9631, 33.8348, 35.1306, 23.2715, 15.7671,\n",
      "        24.6486, 17.4412, 12.2248, 38.0275, 21.9759, 20.5776, 18.4846, 35.2888,\n",
      "        14.3582, 21.8486, 13.8844, 92.9305, 35.8808, 27.3846, 32.8654, 30.1872,\n",
      "        38.6202, 24.6984, 47.8119, 18.6333, 18.7737, 14.4955, 20.3362, 19.5185,\n",
      "        12.6310, 19.6282, 13.1224, 18.2897, 13.5035, 21.3681, 47.9416, 26.5837,\n",
      "        28.8230, 14.0329, 23.3245, 11.6795, 24.3653, 29.0650, 68.4945, 37.5427,\n",
      "        69.9440, 26.2600, 74.3144, 35.0984, 57.8037, 47.0269, 49.7607, 88.6367],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([31.1889, 66.4704, 44.2629, 44.9486, 45.7070, 45.1493, 45.5036, 68.2545],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "pruning 40 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     25\n",
      "     ╠════╗\n",
      "     ║    6\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    21\n",
      "    ╠════╗\n",
      "    ║    8\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   38\n",
      "   ╠════╗\n",
      "   ║    7\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  69\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 101\n",
      "╔╝\n",
      "│\n",
      "100\n",
      "=====================\n",
      "===LOOPS FINISHED :0 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 69 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     31\n",
      "     ╠════╗\n",
      "     ║    12\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    27\n",
      "    ╠════╗\n",
      "    ║    21\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   47\n",
      "   ╠════╗\n",
      "   ║    13\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  78\n",
      "  ╠════╗\n",
      "  ║    19\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 104\n",
      "╔╝\n",
      "│\n",
      "100\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.17220480740070343, 7.292861461639404)\n",
      "Mean, Std: (1.0, 0.9290634989738464)\n",
      "remove_below 0.28331610560417175 true: 14.821784937806527\n",
      "Significance:\n",
      "tensor([51.9603, 41.9269, 29.5227, 34.4102, 50.7539, 38.3919, 45.8056, 43.6801,\n",
      "        51.2680, 45.6061, 43.0469, 36.9073, 50.5007], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 87.9459, 119.0415,  67.7704, 106.4473,  84.8690,  66.2178,  78.9207,\n",
      "        120.9738,  98.5600,  64.6584, 100.0887, 120.5934, 180.5273, 105.2483,\n",
      "         89.7455,  68.8610,  35.2250, 112.7640, 141.1785,  81.3507,  79.5526,\n",
      "         84.0504, 100.3861,  61.1402,  63.1623,  17.1948,   9.0090,  28.1521,\n",
      "         16.5650,  13.3397,  16.4642], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False,  True,\n",
      "        False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([18.3343, 15.4430, 14.5433, 17.6720, 13.8874, 17.2024, 18.8989, 17.9301,\n",
      "        14.4476, 16.6245, 21.1007, 20.1546, 18.0178, 19.1939, 13.3725, 17.1919,\n",
      "        16.5547, 25.0241, 14.9169, 14.4356, 15.5727, 16.4152, 16.0562, 17.2062,\n",
      "        15.7982, 12.1670, 17.6914, 18.3507, 15.6803, 18.2759, 14.7666, 13.8280,\n",
      "        14.8972, 13.6658, 20.0575, 12.8550, 16.9857, 13.9745, 13.5516, 19.0779,\n",
      "        22.1552, 22.4595, 14.9839, 20.1423, 15.9941, 17.8611, 16.8497, 15.7350,\n",
      "        20.1910, 19.3820, 16.9137, 17.3807, 13.6392, 17.6961, 14.5309, 22.1257,\n",
      "        18.8808, 17.4329, 19.0915, 18.5985, 14.2338, 17.5910, 15.8654, 15.0441,\n",
      "        15.8725, 15.0972, 15.8840, 17.7144, 16.8310, 15.2573, 14.8218, 14.3644,\n",
      "        16.4023, 17.5212, 15.7805, 22.2711, 18.5348, 16.0859, 13.1971, 17.4426,\n",
      "        15.2748, 14.9681, 18.2384, 14.1330, 15.8216, 14.6740, 17.0621, 18.7661,\n",
      "        14.0393, 14.4229, 15.4283, 18.4712, 16.2340, 14.4435, 18.1817, 18.6672,\n",
      "        23.7380, 24.8187, 25.8344, 27.4797, 27.5007, 12.7565, 10.3675, 11.4419],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False,  True, False, False, False,  True, False,\n",
      "        False, False, False, False,  True, False, False, False, False,  True,\n",
      "        False, False, False,  True, False,  True, False, False, False, False,\n",
      "         True,  True, False,  True, False,  True, False,  True,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True,  True, False,  True,  True, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False,  True, False,  True, False, False,  True,  True,\n",
      "        False, False, False,  True,  True, False, False, False, False, False,\n",
      "        False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([28.5462, 74.4353, 48.8887, 51.4716, 29.6259, 48.7128, 42.5295, 39.3271,\n",
      "        50.9735, 46.8533, 27.0196, 50.0359, 57.2218, 43.6967, 37.9119, 43.2260,\n",
      "        37.4270, 50.5743, 47.9339, 48.5127, 40.0601], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 26.5808, 173.5280,  60.3578,  46.1505,  36.6765,  53.4224,  39.4202,\n",
      "         34.0486,  43.5613,  34.5051,  34.4877,  35.1001], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([105.9196,  85.6897, 107.4193, 177.1772, 196.2774, 117.6060,  76.5303,\n",
      "        108.2299, 117.1008,  59.2911, 273.7655,  75.7858, 261.0034, 141.9658,\n",
      "        183.8565,  68.6328,  63.9060, 301.3240, 110.7429, 204.3132, 381.5287,\n",
      "         57.3653,  29.7488,  85.6544, 115.1856,  87.9215, 142.2118],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False,  True,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 39.9942, 179.1667, 192.2295,  83.6952,  44.1780, 122.9599,  54.1035,\n",
      "         69.0202, 105.4242,  75.1088,  31.1210, 225.8633, 210.0323,  57.0546,\n",
      "         53.4885,  71.1434,  88.3622,  64.7337,  69.2909,  62.8600,  87.5348,\n",
      "         73.1397,  45.3364,  65.6458, 108.5455,  65.1130,  47.1766,  94.2904,\n",
      "         47.5336, 107.6057,  40.2469,  97.5554, 139.4249, 149.7115, 111.2677,\n",
      "        107.6331,  97.5154, 211.1122,  33.5549,  79.9640,  22.7170,  42.7076,\n",
      "         28.0593,  34.5462,  38.4305,  33.2110,  47.5064], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 31.3795,  38.7619,  74.6383,  52.6570,  55.0556,  51.6400,  34.3097,\n",
      "         45.5115,  41.2597,  78.3815,  32.8835,  35.7210,  49.3764,  31.9431,\n",
      "         53.2911,  41.5251,  21.3700, 101.8230,  20.7922,  49.0817,  44.7109,\n",
      "         62.6987,  35.5478,  37.3655,  62.0584,  28.4146,  59.3282,  38.1726,\n",
      "         46.4665,  34.8735,  45.5638,  28.8486,  37.1092, 138.0367,  59.4808,\n",
      "         30.9545,  68.5830,  41.0978,  83.8181,  39.1143,  89.4874,  33.0444,\n",
      "         37.9074,  20.5841,  19.1149,  30.7897,  16.1031,  26.1956,  49.3408,\n",
      "         49.1704,  25.7265,  38.0609,  86.6092,  61.2905,  54.7783,  31.9341,\n",
      "         48.2912,  20.2317,  26.4235,  88.9771,  65.9794,  84.5981,  39.5025,\n",
      "         93.8198,  52.8698,  76.2707,  63.8525,  66.5919,  82.7582,  16.8872,\n",
      "         30.6623,  16.1918,  20.1425,  20.6923,  55.2652,  25.7694,  23.4968,\n",
      "         31.0766], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False,  True, False,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True, False, False, False,  True,\n",
      "         True, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([38.5845, 47.1935, 32.6999, 46.1743, 18.1166, 33.6551, 25.2076, 40.1160,\n",
      "        32.5031, 34.3704, 39.4016, 24.6708, 51.4013, 55.8379, 37.5146, 35.9305,\n",
      "        44.0371, 49.6064, 31.7395], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "pruning 50 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     27\n",
      "     ╠════╗\n",
      "     ║    11\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    24\n",
      "    ╠════╗\n",
      "    ║    20\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   45\n",
      "   ╠════╗\n",
      "   ║    12\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  69\n",
      "  ╠════╗\n",
      "  ║    19\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 75\n",
      "╔╝\n",
      "│\n",
      "100\n",
      "=====================\n",
      "===LOOPS FINISHED :1 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 71 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     32\n",
      "     ╠════╗\n",
      "     ║    18\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    29\n",
      "    ╠════╗\n",
      "    ║    32\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   56\n",
      "   ╠════╗\n",
      "   ║    19\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  77\n",
      "  ╠════╗\n",
      "  ║    31\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 79\n",
      "╔╝\n",
      "│\n",
      "100\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.11784704029560089, 7.508367538452148)\n",
      "Mean, Std: (0.9999999403953552, 0.9722679257392883)\n",
      "remove_below 0.34298115968704224 true: 19.56260183641635\n",
      "Significance:\n",
      "tensor([38.3228, 51.3449, 31.0247, 33.8964, 43.3232, 40.8503, 37.8105, 37.2553,\n",
      "        39.1778, 32.9133, 79.5452, 33.0307, 39.6319, 40.2456, 31.3873, 39.1138,\n",
      "        39.9800, 34.9961, 24.8156], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 79.7535,  99.5836,  68.6108, 140.1584,  98.0965,  80.6913,  90.3201,\n",
      "        124.4683,  72.9746,  52.1962, 117.7265, 184.1847,  95.6760,  65.3341,\n",
      "         64.0591,  93.9750, 128.0982,  85.3725,  89.3943,  80.9981, 133.2875,\n",
      "         88.5159,  96.5137,  31.4980,  53.7716,  59.1497,  32.4989,   8.7143,\n",
      "         11.9694,   6.7216,  16.6659,   8.2284], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([23.0437, 20.6153, 25.4060, 24.3453, 26.5578, 19.8714, 20.5166, 22.5950,\n",
      "        25.7909, 22.7800, 26.0951, 23.9784, 23.3239, 30.3456, 23.4780, 19.0575,\n",
      "        21.9297, 22.3418, 21.2050, 22.2419, 26.4077, 23.0756, 23.3901, 20.4452,\n",
      "        25.6483, 20.6481, 25.9074, 30.0911, 27.3656, 20.0684, 26.6553, 26.2072,\n",
      "        23.0575, 21.2579, 20.5704, 27.8598, 30.0127, 26.5780, 23.5767, 26.9967,\n",
      "        22.7496, 26.5985, 28.1107, 24.8125, 25.2904, 24.8021, 23.6111, 24.7018,\n",
      "        22.1702, 22.7294, 24.0475, 20.8880, 23.6803, 26.4658, 22.9807, 21.9470,\n",
      "        28.0404, 25.9421, 26.4874, 23.1443, 20.4361, 22.6514, 21.5487, 24.9186,\n",
      "        21.9393, 25.5055, 27.0169, 26.4812, 23.3849, 31.2963, 27.7078, 31.8963,\n",
      "        32.1837, 35.7390, 34.0183,  8.7583,  7.0687,  8.1055,  7.2100],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([36.8489, 40.3554, 49.2016, 26.4608, 37.2393, 39.2754, 38.0343, 35.6114,\n",
      "        40.4847, 30.9099, 47.1121, 32.7537, 34.5657, 34.4441, 46.8993, 34.0014,\n",
      "        34.3046, 46.7147, 40.7069, 34.0552, 22.7642, 26.7687, 28.4017, 26.4257,\n",
      "        23.6001, 29.3296, 27.7653, 31.5763, 26.5967, 27.0532, 31.1175, 25.5760],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 25.0263, 220.8087,  48.3073,  39.1022,  36.0613,  40.6358,  41.9440,\n",
      "         33.9117,  38.6453,  34.3534,  36.1217,  22.1569,  19.5626,  28.4393,\n",
      "         36.6935,  33.2713,  29.2737,  22.1779], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([140.0728, 112.3950, 195.7386, 306.2699, 145.5766,  78.5610, 155.7117,\n",
      "        346.3930,  96.4853, 352.6496, 198.2374, 196.8053,  69.4215,  83.4610,\n",
      "        377.4378, 143.1864, 228.3933, 428.2544, 147.7959,  59.9596,  87.7108,\n",
      "        125.6745, 152.2396, 195.1524,  20.8859,  22.9892,  18.0726,  18.8870,\n",
      "         23.4421], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 44.4850, 165.7446, 213.7778,  49.1081, 113.1031,  47.9256,  78.3643,\n",
      "        112.9061,  62.6686,  40.3299, 207.0043, 214.6704,  58.8684,  64.4703,\n",
      "         70.9934, 100.2255,  67.1851,  99.4857,  81.0895,  88.2888,  83.6644,\n",
      "         59.4737,  95.6027, 130.2298,  48.0441,  53.3558,  93.3695,  61.0218,\n",
      "        109.8553,  34.3929, 116.3508, 166.3381, 151.2858, 125.5571,  98.1578,\n",
      "        120.9403, 217.1870,  73.4449, 100.9271,  77.1955,  37.5310,  65.8130,\n",
      "         52.7311,  62.3506,  93.3108,  15.5275,  16.2133,  31.3086,  15.1595,\n",
      "         17.5464,  18.4278,  16.0943,  12.0707,  10.0750,  15.8172,  24.1251],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 34.8110,  59.5589,  83.2114,  69.7233,  57.0093,  40.3097,  69.3455,\n",
      "         71.5712,  47.8514,  59.1583,  39.8061,  80.7870,  52.2234,  29.2267,\n",
      "        126.5811,  30.7175,  61.4265,  47.8608,  83.3862,  50.4087,  55.7451,\n",
      "         84.9917,  37.7944,  67.5746,  42.9154,  50.5785,  52.6253,  39.1084,\n",
      "         62.8807, 169.3206,  64.9313,  54.7498,  83.3989,  74.4740, 111.0486,\n",
      "         52.9875, 108.0593,  54.5662,  37.3629,  24.5900,  29.0565,  39.1036,\n",
      "         35.6802,  28.8196,  66.1789,  80.6400,  36.6838,  40.6119,  79.4643,\n",
      "         78.9904,  56.2251,  38.9479,  34.0225,  82.0570,  52.2116, 109.0820,\n",
      "         52.8548,  96.7196,  89.6939,  80.0366,  30.7872,  75.1261,  31.5663,\n",
      "         32.4261,  49.3535,  74.9535,  50.9001,  35.4283,  60.2261,  14.1231,\n",
      "         12.1929,  12.7590,  13.2904,  14.6783,  11.4991,  10.5254,  19.6966],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False,  True, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False,  True, False,  True,\n",
      "         True,  True,  True,  True,  True,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([35.4073, 39.5058, 23.2638, 22.2930, 25.8596, 36.6067, 31.2352, 36.8452,\n",
      "        37.5952, 24.7813, 37.6990, 31.1083, 41.4422, 41.7645, 31.9943, 29.5159,\n",
      "        32.9011, 35.4852, 27.7457, 32.0969, 35.6331, 23.2090, 28.8613, 31.0955,\n",
      "        33.7103, 28.3255, 30.4239, 36.8409, 30.1586, 33.2659, 28.0493],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False], device='cuda:0')\n",
      "pruning 49 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     27\n",
      "     ╠════╗\n",
      "     ║    16\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    27\n",
      "    ╠════╗\n",
      "    ║    29\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   45\n",
      "   ╠════╗\n",
      "   ║    16\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  64\n",
      "  ╠════╗\n",
      "  ║    30\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 70\n",
      "╔╝\n",
      "│\n",
      "100\n",
      "=====================\n",
      "===LOOPS FINISHED :2 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 72 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     34\n",
      "     ╠════╗\n",
      "     ║    22\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    32\n",
      "    ╠════╗\n",
      "    ║    39\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   54\n",
      "   ╠════╗\n",
      "   ║    23\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  67\n",
      "  ╠════╗\n",
      "  ║    52\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 73\n",
      "╔╝\n",
      "│\n",
      "100\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.07295168191194534, 7.3780198097229)\n",
      "Mean, Std: (1.0, 0.9924499988555908)\n",
      "remove_below 0.24474456906318665 true: 14.651596834164318\n",
      "Significance:\n",
      "tensor([51.0117, 41.2123, 25.1683, 57.8832, 39.2307, 39.1956, 21.7188, 42.0842,\n",
      "        97.0670, 30.7621, 35.7598, 43.2440, 37.2424, 30.1626, 36.0937, 26.7707,\n",
      "        20.6318, 28.6216, 34.7739, 24.3996, 23.7586, 20.1020, 22.0941],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 67.9352, 101.4102,  68.5744,  91.2322,  85.6644, 104.7742,  91.9605,\n",
      "         85.6240,  51.8692,  71.7558, 149.6870, 153.6257,  97.7380,  69.4183,\n",
      "         62.9297,  87.1457, 122.4894,  86.2243, 105.1747,  73.5543, 145.3921,\n",
      "        101.8705,  90.8655,  36.4119,  64.2210,  82.8021,  72.1190,   5.0171,\n",
      "          4.9801,   5.1548,   7.5123,   4.3672,   6.5242,   6.7470],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([30.0737, 29.6450, 30.8681, 32.6744, 25.9089, 24.3301, 30.8261, 32.1941,\n",
      "        29.7456, 33.3561, 29.3789, 29.3451, 35.9691, 24.4789, 26.1313, 26.5436,\n",
      "        23.2709, 30.8792, 29.2359, 27.6782, 24.6594, 28.9191, 25.1345, 26.8308,\n",
      "        33.3789, 28.0413, 21.2215, 31.0013, 29.1280, 31.0257, 22.5632, 26.7349,\n",
      "        28.9454, 34.3087, 31.2268, 26.9205, 27.6052, 28.2077, 30.0852, 31.5637,\n",
      "        26.9192, 27.9374, 27.6048, 28.9800, 30.5347, 32.0922, 24.8849, 29.3658,\n",
      "        29.7568, 27.0971, 26.3826, 33.2741, 30.4977, 28.8457, 26.7052, 22.1144,\n",
      "        28.5080, 26.4695, 28.8450, 26.5402, 29.7558, 26.1732, 31.0296, 29.3609,\n",
      "        31.2083, 35.7998, 36.0480, 35.7360, 39.4652, 39.1113,  5.0818,  5.0243,\n",
      "         5.5847], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False,  True, False,  True, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([47.5706, 44.6252, 25.9919, 27.9591, 38.0906, 38.8929, 41.9368, 37.1605,\n",
      "        40.0366, 29.6702, 36.0847, 29.3807, 47.3806, 31.5599, 34.5758, 47.7550,\n",
      "        40.5552, 28.0912, 27.5259, 36.5711, 25.4769, 21.5993, 25.3519, 29.4991,\n",
      "        34.8268, 25.6351, 27.6931, 27.9492, 31.9360, 19.9393, 19.0830, 24.7109,\n",
      "        23.1287, 19.2069, 18.5226, 16.5920, 16.9503, 14.5788, 17.7616],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 26.4637, 175.9007,  48.8087,  33.6229,  27.3117,  39.5476,  42.9598,\n",
      "         36.4726,  38.9776,  40.1930,  28.4908,  21.0077,  31.3232,  35.0182,\n",
      "         43.9018,  28.3708,  24.4912,  33.5330,  23.1157,  17.3932,  26.0273,\n",
      "         27.6900], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([143.1958, 127.7887, 232.7074, 376.2366, 162.0560,  90.1427, 217.7316,\n",
      "        386.4614,  91.3803, 341.6800, 166.3418, 238.1053,  60.2493,  77.9848,\n",
      "        394.3452, 152.0516, 205.8224, 441.6841, 172.4261,  62.6290,  94.9104,\n",
      "        124.5587, 198.7397, 174.4178,  43.0113,  56.6269,  35.1396,   9.9832,\n",
      "         11.3420,  13.2500,  13.3231,  11.6195], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False,  True,  True,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 50.6795, 153.1830, 280.7235,  62.3273, 117.2305,  51.7472,  89.2568,\n",
      "        134.4729,  74.6666,  53.2836, 189.0157, 232.3084,  69.6702,  73.2517,\n",
      "         72.2468, 139.5666,  59.8361, 119.4342, 111.2684, 106.6805, 110.9346,\n",
      "         96.0592, 141.4568,  42.7458,  58.1185, 102.1631,  64.1144, 142.7105,\n",
      "         43.7985, 124.8436, 192.4507, 152.2206, 156.8183, 110.8988, 130.3402,\n",
      "        251.8759,  95.7443, 128.0199, 119.9083,  57.2068,  88.0281,  69.4227,\n",
      "        129.0775,  61.3138,  57.3954,  11.2684,  12.3652,  10.2142,  10.5597,\n",
      "          8.0526,  10.8524,   9.8743,   7.2940,   9.0052], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 43.3534,  55.9713, 101.0335,  88.8807,  75.9136,  45.3416,  79.0288,\n",
      "        102.5286,  62.2867,  70.2633,  52.9750,  88.0962,  63.1763, 168.4745,\n",
      "         75.7771, 109.9387,  54.1034,  73.0533,  99.7329,  50.1843,  70.4408,\n",
      "         56.1763,  68.6277,  70.3420,  46.1265,  71.9159, 178.9918,  90.1974,\n",
      "         95.7061, 106.2936,  98.7438, 116.0830,  54.1343, 127.5287,  79.2260,\n",
      "         39.0872,  34.3304,  43.9411,  46.7676,  40.5780,  93.0173, 102.6207,\n",
      "         46.8637,  47.1070, 103.2630,  90.4872,  67.9256,  60.0097,  42.7425,\n",
      "         79.8388,  62.2694, 106.5246,  57.1165, 122.9392, 112.5681,  90.8593,\n",
      "         43.8764,  84.7310,  40.2284,  64.3925,  71.4149,  69.1911,  76.2762,\n",
      "         45.9598,  11.0295,  10.4785,  10.6346], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([29.4040, 38.9947, 24.9445, 21.4885, 17.6291, 37.9951, 22.6150, 23.4779,\n",
      "        37.4374, 33.0573, 29.5429, 22.2399, 28.1595, 31.4116, 30.4563, 37.9222,\n",
      "        35.0355, 37.0162, 24.0782, 33.6309, 35.1525, 30.5120, 20.9116, 32.9151,\n",
      "        31.7998, 29.9166, 37.8769, 33.3228, 24.7507, 40.0835, 28.1559, 35.2752,\n",
      "        27.0544, 22.3570, 23.3867, 21.9519, 23.1728, 20.0358, 20.8488, 28.5922,\n",
      "        22.6312, 19.9128, 14.6516, 22.8751, 25.7219, 26.9742, 19.0206, 25.1936,\n",
      "        19.3421, 29.5140, 26.4133, 16.9544], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False], device='cuda:0')\n",
      "pruning 49 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     26\n",
      "     ╠════╗\n",
      "     ║    21\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    25\n",
      "    ╠════╗\n",
      "    ║    37\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   44\n",
      "   ╠════╗\n",
      "   ║    21\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  61\n",
      "  ╠════╗\n",
      "  ║    49\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 63\n",
      "╔╝\n",
      "│\n",
      "100\n",
      "=====================\n",
      "===LOOPS FINISHED :3 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 74 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     35\n",
      "     ╠════╗\n",
      "     ║    29\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    31\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    1\n",
      "    ║    ╠════╝\n",
      "    ║    50\n",
      "    ║    ╠════╗\n",
      "    ║    ║    3\n",
      "    ║    ╠════╝\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   53\n",
      "   ╠════╗\n",
      "   ║    25\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  65\n",
      "  ╠════╗\n",
      "  ║    64\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 65\n",
      "╔╝\n",
      "│\n",
      "100\n",
      "Computing Network Siginificance\n",
      "Significance Stat:\n",
      "Min, Max: (0.04906795546412468, 7.5425543785095215)\n",
      "Mean, Std: (1.0000001192092896, 1.074583649635315)\n",
      "remove_below 0.17580267786979675 true: 10.01737934639987\n",
      "Significance:\n",
      "tensor([44.0550, 48.6391, 30.7270, 40.4542, 41.1871, 41.2453, 24.3705, 33.2434,\n",
      "        99.8060, 31.3628, 43.5789, 35.8084, 33.7666, 31.1903, 21.6888, 23.9537,\n",
      "        31.6718, 29.6779, 26.5353, 29.2018, 40.0702, 22.8596, 24.4295, 13.6141,\n",
      "        23.9915], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 63.7291,  96.2311,  88.8278,  88.6093,  82.8946, 101.4590,  95.5299,\n",
      "         96.0453,  49.5275,  82.2612, 174.2518, 155.5162, 112.1079,  67.3087,\n",
      "         61.3760,  89.5509,  99.7004,  95.0776,  71.9342, 165.4494, 108.1187,\n",
      "        114.0057,  41.7696,  64.9275,  80.6309,  91.8447,   3.9422,   4.5607,\n",
      "          5.8402,   3.7954,   5.9557,   3.7836,   4.7009,   3.8841,   4.5218],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([32.2139, 31.0865, 30.7382, 29.5607, 27.4259, 34.0897, 37.7149, 30.5414,\n",
      "        36.5034, 32.3141, 29.6696, 42.3611, 29.2078, 30.8462, 27.9346, 30.6971,\n",
      "        32.8881, 29.7267, 31.0727, 32.5706, 26.3133, 28.6308, 37.0966, 28.6641,\n",
      "        28.3784, 36.4171, 28.9297, 29.0874, 24.8358, 32.4081, 28.8829, 32.4248,\n",
      "        28.5608, 35.0123, 33.8160, 30.7448, 31.4737, 33.8967, 32.7100, 32.3178,\n",
      "        23.2469, 30.8334, 32.6532, 29.5703, 32.7020, 36.5280, 31.7419, 30.3820,\n",
      "        27.5472, 33.0385, 30.1988, 33.3987, 32.0308, 34.8207, 32.1377, 31.2593,\n",
      "        35.9608, 40.0495, 35.4129, 38.3818, 35.1031, 41.1413, 38.8764,  3.9017,\n",
      "         2.7959], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False,  True, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False,  True, False, False,  True, False,\n",
      "        False, False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([37.8111, 44.9540, 28.7252, 35.3704, 36.5459, 33.6912, 28.9561, 39.0323,\n",
      "        22.6552, 28.5562, 29.0743, 45.6679, 27.6706, 32.6177, 41.2602, 34.8316,\n",
      "        24.6500, 32.6960, 38.3731, 20.9013, 25.0861, 26.6871, 25.9527, 33.5851,\n",
      "        25.4040, 25.1735, 28.7622, 29.2176, 22.3511, 25.4334, 24.6149, 17.1837,\n",
      "        20.4175, 20.7482, 20.8710, 23.5489, 18.9075, 12.1971, 11.0150, 12.1398,\n",
      "        12.0102, 14.4685, 16.4089, 16.5110, 15.4043, 11.9679, 18.2122, 13.3722,\n",
      "        25.6579, 15.6845], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 23.4253, 168.7185,  45.6899,  32.7350,  28.2918,  36.2342,  42.0459,\n",
      "         35.1121,  36.9260,  39.5646,  28.8142,  20.9218,  33.6063,  36.7814,\n",
      "         42.2783,  26.0230,  28.5626,  29.6546,  21.7388,  21.5019,  25.7431,\n",
      "         17.3415,  14.3168,  21.0528,  19.5531,  16.9804,  22.8775,  18.3695,\n",
      "         13.0557], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([5.7069], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([13.6167,  9.1089,  8.2955], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([169.1432, 130.8401, 235.3321, 400.9892, 161.0161, 113.8472, 270.2268,\n",
      "        390.4120,  76.6397, 174.0750, 290.8929,  64.8341,  81.7259, 411.6854,\n",
      "        167.0824, 205.0967, 429.7809, 201.6931,  85.3058,  82.4960, 127.6589,\n",
      "        259.0218, 167.6537,  51.5619,  41.7153,  17.3524,  13.0302,   9.8490,\n",
      "          7.3928,   6.0115,   9.8602], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False,  True,  True,  True,  True,\n",
      "         True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 53.0909, 151.6892, 341.9458,  83.2455, 115.1210,  48.9957,  72.3410,\n",
      "        153.8284,  76.6840,  52.0998, 198.6487, 264.4468,  72.4454,  73.2674,\n",
      "         70.5565, 147.0716,  61.6931, 108.4036, 110.1666, 104.3559, 111.1338,\n",
      "        112.7918, 135.3092,  42.3071,  58.4283,  81.1613,  67.9959, 137.3669,\n",
      "         55.8409, 130.5209, 213.6600, 126.4656, 180.1471, 103.5945, 248.8749,\n",
      "         97.5075, 140.0259, 115.3660,  58.1506,  97.2883,  70.4066, 137.6652,\n",
      "         78.4895,  70.0625,   8.4439,   5.4194,  14.9350,   7.7991,   5.8554,\n",
      "          6.8037,   8.0092,   7.9576,   7.8499], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True, False,  True,  True,  True,\n",
      "         True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 49.5891,  51.9291,  99.8920,  87.4768,  47.1987, 104.5695,  67.7442,\n",
      "         66.5946,  60.6616, 109.9286,  78.8333, 180.3260,  72.7940, 129.9353,\n",
      "         53.8016,  90.6879,  94.3537,  65.1518,  74.6784,  60.6773,  78.6988,\n",
      "         77.3525,  49.6989,  71.8916, 201.4617,  94.5515, 101.3148, 109.2980,\n",
      "        123.9704, 117.9916,  59.0010, 133.5771,  79.3845,  49.6620,  41.1526,\n",
      "         42.8282,  53.4196,  41.9421, 101.7887, 127.9251,  48.0362,  48.9797,\n",
      "        122.3663,  94.0129,  65.5708,  64.3241,  46.0251,  79.2202,  69.9581,\n",
      "         96.3126,  64.0633, 115.6851,  91.2361,  50.1996,  83.7540,  44.5709,\n",
      "         74.3989,  66.3614,  80.2002,  75.8194,  50.9945,  10.8925,  10.2681,\n",
      "         12.4019,  11.8911], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([22.3142, 33.1917, 22.7524, 17.8304, 26.5332, 19.0871, 21.1558, 20.0140,\n",
      "        22.0351, 31.8096, 25.3166, 22.7077, 33.6353, 20.9537, 22.8793, 25.5859,\n",
      "        33.6397, 27.9046, 33.0839, 30.6269, 26.2115, 26.8878, 25.6866, 27.4042,\n",
      "        30.3596, 23.5015, 23.7553, 23.1046, 21.5718, 19.6108, 21.3933, 22.1399,\n",
      "        16.0614, 27.5718, 27.1557, 23.2497, 19.9320, 28.4205, 28.7199, 28.3136,\n",
      "        19.6569, 22.5102, 31.4910, 30.8350, 37.8123, 24.9674, 22.2396, 23.0885,\n",
      "        19.1164, 12.5719, 14.2305,  4.5797, 16.0724, 12.7080, 14.2093, 11.0277,\n",
      "         9.4376, 11.8675, 12.9197,  8.0348, 16.5593, 10.0174, 10.9423, 10.7343],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False,  True, False, False, False,\n",
      "        False,  True, False, False,  True, False,  True, False, False,  True,\n",
      "        False,  True, False, False], device='cuda:0')\n",
      "pruning 55 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     26\n",
      "     ╠════╗\n",
      "     ║    28\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    24\n",
      "    ╠════╗\n",
      "    ║    49\n",
      "    ║    ╠════╗\n",
      "    ║    ║    1\n",
      "    ║    ╠════╝\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   42\n",
      "   ╠════╗\n",
      "   ║    25\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  61\n",
      "  ╠════╗\n",
      "  ║    53\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 57\n",
      "╔╝\n",
      "│\n",
      "100\n",
      "=====================\n",
      "===LOOPS FINISHED :4 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 75 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     29\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    4\n",
      "     ║    ╠════╝\n",
      "     ║    31\n",
      "     ║    ╠════╗\n",
      "     ║    ║    4\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    30\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    1\n",
      "    ║    ╠════╝\n",
      "    ║    65\n",
      "    ║    ╠════╗\n",
      "    ║    ║    6\n",
      "    ║    ╠════╝\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   45\n",
      "   ╠════╗\n",
      "   ║    34\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  62\n",
      "  ╠════╗\n",
      "  ║    71\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 59\n",
      "╔╝\n",
      "│\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "/home/tsuman/All_Files/Program_Files/miniconda/lib/python3.9/site-packages/torch/nn/modules/module.py:1033: \n",
    "UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. \n",
    "This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
    "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
    "'''\n",
    "\n",
    "trainer.loop(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check if parameter in param_groupd\n",
    "c = 0\n",
    "for p in optimizer.param_groups[0]['params']:\n",
    "    print(p.shape)\n",
    "    c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.root_net.residual.fc1.shortcut.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_x, train_y in train_loader:\n",
    "    train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "    yout = dynet(train_x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for p in dynet.parameters():\n",
    "    print(p.shape)\n",
    "    for _p in optimizer.param_groups[0]['params']:\n",
    "        if _p is p:\n",
    "            print('Found')\n",
    "    print()\n",
    "    c += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.training_func()\n",
    "# trainer.pruning_func()\n",
    "# trainer.maintainance_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    corrects = 0\n",
    "    dynet.eval()\n",
    "    for test_x, test_y in test_loader:\n",
    "        test_x  = test_x.to(device)\n",
    "        yout = dynet.forward(test_x)\n",
    "        outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "        correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "        corrects += correct\n",
    "    dynet.train()\n",
    "    acc = corrects/len(test_dataset)*100\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     corrects = 0\n",
    "#     dynet.train()\n",
    "#     for test_x, test_y in train_loader:\n",
    "#         test_x  = test_x.to(device)\n",
    "#         yout = dynet.forward(test_x)\n",
    "#         outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "#         correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "#         corrects += correct\n",
    "#     acc = corrects/len(train_dataset)*100\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr 66.908 -> 62.422 ## the adding neuron function is wrong.. not preserving the function.\n",
    "# te 71.77 -> 41.959999999999994\n",
    "\n",
    "# te -> 53.32, 53.32\n",
    "# 68.51 -> 68.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.adding_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.beta_del_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accs_all, label=\"train\")\n",
    "plt.plot(accs_test, label=\"test\")\n",
    "ymin, ymax = plt.gca().get_ylim()\n",
    "plt.text(0, 0.8*ymin+0.2*ymax, f\"Train-> max:{max(accs_all):.3f} end:{accs_all[-1]:.3f} \\nTest-> max:{max(accs_test):.3f} end:{accs_test[-1]:.3f}\")\n",
    "                    \n",
    "plt.legend()\n",
    "plt.savefig(f\"output/plots/{index}_{name}_cifar10_{exp_index}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.non_linearity.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_allocated(device=\"cuda:0\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
