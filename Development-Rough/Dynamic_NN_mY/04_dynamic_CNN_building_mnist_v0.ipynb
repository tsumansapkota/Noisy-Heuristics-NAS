{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "\n",
    "import mylibrary.datasets as datasets\n",
    "import mylibrary.nnlib as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hidden_neuron_number(i, o):\n",
    "    return (max(i,o)*(min(i,o)**2))**(1/3)\n",
    "\n",
    "\n",
    "class Shortcut_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=(3,3), stride=1):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self._kernel = np.array(kernel, dtype=int)\n",
    "        self._padding = tuple(((self._kernel-1)/2).astype(int))\n",
    "        self._stride = stride\n",
    "        _wd = nn.Conv2d(input_dim, output_dim, self._kernel, stride=self._stride,\n",
    "                        padding=self._padding, bias=False).weight.data\n",
    "        ## Shape = OutputDim, InputDim, Kernel0, Kernel1\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "        del _wd\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.shape[1] > 0 and self.weight.shape[0] > 0:\n",
    "            return F.conv2d(x, self.weight, stride=self._stride, padding=self._padding)\n",
    "        ### output dim is 0\n",
    "        elif self.weight.shape[0] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###       #out_dim #inp_dim            #kernel\n",
    "            w = torch.zeros(1, 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return torch.zeros(o.shape[0], 0, o.shape[2], o.shape[3], dtype=x.dtype, device=x.device)\n",
    "        ### input dim is 0\n",
    "        elif x.shape[1] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###             #out_dim            #inp_dim            #kernel\n",
    "            w = torch.zeros(self.weight.shape[0], 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return o.data\n",
    "        else:\n",
    "            raise(f\"Unknown shape of input {x.shape} or weight {self.weight.shape}\")\n",
    "\n",
    "#     def decay_std_ratio(self, factor):\n",
    "#         self.weight.data = self.weight.data - self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "        \n",
    "#     def decay_std_ratio_grad(self, factor):\n",
    "#         self.weight.grad = self.weight.grad + self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        pass\n",
    "    \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "        pass\n",
    "            \n",
    "     \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "        _w = self.weight.data[remaining, :]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "        _w = self.weight.data[:, remaining]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        _w = torch.cat((self.weight.data, torch.zeros(o, num, k0, k1, dtype=self.weight.data.dtype,\n",
    "                                                      device=self.weight.data.device)), dim=1)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i)\n",
    "        _new = torch.empty(num, i, k0, k1, dtype=self.weight.data.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        _w = torch.cat((self.weight.data, _new), dim=0)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)        \n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}S▚:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = Shortcut_Conv(\"tree\", 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 3, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(1, 1, 28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1, 5, (3,3), padding=(1,1), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 3, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(inp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv2d(inp, sc.weight, stride = sc._stride, padding=sc._padding).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(5,1,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.randn(1, 1, 2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 4.1030, -0.6012],\n",
       "          [ 0.6821, -4.1699]],\n",
       "\n",
       "         [[-0.2095,  1.1301],\n",
       "          [ 3.2032,  5.9024]],\n",
       "\n",
       "         [[-2.8604,  4.0165],\n",
       "          [ 2.1841, -0.5844]],\n",
       "\n",
       "         [[-1.4028,  2.4924],\n",
       "          [ 0.2415, -4.3600]],\n",
       "\n",
       "         [[-4.4734, -4.0532],\n",
       "          [ 1.0835,  2.7726]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv2d(i, w, stride = sc._stride, padding=sc._padding) + b.view(1,-1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(1, 0, 28, 28))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1, 0, 28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(5,2,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 3, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 3, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[:, [1], :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ = np.random.randn(5,2,3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_[[0,1,2], [0,1], :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearity_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.bias = nn.Parameter(torch.zeros(io_dim))\n",
    "        self.actf = actf_obj\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actf(x+self.bias.view(1,-1,1,1))\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        _b = torch.cat((self.bias.data, torch.zeros(num, dtype=self.bias.data.dtype,\n",
    "                                                    device=self.bias.data.device)))\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        _b = self.bias.data[remaining]\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearity(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.bias = nn.Parameter(torch.zeros(io_dim))\n",
    "        self.actf = actf_obj\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actf(x+self.bias)\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        _b = torch.cat((self.bias.data, torch.zeros(num, dtype=self.bias.data.dtype,\n",
    "                                                    device=self.bias.data.device)))\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        _b = self.bias.data[remaining]\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1, 5, (3,3), padding=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.2322,  0.1578,  0.1843,  0.1363,  0.3134], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, hidden_dim, output_dim, stride=1):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "#         self.stride = stride\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = HierarchicalResidual_Conv(self.tree, input_dim, hidden_dim, stride=stride) \n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim)\n",
    "        self.fc1 = HierarchicalResidual_Conv(self.tree, hidden_dim, output_dim)\n",
    "        self.fc1.shortcut.weight.data *= 0.\n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "        self.apnz = self.apnz/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "        self.count = None\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "            self.significance += z.pow(2).sum(dim=0)\n",
    "            self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "    #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "            self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        \n",
    "        self.to_remove = torch.nonzero(self.significance<=below).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "#         max_dim = np.ceil((self.tree.parent_dict[self].input_dim+\\\n",
    "#             self.tree.parent_dict[self].output_dim)/2)\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None:\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.stride = 1\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, stride=self.stride).to(self.tree.device)\n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None ## this can be Residual Layer or None\n",
    "        ##### only one of shortcut or residual can be None at a time\n",
    "        self.forward = self.forward_shortcut\n",
    "        \n",
    "        self.std_ratio = 0. ## 0-> all variation due to shortcut, 1-> residual\n",
    "        self.target_std_ratio = 0. ##\n",
    "    \n",
    "    def forward_both(self, r):\n",
    "        s = self.shortcut(r)\n",
    "        r = self.residual(r)\n",
    "\n",
    "        if self.residual.hook is None: ### dont execute when computing significance\n",
    "            s_std = torch.std(s, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            r_std = torch.std(r, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            stdr = r_std/(s_std+r_std)\n",
    "\n",
    "            self.std_ratio = self.tree.beta_std_ratio*self.std_ratio + (1-self.tree.beta_std_ratio)*stdr.data\n",
    "            if r_std.min() > 1e-9:\n",
    "                ## recover for the fact that when decaying neurons, target ratio should also be reducing\n",
    "                if self.tree.total_decay_steps:\n",
    "                    i, o = self.shortcut.weight.shape[1],self.shortcut.weight.shape[0]\n",
    "                    if self.shortcut.to_remove is not None:\n",
    "                        i -= len(self.shortcut.to_remove)\n",
    "                    if self.shortcut.to_freeze is not None:\n",
    "                        o -= len(self.shortcut.to_freeze)\n",
    "                    h = self.residual.hidden_dim\n",
    "                    if self.residual.to_remove is not None:\n",
    "                        h -= len(self.residual.to_remove)\n",
    "                    \n",
    "#                     tr = h/np.ceil((i+o)/2 +1)\n",
    "                    tr = h/_get_hidden_neuron_number(i, o)\n",
    "                    self.compute_target_std_ratio(tr)\n",
    "                else:\n",
    "                    self.compute_target_std_ratio()\n",
    "                self.get_std_loss(stdr)\n",
    "        return s+r\n",
    "    \n",
    "    def forward_shortcut(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def forward_residual(self, x):\n",
    "        self.compute_target_std_ratio()\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def compute_target_std_ratio(self, tr = None):\n",
    "        if tr is None:\n",
    "#             tr = self.residual.hidden_dim/np.ceil((self.input_dim+self.output_dim)/2 +1)\n",
    "            tr = self.residual.hidden_dim/_get_hidden_neuron_number(self.input_dim, self.output_dim)\n",
    "#             tr = self.residual.hidden_dim/np.ceil(self.output_dim/2 +1)\n",
    "\n",
    "        tr = np.clip(tr, 0., 1.)\n",
    "        self.target_std_ratio = self.tree.beta_std_ratio*self.target_std_ratio +\\\n",
    "                                (1-self.tree.beta_std_ratio)*tr\n",
    "        pass        \n",
    "    \n",
    "    def get_std_loss(self, stdr):\n",
    "        del_std = self.target_std_ratio-stdr\n",
    "        del_std_loss = (del_std**2 + torch.abs(del_std)).mean()\n",
    "#         del_std_loss = (del_std**2).mean()\n",
    "        self.tree.std_loss += del_std_loss\n",
    "        return\n",
    "            \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_freezing_connection(to_freeze)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_decaying_connection(to_remove)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_freezed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.remove_freezed_connection(remaining)\n",
    "            if self.shortcut: self.std_ratio = self.std_ratio[:, remaining]\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_decayed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_input_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_output_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.add_output_connection(num)\n",
    "            # if torch.is_tensor(self.std_ratio):\n",
    "            if self.shortcut:\n",
    "                self.std_ratio = torch.cat((self.std_ratio, torch.zeros(1, num, device=self.tree.device)), dim=1)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        \n",
    "        if self.residual is None:\n",
    "            # print(f\"Adding {num} hidden units.. in new residual_layer\")\n",
    "            self.residual = Residual_Conv(self.tree, self.input_dim,\n",
    "                                          num, self.output_dim, stride=self.stride).to(self.tree.device)\n",
    "            \n",
    "            self.tree.parent_dict[self.residual] = self\n",
    "            if self.shortcut is None:\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            else:\n",
    "                self.forward = self.forward_both\n",
    "                self.std_ratio = torch.zeros(1, self.output_dim, device=self.tree.device)\n",
    "                \n",
    "        else:\n",
    "            # print(f\"Adding {num} hidden units..\")\n",
    "            self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            if self.std_ratio.min()>0.98 and self.target_std_ratio>0.98:\n",
    "                del self.tree.parent_dict[self.shortcut]\n",
    "                del self.shortcut\n",
    "                self.shortcut = None\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            \n",
    "        elif self.target_std_ratio<0.95:\n",
    "            self.shortcut = Shortcut_Conv(self.tree, self.input_dim, self.output_dim, stride=self.stride)\n",
    "            self.shortcut.weight.data *= 0.\n",
    "            self.forward = self.forward_both\n",
    "            \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.residual.hidden_dim < 1:\n",
    "            del self.tree.parent_dict[self.residual]\n",
    "            del self.residual\n",
    "            ### its parent (Residual_Conv) removes it from dynamic list if possible\n",
    "            self.residual = None\n",
    "            self.forward = self.forward_shortcut\n",
    "            self.std_ratio = 0.\n",
    "            return\n",
    "        \n",
    "#         max_dim = np.ceil((self.input_dim+self.output_dim)/2)\n",
    "        # max_dim = min((self.input_dim, self.output_dim))+1\n",
    "        max_dim = _get_hidden_neuron_number(self.input_dim, self.output_dim) + 1 \n",
    "        # print(\"MaxDIM\", max_dim, self.residual.hidden_dim)\n",
    "        if self.residual.hidden_dim > max_dim:\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc0)\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc1)\n",
    "            # print(\"Added\", self.residual)\n",
    "            \n",
    "        # self.residual.fc0.morph_network()\n",
    "        # self.residual.fc1.morph_network()\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        stdr = self.std_ratio\n",
    "        if torch.is_tensor(self.std_ratio):\n",
    "            stdr = self.std_ratio.min()\n",
    "            \n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{self.target_std_ratio}, s:{stdr}\")\n",
    "        if self.shortcut:\n",
    "            self.shortcut.print_network_debug(depth+1)\n",
    "        if self.residual:\n",
    "            self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        if self.residual is None:\n",
    "            return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            print(f\"{pre_string}╠════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}║    \")\n",
    "            print(f\"{pre_string}╠════╝\")\n",
    "        else:\n",
    "            print(f\"{pre_string}╚════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}     \")\n",
    "            print(f\"{pre_string}╔════╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Conv Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation, hidden_dim, post_activation=None):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "        self.post_activation = post_activation\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = hrnet0\n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = hrnet1\n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        if self.post_activation:\n",
    "            x = self.post_activation(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "        self.apnz = self.apnz/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "        self.count = None\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "            self.significance += z.pow(2).sum(dim=0)\n",
    "            self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "    #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "            self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        \n",
    "        self.to_remove = torch.nonzero(self.significance<=below).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None:\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation=nn.ReLU(), post_activation=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = hrnet0.input_dim\n",
    "        self.output_dim = hrnet1.output_dim\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = None\n",
    "        self.residual = Residual_Conv_Connector(self.tree, hrnet0, hrnet1,\n",
    "                                                activation, hrnet0.output_dim, post_activation)\n",
    "        self.tree.parent_dict[self.residual] = self\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.residual.fc1.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.residual.fc1.add_output_connection(num)\n",
    "        \n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):  \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        print(f\"{pre_string}╚╗\")\n",
    "        self.residual.print_network(f\"{pre_string} \")\n",
    "        print(f\"{pre_string}╔╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut only Hierarchical Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        _wd = nn.Linear(input_dim, output_dim, bias=False).weight.data\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## input_dim        ## output_dim\n",
    "        if x.shape[1] + self.weight.shape[1] > 0:\n",
    "            return x.matmul(self.weight.t())\n",
    "        else:\n",
    "            # print(x.shape, self.weight.shape)\n",
    "            # print(x.matmul(self.weight.t()))\n",
    "            if x.shape[1] + self.weight.shape[1] == 0:\n",
    "                return torch.zeros(x.shape[0], self.weight.shape[0], dtype=x.dtype, device=x.device)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        pass\n",
    "    \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "        pass\n",
    "            \n",
    "     \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "        _w = self.weight.data[remaining, :]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "        _w = self.weight.data[:, remaining]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        _w = torch.cat((self.weight.data, torch.zeros(o, num, dtype=self.weight.data.dtype,\n",
    "                                                      device=self.weight.data.device)), dim=1)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i)\n",
    "        _new = torch.empty(num, i, dtype=self.bias.weight.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        _w = torch.cat((self.weight.data, _new), dim=0)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)        \n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}S:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n",
    "\n",
    "\n",
    "class HierarchicalResidual_Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=None, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        ## this can be Shortcut Layer or None\n",
    "        if kernel is None:\n",
    "            self.shortcut = Shortcut(tree, self.input_dim, self.output_dim) \n",
    "        else:\n",
    "            self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, kernel, stride) \n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.shortcut.start_freezing_connection(to_freeze)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.shortcut.start_decaying_connection(to_remove)\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.shortcut.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.shortcut.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.shortcut.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.shortcut.add_output_connection(num)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        print(\"Cannot Add Hidden neuron to Shortcut Only Layer\")\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        pass\n",
    "        \n",
    "    def morph_network(self):\n",
    "        pass\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.shortcut.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree and Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree_State():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.DYNAMIC_LIST = set() ## residual parent is added, to make code effecient.\n",
    "        ## the parents which is not intended to have residual connection should not be added.\n",
    "        self.beta_std_ratio = None\n",
    "        self.beta_del_neuron = None\n",
    "        self.device = 'cpu'\n",
    "    \n",
    "        self.parent_dict = {}\n",
    "    \n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual:set = None\n",
    "        self.freeze_connection_shortcut:set = None\n",
    "        self.decay_connection_shortcut:set = None\n",
    "\n",
    "        self.decay_rate_std = 0.001\n",
    "\n",
    "        self.add_to_remove_ratio = 2.\n",
    "        pass\n",
    "    \n",
    "    def get_decay_factor(self):\n",
    "        ratio = self.current_decay_step/self.total_decay_steps\n",
    "#         self.decay_factor = np.exp(-2*ratio)*(1-ratio)\n",
    "        self.decay_factor = (1-ratio)**2\n",
    "        pass\n",
    "    \n",
    "    def clear_decay_variables(self):\n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual = None\n",
    "        self.freeze_connection_shortcut = None\n",
    "        self.decay_connection_shortcut = None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constructing Hierarchical Residual CNN (Resnet Inspired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Tree_State()\n",
    "hrn0 = HierarchicalResidual_Conv(tree, 1, 8)\n",
    "hrn1 = HierarchicalResidual_Conv(tree, 8, 16)\n",
    "hrn2 = HierarchicalResidual_Conv(tree, 16, 32)\n",
    "hrn3 = HierarchicalResidual_Conv(tree, 32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrn01 = HierarchicalResidual_Connector(tree, hrn0, hrn1)\n",
    "hrn012 = HierarchicalResidual_Connector(tree, hrn01, hrn2)\n",
    "hrn0123 = HierarchicalResidual_Connector(tree, hrn012, hrn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrnfc = HierarchicalResidual_Shortcut(tree, 64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_and_reshape(x):\n",
    "    x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    return x\n",
    "\n",
    "hrn0123fc = HierarchicalResidual_Connector(tree, hrn0123, hrnfc,\n",
    "                                           activation=lambda x: x, post_activation=pool_and_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0102, -0.0079, -0.0129,  0.0178, -0.0268, -0.0098,  0.0014,  0.0122,\n",
       "         -0.0068,  0.0120]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrn0123fc(inp) ## worked at once.. !! :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, device, input_dim = 1, hidden_dims = [8, 16, 32, 64], output_dim = 10, final_activation=None,\n",
    "                 num_stat=5, num_std=100, decay_rate_std=0.001):\n",
    "        super().__init__()\n",
    "        self.tree = Tree_State()\n",
    "        self.tree.beta_del_neuron = (num_stat-1)/num_stat\n",
    "        self.tree.beta_std_ratio = (num_std-1)/num_std\n",
    "        self.tree.decay_rate_std = decay_rate_std\n",
    "        self.tree.device = device\n",
    "        \n",
    "        self.root_net = None\n",
    "        self._construct_root_net(input_dim, hidden_dims, output_dim)\n",
    "        \n",
    "#         self.tree.DYNAMIC_LIST.add(self.root_net)\n",
    "        self.tree.parent_dict[self.root_net] = None\n",
    "        \n",
    "        if final_activation is None:\n",
    "            final_activation = lambda x: x\n",
    "        self.non_linearity = NonLinearity(\"Root\", output_dim, final_activation)\n",
    "        \n",
    "        self.neurons_added = 0\n",
    "\n",
    "        self._remove_below = None ## temporary variable\n",
    "        \n",
    "    def _construct_root_net(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 1, 8, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 8, 8)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 8, 16, stride=2)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2)\n",
    "        hrn3 = HierarchicalResidual_Conv(self.tree, 32, 32, stride=2)\n",
    "\n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2)\n",
    "        hrnR0123 = HierarchicalResidual_Connector(self.tree, hrnR012, hrn3)\n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 32, 10)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "#         actf = lambda x: x\n",
    "        actf = nn.ReLU()\n",
    "        hrnR0123fc = HierarchicalResidual_Connector(self.tree, hrnR0123, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR0123fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "        morphables = [self.root_net, hrnR0123, hrnR012, hrnR01, hrnR0, hrn3, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.non_linearity(self.root_net(x))\n",
    "\n",
    "    def add_neurons(self, num):\n",
    "        num_stat = num//2\n",
    "        num_random = num - num_stat\n",
    "        \n",
    "        DL = list(self.tree.DYNAMIC_LIST)\n",
    "        if num_random>0:\n",
    "            rands = torch.randint(high=len(DL), size=(num_random,))\n",
    "            index, count = torch.unique(rands, sorted=False, return_counts=True)\n",
    "            for i, idx in enumerate(index):\n",
    "                DL[idx].add_hidden_neuron(int(count[i]))\n",
    "\n",
    "        if num_stat>0:\n",
    "            del_neurons = []\n",
    "            for hr in DL:\n",
    "                if hr.residual:\n",
    "                    del_neurons.append(hr.residual.del_neurons)#+1e-7)\n",
    "                else:\n",
    "                    del_neurons.append(0.)#1e-7) ## residual layer yet not created \n",
    "            \n",
    "            prob_stat = torch.tensor(del_neurons)\n",
    "            prob_stat = torch.log(torch.exp(prob_stat)+1.)\n",
    "            m = torch.distributions.multinomial.Multinomial(total_count=num_stat,\n",
    "                                                            probs= prob_stat)\n",
    "            count = m.sample()#.type(torch.long)\n",
    "            for i, hr in enumerate(DL):\n",
    "                if count[i] < 1: continue\n",
    "                hr.add_hidden_neuron(int(count[i]))\n",
    "        \n",
    "        self.neurons_added += num \n",
    "        pass\n",
    "\n",
    "    def identify_removable_neurons(self, num=None, threshold_min=0., threshold_max=1.):\n",
    "        \n",
    "        all_sig = []\n",
    "        self.all_sig_ = []\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                all_sig.append(hr.residual.significance)\n",
    "        all_sigs = torch.cat(all_sig)\n",
    "        del all_sig\n",
    "        print(\"All_sigs\", all_sigs)\n",
    "        \n",
    "        \n",
    "#         print(\"Normalization\", (all_sigs/all_sigs.sum()).sum())\n",
    "        \n",
    "        ### Normalizes such that importance 1 is average importance\n",
    "        normalizer = float(torch.sum(all_sigs))/len(all_sigs)\n",
    "        all_sig = all_sigs/normalizer\n",
    "\n",
    "        ### Normalizes to range [0, 1]\n",
    "#         max_sig = all_sigs.max()\n",
    "#         all_sig = all_sigs/(max_sig+1e-9)\n",
    "        print(\"All_sig\", all_sig)\n",
    "        print(\"Sig sum\", all_sig.sum())\n",
    "        all_sig = all_sig[all_sig<threshold_max]\n",
    "        if len(all_sig)<1: ## if all significance is above threshold max \n",
    "            return 0, None, all_sigs\n",
    "        all_sig = torch.sort(all_sig)[0] ### sorted significance scores\n",
    "        \n",
    "        self.all_sig_ = all_sig\n",
    "        \n",
    "        if not num:num = int(np.ceil(self.neurons_added/self.tree.add_to_remove_ratio))\n",
    "        ## reset the neurons_added number if decay is started\n",
    "\n",
    "        remove_below = threshold_min\n",
    "        if num>len(all_sig):\n",
    "            remove_below = float(all_sig[-1])\n",
    "        elif num>0:\n",
    "            remove_below = float(all_sig[num-1])\n",
    "        \n",
    "        ### sig < threshold_min is always removed; whatsoever\n",
    "        if remove_below < threshold_min:\n",
    "            remove_below = threshold_min\n",
    "            \n",
    "        print(\"remove_below\", remove_below)\n",
    "        remove_below *= normalizer\n",
    "#         remove_below *= max_sig\n",
    "        print(\"remove_below\", remove_below)\n",
    "\n",
    "        self._remove_below = remove_below\n",
    "        return remove_below, all_sigs\n",
    "\n",
    "    def decay_neuron_start(self, decay_steps=1000):\n",
    "        if self._remove_below is None: return 0\n",
    "        \n",
    "        self.neurons_added = 0 ## resetting this variable\n",
    "        \n",
    "        self.tree.total_decay_steps = decay_steps\n",
    "        self.tree.current_decay_step = 0\n",
    "        self.tree.remove_neuron_residual = set()\n",
    "        self.tree.freeze_connection_shortcut = set()\n",
    "        self.tree.decay_connection_shortcut = set()\n",
    "        \n",
    "        count_remove = 0\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                count_remove += hr.residual.identify_removable_neurons(below=self._remove_below)\n",
    "        if count_remove<1:\n",
    "            self.tree.clear_decay_variables()\n",
    "        return count_remove\n",
    "    \n",
    "    def decay_neuron_step(self):\n",
    "        if self.tree.total_decay_steps is None:\n",
    "            return\n",
    "        \n",
    "        self.tree.current_decay_step += 1\n",
    "        \n",
    "        if self.tree.current_decay_step < self.tree.total_decay_steps:\n",
    "            self.tree.get_decay_factor()\n",
    "            for sh in self.tree.decay_connection_shortcut:\n",
    "                sh.decay_connection_step()\n",
    "            for sh in self.tree.freeze_connection_shortcut:\n",
    "                sh.freeze_connection_step()\n",
    "        else:\n",
    "            for rs in self.tree.remove_neuron_residual:\n",
    "                rs.remove_decayed_neurons()\n",
    "            \n",
    "            self.tree.clear_decay_variables()\n",
    "            \n",
    "    def compute_del_neurons(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.compute_del_neurons()\n",
    "    \n",
    "    def maintain_network(self):\n",
    "        self.root_net.maintain_shortcut_connection()\n",
    "        self.root_net.morph_network()\n",
    "        \n",
    "    def start_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.start_computing_significance()\n",
    "\n",
    "    def finish_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.finish_computing_significance()\n",
    "            \n",
    "    def print_network_debug(self):\n",
    "        self.root_net.print_network_debug(0)\n",
    "        \n",
    "    def print_network(self):\n",
    "        print(self.root_net.input_dim)\n",
    "        self.root_net.print_network()\n",
    "        print(\"│\")\n",
    "        print(self.root_net.output_dim)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dycnn = Dynamic_CNN(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(2, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0006, -0.0045, -0.0017,  0.0066, -0.0030,  0.0052,  0.0035,  0.0054,\n",
       "          0.0029, -0.0037],\n",
       "        [ 0.0006, -0.0046, -0.0018,  0.0070, -0.0032,  0.0056,  0.0037,  0.0056,\n",
       "          0.0030, -0.0039]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dycnn(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dycnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dycnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = datasets.MNIST()\n",
    "# train_data, train_label_, test_data, test_label_ = mnist.load(dirs=\"../../_Datasets/MNIST\")\n",
    "mnist = datasets.FashionMNIST()\n",
    "train_data, train_label_, test_data, test_label_ = mnist.load(dirs=\"../../_Datasets/F-MNIST\")\n",
    "\n",
    "train_data = train_data / 255.\n",
    "test_data = test_data / 255.\n",
    "\n",
    "train_data = train_data.reshape(-1, 1, 28,28)\n",
    "test_data = test_data.reshape(-1, 1, 28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "# learning_rate = 0.00003\n",
    "batch_size = 50\n",
    "\n",
    "# train_label = tnn.Logits.index_to_logit(train_label_)\n",
    "train_size = len(train_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1200)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size, train_size // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 200)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(test_label_)\n",
    "test_size, test_size // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting data to pytorch format\n",
    "train_data = torch.Tensor(train_data).to(device)\n",
    "test_data = torch.Tensor(test_data).to(device)\n",
    "train_label = torch.LongTensor(train_label_).to(device)\n",
    "test_label = torch.LongTensor(test_label_).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet = Dynamic_CNN(device).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameters\n",
    "num_add_neuron = 30#25#10\n",
    "num_decay_steps = int(train_size/batch_size*3)#3\n",
    "threshold_max = 2\n",
    "threshold_min = 0.001\n",
    "\n",
    "train_epoch_min = 1 #1\n",
    "train_epoch_max = 6 #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.add_to_remove_ratio = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decay_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoTrainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_func = None\n",
    "        self.adding_func = None\n",
    "        self.pruning_func = None\n",
    "        self.maintainance_func = None\n",
    "        self.extra_func = None\n",
    "        \n",
    "    def loop(self, count = 15):\n",
    "        cb = count\n",
    "        for i in range(count):\n",
    "            if i>0:\n",
    "                self.adding_func()\n",
    "            else:\n",
    "                global optimizer\n",
    "                dynet.print_network()    \n",
    "                optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "            \n",
    "            self.training_func()\n",
    "            \n",
    "            if i>0:\n",
    "                self.pruning_func()\n",
    "            \n",
    "            self.maintainance_func()\n",
    "            if self.extra_func:\n",
    "                self.extra_func()\n",
    "            \n",
    "            print(f\"=====================\")\n",
    "            print(f\"===LOOPS FINISHED :{i} ===\")\n",
    "        self.training_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when to stop training functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeff(num_iter, coeff0, coeff1, coeff2, coeff_opt, loss_list):\n",
    "    if len(loss_list)<10: return np.array([0]), np.array([0]), float(coeff0.data.cpu()[0])\n",
    "    \n",
    "    _t = torch.tensor(loss_list)\n",
    "    _t = (_t - _t[-1])/(_t[0]-_t.min()) ## normalize to make first point at 1 and last at 0 \n",
    "    _t = torch.clamp(_t, -1.1, 1.1)\n",
    "    _x = torch.linspace(0, 1, steps=len(_t))\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        coeff_opt.zero_grad()\n",
    "        _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "        _loss = ((_y - _t)**2).mean()\n",
    "        _loss.backward()\n",
    "        coeff_opt.step()\n",
    "\n",
    "        coeff0.data = torch.clamp(coeff0.data, -20., 20.)\n",
    "        coeff1.data = torch.clamp(coeff1.data, 0.7, 2.)\n",
    "        coeff2.data = torch.clamp(coeff2.data, -0.2,0.1)\n",
    "        \n",
    "    if torch.isnan(coeff0.data[0]):\n",
    "        coeff0.data[0] = 0.\n",
    "        coeff1.data[0] = 0.\n",
    "        coeff2.data[0] = 1. ## this gives signal\n",
    "        \n",
    "    _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "    return _x.numpy(), _t.numpy(), _y.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.FloatTensor([1, 5, np.nan])\n",
    "# if torch.isnan(a[0]):\n",
    "#     print('___')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data():\n",
    "    global train_data, train_label\n",
    "    randidx = random.sample(range(len(train_label)), k=len(train_label))\n",
    "    train_data = train_data[randidx]\n",
    "    train_label = train_label[randidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global variables\n",
    "optimizer = None\n",
    "coeff_opt = None\n",
    "\n",
    "loss_all = []\n",
    "accs_all = []\n",
    "accs_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neurons_func():\n",
    "    global optimizer\n",
    "    dynet.add_neurons(num_add_neuron)\n",
    "    dynet.print_network()    \n",
    "    optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_network_func():\n",
    "    global optimizer, loss_all, accs_all\n",
    "    \n",
    "    coeff0 = torch.zeros(1, requires_grad=True)\n",
    "    coeff1 = torch.zeros(1, requires_grad=True)\n",
    "    coeff2 = torch.zeros(1, requires_grad=True)\n",
    "    coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "    loss_list = []\n",
    "    prev_loss = None\n",
    "    beta_loss = (1000-1)/1000\n",
    "    loss_ = []\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "\n",
    "    steps_ = -1\n",
    "    for epoch in range(train_epoch_max):\n",
    "        \n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "#         print(epoch, train_size // batch_size)\n",
    "#         loss_ = []\n",
    "        shuffle_data()\n",
    "        for index in range(train_size // batch_size):\n",
    "            steps_ += 1\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "            train_x = train_data[index * batch_size:(index + 1) * batch_size]\n",
    "            train_y = train_label[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "            dynet.decay_neuron_step()\n",
    "            dynet.tree.std_loss = 0.    \n",
    "\n",
    "            yout = dynet(train_x)\n",
    "            loss = criterion(yout, train_y) + dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "            \n",
    "            if steps_>100:\n",
    "                prev_loss = (1-beta_loss)*float(loss)+beta_loss*prev_loss\n",
    "                loss_list.append(prev_loss)\n",
    "#                 loss_.append(float(loss))\n",
    "#                 loss_list.append(float(loss))\n",
    "            elif steps_ == 100:\n",
    "                loss_.append(float(loss))\n",
    "                prev_loss = np.mean(loss_)\n",
    "                loss_ = []\n",
    "            else:\n",
    "                loss_.append(float(loss))\n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=False)\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "#             targets = tnn.Logits.logit_to_index(train_y.data.cpu().numpy())\n",
    "            targets = train_y.data.cpu().numpy()\n",
    "\n",
    "            correct = (outputs == targets).sum()\n",
    "            train_acc += correct\n",
    "            train_count += len(outputs)\n",
    "#             print(train_size)\n",
    "\n",
    "#             if steps_%50 == 0 and steps_>100:\n",
    "#                 loss = np.mean(loss_)\n",
    "#                 loss_ = []\n",
    "#                 loss_list.append(loss)\n",
    "            \n",
    "            if steps_%100 == 0 and steps_>0:\n",
    "                if len(loss_list)>0:\n",
    "                    max_indx = np.argmax(loss_list)\n",
    "                    loss_list = loss_list[max_indx:]\n",
    "#                 loss_all.append(float(loss))\n",
    "                \n",
    "                _x, _t, _y = update_coeff(50, coeff0, coeff1, coeff2, coeff_opt, loss_list)\n",
    "                _c = float(coeff0.data.cpu()[0])\n",
    "    #             if coeff2.data[0] > 0.5: ## this is a signal to reset optimizer\n",
    "                coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "                _info = f'ES: {epoch}:{steps_}, coeff:{_c:.3f}/{-5}, \\nLoss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "#                 print(_info)\n",
    "\n",
    "                ax.clear()\n",
    "                if len(_x)>0:\n",
    "                    ax.plot(_x, _t, c='c')\n",
    "                    ax.plot(_x, _y, c='m')\n",
    "                xmin, xmax = ax.get_xlim()\n",
    "                ymin, ymax = ax.get_ylim()\n",
    "                ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                ax2.clear()\n",
    "                if len(accs_all)>0:\n",
    "                    acc_tr = accs_all\n",
    "                    if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                    ax2.plot(acc_tr, label=\"train\")\n",
    "                    \n",
    "                    ymin, ymax = ax2.get_ylim()\n",
    "#                     print()\n",
    "                    ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                    acc_tr = accs_test\n",
    "                    if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                    ax2.plot(acc_tr, label=\"test\")\n",
    "                    ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                    ax2.legend()\n",
    "\n",
    "                \n",
    "                fig.canvas.draw()\n",
    "                plt.savefig(\"./files/_temp_train_plot.png\")\n",
    "#                 plt.pause(0.01)\n",
    "#                 print(\"\\n\")\n",
    "\n",
    "                if _c < -5 and epoch>train_epoch_min: \n",
    "                    break\n",
    "                    \n",
    "        accs_all.append(train_acc/train_count*100.)\n",
    "        with torch.no_grad():\n",
    "            corrects = 0\n",
    "            for index in range(test_size // batch_size):\n",
    "                test_x = test_data[index * batch_size:(index + 1) * batch_size]\n",
    "                test_y = test_label_[index * batch_size:(index + 1) * batch_size]\n",
    "                yout = dynet.forward(test_x)\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                correct = (outputs == np.array(test_y)).sum()\n",
    "                corrects += correct\n",
    "            accs_test.append(corrects/test_size*100)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_func():\n",
    "    global optimizer\n",
    "    optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    dynet.start_computing_significance()\n",
    "\n",
    "#     steps_ = 0\n",
    "#     breakall = True\n",
    "    for index in range(train_size // batch_size):\n",
    "\n",
    "        train_x = train_data[index * batch_size:(index + 1) * batch_size]\n",
    "        train_y = train_label[index * batch_size:(index + 1) * batch_size]\n",
    "        dynet.tree.std_loss = 0.    \n",
    "\n",
    "        yout = dynet(train_x)\n",
    "\n",
    "        yout.backward(gradient=torch.ones_like(yout))\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "#             steps_+=1\n",
    "\n",
    "\n",
    "    dynet.finish_computing_significance()\n",
    "    dynet.identify_removable_neurons(num=None,\n",
    "                                 threshold_min = threshold_min,\n",
    "                                 threshold_max = threshold_max)\n",
    "    num_remove = dynet.decay_neuron_start(decay_steps=num_decay_steps)\n",
    "    if num_remove > 0:\n",
    "        print(f\"pruning {num_remove} neurons.\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,4))\n",
    "        ax = fig.add_subplot(121)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        \n",
    "        loss_list = []\n",
    "        steps_ = -1\n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "        breakall=False\n",
    "        for epoch in range(train_epoch_max):\n",
    "            loss_ = []\n",
    "            shuffle_data()\n",
    "            for index in range(train_size // batch_size):\n",
    "                steps_ += 1\n",
    "                train_x = train_data[index * batch_size:(index + 1) * batch_size]\n",
    "                train_y = train_label[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "                dynet.decay_neuron_step()\n",
    "                dynet.tree.std_loss = 0.    \n",
    "\n",
    "                yout = dynet(train_x)\n",
    "                loss = criterion(yout, train_y) + dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "\n",
    "                loss_.append(float(loss))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=False)\n",
    "                optimizer.step()\n",
    "\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "#                 targets = tnn.Logits.logit_to_index(train_y.data.cpu().numpy())\n",
    "                targets = train_y.data.cpu().numpy()\n",
    "                correct = (outputs == targets).sum()\n",
    "                train_acc += correct\n",
    "                train_count += len(outputs)\n",
    "\n",
    "                dynet.decay_neuron_step()\n",
    "                \n",
    "                if steps_%50 == 0 and steps_>0:\n",
    "                    loss = np.mean(loss_)\n",
    "                    loss_ = []\n",
    "                    loss_list.append(loss)\n",
    "                \n",
    "                if steps_%100 == 0 and steps_>0:\n",
    "                    \n",
    "                    _info = f'ES: {epoch}:{steps_}, Loss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "#                     print(_info)\n",
    "                    ax.clear()\n",
    "                    out = (yout.data.cpu().numpy()>0.5).astype(int)\n",
    "                    ax.plot(loss_list)\n",
    "                    \n",
    "                    xmin, xmax = ax.get_xlim()\n",
    "                    ymin, ymax = ax.get_ylim()\n",
    "                    ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                    ax2.clear()\n",
    "                    if len(accs_all)>0:\n",
    "                        acc_tr = accs_all\n",
    "                        if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                        ax2.plot(acc_tr, label=\"train\")\n",
    "\n",
    "                        ymin, ymax = ax2.get_ylim()\n",
    "                        ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                        acc_tr = accs_test\n",
    "                        if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                        ax2.plot(acc_tr, label=\"test\")\n",
    "                        ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                        ax2.legend()\n",
    "                    \n",
    "                    fig.canvas.draw()\n",
    "                    plt.savefig(\"./files/_temp_prune_plot.png\")\n",
    "#                     plt.pause(0.01)\n",
    "#                     print(\"\\n\")\n",
    "                    \n",
    "                if steps_>num_decay_steps+5: breakall=True\n",
    "                if breakall: break\n",
    "            if breakall: break\n",
    "                \n",
    "        accs_all.append(train_acc/train_count*100.)\n",
    "        with torch.no_grad():\n",
    "            corrects = 0\n",
    "            for index in range(test_size // batch_size):\n",
    "                test_x = test_data[index * batch_size:(index + 1) * batch_size]\n",
    "                test_y = test_label_[index * batch_size:(index + 1) * batch_size]\n",
    "                yout = dynet.forward(test_x)\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                correct = (outputs == np.array(test_y)).sum()\n",
    "                corrects += correct\n",
    "            accs_test.append(corrects/test_size*100)\n",
    "        plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintain_network():\n",
    "    dynet.compute_del_neurons()\n",
    "    dynet.maintain_network()\n",
    "    dynet.print_network()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set all functions and begin automated loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AutoTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.adding_func = add_neurons_func\n",
    "trainer.training_func = training_network_func\n",
    "trainer.pruning_func = pruning_func\n",
    "trainer.maintainance_func = maintain_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     8\n",
      "    ╔╝\n",
      "    8\n",
      "   ╔╝\n",
      "   16\n",
      "  ╔╝\n",
      "  32\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     8\n",
      "    ╔╝\n",
      "    8\n",
      "   ╔╝\n",
      "   16\n",
      "  ╔╝\n",
      "  32\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :0 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    10\n",
      "    ╠════╗\n",
      "    ║    6\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   19\n",
      "   ╠════╗\n",
      "   ║    4\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  35\n",
      "  ╠════╗\n",
      "  ║    2\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 36\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.9916e+05, 4.8590e+06, 2.8301e+06, 6.5908e+06, 4.6538e+04, 8.6415e+04,\n",
      "        7.0863e+04, 2.2915e+04, 4.6567e+06, 3.8539e+06, 6.0367e+06, 9.8640e+05,\n",
      "        1.0953e+06, 8.4874e+02, 4.9769e+04, 1.3289e+06, 3.3533e+05, 1.6105e+05,\n",
      "        2.0642e+05, 6.4997e+06, 3.4815e+05, 8.4173e+06, 0.0000e+00, 2.0780e+07,\n",
      "        2.4343e+04, 1.2781e+06, 5.2456e+05, 2.3777e+06, 6.7724e+05, 4.1768e+05,\n",
      "        2.8826e+06, 2.0123e+05, 1.6085e+03, 2.9303e+00, 3.3905e+03, 9.3969e+03],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8106, 0.6310, 0.9905, 0.9971, 0.0594, 0.4699, 0.8082, 0.9455, 0.8330,\n",
      "        0.7284, 0.9937, 0.8950, 0.7932, 0.8222, 0.9724, 0.9796, 0.5244, 0.7949,\n",
      "        0.5469, 0.6602, 0.1061, 0.9964, 0.0000, 0.9941, 1.0000, 0.4720, 0.6142,\n",
      "        0.7587, 0.8679, 0.7457, 0.6792, 1.0000, 0.3546, 0.6241, 0.6503, 0.6757],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.2966e+05, 7.1728e+06, 1.0748e+05, 7.7706e+04, 1.7509e+05, 1.8323e+05,\n",
      "        5.4366e+04, 4.9968e+03, 3.1099e+06, 4.1864e+06, 1.5242e+05, 4.1424e+05,\n",
      "        9.0581e+05, 6.0361e+02, 5.4858e+03, 1.0837e+05, 6.3797e+05, 1.3210e+05,\n",
      "        3.7407e+05, 8.8343e+06, 1.2449e+06, 1.2272e+05, 0.0000e+00, 4.9259e+05,\n",
      "        5.8039e-03, 2.6994e+06, 8.0958e+05, 2.2947e+06, 3.5793e+05, 4.2488e+05,\n",
      "        3.6986e+06, 4.7976e-02, 4.1522e+03, 4.4056e+00, 4.7423e+03, 1.2191e+04],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.5799e+04, 6.9027e+04, 2.6868e+06, 8.3501e+06, 1.1888e+05, 4.9200e+04,\n",
      "        1.3117e+05, 2.9258e+05, 1.6301e+05, 2.5789e+05, 8.2492e+04, 9.8200e+05,\n",
      "        1.6874e+04, 7.9333e+05, 5.3067e+06, 1.9183e+04, 3.9981e+06, 2.2858e+05,\n",
      "        6.2429e+04, 5.5583e+04, 1.4025e+05, 1.3495e+06, 1.9313e+04, 4.2851e+06,\n",
      "        3.7774e+05, 2.1885e+05, 7.3895e+05, 3.7993e+05, 6.7005e+05, 6.6553e+06,\n",
      "        1.7231e+05, 2.7300e+06, 1.7252e+04, 9.4529e+04, 3.9331e+03],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9983, 0.9052, 0.9902, 0.9984, 0.9990, 1.0000, 0.9827, 1.0000, 0.9787,\n",
      "        0.9931, 0.7346, 0.6514, 1.0000, 0.4308, 0.9922, 0.9922, 1.0000, 0.9913,\n",
      "        0.8831, 0.0992, 0.9536, 0.4106, 0.9945, 0.9923, 1.0000, 0.8072, 0.9455,\n",
      "        0.9999, 0.9824, 0.9937, 0.9077, 0.6638, 0.5941, 0.3742, 0.4495],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.7767e+02, 2.6162e+04, 1.0546e+05, 5.4630e+04, 4.9391e+02, 1.1730e-02,\n",
      "        9.0926e+03, 5.7201e+01, 1.3868e+04, 7.1050e+03, 8.7587e+04, 1.3691e+06,\n",
      "        4.0232e-03, 1.8062e+06, 1.6588e+05, 5.9660e+02, 9.5321e-01, 7.9931e+03,\n",
      "        2.9194e+04, 2.0027e+05, 2.6046e+04, 3.1813e+06, 4.2119e+02, 1.3220e+05,\n",
      "        1.8012e-01, 1.6878e+05, 1.6114e+05, 1.0978e+02, 4.7262e+04, 1.6895e+05,\n",
      "        6.3636e+04, 3.6709e+06, 2.8013e+04, 2.3664e+05, 8.6610e+03],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.0147e-09, 5.5895e+04, 2.1403e+02, 7.9636e+04], device='cuda:0')\n",
      "Apnz\n",
      "tensor([7.0578e-06, 5.5514e-01, 3.8732e-02, 5.3594e-01], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([8.0587e-09, 9.9461e+04, 8.2297e+02, 1.4782e+05], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.5398e+07, 1.7149e+04, 1.9716e+06, 1.1272e+07, 1.7967e+05, 1.6620e+06,\n",
      "        5.1847e+06, 4.1279e+04, 2.2873e+07, 1.1265e+06, 1.6759e+05, 9.6572e+05,\n",
      "        4.4436e+05, 2.1818e+06, 7.5894e+05, 4.0263e+06, 1.6503e+04, 4.4542e+03,\n",
      "        5.7117e+04], device='cuda:0')\n",
      "Apnz\n",
      "tensor([1.0000, 0.9993, 1.0000, 0.8841, 0.9974, 1.0000, 0.8434, 1.0000, 1.0000,\n",
      "        0.8585, 1.0000, 0.2960, 0.9960, 0.3248, 1.0000, 0.9125, 0.4329, 0.9815,\n",
      "        0.7279], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([8.4396e+00, 4.9346e+01, 4.7006e-01, 5.2252e+06, 1.8642e+03, 3.5663e+00,\n",
      "        3.2474e+06, 9.8418e-03, 5.4533e+00, 6.3765e+05, 1.1867e+01, 2.7195e+06,\n",
      "        7.0856e+03, 5.8927e+06, 1.8095e-01, 1.4086e+06, 3.7435e+04, 3.3007e+02,\n",
      "        6.2161e+04], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([5.9668e+03, 1.0182e+04, 5.0934e+05, 0.0000e+00, 2.2557e+02, 2.0504e+05],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.0837, 0.5926, 0.8870, 0.0000, 0.7765, 0.3101], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.1868e+04, 1.6590e+04, 2.3025e+05, 0.0000e+00, 2.0170e+02, 5.6587e+05],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([5.7101e+07, 1.5272e+07, 6.2002e+06, 1.3916e+06, 6.7880e+06, 3.6808e+05,\n",
      "        5.6215e+06, 1.4122e+06, 3.8819e+04, 1.1078e+06, 5.7905e+04],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9993, 0.9998, 0.9789, 0.5149, 1.0000, 0.4754, 0.9626, 0.5155, 0.2282,\n",
      "        0.9784, 0.5099], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.5197e+05, 1.2500e+04, 5.2442e+05, 2.7002e+06, 2.8969e+02, 7.7235e+05,\n",
      "        8.4141e+05, 2.7366e+06, 1.1984e+05, 9.5710e+04, 1.1353e+05],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2675.3303,  579.0768, 1670.5968], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5883, 0.5044, 0.6689], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4405.5972, 1148.0179, 2212.5710], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([9.3770e+05, 2.6615e+05, 1.1837e+06, 3.9218e+06, 7.4143e+07, 7.8382e+07,\n",
      "        5.2454e+04, 8.6926e+06, 3.0528e+04, 6.1387e+04], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9991, 1.0000, 0.9786, 0.9996, 1.0000, 1.0000, 0.6198, 0.9782, 0.5662,\n",
      "        0.8767], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.3318e+03, 6.3456e-02, 1.0133e+05, 6.9968e+03, 1.7677e+01, 1.8688e+01,\n",
      "        7.9773e+04, 7.5972e+05, 5.2971e+04, 3.0274e+04], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([38631.0625, 50836.1016], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2768, 0.5533], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([111758.8672,  90826.2578], device='cuda:0')\n",
      "All_sigs tensor([5.2966e+05, 7.1728e+06, 1.0748e+05, 7.7706e+04, 1.7509e+05, 1.8323e+05,\n",
      "        5.4366e+04, 4.9968e+03, 3.1099e+06, 4.1864e+06, 1.5242e+05, 4.1424e+05,\n",
      "        9.0581e+05, 6.0361e+02, 5.4858e+03, 1.0837e+05, 6.3797e+05, 1.3210e+05,\n",
      "        3.7407e+05, 8.8343e+06, 1.2449e+06, 1.2272e+05, 0.0000e+00, 4.9259e+05,\n",
      "        5.8039e-03, 2.6994e+06, 8.0958e+05, 2.2947e+06, 3.5793e+05, 4.2488e+05,\n",
      "        3.6986e+06, 4.7976e-02, 4.1522e+03, 4.4056e+00, 4.7423e+03, 1.2191e+04,\n",
      "        1.7767e+02, 2.6162e+04, 1.0546e+05, 5.4630e+04, 4.9391e+02, 1.1730e-02,\n",
      "        9.0926e+03, 5.7201e+01, 1.3868e+04, 7.1050e+03, 8.7587e+04, 1.3691e+06,\n",
      "        4.0232e-03, 1.8062e+06, 1.6588e+05, 5.9660e+02, 9.5321e-01, 7.9931e+03,\n",
      "        2.9194e+04, 2.0027e+05, 2.6046e+04, 3.1813e+06, 4.2119e+02, 1.3220e+05,\n",
      "        1.8012e-01, 1.6878e+05, 1.6114e+05, 1.0978e+02, 4.7262e+04, 1.6895e+05,\n",
      "        6.3636e+04, 3.6709e+06, 2.8013e+04, 2.3664e+05, 8.6610e+03, 8.0587e-09,\n",
      "        9.9461e+04, 8.2297e+02, 1.4782e+05, 8.4396e+00, 4.9346e+01, 4.7006e-01,\n",
      "        5.2252e+06, 1.8642e+03, 3.5663e+00, 3.2474e+06, 9.8418e-03, 5.4533e+00,\n",
      "        6.3765e+05, 1.1867e+01, 2.7195e+06, 7.0856e+03, 5.8927e+06, 1.8095e-01,\n",
      "        1.4086e+06, 3.7435e+04, 3.3007e+02, 6.2161e+04, 2.1868e+04, 1.6590e+04,\n",
      "        2.3025e+05, 0.0000e+00, 2.0170e+02, 5.6587e+05, 1.5197e+05, 1.2500e+04,\n",
      "        5.2442e+05, 2.7002e+06, 2.8969e+02, 7.7235e+05, 8.4141e+05, 2.7366e+06,\n",
      "        1.1984e+05, 9.5710e+04, 1.1353e+05, 4.4056e+03, 1.1480e+03, 2.2126e+03,\n",
      "        3.3318e+03, 6.3456e-02, 1.0133e+05, 6.9968e+03, 1.7677e+01, 1.8688e+01,\n",
      "        7.9773e+04, 7.5972e+05, 5.2971e+04, 3.0274e+04, 1.1176e+05, 9.0826e+04],\n",
      "       device='cuda:0')\n",
      "All_sig tensor([8.2648e-01, 1.1192e+01, 1.6772e-01, 1.2125e-01, 2.7321e-01, 2.8592e-01,\n",
      "        8.4833e-02, 7.7971e-03, 4.8527e+00, 6.5325e+00, 2.3783e-01, 6.4638e-01,\n",
      "        1.4134e+00, 9.4188e-04, 8.5601e-03, 1.6911e-01, 9.9549e-01, 2.0612e-01,\n",
      "        5.8370e-01, 1.3785e+01, 1.9426e+00, 1.9149e-01, 0.0000e+00, 7.6864e-01,\n",
      "        9.0564e-09, 4.2122e+00, 1.2633e+00, 3.5807e+00, 5.5852e-01, 6.6299e-01,\n",
      "        5.7713e+00, 7.4863e-08, 6.4791e-03, 6.8746e-06, 7.4000e-03, 1.9024e-02,\n",
      "        2.7723e-04, 4.0824e-02, 1.6456e-01, 8.5245e-02, 7.7070e-04, 1.8304e-08,\n",
      "        1.4188e-02, 8.9257e-05, 2.1640e-02, 1.1087e-02, 1.3667e-01, 2.1364e+00,\n",
      "        6.2778e-09, 2.8185e+00, 2.5884e-01, 9.3094e-04, 1.4874e-06, 1.2473e-02,\n",
      "        4.5554e-02, 3.1250e-01, 4.0642e-02, 4.9642e+00, 6.5722e-04, 2.0628e-01,\n",
      "        2.8106e-07, 2.6337e-01, 2.5144e-01, 1.7131e-04, 7.3748e-02, 2.6363e-01,\n",
      "        9.9298e-02, 5.7281e+00, 4.3712e-02, 3.6925e-01, 1.3515e-02, 1.2575e-14,\n",
      "        1.5520e-01, 1.2842e-03, 2.3066e-01, 1.3169e-05, 7.7001e-05, 7.3348e-07,\n",
      "        8.1535e+00, 2.9089e-03, 5.5650e-06, 5.0674e+00, 1.5357e-08, 8.5093e-06,\n",
      "        9.9500e-01, 1.8517e-05, 4.2436e+00, 1.1057e-02, 9.1950e+00, 2.8235e-07,\n",
      "        2.1980e+00, 5.8414e-02, 5.1504e-04, 9.6997e-02, 3.4123e-02, 2.5888e-02,\n",
      "        3.5928e-01, 0.0000e+00, 3.1473e-04, 8.8299e-01, 2.3714e-01, 1.9505e-02,\n",
      "        8.1831e-01, 4.2134e+00, 4.5204e-04, 1.2052e+00, 1.3130e+00, 4.2702e+00,\n",
      "        1.8700e-01, 1.4935e-01, 1.7715e-01, 6.8745e-03, 1.7914e-03, 3.4525e-03,\n",
      "        5.1990e-03, 9.9017e-08, 1.5812e-01, 1.0918e-02, 2.7583e-05, 2.9160e-05,\n",
      "        1.2448e-01, 1.1855e+00, 8.2657e-02, 4.7240e-02, 1.7439e-01, 1.4173e-01],\n",
      "       device='cuda:0')\n",
      "Sig sum tensor(126.0000, device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 640.856126984127\n",
      "pruning 31 neurons.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-51c6935b299d>:69: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  self.to_remove = torch.nonzero(self.significance<=below).reshape(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     10\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    7\n",
      "    ╠════╗\n",
      "    ║    4\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   10\n",
      "   ╠════╗\n",
      "   ║    3\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  25\n",
      "  ╠════╗\n",
      "  ║    2\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 31\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :1 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    7\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    9\n",
      "    ╠════╗\n",
      "    ║    6\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   12\n",
      "   ╠════╗\n",
      "   ║    8\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  29\n",
      "  ╠════╗\n",
      "  ║    7\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 35\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([7.6903e+05, 7.7502e+06, 1.0255e+06, 6.0953e+05, 1.8772e+05, 1.5826e+05,\n",
      "        1.8876e+04, 1.0288e+04, 1.3975e+06, 8.1968e+05, 3.3180e+06, 5.5007e+05,\n",
      "        7.2997e+04, 1.7894e+04, 3.8130e+05, 3.9821e+05, 3.8096e+05, 1.2319e+05,\n",
      "        3.4850e+06, 4.0576e+05, 7.3242e+06, 1.2326e+07, 2.0863e+06, 8.1264e+05,\n",
      "        3.1582e+06, 5.2213e+05, 3.1618e+05, 3.8309e+06, 6.4080e+04, 1.4051e+04,\n",
      "        1.7648e+04, 9.4074e+01, 1.1298e+02, 1.1668e+04, 2.0332e+04],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8298, 0.6957, 0.9701, 0.7247, 0.0944, 0.4886, 0.4544, 0.8981, 0.5360,\n",
      "        0.6234, 0.8483, 0.9039, 0.2690, 0.9524, 0.9129, 0.4982, 0.7367, 0.4466,\n",
      "        0.5467, 0.1855, 0.9778, 0.9998, 0.5400, 0.7093, 0.7446, 0.9040, 0.7274,\n",
      "        0.7368, 0.3281, 0.6435, 0.5513, 0.2298, 0.3509, 0.4366, 0.4060],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.2369e+05, 9.4341e+06, 1.2280e+05, 6.7128e+05, 6.8001e+05, 3.2374e+05,\n",
      "        4.1199e+04, 4.1951e+03, 2.5935e+06, 1.2346e+06, 2.0137e+06, 2.1149e+05,\n",
      "        2.1345e+05, 3.4046e+03, 1.3292e+05, 7.9934e+05, 4.0130e+05, 2.7270e+05,\n",
      "        6.3197e+06, 1.3220e+06, 6.4977e+05, 1.1708e+04, 3.8390e+06, 9.4482e+05,\n",
      "        3.2269e+06, 2.0047e+05, 3.4475e+05, 4.0336e+06, 1.7222e+05, 2.0036e+04,\n",
      "        3.1673e+04, 2.8982e+02, 2.9333e+02, 2.6296e+04, 4.8306e+04],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.2047e+05, 1.6248e+06, 1.4069e+07, 2.7916e+05, 3.7777e+05, 7.6274e+05,\n",
      "        1.9558e+05, 1.7308e+06, 3.6010e+06, 4.8142e+05, 4.7352e+05, 2.5911e+05,\n",
      "        1.4371e+05, 3.0303e+05, 4.4079e+06, 3.0060e+06, 3.4278e+05, 8.8071e+05,\n",
      "        4.7530e+05, 1.6851e+06, 8.0442e+05, 1.5347e+06, 9.8821e+05, 2.4201e+06,\n",
      "        1.6785e+04, 1.9618e+03, 8.6909e+04, 1.4713e+04, 6.6669e+04],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9363, 0.9978, 0.9999, 0.9391, 0.9511, 0.9679, 0.8198, 0.8534, 0.4996,\n",
      "        0.9901, 0.9588, 0.7375, 0.1381, 0.9512, 0.4883, 0.9951, 0.8031, 0.9389,\n",
      "        0.8262, 0.9463, 0.7432, 0.4220, 0.9360, 0.7526, 0.4953, 0.5333, 0.4873,\n",
      "        0.3518, 0.4491], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.6158e+04, 1.4600e+04, 6.4908e+03, 6.8026e+04, 7.3881e+04, 9.7836e+04,\n",
      "        1.4099e+05, 1.0153e+06, 7.2083e+06, 1.9038e+04, 7.8009e+04, 2.7207e+05,\n",
      "        4.9543e+05, 5.9197e+04, 9.0229e+06, 5.8533e+04, 2.7001e+05, 2.1542e+05,\n",
      "        3.3045e+05, 3.6214e+05, 8.2642e+05, 3.5481e+06, 2.5304e+05, 2.3953e+06,\n",
      "        3.3887e+04, 3.6625e+03, 1.7822e+05, 3.8149e+04, 1.4692e+05],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.2769e+06, 4.5006e+05, 3.2303e+06, 4.0599e+03, 4.0549e+04, 2.1291e+05,\n",
      "        7.4965e+02, 9.6784e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5908, 0.4116, 0.7993, 0.0901, 0.6009, 0.3301, 0.6255, 0.5093],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.0900e+06, 1.0593e+06, 2.5937e+06, 1.4776e+04, 6.4734e+04, 5.7049e+05,\n",
      "        1.1228e+03, 1.8997e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([13314858.0000,  2775413.2500,  6048216.0000,  1243302.2500,\n",
      "         4864882.0000,   172541.9219,  7173625.0000,   592902.5000,\n",
      "        24036626.0000,  3543258.0000,    30640.9258,   351280.6875],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8411, 0.9832, 0.8227, 0.5821, 0.6116, 0.6855, 0.4516, 0.7789, 0.9332,\n",
      "        0.6359, 0.5215, 0.5797], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([ 8460373.0000,   186032.5156,  4290442.5000,  2078335.6250,\n",
      "         7558452.0000,   217057.6406, 15735752.0000,   524424.3750,\n",
      "         6426569.0000,  5159718.0000,    58644.3945,   590621.4375],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.7410e+06, 3.3630e+05, 1.3202e+08, 5.6971e+05, 3.0524e+05, 1.2345e+05],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2842, 0.5606, 1.0000, 0.4496, 0.8330, 0.4664], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.9848e+06, 5.9104e+05, 1.2590e+02, 1.2542e+06, 2.0395e+05, 2.6349e+05],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.6463e+07, 4.0546e+06, 9.1703e+06, 1.6726e+06, 2.0347e+05, 4.5114e+06,\n",
      "        1.5653e+06, 6.3261e+05, 3.9118e+07, 3.1326e+05, 6.0087e+04, 2.7231e+04],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([1.0000, 1.0000, 0.9901, 0.5175, 0.4821, 0.9700, 0.5121, 0.1493, 0.9868,\n",
      "        0.8665, 0.4887, 0.0597], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([6.6335e+02, 9.6669e-01, 3.6409e+05, 3.2279e+06, 4.2150e+05, 5.4055e+05,\n",
      "        3.0551e+06, 2.1527e+06, 2.0654e+06, 1.6726e+05, 1.2290e+05, 1.0242e+05],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([ 432016.5625,  370530.3750, 1734156.0000, 1046086.8750,   86537.5078,\n",
      "        1552750.1250,    4487.0845], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9758, 0.8019, 0.9072, 0.8719, 0.3846, 0.9230, 0.2627],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([ 41856.3203, 293546.0312, 643414.5625, 536197.8750, 213034.8281,\n",
      "        478557.4062,  13233.7871], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.9466e+07, 2.7748e+06, 1.7152e+07, 1.6377e+06, 1.8423e+07, 1.3658e+06,\n",
      "        1.7962e+06, 4.1971e+04, 9.8517e+05], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9997, 0.9617, 0.7759, 0.5308, 0.9919, 0.9769, 0.8740, 0.3206, 0.7651],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([   50216.9102,   425146.4688, 15374565.0000,  3073282.0000,\n",
      "          598167.3750,   126056.2969,   905232.1250,   114056.3750,\n",
      "          925481.4375], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([ 55117.6484, 497522.3438,  51607.1445,  39088.6289,   5320.3623,\n",
      "           745.1192,  33266.6133], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2888, 0.3326, 0.6543, 0.2905, 0.5083, 0.3503, 0.4483],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([ 156790.3750, 1328224.7500,   71355.6328,  110929.1250,   10464.3506,\n",
      "           1936.2731,   73418.0859], device='cuda:0')\n",
      "All_sigs tensor([5.2369e+05, 9.4341e+06, 1.2280e+05, 6.7128e+05, 6.8001e+05, 3.2374e+05,\n",
      "        4.1199e+04, 4.1951e+03, 2.5935e+06, 1.2346e+06, 2.0137e+06, 2.1149e+05,\n",
      "        2.1345e+05, 3.4046e+03, 1.3292e+05, 7.9934e+05, 4.0130e+05, 2.7270e+05,\n",
      "        6.3197e+06, 1.3220e+06, 6.4977e+05, 1.1708e+04, 3.8390e+06, 9.4482e+05,\n",
      "        3.2269e+06, 2.0047e+05, 3.4475e+05, 4.0336e+06, 1.7222e+05, 2.0036e+04,\n",
      "        3.1673e+04, 2.8982e+02, 2.9333e+02, 2.6296e+04, 4.8306e+04, 5.6158e+04,\n",
      "        1.4600e+04, 6.4908e+03, 6.8026e+04, 7.3881e+04, 9.7836e+04, 1.4099e+05,\n",
      "        1.0153e+06, 7.2083e+06, 1.9038e+04, 7.8009e+04, 2.7207e+05, 4.9543e+05,\n",
      "        5.9197e+04, 9.0229e+06, 5.8533e+04, 2.7001e+05, 2.1542e+05, 3.3045e+05,\n",
      "        3.6214e+05, 8.2642e+05, 3.5481e+06, 2.5304e+05, 2.3953e+06, 3.3887e+04,\n",
      "        3.6625e+03, 1.7822e+05, 3.8149e+04, 1.4692e+05, 2.0900e+06, 1.0593e+06,\n",
      "        2.5937e+06, 1.4776e+04, 6.4734e+04, 5.7049e+05, 1.1228e+03, 1.8997e+03,\n",
      "        8.4604e+06, 1.8603e+05, 4.2904e+06, 2.0783e+06, 7.5585e+06, 2.1706e+05,\n",
      "        1.5736e+07, 5.2442e+05, 6.4266e+06, 5.1597e+06, 5.8644e+04, 5.9062e+05,\n",
      "        4.9848e+06, 5.9104e+05, 1.2590e+02, 1.2542e+06, 2.0395e+05, 2.6349e+05,\n",
      "        6.6335e+02, 9.6669e-01, 3.6409e+05, 3.2279e+06, 4.2150e+05, 5.4055e+05,\n",
      "        3.0551e+06, 2.1527e+06, 2.0654e+06, 1.6726e+05, 1.2290e+05, 1.0242e+05,\n",
      "        4.1856e+04, 2.9355e+05, 6.4341e+05, 5.3620e+05, 2.1303e+05, 4.7856e+05,\n",
      "        1.3234e+04, 5.0217e+04, 4.2515e+05, 1.5375e+07, 3.0733e+06, 5.9817e+05,\n",
      "        1.2606e+05, 9.0523e+05, 1.1406e+05, 9.2548e+05, 1.5679e+05, 1.3282e+06,\n",
      "        7.1356e+04, 1.1093e+05, 1.0464e+04, 1.9363e+03, 7.3418e+04],\n",
      "       device='cuda:0')\n",
      "All_sig tensor([3.8299e-01, 6.8994e+00, 8.9809e-02, 4.9092e-01, 4.9730e-01, 2.3676e-01,\n",
      "        3.0130e-02, 3.0680e-03, 1.8967e+00, 9.0291e-01, 1.4726e+00, 1.5467e-01,\n",
      "        1.5610e-01, 2.4899e-03, 9.7205e-02, 5.8457e-01, 2.9348e-01, 1.9943e-01,\n",
      "        4.6217e+00, 9.6683e-01, 4.7519e-01, 8.5620e-03, 2.8076e+00, 6.9096e-01,\n",
      "        2.3599e+00, 1.4660e-01, 2.5212e-01, 2.9499e+00, 1.2595e-01, 1.4653e-02,\n",
      "        2.3163e-02, 2.1195e-04, 2.1452e-04, 1.9231e-02, 3.5327e-02, 4.1070e-02,\n",
      "        1.0677e-02, 4.7468e-03, 4.9749e-02, 5.4031e-02, 7.1549e-02, 1.0311e-01,\n",
      "        7.4251e-01, 5.2716e+00, 1.3923e-02, 5.7049e-02, 1.9897e-01, 3.6232e-01,\n",
      "        4.3292e-02, 6.5987e+00, 4.2807e-02, 1.9746e-01, 1.5754e-01, 2.4167e-01,\n",
      "        2.6484e-01, 6.0438e-01, 2.5948e+00, 1.8506e-01, 1.7518e+00, 2.4782e-02,\n",
      "        2.6784e-03, 1.3033e-01, 2.7899e-02, 1.0745e-01, 1.5284e+00, 7.7472e-01,\n",
      "        1.8968e+00, 1.0806e-02, 4.7342e-02, 4.1721e-01, 8.2115e-04, 1.3893e-03,\n",
      "        6.1873e+00, 1.3605e-01, 3.1377e+00, 1.5199e+00, 5.5277e+00, 1.5874e-01,\n",
      "        1.1508e+01, 3.8352e-01, 4.6999e+00, 3.7734e+00, 4.2888e-02, 4.3193e-01,\n",
      "        3.6455e+00, 4.3224e-01, 9.2077e-05, 9.1724e-01, 1.4915e-01, 1.9269e-01,\n",
      "        4.8512e-04, 7.0696e-07, 2.6626e-01, 2.3606e+00, 3.0826e-01, 3.9532e-01,\n",
      "        2.2342e+00, 1.5743e+00, 1.5105e+00, 1.2232e-01, 8.9876e-02, 7.4904e-02,\n",
      "        3.0610e-02, 2.1468e-01, 4.7054e-01, 3.9213e-01, 1.5580e-01, 3.4998e-01,\n",
      "        9.6782e-03, 3.6725e-02, 3.1092e-01, 1.1244e+01, 2.2476e+00, 4.3745e-01,\n",
      "        9.2188e-02, 6.6202e-01, 8.3412e-02, 6.7682e-01, 1.1466e-01, 9.7136e-01,\n",
      "        5.2184e-02, 8.1125e-02, 7.6528e-03, 1.4160e-03, 5.3692e-02],\n",
      "       device='cuda:0')\n",
      "Sig sum tensor(125.0000, device='cuda:0')\n",
      "remove_below 0.009678150527179241\n",
      "remove_below 13233.788441562652\n",
      "pruning 15 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     10\n",
      "     ╠════╗\n",
      "     ║    6\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    9\n",
      "    ╠════╗\n",
      "    ║    5\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   12\n",
      "   ╠════╗\n",
      "   ║    6\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  27\n",
      "  ╠════╗\n",
      "  ║    5\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 30\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :2 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     14\n",
      "     ╠════╗\n",
      "     ║    8\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    11\n",
      "    ╠════╗\n",
      "    ║    10\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   13\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  30\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 35\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([8.8432e+05, 1.2044e+07, 8.6457e+05, 5.7154e+05, 2.8620e+05, 2.8076e+05,\n",
      "        2.2682e+04, 2.0062e+06, 3.4996e+05, 2.2374e+06, 4.0389e+05, 8.1776e+04,\n",
      "        6.0740e+05, 4.7631e+05, 8.1944e+05, 4.8303e+04, 6.2471e+06, 4.7355e+05,\n",
      "        9.0051e+06, 5.5518e+06, 5.7032e+05, 4.7834e+06, 4.1114e+05, 3.2179e+05,\n",
      "        5.2316e+06, 1.4222e+05, 2.8430e+04, 2.3032e+04, 4.4430e+04, 1.8109e+05,\n",
      "        6.4016e+02, 1.9736e+04, 4.1378e-01, 2.1242e+04, 1.4205e+03],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8373, 0.7150, 0.9490, 0.5618, 0.1121, 0.4990, 0.5672, 0.4753, 0.5032,\n",
      "        0.7870, 0.6663, 0.2713, 0.9707, 0.4694, 0.7510, 0.1965, 0.5345, 0.1799,\n",
      "        0.9487, 0.6623, 0.6637, 0.7577, 0.8703, 0.6126, 0.7488, 0.3253, 0.5874,\n",
      "        0.5252, 0.4084, 0.4323, 0.2454, 0.2173, 0.2326, 0.2083, 0.2804],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.7560e+05, 1.3730e+07, 1.7631e+05, 1.0019e+06, 1.0165e+06, 5.6262e+05,\n",
      "        3.9268e+04, 4.2107e+06, 6.9537e+05, 1.9062e+06, 5.3913e+05, 2.3835e+05,\n",
      "        7.1079e+04, 1.0108e+06, 8.1627e+05, 1.5524e+05, 1.1632e+07, 1.5534e+06,\n",
      "        1.8464e+06, 7.4994e+06, 7.6729e+05, 4.6360e+06, 2.1332e+05, 4.9865e+05,\n",
      "        5.2572e+06, 3.8383e+05, 4.6919e+04, 4.3746e+04, 1.0514e+05, 4.1120e+05,\n",
      "        1.9323e+03, 6.1794e+04, 1.2702e+00, 6.7266e+04, 4.0891e+03],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.1211e+05, 3.6420e+06, 3.6649e+05, 9.0381e+05, 7.5145e+05, 1.9467e+05,\n",
      "        4.2915e+06, 4.8723e+06, 1.1041e+06, 8.0044e+05, 3.1356e+05, 1.4262e+05,\n",
      "        2.1210e+05, 4.6694e+06, 5.6951e+06, 5.3533e+05, 6.1648e+05, 1.9682e+05,\n",
      "        4.0475e+06, 7.2865e+05, 4.7447e+06, 5.1750e+05, 1.8082e+06, 4.7906e+04,\n",
      "        6.1176e+05, 1.5740e+05, 2.2278e+05, 1.7297e+03, 1.9949e+03, 3.9438e+03],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9341, 0.9978, 0.9653, 0.9324, 0.9763, 0.8572, 0.8502, 0.5060, 0.9808,\n",
      "        0.9862, 0.7219, 0.2142, 0.9415, 0.4660, 1.0000, 0.7587, 0.9197, 0.6784,\n",
      "        0.9989, 0.8018, 0.4167, 0.9027, 0.5882, 0.5076, 0.4840, 0.5207, 0.4634,\n",
      "        0.3151, 0.3992, 0.3408], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.5947e+04, 3.2166e+04, 5.0920e+04, 2.4448e+05, 7.1310e+04, 1.1120e+05,\n",
      "        2.5719e+06, 9.6286e+06, 8.4624e+04, 4.4332e+04, 3.4882e+05, 4.4827e+05,\n",
      "        4.9670e+04, 9.9744e+06, 1.3578e+00, 5.1672e+05, 1.9793e+05, 2.5317e+05,\n",
      "        1.7124e+04, 5.7778e+05, 1.1070e+07, 2.0131e+05, 2.9786e+06, 9.4364e+04,\n",
      "        1.2626e+06, 3.0176e+05, 4.7820e+05, 4.7388e+03, 4.7942e+03, 1.0399e+04],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.2539e+06, 7.6087e+05, 5.0258e+06, 1.1045e+05, 3.1127e+05, 2.1898e+06,\n",
      "        2.2683e+03, 1.4683e+02, 3.3432e+04, 1.7993e+04], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.6185, 0.4286, 0.8635, 0.2788, 0.5662, 0.3391, 0.3030, 0.4596, 0.2557,\n",
      "        0.2471], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.9652e+06, 1.7390e+06, 2.7436e+06, 3.1864e+05, 5.4010e+05, 5.7889e+06,\n",
      "        6.3245e+03, 3.1739e+02, 9.9534e+04, 5.4187e+04], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([20134774.0000,  9055558.0000,  9583471.0000,  2560291.2500,\n",
      "         3699569.0000,   772588.3750,  6292381.0000,   693414.5625,\n",
      "         7455488.5000,  8349149.0000,   102055.7578,  1763709.2500,\n",
      "           21185.4473], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8412, 0.9134, 0.8324, 0.5854, 0.6138, 0.6514, 0.4342, 0.9370, 0.8537,\n",
      "        0.4986, 0.8037, 0.8036, 0.2074], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([12789038.0000,  3137983.2500,  6424166.5000,  4245482.5000,\n",
      "         5715385.5000,  1077190.8750, 14241115.0000,   174714.9062,\n",
      "         4363926.5000, 16743564.0000,    80118.0625,  1385717.8750,\n",
      "           67167.7109], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.5613e+06, 3.7239e+06, 4.2764e+05, 1.4607e+06, 2.7420e+06, 7.0213e+03,\n",
      "        3.4581e+03, 5.0587e+04, 7.4284e+01, 5.7694e+03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2206, 0.5645, 0.4616, 0.8192, 0.4796, 0.3639, 0.0760, 0.5458, 0.2521,\n",
      "        0.2794], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.8677e+06, 6.4868e+06, 9.2096e+05, 1.0561e+06, 5.7079e+06, 1.7864e+04,\n",
      "        1.2781e+04, 9.1904e+04, 2.2223e+02, 1.6630e+04], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.7356e+07, 4.1516e+06, 8.8409e+04, 1.7967e+07, 3.4453e+06, 1.2721e+06,\n",
      "        6.7653e+07, 3.4775e+06, 4.4861e+05, 8.4342e+04, 3.1938e+04, 1.3246e+04,\n",
      "        5.4691e+03, 1.7545e+04], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9942, 0.5175, 0.5072, 0.9721, 0.5078, 0.1780, 0.9882, 0.8586, 0.5041,\n",
      "        0.0638, 0.5299, 0.5899, 0.1059, 0.7780], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([ 866889.1250, 8012667.0000,  174263.9375, 2004453.7500, 6783732.0000,\n",
      "        4182758.0000, 3182183.5000, 1967379.8750,  889832.1250,  315837.4688,\n",
      "          60054.7227,   21726.3848,   19559.4668,   15577.4424],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.5739e+06, 2.7875e+05, 1.4774e+07, 6.2888e+06, 1.1313e+06, 1.8905e+07,\n",
      "        1.3935e+04, 6.0989e+03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9970, 0.3902, 0.8681, 0.8499, 0.2800, 0.9600, 0.4147, 0.3041],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([  18612.5293,  679911.0625, 7796373.0000, 3775718.5000, 3258408.0000,\n",
      "        3022190.2500,   32626.7227,   16976.5332], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.6672e+07, 1.1648e+06, 5.8203e+07, 4.4719e+06, 6.0494e+06, 3.8788e+06,\n",
      "        1.9417e+06, 3.4052e+04, 1.0812e+07, 1.0258e+05, 2.5976e+03],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8645, 0.9578, 0.8020, 0.6109, 0.9938, 0.9962, 0.8475, 0.4570, 0.9083,\n",
      "        0.1991, 0.5868], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.9874e+07, 1.9642e+05, 4.6104e+07, 6.9595e+06, 1.5002e+05, 5.9487e+04,\n",
      "        1.1844e+06, 7.3960e+04, 3.9663e+06, 3.2864e+05, 4.2938e+03],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.1221e+05, 5.3977e+05, 1.5460e+06, 1.1724e+05, 2.5059e+05, 1.4670e+03,\n",
      "        9.0672e+03, 7.9972e+04, 1.6990e+04], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2946, 0.4496, 0.6387, 0.3427, 0.4369, 0.4050, 0.5371, 0.3967, 0.2711],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([ 598754.9375, 1188250.8750, 2234352.7500,  308247.1875,  564440.8750,\n",
      "           3491.2429,   16787.4688,  192988.1562,   49532.7812],\n",
      "       device='cuda:0')\n",
      "All_sigs tensor([5.7560e+05, 1.3730e+07, 1.7631e+05, 1.0019e+06, 1.0165e+06, 5.6262e+05,\n",
      "        3.9268e+04, 4.2107e+06, 6.9537e+05, 1.9062e+06, 5.3913e+05, 2.3835e+05,\n",
      "        7.1079e+04, 1.0108e+06, 8.1627e+05, 1.5524e+05, 1.1632e+07, 1.5534e+06,\n",
      "        1.8464e+06, 7.4994e+06, 7.6729e+05, 4.6360e+06, 2.1332e+05, 4.9865e+05,\n",
      "        5.2572e+06, 3.8383e+05, 4.6919e+04, 4.3746e+04, 1.0514e+05, 4.1120e+05,\n",
      "        1.9323e+03, 6.1794e+04, 1.2702e+00, 6.7266e+04, 4.0891e+03, 5.5947e+04,\n",
      "        3.2166e+04, 5.0920e+04, 2.4448e+05, 7.1310e+04, 1.1120e+05, 2.5719e+06,\n",
      "        9.6286e+06, 8.4624e+04, 4.4332e+04, 3.4882e+05, 4.4827e+05, 4.9670e+04,\n",
      "        9.9744e+06, 1.3578e+00, 5.1672e+05, 1.9793e+05, 2.5317e+05, 1.7124e+04,\n",
      "        5.7778e+05, 1.1070e+07, 2.0131e+05, 2.9786e+06, 9.4364e+04, 1.2626e+06,\n",
      "        3.0176e+05, 4.7820e+05, 4.7388e+03, 4.7942e+03, 1.0399e+04, 4.9652e+06,\n",
      "        1.7390e+06, 2.7436e+06, 3.1864e+05, 5.4010e+05, 5.7889e+06, 6.3245e+03,\n",
      "        3.1739e+02, 9.9534e+04, 5.4187e+04, 1.2789e+07, 3.1380e+06, 6.4242e+06,\n",
      "        4.2455e+06, 5.7154e+06, 1.0772e+06, 1.4241e+07, 1.7471e+05, 4.3639e+06,\n",
      "        1.6744e+07, 8.0118e+04, 1.3857e+06, 6.7168e+04, 4.8677e+06, 6.4868e+06,\n",
      "        9.2096e+05, 1.0561e+06, 5.7079e+06, 1.7864e+04, 1.2781e+04, 9.1904e+04,\n",
      "        2.2223e+02, 1.6630e+04, 8.6689e+05, 8.0127e+06, 1.7426e+05, 2.0045e+06,\n",
      "        6.7837e+06, 4.1828e+06, 3.1822e+06, 1.9674e+06, 8.8983e+05, 3.1584e+05,\n",
      "        6.0055e+04, 2.1726e+04, 1.9559e+04, 1.5577e+04, 1.8613e+04, 6.7991e+05,\n",
      "        7.7964e+06, 3.7757e+06, 3.2584e+06, 3.0222e+06, 3.2627e+04, 1.6977e+04,\n",
      "        1.9874e+07, 1.9642e+05, 4.6104e+07, 6.9595e+06, 1.5002e+05, 5.9487e+04,\n",
      "        1.1844e+06, 7.3960e+04, 3.9663e+06, 3.2864e+05, 4.2938e+03, 5.9875e+05,\n",
      "        1.1883e+06, 2.2344e+06, 3.0825e+05, 5.6444e+05, 3.4912e+03, 1.6787e+04,\n",
      "        1.9299e+05, 4.9533e+04], device='cuda:0')\n",
      "All_sig tensor([2.3667e-01, 5.6453e+00, 7.2494e-02, 4.1193e-01, 4.1793e-01, 2.3133e-01,\n",
      "        1.6146e-02, 1.7313e+00, 2.8591e-01, 7.8377e-01, 2.2167e-01, 9.8002e-02,\n",
      "        2.9225e-02, 4.1563e-01, 3.3562e-01, 6.3831e-02, 4.7825e+00, 6.3870e-01,\n",
      "        7.5916e-01, 3.0835e+00, 3.1548e-01, 1.9061e+00, 8.7710e-02, 2.0503e-01,\n",
      "        2.1616e+00, 1.5782e-01, 1.9292e-02, 1.7987e-02, 4.3231e-02, 1.6907e-01,\n",
      "        7.9448e-04, 2.5408e-02, 5.2225e-07, 2.7657e-02, 1.6813e-03, 2.3003e-02,\n",
      "        1.3225e-02, 2.0936e-02, 1.0052e-01, 2.9320e-02, 4.5722e-02, 1.0575e+00,\n",
      "        3.9589e+00, 3.4794e-02, 1.8228e-02, 1.4342e-01, 1.8431e-01, 2.0423e-02,\n",
      "        4.1011e+00, 5.5829e-07, 2.1246e-01, 8.1383e-02, 1.0410e-01, 7.0407e-03,\n",
      "        2.3756e-01, 4.5517e+00, 8.2772e-02, 1.2247e+00, 3.8799e-02, 5.1912e-01,\n",
      "        1.2407e-01, 1.9662e-01, 1.9484e-03, 1.9712e-03, 4.2758e-03, 2.0415e+00,\n",
      "        7.1501e-01, 1.1281e+00, 1.3102e-01, 2.2207e-01, 2.3802e+00, 2.6004e-03,\n",
      "        1.3050e-04, 4.0925e-02, 2.2280e-02, 5.2584e+00, 1.2902e+00, 2.6414e+00,\n",
      "        1.7456e+00, 2.3500e+00, 4.4290e-01, 5.8554e+00, 7.1837e-02, 1.7943e+00,\n",
      "        6.8844e+00, 3.2942e-02, 5.6976e-01, 2.7617e-02, 2.0014e+00, 2.6672e+00,\n",
      "        3.7866e-01, 4.3423e-01, 2.3469e+00, 7.3452e-03, 5.2552e-03, 3.7788e-02,\n",
      "        9.1373e-05, 6.8377e-03, 3.5643e-01, 3.2945e+00, 7.1651e-02, 8.2416e-01,\n",
      "        2.7892e+00, 1.7198e+00, 1.3084e+00, 8.0892e-01, 3.6587e-01, 1.2986e-01,\n",
      "        2.4692e-02, 8.9331e-03, 8.0422e-03, 6.4049e-03, 7.6528e-03, 2.7956e-01,\n",
      "        3.2056e+00, 1.5524e+00, 1.3397e+00, 1.2426e+00, 1.3415e-02, 6.9801e-03,\n",
      "        8.1714e+00, 8.0761e-02, 1.8956e+01, 2.8615e+00, 6.1684e-02, 2.4459e-02,\n",
      "        4.8699e-01, 3.0410e-02, 1.6308e+00, 1.3513e-01, 1.7655e-03, 2.4619e-01,\n",
      "        4.8857e-01, 9.1869e-01, 1.2674e-01, 2.3208e-01, 1.4355e-03, 6.9024e-03,\n",
      "        7.9350e-02, 2.0366e-02], device='cuda:0')\n",
      "Sig sum tensor(140.0000, device='cuda:0')\n",
      "remove_below 0.006837700493633747\n",
      "remove_below 16630.08624393493\n",
      "pruning 15 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     13\n",
      "     ╠════╗\n",
      "     ║    8\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    10\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   13\n",
      "   ╠════╗\n",
      "   ║    8\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  26\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :3 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    13\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    13\n",
      "    ╠════╗\n",
      "    ║    9\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   15\n",
      "   ╠════╗\n",
      "   ║    11\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  30\n",
      "  ╠════╗\n",
      "  ║    11\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 38\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([8.5994e+05, 1.3887e+07, 1.0434e+06, 5.4009e+05, 4.0475e+05, 2.9592e+05,\n",
      "        3.3662e+04, 1.8795e+06, 4.4595e+05, 2.4127e+06, 4.0977e+05, 6.5677e+04,\n",
      "        5.1837e+05, 4.3054e+05, 1.0025e+06, 1.0536e+05, 7.4208e+06, 8.2019e+05,\n",
      "        1.0929e+07, 5.2115e+06, 6.1362e+05, 4.6042e+06, 4.7744e+05, 3.7967e+05,\n",
      "        3.5549e+06, 1.7409e+05, 2.9071e+04, 2.3788e+04, 1.8237e+05, 2.3601e+05,\n",
      "        7.8010e+04, 1.1561e+05, 2.7838e+02, 8.2382e+01, 1.2783e+00, 3.0248e+02,\n",
      "        6.2652e+01, 8.4378e+00], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8352, 0.7160, 0.9302, 0.3637, 0.1570, 0.4763, 0.5867, 0.4782, 0.4383,\n",
      "        0.8276, 0.6599, 0.2639, 0.9515, 0.4368, 0.7274, 0.2401, 0.5470, 0.2132,\n",
      "        0.9587, 0.6282, 0.6832, 0.7389, 0.8929, 0.5722, 0.6465, 0.3426, 0.5287,\n",
      "        0.5758, 0.5045, 0.3921, 0.1823, 0.1670, 0.1943, 0.0673, 0.1992, 0.4088,\n",
      "        0.1386, 0.1776], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.6673e+05, 1.5774e+07, 2.9145e+05, 1.3746e+06, 1.3648e+06, 6.1991e+05,\n",
      "        5.5654e+04, 3.9226e+06, 1.0019e+06, 1.6639e+06, 5.5738e+05, 1.9339e+05,\n",
      "        1.0053e+05, 9.6994e+05, 1.0931e+06, 3.2025e+05, 1.3447e+07, 2.5812e+06,\n",
      "        1.8063e+06, 7.7495e+06, 7.7747e+05, 4.8091e+06, 2.0459e+05, 6.4963e+05,\n",
      "        5.0260e+06, 4.5779e+05, 5.4809e+04, 4.0360e+04, 3.6145e+05, 5.7391e+05,\n",
      "        2.5514e+05, 3.8521e+05, 8.9711e+02, 3.0734e+02, 4.0945e+00, 7.1529e+02,\n",
      "        2.1587e+02, 2.7757e+01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.9614e+05, 5.4889e+06, 1.3631e+05, 1.2232e+06, 7.6447e+05, 2.0097e+05,\n",
      "        5.8702e+06, 5.4768e+06, 1.2462e+06, 7.3221e+05, 3.2938e+05, 2.2231e+05,\n",
      "        2.9822e+05, 4.3230e+06, 4.8206e+05, 8.4399e+05, 1.9005e+05, 6.9508e+06,\n",
      "        1.0093e+06, 6.3189e+06, 4.6950e+05, 1.7766e+06, 3.3302e+04, 1.6722e+06,\n",
      "        2.1804e+05, 3.8415e+05, 4.1887e+03, 2.3180e+04, 4.3247e+04, 2.8225e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9275, 0.9943, 0.8506, 0.9270, 0.9767, 0.8977, 0.8233, 0.5001, 0.9324,\n",
      "        0.9752, 0.7118, 0.2107, 0.9011, 0.4313, 0.6535, 0.8845, 0.6372, 0.9995,\n",
      "        0.7792, 0.4164, 0.9028, 0.5389, 0.4996, 0.4695, 0.6007, 0.4558, 0.2904,\n",
      "        0.1973, 0.4734, 0.3090], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([8.5917e+04, 1.2416e+05, 8.1461e+04, 3.5711e+05, 7.1232e+04, 8.2213e+04,\n",
      "        4.1482e+06, 1.0952e+07, 3.3706e+05, 7.2703e+04, 3.7967e+05, 7.0186e+05,\n",
      "        1.1798e+05, 9.8339e+06, 6.6816e+05, 3.8995e+05, 2.7577e+05, 1.4007e+04,\n",
      "        8.9141e+05, 1.4750e+07, 1.8262e+05, 3.2768e+06, 6.6659e+04, 3.5482e+06,\n",
      "        3.4826e+05, 8.3621e+05, 1.1889e+04, 7.4426e+04, 9.1089e+04, 7.8016e+02],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.9286e+06, 2.7906e+05, 6.3357e+06, 2.0269e+05, 2.8336e+05, 4.2625e+06,\n",
      "        1.7990e+05, 1.1844e+05, 2.6200e+03, 3.7013e+02, 7.9724e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5896, 0.4296, 0.8806, 0.3143, 0.5694, 0.3063, 0.2538, 0.2777, 0.4630,\n",
      "        0.1455, 0.2183], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([8.0912e+06, 6.3666e+05, 3.0263e+06, 5.5595e+05, 4.8808e+05, 1.1827e+07,\n",
      "        5.3694e+05, 3.4223e+05, 5.6276e+03, 1.2652e+03, 2.4928e+03],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.0881e+07, 1.4333e+07, 1.0913e+07, 2.9563e+06, 5.2769e+06, 2.2438e+06,\n",
      "        6.8884e+06, 1.5169e+06, 5.3536e+06, 1.6537e+07, 1.2935e+05, 5.3652e+06,\n",
      "        6.2754e+04, 8.6511e+03, 5.8479e+03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8389, 0.7828, 0.8272, 0.6765, 0.6271, 0.6295, 0.4685, 0.9270, 0.8748,\n",
      "        0.5154, 0.8268, 0.8436, 0.2644, 0.4220, 0.5321], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.3456e+07, 1.2451e+07, 7.5410e+06, 3.8251e+06, 7.8716e+06, 3.3258e+06,\n",
      "        1.4644e+07, 4.4298e+05, 2.6802e+06, 3.2054e+07, 8.9600e+04, 3.3569e+06,\n",
      "        1.8464e+05, 2.0000e+04, 1.0946e+04], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2218944.7500, 6054550.0000,  832596.2500, 8275964.0000, 6767694.5000,\n",
      "          80875.3750,  749697.8125,   70276.3828,   28547.0273],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2179, 0.5700, 0.4704, 0.8075, 0.4923, 0.3758, 0.6333, 0.1223, 0.0634],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([ 6941493.5000, 10414591.0000,  1763660.8750,  6371724.5000,\n",
      "        13743598.0000,   201944.8281,  1099794.0000,   246733.3750,\n",
      "          106950.1875], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([8.4774e+07, 5.6838e+06, 4.0097e+05, 3.0548e+07, 7.6040e+06, 2.3323e+06,\n",
      "        3.6852e+07, 1.6461e+07, 8.5642e+05, 1.0996e+05, 5.3916e+05, 2.2555e+05,\n",
      "        8.0415e+04, 3.5012e+05, 2.7481e+04], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9943, 0.5182, 0.5063, 0.9710, 0.5063, 0.2196, 0.9890, 0.8684, 0.5221,\n",
      "        0.0644, 0.5963, 0.6219, 0.1715, 0.7750, 0.3985], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([ 1927679.8750, 10954046.0000,   791877.5000,  3545753.0000,\n",
      "        15016666.0000,  7280145.0000,  1619959.7500,  8665382.0000,\n",
      "         1637115.2500,   411496.0625,   870639.0000,   341117.9375,\n",
      "          266508.4062,   315046.0000,    66116.8047], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.4774e+06, 5.9690e+05, 3.0916e+07, 1.6116e+07, 1.1384e+07, 2.8977e+07,\n",
      "        3.7862e+05, 9.4012e+04, 1.4571e+03, 5.6100e+04, 3.2291e+04, 9.5318e+04,\n",
      "        1.1563e+04], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9859, 0.3985, 0.8682, 0.8790, 0.3613, 0.9573, 0.5076, 0.3149, 0.2473,\n",
      "        0.2035, 0.0708, 0.1712, 0.2040], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.3935e+05, 1.4361e+06, 1.6298e+07, 7.8018e+06, 2.9083e+07, 4.9442e+06,\n",
      "        7.4575e+05, 2.5763e+05, 4.3872e+03, 1.7873e+05, 1.2002e+05, 3.1601e+05,\n",
      "        3.6818e+04], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.6030e+07, 1.3146e+06, 8.8075e+07, 5.6148e+06, 1.0487e+06, 1.2372e+07,\n",
      "        4.0187e+06, 9.8527e+04, 1.5987e+07, 8.5934e+05, 5.0402e+03, 1.6520e+04,\n",
      "        9.7703e+04], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8248, 0.9567, 0.7661, 0.6338, 0.9870, 0.9869, 0.8588, 0.5925, 0.9006,\n",
      "        0.2250, 0.4974, 0.1311, 0.4416], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.2250e+07, 2.2757e+05, 8.2408e+07, 8.2252e+06, 5.4454e+04, 6.4934e+05,\n",
      "        2.2703e+06, 1.6060e+05, 6.3539e+06, 2.6641e+06, 1.0133e+04, 5.7421e+04,\n",
      "        2.1825e+05], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.4624e+05, 7.2543e+05, 4.2280e+06, 9.9026e+04, 7.2438e+05, 8.8146e+04,\n",
      "        2.9392e+05, 1.4740e+05, 1.5813e+03, 1.4283e+03, 5.9634e+03],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3310, 0.5098, 0.5959, 0.3854, 0.4053, 0.6429, 0.3597, 0.2524, 0.2091,\n",
      "        0.2232, 0.2771], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.1940e+06, 1.4223e+06, 6.8346e+06, 2.4344e+05, 1.7233e+06, 1.2591e+05,\n",
      "        7.5280e+05, 4.4080e+05, 5.0023e+03, 4.4381e+03, 1.7243e+04],\n",
      "       device='cuda:0')\n",
      "All_sigs tensor([5.6673e+05, 1.5774e+07, 2.9145e+05, 1.3746e+06, 1.3648e+06, 6.1991e+05,\n",
      "        5.5654e+04, 3.9226e+06, 1.0019e+06, 1.6639e+06, 5.5738e+05, 1.9339e+05,\n",
      "        1.0053e+05, 9.6994e+05, 1.0931e+06, 3.2025e+05, 1.3447e+07, 2.5812e+06,\n",
      "        1.8063e+06, 7.7495e+06, 7.7747e+05, 4.8091e+06, 2.0459e+05, 6.4963e+05,\n",
      "        5.0260e+06, 4.5779e+05, 5.4809e+04, 4.0360e+04, 3.6145e+05, 5.7391e+05,\n",
      "        2.5514e+05, 3.8521e+05, 8.9711e+02, 3.0734e+02, 4.0945e+00, 7.1529e+02,\n",
      "        2.1587e+02, 2.7757e+01, 8.5917e+04, 1.2416e+05, 8.1461e+04, 3.5711e+05,\n",
      "        7.1232e+04, 8.2213e+04, 4.1482e+06, 1.0952e+07, 3.3706e+05, 7.2703e+04,\n",
      "        3.7967e+05, 7.0186e+05, 1.1798e+05, 9.8339e+06, 6.6816e+05, 3.8995e+05,\n",
      "        2.7577e+05, 1.4007e+04, 8.9141e+05, 1.4750e+07, 1.8262e+05, 3.2768e+06,\n",
      "        6.6659e+04, 3.5482e+06, 3.4826e+05, 8.3621e+05, 1.1889e+04, 7.4426e+04,\n",
      "        9.1089e+04, 7.8016e+02, 8.0912e+06, 6.3666e+05, 3.0263e+06, 5.5595e+05,\n",
      "        4.8808e+05, 1.1827e+07, 5.3694e+05, 3.4223e+05, 5.6276e+03, 1.2652e+03,\n",
      "        2.4928e+03, 1.3456e+07, 1.2451e+07, 7.5410e+06, 3.8251e+06, 7.8716e+06,\n",
      "        3.3258e+06, 1.4644e+07, 4.4298e+05, 2.6802e+06, 3.2054e+07, 8.9600e+04,\n",
      "        3.3569e+06, 1.8464e+05, 2.0000e+04, 1.0946e+04, 6.9415e+06, 1.0415e+07,\n",
      "        1.7637e+06, 6.3717e+06, 1.3744e+07, 2.0194e+05, 1.0998e+06, 2.4673e+05,\n",
      "        1.0695e+05, 1.9277e+06, 1.0954e+07, 7.9188e+05, 3.5458e+06, 1.5017e+07,\n",
      "        7.2801e+06, 1.6200e+06, 8.6654e+06, 1.6371e+06, 4.1150e+05, 8.7064e+05,\n",
      "        3.4112e+05, 2.6651e+05, 3.1505e+05, 6.6117e+04, 1.3935e+05, 1.4361e+06,\n",
      "        1.6298e+07, 7.8018e+06, 2.9083e+07, 4.9442e+06, 7.4575e+05, 2.5763e+05,\n",
      "        4.3872e+03, 1.7873e+05, 1.2002e+05, 3.1601e+05, 3.6818e+04, 3.2250e+07,\n",
      "        2.2757e+05, 8.2408e+07, 8.2252e+06, 5.4454e+04, 6.4934e+05, 2.2703e+06,\n",
      "        1.6060e+05, 6.3539e+06, 2.6641e+06, 1.0133e+04, 5.7421e+04, 2.1825e+05,\n",
      "        1.1940e+06, 1.4223e+06, 6.8346e+06, 2.4344e+05, 1.7233e+06, 1.2591e+05,\n",
      "        7.5280e+05, 4.4080e+05, 5.0023e+03, 4.4381e+03, 1.7243e+04],\n",
      "       device='cuda:0')\n",
      "All_sig tensor([1.5868e-01, 4.4168e+00, 8.1605e-02, 3.8490e-01, 3.8214e-01, 1.7358e-01,\n",
      "        1.5583e-02, 1.0983e+00, 2.8053e-01, 4.6589e-01, 1.5607e-01, 5.4149e-02,\n",
      "        2.8150e-02, 2.7159e-01, 3.0606e-01, 8.9671e-02, 3.7651e+00, 7.2276e-01,\n",
      "        5.0577e-01, 2.1699e+00, 2.1769e-01, 1.3466e+00, 5.7286e-02, 1.8190e-01,\n",
      "        1.4073e+00, 1.2818e-01, 1.5347e-02, 1.1301e-02, 1.0121e-01, 1.6070e-01,\n",
      "        7.1441e-02, 1.0786e-01, 2.5119e-04, 8.6055e-05, 1.1465e-06, 2.0028e-04,\n",
      "        6.0444e-05, 7.7719e-06, 2.4057e-02, 3.4766e-02, 2.2809e-02, 9.9992e-02,\n",
      "        1.9945e-02, 2.3020e-02, 1.1615e+00, 3.0666e+00, 9.4378e-02, 2.0357e-02,\n",
      "        1.0631e-01, 1.9652e-01, 3.3035e-02, 2.7535e+00, 1.8709e-01, 1.0919e-01,\n",
      "        7.7215e-02, 3.9219e-03, 2.4960e-01, 4.1300e+00, 5.1133e-02, 9.1751e-01,\n",
      "        1.8665e-02, 9.9351e-01, 9.7515e-02, 2.3414e-01, 3.3290e-03, 2.0839e-02,\n",
      "        2.5505e-02, 2.1845e-04, 2.2656e+00, 1.7827e-01, 8.4736e-01, 1.5567e-01,\n",
      "        1.3666e-01, 3.3117e+00, 1.5034e-01, 9.5825e-02, 1.5758e-03, 3.5425e-04,\n",
      "        6.9799e-04, 3.7678e+00, 3.4862e+00, 2.1115e+00, 1.0710e+00, 2.2041e+00,\n",
      "        9.3123e-01, 4.1005e+00, 1.2404e-01, 7.5048e-01, 8.9752e+00, 2.5088e-02,\n",
      "        9.3994e-01, 5.1699e-02, 5.6002e-03, 3.0648e-03, 1.9436e+00, 2.9161e+00,\n",
      "        4.9383e-01, 1.7841e+00, 3.8482e+00, 5.6545e-02, 3.0794e-01, 6.9086e-02,\n",
      "        2.9946e-02, 5.3975e-01, 3.0672e+00, 2.2173e-01, 9.9282e-01, 4.2047e+00,\n",
      "        2.0385e+00, 4.5359e-01, 2.4263e+00, 4.5840e-01, 1.1522e-01, 2.4378e-01,\n",
      "        9.5514e-02, 7.4623e-02, 8.8214e-02, 1.8513e-02, 3.9019e-02, 4.0212e-01,\n",
      "        4.5634e+00, 2.1845e+00, 8.1433e+00, 1.3844e+00, 2.0881e-01, 7.2137e-02,\n",
      "        1.2284e-03, 5.0044e-02, 3.3605e-02, 8.8483e-02, 1.0309e-02, 9.0302e+00,\n",
      "        6.3720e-02, 2.3074e+01, 2.3031e+00, 1.5247e-02, 1.8182e-01, 6.3569e-01,\n",
      "        4.4967e-02, 1.7791e+00, 7.4595e-01, 2.8372e-03, 1.6078e-02, 6.1110e-02,\n",
      "        3.3434e-01, 3.9826e-01, 1.9137e+00, 6.8163e-02, 4.8252e-01, 3.5256e-02,\n",
      "        2.1078e-01, 1.2342e-01, 1.4007e-03, 1.2427e-03, 4.8281e-03],\n",
      "       device='cuda:0')\n",
      "Sig sum tensor(155., device='cuda:0')\n",
      "remove_below 0.0030648100655525923\n",
      "remove_below 10945.659662623368\n",
      "pruning 15 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    12\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    12\n",
      "    ╠════╗\n",
      "    ║    9\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   14\n",
      "   ╠════╗\n",
      "   ║    8\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  29\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :4 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     22\n",
      "     ╠════╗\n",
      "     ║    16\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    16\n",
      "    ╠════╗\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   16\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    11\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 36\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.0924e+05, 1.2425e+07, 1.0148e+06, 9.0946e+05, 5.6111e+05, 4.0286e+05,\n",
      "        3.8965e+04, 1.8367e+06, 4.3067e+05, 2.6230e+06, 3.9651e+05, 7.7256e+04,\n",
      "        4.6700e+05, 7.0118e+05, 8.4721e+05, 8.6502e+04, 6.3690e+06, 6.2387e+05,\n",
      "        1.0659e+07, 7.2404e+06, 8.4656e+05, 6.0968e+06, 4.1721e+05, 2.8658e+05,\n",
      "        3.0300e+06, 1.6381e+05, 3.5336e+04, 1.5261e+04, 1.8978e+05, 3.4500e+05,\n",
      "        1.5582e+05, 2.2713e+05, 2.3248e+02, 2.3143e+02, 1.0697e+02, 2.0836e+01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8152, 0.6922, 0.8952, 0.4442, 0.2473, 0.4884, 0.5706, 0.4874, 0.4329,\n",
      "        0.7741, 0.6478, 0.2886, 0.9477, 0.4683, 0.7019, 0.2146, 0.5260, 0.1959,\n",
      "        0.9662, 0.6685, 0.7036, 0.7882, 0.8305, 0.5312, 0.6159, 0.3306, 0.5055,\n",
      "        0.5524, 0.4621, 0.4345, 0.1404, 0.1482, 0.1628, 0.1760, 0.1107, 0.0881],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.5024e+05, 1.5296e+07, 4.2530e+05, 2.0219e+06, 1.6894e+06, 8.2444e+05,\n",
      "        6.6923e+04, 3.7663e+06, 9.7698e+05, 2.3698e+06, 5.5856e+05, 2.1983e+05,\n",
      "        9.7646e+04, 1.4913e+06, 1.0103e+06, 2.7174e+05, 1.2075e+07, 2.0066e+06,\n",
      "        1.4418e+06, 9.6000e+06, 1.0036e+06, 5.1656e+06, 2.8291e+05, 5.3735e+05,\n",
      "        4.6547e+06, 4.3859e+05, 6.9889e+04, 2.7321e+04, 4.0830e+05, 7.8042e+05,\n",
      "        5.3575e+05, 7.7390e+05, 7.7853e+02, 7.6283e+02, 3.8055e+02, 7.6004e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.9093e+05, 6.8936e+06, 1.0553e+05, 2.2926e+06, 4.8222e+05, 1.3156e+05,\n",
      "        8.3501e+06, 3.5464e+06, 2.0330e+06, 3.4700e+05, 1.9318e+05, 3.7339e+05,\n",
      "        1.9497e+05, 3.4618e+06, 2.8304e+05, 6.6052e+05, 1.4858e+05, 7.9213e+06,\n",
      "        7.0400e+05, 6.2329e+06, 3.3090e+05, 8.9574e+05, 9.1246e+04, 1.9220e+06,\n",
      "        1.2968e+05, 4.7018e+05, 3.5633e+04, 1.7515e+05, 2.6001e+05, 7.5226e+01,\n",
      "        2.8877e+03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9064, 0.9843, 0.8367, 0.9401, 0.9237, 0.8356, 0.8310, 0.5168, 0.9094,\n",
      "        0.9536, 0.6745, 0.2015, 0.8710, 0.4522, 0.6498, 0.8594, 0.6147, 0.9475,\n",
      "        0.7419, 0.4307, 0.8993, 0.4425, 0.4966, 0.4513, 0.5342, 0.5223, 0.4079,\n",
      "        0.2590, 0.5082, 0.2669, 0.1392], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.0890e+05, 4.3428e+05, 6.8937e+04, 5.4968e+05, 1.4726e+05, 8.6510e+04,\n",
      "        5.6447e+06, 6.8539e+06, 7.3704e+05, 6.4467e+04, 2.5149e+05, 1.1926e+06,\n",
      "        1.0063e+05, 7.5855e+06, 3.9653e+05, 3.7142e+05, 2.2900e+05, 1.6632e+06,\n",
      "        7.2678e+05, 1.4194e+07, 1.3326e+05, 1.9976e+06, 1.8373e+05, 4.2182e+06,\n",
      "        2.4163e+05, 8.9840e+05, 8.4395e+04, 5.1913e+05, 5.1147e+05, 2.2059e+02,\n",
      "        9.9428e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.2761e+06, 1.5171e+05, 5.2019e+06, 3.1149e+05, 1.3772e+05, 9.7621e+06,\n",
      "        5.4295e+05, 2.2178e+05, 2.5385e+03, 1.2712e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5701, 0.4328, 0.8642, 0.3032, 0.6797, 0.3724, 0.2599, 0.2903, 0.1817,\n",
      "        0.1467], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.6331e+06, 3.4421e+05, 2.8258e+06, 8.6816e+05, 1.7647e+05, 2.4508e+07,\n",
      "        1.6074e+06, 6.2957e+05, 8.3085e+03, 4.3387e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.2144e+07, 2.1949e+07, 1.5745e+07, 2.3061e+06, 3.4885e+06, 2.1183e+06,\n",
      "        3.8378e+06, 1.7119e+06, 6.4204e+06, 2.0389e+07, 9.4451e+04, 8.1401e+06,\n",
      "        1.2465e+05, 1.7007e+05, 1.3498e+03, 2.4851e+03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8460, 0.7434, 0.8126, 0.6459, 0.6047, 0.6306, 0.4627, 0.7915, 0.8745,\n",
      "        0.5063, 0.7657, 0.8610, 0.3343, 0.4818, 0.2460, 0.1761],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.9798e+07, 2.2524e+07, 1.1805e+07, 3.2662e+06, 5.5155e+06, 3.1303e+06,\n",
      "        8.2478e+06, 1.4278e+06, 3.2226e+06, 4.0263e+07, 8.8506e+04, 4.5257e+06,\n",
      "        3.3190e+05, 3.5252e+05, 4.0707e+03, 8.1898e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.3760e+06, 6.5302e+06, 1.1086e+06, 1.3427e+07, 7.9773e+06, 1.7806e+05,\n",
      "        1.3306e+06, 2.0302e+05, 2.0326e+05, 3.3819e+01, 1.1112e+02, 1.6229e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1698, 0.5672, 0.4496, 0.8091, 0.4919, 0.3463, 0.5571, 0.1940, 0.0708,\n",
      "        0.1885, 0.3787, 0.0789], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.5695e+06, 1.1305e+07, 2.4410e+06, 1.0252e+07, 1.6213e+07, 4.6559e+05,\n",
      "        2.3573e+06, 6.5451e+05, 7.5547e+05, 1.0978e+02, 2.7617e+02, 5.9794e+02],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([9.2778e+07, 5.1909e+06, 4.8385e+05, 3.6021e+07, 9.7062e+06, 1.9838e+06,\n",
      "        2.9987e+07, 7.8353e+06, 5.0374e+05, 7.9722e+04, 4.1017e+05, 4.2796e+05,\n",
      "        5.0560e+04, 4.5332e+05, 1.5021e+05, 2.4713e+04, 4.0916e+04, 1.1174e+04,\n",
      "        4.3021e+03, 1.2724e+04, 1.0876e+04, 4.9216e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9874, 0.5196, 0.5022, 0.9709, 0.5049, 0.2844, 0.9896, 0.8676, 0.5395,\n",
      "        0.0650, 0.6028, 0.6535, 0.1973, 0.7926, 0.3961, 0.4201, 0.4691, 0.4734,\n",
      "        0.4134, 0.7018, 0.0799, 0.0197], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.6830e+06, 9.9745e+06, 9.6347e+05, 4.1935e+06, 1.9223e+07, 5.6783e+06,\n",
      "        1.2452e+06, 4.1491e+06, 9.2786e+05, 2.9815e+05, 6.5175e+05, 5.9320e+05,\n",
      "        1.6234e+05, 3.7609e+05, 3.6283e+05, 5.7328e+04, 8.6883e+04, 2.3536e+04,\n",
      "        1.0095e+04, 1.5178e+04, 4.0029e+04, 1.9299e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.5238e+06, 4.1127e+05, 2.8504e+07, 1.9197e+07, 9.5870e+06, 3.3419e+07,\n",
      "        5.0036e+05, 3.9477e+05, 1.4932e+05, 2.6055e+05, 3.1697e+05, 6.2589e+04,\n",
      "        8.2708e+03, 1.1816e+04, 7.2999e+01, 1.2817e+03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9751, 0.3951, 0.8660, 0.8772, 0.3882, 0.9555, 0.5339, 0.3446, 0.3332,\n",
      "        0.0930, 0.2259, 0.2280, 0.4315, 0.1661, 0.1090, 0.0547],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.5175e+05, 9.9510e+05, 1.5279e+07, 9.4262e+06, 2.3460e+07, 5.9451e+06,\n",
      "        9.3284e+05, 1.0350e+06, 3.9829e+05, 9.4533e+05, 9.8151e+05, 1.9327e+05,\n",
      "        1.8808e+04, 3.9413e+04, 2.6017e+02, 4.8463e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.4244e+07, 8.0640e+05, 1.1563e+08, 2.9316e+06, 1.9797e+06, 1.2257e+07,\n",
      "        6.4718e+06, 1.3831e+05, 1.1527e+07, 1.9450e+06, 6.1030e+04, 6.4878e+05,\n",
      "        8.4868e+02, 7.8020e+03, 1.5195e+02, 1.5012e+04], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8333, 0.9566, 0.7236, 0.6079, 0.9756, 0.9807, 0.8604, 0.6118, 0.8669,\n",
      "        0.2808, 0.1602, 0.5340, 0.1162, 0.2956, 0.1446, 0.3716],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.2834e+07, 1.4003e+05, 1.2786e+08, 4.5984e+06, 1.9357e+05, 9.4849e+05,\n",
      "        3.6134e+06, 2.1474e+05, 6.1351e+06, 5.5953e+06, 2.0502e+05, 1.2093e+06,\n",
      "        3.0001e+03, 2.1982e+04, 5.1993e+02, 3.7731e+04], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.8995e+05, 3.6920e+05, 7.4214e+06, 3.1883e+04, 1.1869e+06, 8.7050e+04,\n",
      "        7.9050e+05, 3.7064e+05, 5.7523e+04, 1.0875e+03, 1.5373e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3117, 0.4628, 0.5912, 0.3595, 0.3797, 0.6536, 0.3867, 0.2991, 0.3035,\n",
      "        0.1203, 0.2965], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([7.9826e+05, 7.9331e+05, 1.2136e+07, 8.1677e+04, 2.9450e+06, 1.2063e+05,\n",
      "        1.9393e+06, 1.0391e+06, 1.6027e+05, 3.8268e+03, 4.3263e+02],\n",
      "       device='cuda:0')\n",
      "All_sigs tensor([4.5024e+05, 1.5296e+07, 4.2530e+05, 2.0219e+06, 1.6894e+06, 8.2444e+05,\n",
      "        6.6923e+04, 3.7663e+06, 9.7698e+05, 2.3698e+06, 5.5856e+05, 2.1983e+05,\n",
      "        9.7646e+04, 1.4913e+06, 1.0103e+06, 2.7174e+05, 1.2075e+07, 2.0066e+06,\n",
      "        1.4418e+06, 9.6000e+06, 1.0036e+06, 5.1656e+06, 2.8291e+05, 5.3735e+05,\n",
      "        4.6547e+06, 4.3859e+05, 6.9889e+04, 2.7321e+04, 4.0830e+05, 7.8042e+05,\n",
      "        5.3575e+05, 7.7390e+05, 7.7853e+02, 7.6283e+02, 3.8055e+02, 7.6004e+01,\n",
      "        1.0890e+05, 4.3428e+05, 6.8937e+04, 5.4968e+05, 1.4726e+05, 8.6510e+04,\n",
      "        5.6447e+06, 6.8539e+06, 7.3704e+05, 6.4467e+04, 2.5149e+05, 1.1926e+06,\n",
      "        1.0063e+05, 7.5855e+06, 3.9653e+05, 3.7142e+05, 2.2900e+05, 1.6632e+06,\n",
      "        7.2678e+05, 1.4194e+07, 1.3326e+05, 1.9976e+06, 1.8373e+05, 4.2182e+06,\n",
      "        2.4163e+05, 8.9840e+05, 8.4395e+04, 5.1913e+05, 5.1147e+05, 2.2059e+02,\n",
      "        9.9428e+03, 5.6331e+06, 3.4421e+05, 2.8258e+06, 8.6816e+05, 1.7647e+05,\n",
      "        2.4508e+07, 1.6074e+06, 6.2957e+05, 8.3085e+03, 4.3387e+02, 1.9798e+07,\n",
      "        2.2524e+07, 1.1805e+07, 3.2662e+06, 5.5155e+06, 3.1303e+06, 8.2478e+06,\n",
      "        1.4278e+06, 3.2226e+06, 4.0263e+07, 8.8506e+04, 4.5257e+06, 3.3190e+05,\n",
      "        3.5252e+05, 4.0707e+03, 8.1898e+03, 4.5695e+06, 1.1305e+07, 2.4410e+06,\n",
      "        1.0252e+07, 1.6213e+07, 4.6559e+05, 2.3573e+06, 6.5451e+05, 7.5547e+05,\n",
      "        1.0978e+02, 2.7617e+02, 5.9794e+02, 4.6830e+06, 9.9745e+06, 9.6347e+05,\n",
      "        4.1935e+06, 1.9223e+07, 5.6783e+06, 1.2452e+06, 4.1491e+06, 9.2786e+05,\n",
      "        2.9815e+05, 6.5175e+05, 5.9320e+05, 1.6234e+05, 3.7609e+05, 3.6283e+05,\n",
      "        5.7328e+04, 8.6883e+04, 2.3536e+04, 1.0095e+04, 1.5178e+04, 4.0029e+04,\n",
      "        1.9299e+03, 2.5175e+05, 9.9510e+05, 1.5279e+07, 9.4262e+06, 2.3460e+07,\n",
      "        5.9451e+06, 9.3284e+05, 1.0350e+06, 3.9829e+05, 9.4533e+05, 9.8151e+05,\n",
      "        1.9327e+05, 1.8808e+04, 3.9413e+04, 2.6017e+02, 4.8463e+03, 2.2834e+07,\n",
      "        1.4003e+05, 1.2786e+08, 4.5984e+06, 1.9357e+05, 9.4849e+05, 3.6134e+06,\n",
      "        2.1474e+05, 6.1351e+06, 5.5953e+06, 2.0502e+05, 1.2093e+06, 3.0001e+03,\n",
      "        2.1982e+04, 5.1993e+02, 3.7731e+04, 7.9826e+05, 7.9331e+05, 1.2136e+07,\n",
      "        8.1677e+04, 2.9450e+06, 1.2063e+05, 1.9393e+06, 1.0391e+06, 1.6027e+05,\n",
      "        3.8268e+03, 4.3263e+02], device='cuda:0')\n",
      "All_sig tensor([1.1980e-01, 4.0699e+00, 1.1316e-01, 5.3796e-01, 4.4950e-01, 2.1936e-01,\n",
      "        1.7806e-02, 1.0021e+00, 2.5995e-01, 6.3054e-01, 1.4862e-01, 5.8492e-02,\n",
      "        2.5981e-02, 3.9680e-01, 2.6880e-01, 7.2303e-02, 3.2128e+00, 5.3389e-01,\n",
      "        3.8362e-01, 2.5543e+00, 2.6703e-01, 1.3744e+00, 7.5275e-02, 1.4297e-01,\n",
      "        1.2385e+00, 1.1670e-01, 1.8596e-02, 7.2693e-03, 1.0864e-01, 2.0765e-01,\n",
      "        1.4255e-01, 2.0591e-01, 2.0714e-04, 2.0297e-04, 1.0125e-04, 2.0223e-05,\n",
      "        2.8976e-02, 1.1555e-01, 1.8342e-02, 1.4625e-01, 3.9182e-02, 2.3018e-02,\n",
      "        1.5019e+00, 1.8236e+00, 1.9610e-01, 1.7153e-02, 6.6915e-02, 3.1733e-01,\n",
      "        2.6775e-02, 2.0183e+00, 1.0550e-01, 9.8824e-02, 6.0932e-02, 4.4254e-01,\n",
      "        1.9338e-01, 3.7766e+00, 3.5457e-02, 5.3151e-01, 4.8885e-02, 1.1224e+00,\n",
      "        6.4292e-02, 2.3904e-01, 2.2455e-02, 1.3813e-01, 1.3609e-01, 5.8694e-05,\n",
      "        2.6455e-03, 1.4988e+00, 9.1586e-02, 7.5188e-01, 2.3099e-01, 4.6954e-02,\n",
      "        6.5210e+00, 4.2768e-01, 1.6751e-01, 2.2107e-03, 1.1544e-04, 5.2677e+00,\n",
      "        5.9931e+00, 3.1410e+00, 8.6905e-01, 1.4675e+00, 8.3289e-01, 2.1945e+00,\n",
      "        3.7990e-01, 8.5745e-01, 1.0713e+01, 2.3549e-02, 1.2042e+00, 8.8310e-02,\n",
      "        9.3795e-02, 1.0831e-03, 2.1791e-03, 1.2158e+00, 3.0081e+00, 6.4948e-01,\n",
      "        2.7279e+00, 4.3140e+00, 1.2388e-01, 6.2720e-01, 1.7415e-01, 2.0101e-01,\n",
      "        2.9208e-05, 7.3482e-05, 1.5910e-04, 1.2460e+00, 2.6539e+00, 2.5635e-01,\n",
      "        1.1158e+00, 5.1146e+00, 1.5108e+00, 3.3132e-01, 1.1040e+00, 2.4688e-01,\n",
      "        7.9330e-02, 1.7341e-01, 1.5783e-01, 4.3193e-02, 1.0007e-01, 9.6539e-02,\n",
      "        1.5254e-02, 2.3117e-02, 6.2622e-03, 2.6861e-03, 4.0385e-03, 1.0651e-02,\n",
      "        5.1349e-04, 6.6985e-02, 2.6477e-01, 4.0653e+00, 2.5080e+00, 6.2421e+00,\n",
      "        1.5818e+00, 2.4820e-01, 2.7537e-01, 1.0598e-01, 2.5153e-01, 2.6115e-01,\n",
      "        5.1424e-02, 5.0042e-03, 1.0487e-02, 6.9223e-05, 1.2895e-03, 6.0754e+00,\n",
      "        3.7257e-02, 3.4020e+01, 1.2235e+00, 5.1503e-02, 2.5237e-01, 9.6142e-01,\n",
      "        5.7136e-02, 1.6324e+00, 1.4887e+00, 5.4551e-02, 3.2175e-01, 7.9825e-04,\n",
      "        5.8488e-03, 1.3834e-04, 1.0039e-02, 2.1239e-01, 2.1108e-01, 3.2290e+00,\n",
      "        2.1732e-02, 7.8357e-01, 3.2096e-02, 5.1599e-01, 2.7646e-01, 4.2642e-02,\n",
      "        1.0182e-03, 1.1511e-04], device='cuda:0')\n",
      "Sig sum tensor(170.0000, device='cuda:0')\n",
      "remove_below 0.0010181937832385302\n",
      "remove_below 3826.758815643016\n",
      "pruning 15 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     21\n",
      "     ╠════╗\n",
      "     ║    15\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    14\n",
      "    ╠════╗\n",
      "    ║    9\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   16\n",
      "   ╠════╗\n",
      "   ║    9\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  30\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :5 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     24\n",
      "     ╠════╗\n",
      "     ║    22\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    17\n",
      "    ╠════╗\n",
      "    ║    11\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   22\n",
      "   ╠════╗\n",
      "   ║    14\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  32\n",
      "  ╠════╗\n",
      "  ║    11\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([  544673.2500, 12781334.0000,   809191.0625,  1375561.6250,\n",
      "          660750.1250,   447105.3750,    48611.8359,  2149778.0000,\n",
      "          316061.7188,  3337762.7500,   316137.6562,    91053.2266,\n",
      "          449060.5312,   928877.4375,   730768.0000,   102367.8359,\n",
      "         7490587.5000,   643192.3750, 11832650.0000,  8258633.5000,\n",
      "         1045447.3125,  5773496.5000,   373776.1875,   271051.9375,\n",
      "         2199368.2500,   255405.7344,    50747.8359,    14084.9922,\n",
      "          192470.8750,   371855.1875,   209265.0781,   352542.0938],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8073, 0.6749, 0.8007, 0.4693, 0.2224, 0.4810, 0.5739, 0.5005, 0.3658,\n",
      "        0.8429, 0.5597, 0.2909, 0.9384, 0.4727, 0.6459, 0.2221, 0.5444, 0.2068,\n",
      "        0.9672, 0.6544, 0.6502, 0.7811, 0.7495, 0.5259, 0.5467, 0.3935, 0.4984,\n",
      "        0.6119, 0.4805, 0.4242, 0.1255, 0.1456], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([  419736.0312, 16620274.0000,   645121.4375,  2919861.2500,\n",
      "         2055213.5000,   928163.2500,    82858.7812,  4295666.5000,\n",
      "          801761.0625,  2097830.7500,   556767.2500,   258265.6719,\n",
      "          110731.6484,  1959082.3750,  1035008.3750,   318531.2188,\n",
      "        13651344.0000,  2040846.2500,  1552932.6250, 11416820.0000,\n",
      "         1462885.5000,  5056214.5000,   374512.6250,   514046.8750,\n",
      "         3987783.7500,   619614.5625,   101813.2734,    21866.4395,\n",
      "          399976.6875,   856508.5625,   731984.6250,  1204891.5000],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.2437e+05, 6.3977e+06, 8.5560e+04, 2.3250e+06, 4.0092e+05, 1.4696e+05,\n",
      "        9.6102e+06, 3.0969e+06, 1.8741e+06, 2.2123e+05, 2.4312e+05, 6.8087e+05,\n",
      "        1.5682e+05, 3.0279e+06, 2.0211e+05, 7.6768e+05, 1.0829e+05, 8.3683e+06,\n",
      "        6.3239e+05, 5.7836e+06, 4.4266e+05, 1.0415e+06, 1.4096e+05, 2.7133e+06,\n",
      "        1.6187e+05, 7.4034e+05, 4.5784e+04, 4.5928e+05, 4.8004e+05, 1.0436e+04,\n",
      "        4.8891e+03, 7.8603e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9026, 0.9538, 0.7655, 0.9420, 0.8637, 0.8153, 0.8426, 0.5033, 0.8711,\n",
      "        0.9415, 0.6359, 0.2098, 0.8253, 0.4351, 0.6566, 0.8554, 0.5577, 0.9264,\n",
      "        0.7457, 0.4149, 0.8822, 0.4551, 0.4798, 0.4604, 0.6109, 0.5977, 0.4482,\n",
      "        0.3839, 0.5159, 0.1264, 0.2388, 0.1664], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2640e+05, 1.1835e+06, 8.0253e+04, 5.3958e+05, 2.1855e+05, 1.0859e+05,\n",
      "        6.0499e+06, 6.1526e+06, 9.6662e+05, 5.1728e+04, 3.5408e+05, 2.1520e+06,\n",
      "        1.0959e+05, 6.8421e+06, 2.7765e+05, 4.4400e+05, 1.9159e+05, 2.4639e+06,\n",
      "        6.4330e+05, 1.3535e+07, 2.0858e+05, 2.2702e+06, 2.9331e+05, 5.8569e+06,\n",
      "        2.5194e+05, 1.1914e+06, 1.0105e+05, 1.1318e+06, 9.2955e+05, 3.6467e+04,\n",
      "        1.4886e+04, 2.6209e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.1757e+06, 1.3539e+05, 6.7231e+06, 2.6655e+05, 6.4221e+04, 1.3630e+07,\n",
      "        1.0231e+06, 2.7727e+05, 1.0333e+04, 2.2305e+01, 5.5675e+01, 7.8222e+01,\n",
      "        1.7350e+03, 3.8989e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5832, 0.4450, 0.8671, 0.2741, 0.6905, 0.3986, 0.2506, 0.2910, 0.1835,\n",
      "        0.1216, 0.1667, 0.2542, 0.1815, 0.0582], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([6.9610e+06, 3.0058e+05, 3.5751e+06, 7.7399e+05, 7.9494e+04, 3.2791e+07,\n",
      "        3.0668e+06, 7.8628e+05, 3.3748e+04, 7.8370e+01, 1.8559e+02, 2.3337e+02,\n",
      "        5.6805e+03, 1.4688e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.2188e+07, 2.9686e+07, 1.2829e+07, 2.6378e+06, 3.5527e+06, 1.5962e+06,\n",
      "        3.0592e+06, 2.2351e+06, 8.2718e+06, 2.2333e+07, 1.0144e+05, 8.1410e+06,\n",
      "        1.4515e+05, 2.4783e+05, 1.6594e+04, 6.3439e+04, 8.8658e+02, 2.3265e+02,\n",
      "        3.1354e+02, 8.2132e+01, 3.6402e+02, 8.4012e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8560, 0.7609, 0.7922, 0.6626, 0.6071, 0.6269, 0.4946, 0.8257, 0.8887,\n",
      "        0.4999, 0.6190, 0.8498, 0.3386, 0.4833, 0.3725, 0.2100, 0.1618, 0.1316,\n",
      "        0.2147, 0.1089, 0.2794, 0.1976], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.8543e+07, 2.8398e+07, 1.0662e+07, 3.5596e+06, 5.5829e+06, 2.3825e+06,\n",
      "        6.1842e+06, 1.5580e+06, 3.6812e+06, 4.4674e+07, 1.5459e+05, 4.8914e+06,\n",
      "        3.8398e+05, 5.1221e+05, 4.1650e+04, 2.0046e+05, 2.9726e+03, 8.0812e+02,\n",
      "        9.8484e+02, 2.9275e+02, 1.0493e+03, 2.6963e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.3328e+06, 7.5853e+06, 1.7675e+06, 1.0942e+07, 8.5098e+06, 1.5422e+05,\n",
      "        1.8945e+06, 3.8877e+05, 1.8629e+05, 1.9633e+03, 1.6439e+04],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1540, 0.5672, 0.4543, 0.8171, 0.5042, 0.2922, 0.5497, 0.2243, 0.0745,\n",
      "        0.1593, 0.3292], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.5100e+06, 1.3130e+07, 3.8580e+06, 8.0034e+06, 1.6878e+07, 4.3663e+05,\n",
      "        3.4126e+06, 1.2062e+06, 6.8962e+05, 6.6023e+03, 4.4114e+04],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.1960e+08, 3.9456e+06, 3.9686e+05, 4.5887e+07, 9.6112e+06, 1.4438e+06,\n",
      "        2.3116e+07, 6.1819e+06, 3.4902e+05, 7.8128e+04, 3.5106e+05, 7.5365e+05,\n",
      "        3.5210e+04, 3.4848e+05, 3.1385e+05, 1.5275e+05, 4.5489e+04, 8.0723e+04,\n",
      "        3.2288e+04, 4.1853e+04, 8.8715e+04, 1.4115e+03, 3.9226e+03, 2.1747e+04],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9891, 0.5194, 0.5028, 0.9705, 0.5044, 0.2809, 0.9892, 0.8688, 0.5442,\n",
      "        0.0651, 0.6116, 0.7160, 0.5676, 0.7943, 0.3906, 0.3868, 0.4556, 0.4584,\n",
      "        0.3932, 0.7747, 0.0891, 0.3781, 0.1440, 0.1470], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.1956e+06, 7.5847e+06, 7.8934e+05, 5.4082e+06, 1.9052e+07, 4.1529e+06,\n",
      "        9.9730e+05, 3.2454e+06, 6.3627e+05, 2.9217e+05, 5.4536e+05, 8.5600e+05,\n",
      "        6.0896e+04, 2.8676e+05, 7.6506e+05, 3.7469e+05, 9.9049e+04, 1.7489e+05,\n",
      "        7.8376e+04, 3.7726e+04, 3.2324e+05, 3.5114e+03, 1.3431e+04, 7.4203e+04],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.2012e+06, 2.9062e+05, 2.0769e+07, 1.9928e+07, 6.9260e+06, 3.6173e+07,\n",
      "        5.1741e+05, 5.6465e+05, 1.7789e+05, 3.5019e+05, 6.2792e+05, 1.4634e+05,\n",
      "        7.7682e+04, 1.6633e+05, 1.0394e+04, 1.8809e+03, 2.5012e+03, 3.0569e+03,\n",
      "        2.2247e+03, 1.5203e+03, 2.0822e+03, 7.9720e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9664, 0.3959, 0.8557, 0.8768, 0.4051, 0.9536, 0.5462, 0.3206, 0.3833,\n",
      "        0.0966, 0.2158, 0.2789, 0.4458, 0.2367, 0.0750, 0.0852, 0.1466, 0.4851,\n",
      "        0.0772, 0.0603, 0.2145, 0.1336], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.6412e+05, 7.0220e+05, 1.1984e+07, 9.8200e+06, 1.6481e+07, 6.7127e+06,\n",
      "        9.3911e+05, 1.5344e+06, 4.3881e+05, 1.2654e+06, 1.9697e+06, 4.2210e+05,\n",
      "        1.7219e+05, 5.0784e+05, 3.8460e+04, 6.8823e+03, 8.5378e+03, 6.2959e+03,\n",
      "        8.2123e+03, 5.7144e+03, 6.5423e+03, 2.7629e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.4058e+07, 9.6712e+05, 1.3368e+08, 2.2660e+06, 2.1964e+06, 1.1003e+07,\n",
      "        8.1869e+06, 2.2401e+05, 1.0639e+07, 2.2798e+06, 9.0214e+04, 1.7005e+06,\n",
      "        1.9593e+04, 1.2092e+05, 2.8593e+03, 2.4026e+03, 4.6507e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8397, 0.9686, 0.7229, 0.5902, 0.9686, 0.9796, 0.8496, 0.6018, 0.8565,\n",
      "        0.2860, 0.1553, 0.5797, 0.3428, 0.4073, 0.1461, 0.1547, 0.1497],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.1844e+07, 1.2131e+05, 1.4819e+08, 3.7144e+06, 2.7544e+05, 8.9791e+05,\n",
      "        4.9243e+06, 3.5676e+05, 6.1084e+06, 6.5108e+06, 3.0481e+05, 2.8590e+06,\n",
      "        5.1507e+04, 2.8668e+05, 9.7658e+03, 8.1233e+03, 1.5818e+03],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.9984e+05, 4.9487e+05, 6.0947e+06, 2.3074e+04, 1.9210e+06, 6.2525e+04,\n",
      "        1.0682e+06, 6.4348e+05, 9.1665e+04, 7.5051e+02, 1.9444e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3398, 0.5029, 0.5685, 0.3940, 0.4082, 0.5710, 0.3648, 0.3227, 0.2852,\n",
      "        0.1618, 0.0748], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([7.9188e+05, 9.8402e+05, 1.0518e+07, 5.5929e+04, 4.5472e+06, 1.0729e+05,\n",
      "        2.7143e+06, 1.7433e+06, 2.6210e+05, 2.5163e+03, 7.1954e+02],\n",
      "       device='cuda:0')\n",
      "All_sigs tensor([4.1974e+05, 1.6620e+07, 6.4512e+05, 2.9199e+06, 2.0552e+06, 9.2816e+05,\n",
      "        8.2859e+04, 4.2957e+06, 8.0176e+05, 2.0978e+06, 5.5677e+05, 2.5827e+05,\n",
      "        1.1073e+05, 1.9591e+06, 1.0350e+06, 3.1853e+05, 1.3651e+07, 2.0408e+06,\n",
      "        1.5529e+06, 1.1417e+07, 1.4629e+06, 5.0562e+06, 3.7451e+05, 5.1405e+05,\n",
      "        3.9878e+06, 6.1961e+05, 1.0181e+05, 2.1866e+04, 3.9998e+05, 8.5651e+05,\n",
      "        7.3198e+05, 1.2049e+06, 1.2640e+05, 1.1835e+06, 8.0253e+04, 5.3958e+05,\n",
      "        2.1855e+05, 1.0859e+05, 6.0499e+06, 6.1526e+06, 9.6662e+05, 5.1728e+04,\n",
      "        3.5408e+05, 2.1520e+06, 1.0959e+05, 6.8421e+06, 2.7765e+05, 4.4400e+05,\n",
      "        1.9159e+05, 2.4639e+06, 6.4330e+05, 1.3535e+07, 2.0858e+05, 2.2702e+06,\n",
      "        2.9331e+05, 5.8569e+06, 2.5194e+05, 1.1914e+06, 1.0105e+05, 1.1318e+06,\n",
      "        9.2955e+05, 3.6467e+04, 1.4886e+04, 2.6209e+02, 6.9610e+06, 3.0058e+05,\n",
      "        3.5751e+06, 7.7399e+05, 7.9494e+04, 3.2791e+07, 3.0668e+06, 7.8628e+05,\n",
      "        3.3748e+04, 7.8370e+01, 1.8559e+02, 2.3337e+02, 5.6805e+03, 1.4688e+03,\n",
      "        1.8543e+07, 2.8398e+07, 1.0662e+07, 3.5596e+06, 5.5829e+06, 2.3825e+06,\n",
      "        6.1842e+06, 1.5580e+06, 3.6812e+06, 4.4674e+07, 1.5459e+05, 4.8914e+06,\n",
      "        3.8398e+05, 5.1221e+05, 4.1650e+04, 2.0046e+05, 2.9726e+03, 8.0812e+02,\n",
      "        9.8484e+02, 2.9275e+02, 1.0493e+03, 2.6963e+03, 4.5100e+06, 1.3130e+07,\n",
      "        3.8580e+06, 8.0034e+06, 1.6878e+07, 4.3663e+05, 3.4126e+06, 1.2062e+06,\n",
      "        6.8962e+05, 6.6023e+03, 4.4114e+04, 5.1956e+06, 7.5847e+06, 7.8934e+05,\n",
      "        5.4082e+06, 1.9052e+07, 4.1529e+06, 9.9730e+05, 3.2454e+06, 6.3627e+05,\n",
      "        2.9217e+05, 5.4536e+05, 8.5600e+05, 6.0896e+04, 2.8676e+05, 7.6506e+05,\n",
      "        3.7469e+05, 9.9049e+04, 1.7489e+05, 7.8376e+04, 3.7726e+04, 3.2324e+05,\n",
      "        3.5114e+03, 1.3431e+04, 7.4203e+04, 5.6412e+05, 7.0220e+05, 1.1984e+07,\n",
      "        9.8200e+06, 1.6481e+07, 6.7127e+06, 9.3911e+05, 1.5344e+06, 4.3881e+05,\n",
      "        1.2654e+06, 1.9697e+06, 4.2210e+05, 1.7219e+05, 5.0784e+05, 3.8460e+04,\n",
      "        6.8823e+03, 8.5378e+03, 6.2959e+03, 8.2123e+03, 5.7144e+03, 6.5423e+03,\n",
      "        2.7629e+03, 2.1844e+07, 1.2131e+05, 1.4819e+08, 3.7144e+06, 2.7544e+05,\n",
      "        8.9791e+05, 4.9243e+06, 3.5676e+05, 6.1084e+06, 6.5108e+06, 3.0481e+05,\n",
      "        2.8590e+06, 5.1507e+04, 2.8668e+05, 9.7658e+03, 8.1233e+03, 1.5818e+03,\n",
      "        7.9188e+05, 9.8402e+05, 1.0518e+07, 5.5929e+04, 4.5472e+06, 1.0729e+05,\n",
      "        2.7143e+06, 1.7433e+06, 2.6210e+05, 2.5163e+03, 7.1954e+02],\n",
      "       device='cuda:0')\n",
      "All_sig tensor([1.1275e-01, 4.4647e+00, 1.7330e-01, 7.8436e-01, 5.5209e-01, 2.4933e-01,\n",
      "        2.2258e-02, 1.1539e+00, 2.1538e-01, 5.6354e-01, 1.4956e-01, 6.9378e-02,\n",
      "        2.9746e-02, 5.2627e-01, 2.7804e-01, 8.5567e-02, 3.6672e+00, 5.4823e-01,\n",
      "        4.1717e-01, 3.0669e+00, 3.9298e-01, 1.3583e+00, 1.0061e-01, 1.3809e-01,\n",
      "        1.0712e+00, 1.6645e-01, 2.7350e-02, 5.8740e-03, 1.0745e-01, 2.3008e-01,\n",
      "        1.9663e-01, 3.2367e-01, 3.3955e-02, 3.1793e-01, 2.1559e-02, 1.4495e-01,\n",
      "        5.8710e-02, 2.9169e-02, 1.6252e+00, 1.6528e+00, 2.5966e-01, 1.3896e-02,\n",
      "        9.5118e-02, 5.7809e-01, 2.9440e-02, 1.8380e+00, 7.4586e-02, 1.1927e-01,\n",
      "        5.1467e-02, 6.6189e-01, 1.7281e-01, 3.6360e+00, 5.6031e-02, 6.0986e-01,\n",
      "        7.8793e-02, 1.5733e+00, 6.7680e-02, 3.2005e-01, 2.7144e-02, 3.0404e-01,\n",
      "        2.4971e-01, 9.7961e-03, 3.9988e-03, 7.0404e-05, 1.8699e+00, 8.0744e-02,\n",
      "        9.6037e-01, 2.0792e-01, 2.1355e-02, 8.8086e+00, 8.2384e-01, 2.1122e-01,\n",
      "        9.0658e-03, 2.1053e-05, 4.9854e-05, 6.2689e-05, 1.5260e-03, 3.9456e-04,\n",
      "        4.9812e+00, 7.6285e+00, 2.8641e+00, 9.5622e-01, 1.4997e+00, 6.4000e-01,\n",
      "        1.6613e+00, 4.1853e-01, 9.8887e-01, 1.2001e+01, 4.1528e-02, 1.3140e+00,\n",
      "        1.0315e-01, 1.3760e-01, 1.1188e-02, 5.3850e-02, 7.9854e-04, 2.1708e-04,\n",
      "        2.6456e-04, 7.8642e-05, 2.8188e-04, 7.2431e-04, 1.2115e+00, 3.5272e+00,\n",
      "        1.0364e+00, 2.1500e+00, 4.5340e+00, 1.1729e-01, 9.1673e-01, 3.2403e-01,\n",
      "        1.8525e-01, 1.7736e-03, 1.1850e-02, 1.3957e+00, 2.0375e+00, 2.1204e-01,\n",
      "        1.4528e+00, 5.1180e+00, 1.1156e+00, 2.6790e-01, 8.7181e-01, 1.7092e-01,\n",
      "        7.8486e-02, 1.4650e-01, 2.2995e-01, 1.6358e-02, 7.7032e-02, 2.0552e-01,\n",
      "        1.0065e-01, 2.6608e-02, 4.6981e-02, 2.1054e-02, 1.0134e-02, 8.6832e-02,\n",
      "        9.4327e-04, 3.6080e-03, 1.9933e-02, 1.5154e-01, 1.8863e-01, 3.2193e+00,\n",
      "        2.6379e+00, 4.4273e+00, 1.8032e+00, 2.5227e-01, 4.1219e-01, 1.1788e-01,\n",
      "        3.3994e-01, 5.2912e-01, 1.1339e-01, 4.6255e-02, 1.3642e-01, 1.0331e-02,\n",
      "        1.8488e-03, 2.2935e-03, 1.6913e-03, 2.2061e-03, 1.5351e-03, 1.7575e-03,\n",
      "        7.4219e-04, 5.8679e+00, 3.2587e-02, 3.9807e+01, 9.9780e-01, 7.3991e-02,\n",
      "        2.4121e-01, 1.3228e+00, 9.5838e-02, 1.6409e+00, 1.7490e+00, 8.1882e-02,\n",
      "        7.6802e-01, 1.3836e-02, 7.7011e-02, 2.6234e-03, 2.1822e-03, 4.2491e-04,\n",
      "        2.1272e-01, 2.6434e-01, 2.8256e+00, 1.5024e-02, 1.2215e+00, 2.8822e-02,\n",
      "        7.2915e-01, 4.6830e-01, 7.0408e-02, 6.7597e-04, 1.9329e-04],\n",
      "       device='cuda:0')\n",
      "Sig sum tensor(185., device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 3722.5817945945946\n",
      "pruning 16 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     23\n",
      "     ╠════╗\n",
      "     ║    21\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    16\n",
      "    ╠════╗\n",
      "    ║    11\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   16\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :6 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     24\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    4\n",
      "     ║    ╠════╝\n",
      "     ║    23\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    20\n",
      "    ╠════╗\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   19\n",
      "   ╠════╗\n",
      "   ║    15\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  33\n",
      "  ╠════╗\n",
      "  ║    13\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 35\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.8460e+05, 1.2642e+07, 6.9195e+05, 1.4162e+06, 9.7289e+05, 5.1278e+05,\n",
      "        4.1350e+04, 1.8910e+06, 3.1449e+05, 4.3395e+06, 2.7611e+05, 1.0173e+05,\n",
      "        4.4332e+05, 1.1474e+06, 6.6792e+05, 1.1123e+05, 6.2841e+06, 6.1422e+05,\n",
      "        1.2890e+07, 8.1378e+06, 1.0777e+06, 5.9749e+06, 3.7317e+05, 2.1722e+05,\n",
      "        1.8727e+06, 3.0074e+05, 6.9150e+04, 2.1774e+04, 2.1970e+05, 4.0447e+05,\n",
      "        1.6623e+05, 4.1727e+05, 1.7466e+00, 8.5702e-04, 4.7506e-03],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.7803, 0.6577, 0.7479, 0.4687, 0.2413, 0.4839, 0.5342, 0.4873, 0.3406,\n",
      "        0.8523, 0.4087, 0.3059, 0.9255, 0.4880, 0.6094, 0.2379, 0.5262, 0.2038,\n",
      "        0.9625, 0.6455, 0.6014, 0.7843, 0.7082, 0.5015, 0.5185, 0.3900, 0.4466,\n",
      "        0.6530, 0.4925, 0.4342, 0.1090, 0.1613, 0.1548, 0.0602, 0.0910],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.3803e+05, 1.7309e+07, 6.9781e+05, 3.0099e+06, 2.9524e+06, 1.0587e+06,\n",
      "        7.7048e+04, 3.8779e+06, 8.2948e+05, 2.5632e+06, 6.5311e+05, 2.8245e+05,\n",
      "        1.3217e+05, 2.3501e+06, 1.0435e+06, 3.3908e+05, 1.1909e+07, 1.9563e+06,\n",
      "        1.9313e+06, 1.1538e+07, 1.7183e+06, 5.1553e+06, 4.3551e+05, 4.3317e+05,\n",
      "        3.6068e+06, 7.3384e+05, 1.5306e+05, 3.0220e+04, 4.4601e+05, 9.1548e+05,\n",
      "        5.9244e+05, 1.3999e+06, 5.9048e+00, 3.2216e-03, 1.7273e-02],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.5108e+05, 5.4423e+06, 7.0809e+04, 2.2567e+06, 3.2071e+05, 1.3764e+05,\n",
      "        9.5751e+06, 3.3394e+06, 1.8955e+06, 1.8945e+05, 2.7417e+05, 9.9727e+05,\n",
      "        1.3995e+05, 3.2576e+06, 1.7970e+05, 7.5147e+05, 6.5222e+04, 7.7969e+06,\n",
      "        5.3100e+05, 6.9503e+06, 4.3198e+05, 1.0368e+06, 1.5563e+05, 2.7411e+06,\n",
      "        2.3983e+05, 1.1328e+06, 5.7396e+04, 5.4970e+05, 5.1081e+05, 2.6596e+04,\n",
      "        1.5493e+04, 2.9614e+01, 7.7129e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8834, 0.8906, 0.5923, 0.9452, 0.8057, 0.7844, 0.8434, 0.5136, 0.8491,\n",
      "        0.9376, 0.6408, 0.2163, 0.8074, 0.4428, 0.6757, 0.8257, 0.4891, 0.9294,\n",
      "        0.7351, 0.4199, 0.8777, 0.4369, 0.4917, 0.4496, 0.6702, 0.6283, 0.4576,\n",
      "        0.4039, 0.4988, 0.1405, 0.2353, 0.0849, 0.1058], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.6369e+05, 2.3809e+06, 1.1546e+05, 4.9469e+05, 2.4925e+05, 1.1869e+05,\n",
      "        5.9967e+06, 6.4977e+06, 1.1441e+06, 4.7288e+04, 3.9388e+05, 3.1262e+06,\n",
      "        1.0783e+05, 7.2602e+06, 2.3311e+05, 5.2406e+05, 1.3330e+05, 2.2032e+06,\n",
      "        5.6259e+05, 1.6127e+07, 2.1140e+05, 2.3352e+06, 3.1644e+05, 6.0352e+06,\n",
      "        3.1641e+05, 1.6842e+06, 1.2453e+05, 1.3107e+06, 1.0241e+06, 9.1437e+04,\n",
      "        4.7387e+04, 1.0840e+02, 2.7589e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.8679e+06, 1.0152e+05, 6.8175e+06, 2.1791e+05, 9.3351e+04, 1.2987e+07,\n",
      "        1.3977e+06, 5.3181e+05, 1.6030e+04, 1.7774e+04, 2.1250e+02, 2.7497e+01,\n",
      "        2.7732e-01, 2.8591e+01, 1.2889e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5835, 0.4277, 0.8656, 0.2016, 0.7012, 0.3916, 0.2150, 0.3113, 0.1805,\n",
      "        0.2059, 0.1513, 0.1305, 0.0908, 0.1055, 0.1845], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([8.1098e+06, 2.3239e+05, 3.6662e+06, 6.9589e+05, 1.1158e+05, 3.1604e+07,\n",
      "        4.3890e+06, 1.4650e+06, 5.2548e+04, 5.6461e+04, 7.2142e+02, 9.5633e+01,\n",
      "        1.0085e+00, 1.0230e+02, 4.2045e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.4504e+07, 3.5100e+07, 1.1169e+07, 2.7825e+06, 4.5969e+06, 1.8928e+06,\n",
      "        3.3119e+06, 2.7885e+06, 5.9134e+06, 2.0311e+07, 2.5770e+05, 8.9774e+06,\n",
      "        1.6824e+05, 2.2457e+05, 7.1885e+04, 1.0648e+05, 7.5584e+02, 2.5442e+03,\n",
      "        1.1299e+03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8669, 0.7925, 0.7810, 0.6607, 0.6141, 0.6381, 0.4565, 0.8221, 0.8830,\n",
      "        0.4823, 0.6074, 0.8576, 0.3853, 0.5032, 0.4961, 0.2333, 0.1994, 0.2671,\n",
      "        0.2649], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.8371e+07, 2.9137e+07, 9.7820e+06, 3.7767e+06, 7.0959e+06, 2.7398e+06,\n",
      "        7.1999e+06, 1.9842e+06, 2.7683e+06, 4.2057e+07, 4.0466e+05, 5.1146e+06,\n",
      "        4.1367e+05, 4.4624e+05, 1.4490e+05, 3.2655e+05, 2.4204e+03, 7.4585e+03,\n",
      "        3.3225e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.4212e+06, 1.1047e+07, 2.2246e+06, 7.4703e+06, 9.4595e+06, 3.5559e+05,\n",
      "        2.6309e+06, 6.0239e+05, 1.2861e+05, 3.9132e+04, 2.0866e+05, 1.1456e+03,\n",
      "        4.1813e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1550, 0.5779, 0.4393, 0.8269, 0.4831, 0.2973, 0.5380, 0.2480, 0.0773,\n",
      "        0.2119, 0.3951, 0.0753, 0.0636], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.8037e+06, 1.8650e+07, 4.9893e+06, 5.1717e+06, 1.9558e+07, 9.9952e+05,\n",
      "        4.8621e+06, 1.8121e+06, 4.7468e+05, 1.2336e+05, 5.0489e+05, 4.2372e+03,\n",
      "        1.5662e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.4165e+08, 7.1796e+06, 5.7392e+05, 5.1585e+07, 2.0531e+07, 1.3784e+06,\n",
      "        1.7637e+07, 7.5481e+06, 5.6060e+05, 8.7662e+04, 4.3397e+05, 1.4886e+06,\n",
      "        3.2892e+04, 3.5689e+05, 4.5523e+05, 2.6129e+05, 5.6493e+04, 1.8444e+05,\n",
      "        5.6872e+04, 5.5946e+04, 1.7461e+05, 1.5332e+04, 6.6198e+04, 1.1563e+03],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9895, 0.5194, 0.5025, 0.9700, 0.5038, 0.2859, 0.9888, 0.8715, 0.5493,\n",
      "        0.4438, 0.6166, 0.7052, 0.5738, 0.7943, 0.3900, 0.3795, 0.4466, 0.4550,\n",
      "        0.3916, 0.7695, 0.0906, 0.1800, 0.1727, 0.0294], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.9501e+06, 1.3803e+07, 1.1422e+06, 6.1843e+06, 4.0748e+07, 3.9375e+06,\n",
      "        7.9314e+05, 3.8800e+06, 1.0105e+06, 1.9502e+05, 6.6547e+05, 1.7552e+06,\n",
      "        5.6077e+04, 2.9370e+05, 1.1108e+06, 6.4848e+05, 1.2506e+05, 4.0210e+05,\n",
      "        1.3840e+05, 5.1576e+04, 6.3516e+05, 5.0285e+04, 2.1906e+05, 4.4890e+03],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.5843e+06, 1.3584e+06, 2.2453e+07, 2.2328e+07, 5.5034e+06, 3.6237e+07,\n",
      "        6.6525e+05, 4.5706e+05, 2.8595e+05, 3.1544e+05, 1.5055e+06, 2.0417e+05,\n",
      "        2.0488e+05, 5.1446e+05, 4.1003e+04, 2.2296e+04, 5.2817e+04, 4.5944e+04,\n",
      "        3.1824e+04, 1.1694e+04, 1.0882e+04, 2.4411e+03, 9.0321e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9433, 0.4034, 0.8246, 0.8791, 0.4161, 0.9508, 0.5819, 0.3021, 0.3776,\n",
      "        0.1065, 0.2386, 0.2893, 0.4357, 0.2784, 0.0801, 0.1325, 0.1669, 0.5583,\n",
      "        0.1149, 0.0809, 0.2146, 0.2671, 0.2159], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([8.1294e+05, 3.2417e+06, 1.5756e+07, 1.0799e+07, 1.2854e+07, 7.1371e+06,\n",
      "        1.1126e+06, 1.2758e+06, 7.1185e+05, 1.1273e+06, 4.5850e+06, 5.8039e+05,\n",
      "        4.6248e+05, 1.4849e+06, 1.5088e+05, 7.7364e+04, 1.7601e+05, 8.1180e+04,\n",
      "        1.1267e+05, 4.2991e+04, 3.4186e+04, 7.1566e+03, 2.8328e+03],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([ 283.4637,  294.1883, 2616.2610, 3318.5320], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2513, 0.0542, 0.0604, 0.1260], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([  848.8849,  1112.9536,  9832.6494, 11601.2383], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.9613e+07, 2.1336e+06, 1.8312e+08, 2.3648e+06, 4.2092e+06, 1.3941e+07,\n",
      "        9.8851e+06, 2.6929e+05, 1.2033e+07, 2.0477e+06, 1.1181e+05, 3.5255e+06,\n",
      "        3.5526e+04, 4.8313e+05, 2.2564e+04, 1.9897e+04, 4.4593e+02, 7.7674e+03,\n",
      "        2.5100e+03, 8.5373e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9588, 0.9706, 0.7298, 0.5592, 0.9777, 0.9775, 0.8297, 0.5933, 0.8524,\n",
      "        0.2702, 0.1627, 0.5918, 0.3519, 0.4521, 0.2037, 0.1639, 0.0620, 0.0865,\n",
      "        0.0871, 0.2748], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.8804e+06, 2.5108e+05, 1.9788e+08, 4.1694e+06, 3.7571e+05, 1.2543e+06,\n",
      "        6.7320e+06, 4.3811e+05, 7.1060e+06, 5.9772e+06, 3.7447e+05, 5.7564e+06,\n",
      "        9.2097e+04, 1.0589e+06, 7.1870e+04, 6.6547e+04, 1.6731e+03, 2.8381e+04,\n",
      "        9.1654e+03, 2.4765e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.5070e+05, 5.4473e+05, 6.7592e+06, 1.9025e+04, 2.8829e+06, 4.7557e+04,\n",
      "        1.2790e+06, 9.3434e+05, 8.8055e+04, 3.0402e+00, 4.8615e+00, 1.6416e+02,\n",
      "        2.1627e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3472, 0.5358, 0.5826, 0.4116, 0.4361, 0.5068, 0.3774, 0.3518, 0.2711,\n",
      "        0.0593, 0.0431, 0.0912, 0.1690], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([9.1569e+05, 1.0115e+06, 1.1285e+07, 4.4777e+04, 6.5025e+06, 9.3817e+04,\n",
      "        3.1853e+06, 2.4225e+06, 2.5672e+05, 1.1439e+01, 1.8608e+01, 5.9678e+02,\n",
      "        7.1886e+02], device='cuda:0')\n",
      "All_sigs tensor([3.3803e+05, 1.7309e+07, 6.9781e+05, 3.0099e+06, 2.9524e+06, 1.0587e+06,\n",
      "        7.7048e+04, 3.8779e+06, 8.2948e+05, 2.5632e+06, 6.5311e+05, 2.8245e+05,\n",
      "        1.3217e+05, 2.3501e+06, 1.0435e+06, 3.3908e+05, 1.1909e+07, 1.9563e+06,\n",
      "        1.9313e+06, 1.1538e+07, 1.7183e+06, 5.1553e+06, 4.3551e+05, 4.3317e+05,\n",
      "        3.6068e+06, 7.3384e+05, 1.5306e+05, 3.0220e+04, 4.4601e+05, 9.1548e+05,\n",
      "        5.9244e+05, 1.3999e+06, 5.9048e+00, 3.2216e-03, 1.7273e-02, 1.6369e+05,\n",
      "        2.3809e+06, 1.1546e+05, 4.9469e+05, 2.4925e+05, 1.1869e+05, 5.9967e+06,\n",
      "        6.4977e+06, 1.1441e+06, 4.7288e+04, 3.9388e+05, 3.1262e+06, 1.0783e+05,\n",
      "        7.2602e+06, 2.3311e+05, 5.2406e+05, 1.3330e+05, 2.2032e+06, 5.6259e+05,\n",
      "        1.6127e+07, 2.1140e+05, 2.3352e+06, 3.1644e+05, 6.0352e+06, 3.1641e+05,\n",
      "        1.6842e+06, 1.2453e+05, 1.3107e+06, 1.0241e+06, 9.1437e+04, 4.7387e+04,\n",
      "        1.0840e+02, 2.7589e+02, 8.1098e+06, 2.3239e+05, 3.6662e+06, 6.9589e+05,\n",
      "        1.1158e+05, 3.1604e+07, 4.3890e+06, 1.4650e+06, 5.2548e+04, 5.6461e+04,\n",
      "        7.2142e+02, 9.5633e+01, 1.0085e+00, 1.0230e+02, 4.2045e+02, 1.8371e+07,\n",
      "        2.9137e+07, 9.7820e+06, 3.7767e+06, 7.0959e+06, 2.7398e+06, 7.1999e+06,\n",
      "        1.9842e+06, 2.7683e+06, 4.2057e+07, 4.0466e+05, 5.1146e+06, 4.1367e+05,\n",
      "        4.4624e+05, 1.4490e+05, 3.2655e+05, 2.4204e+03, 7.4585e+03, 3.3225e+03,\n",
      "        4.8037e+06, 1.8650e+07, 4.9893e+06, 5.1717e+06, 1.9558e+07, 9.9952e+05,\n",
      "        4.8621e+06, 1.8121e+06, 4.7468e+05, 1.2336e+05, 5.0489e+05, 4.2372e+03,\n",
      "        1.5662e+03, 5.9501e+06, 1.3803e+07, 1.1422e+06, 6.1843e+06, 4.0748e+07,\n",
      "        3.9375e+06, 7.9314e+05, 3.8800e+06, 1.0105e+06, 1.9502e+05, 6.6547e+05,\n",
      "        1.7552e+06, 5.6077e+04, 2.9370e+05, 1.1108e+06, 6.4848e+05, 1.2506e+05,\n",
      "        4.0210e+05, 1.3840e+05, 5.1576e+04, 6.3516e+05, 5.0285e+04, 2.1906e+05,\n",
      "        4.4890e+03, 8.1294e+05, 3.2417e+06, 1.5756e+07, 1.0799e+07, 1.2854e+07,\n",
      "        7.1371e+06, 1.1126e+06, 1.2758e+06, 7.1185e+05, 1.1273e+06, 4.5850e+06,\n",
      "        5.8039e+05, 4.6248e+05, 1.4849e+06, 1.5088e+05, 7.7364e+04, 1.7601e+05,\n",
      "        8.1180e+04, 1.1267e+05, 4.2991e+04, 3.4186e+04, 7.1566e+03, 2.8328e+03,\n",
      "        8.4888e+02, 1.1130e+03, 9.8326e+03, 1.1601e+04, 4.8804e+06, 2.5108e+05,\n",
      "        1.9788e+08, 4.1694e+06, 3.7571e+05, 1.2543e+06, 6.7320e+06, 4.3811e+05,\n",
      "        7.1060e+06, 5.9772e+06, 3.7447e+05, 5.7564e+06, 9.2097e+04, 1.0589e+06,\n",
      "        7.1870e+04, 6.6547e+04, 1.6731e+03, 2.8381e+04, 9.1654e+03, 2.4765e+02,\n",
      "        9.1569e+05, 1.0115e+06, 1.1285e+07, 4.4777e+04, 6.5025e+06, 9.3817e+04,\n",
      "        3.1853e+06, 2.4225e+06, 2.5672e+05, 1.1439e+01, 1.8608e+01, 5.9678e+02,\n",
      "        7.1886e+02], device='cuda:0')\n",
      "All_sig tensor([8.4650e-02, 4.3345e+00, 1.7475e-01, 7.5374e-01, 7.3934e-01, 2.6511e-01,\n",
      "        1.9295e-02, 9.7112e-01, 2.0772e-01, 6.4189e-01, 1.6355e-01, 7.0731e-02,\n",
      "        3.3098e-02, 5.8852e-01, 2.6133e-01, 8.4915e-02, 2.9822e+00, 4.8989e-01,\n",
      "        4.8364e-01, 2.8894e+00, 4.3029e-01, 1.2910e+00, 1.0906e-01, 1.0847e-01,\n",
      "        9.0323e-01, 1.8377e-01, 3.8331e-02, 7.5677e-03, 1.1169e-01, 2.2926e-01,\n",
      "        1.4836e-01, 3.5057e-01, 1.4787e-06, 8.0676e-10, 4.3255e-09, 4.0993e-02,\n",
      "        5.9623e-01, 2.8915e-02, 1.2388e-01, 6.2417e-02, 2.9722e-02, 1.5017e+00,\n",
      "        1.6272e+00, 2.8651e-01, 1.1842e-02, 9.8636e-02, 7.8289e-01, 2.7004e-02,\n",
      "        1.8181e+00, 5.8375e-02, 1.3124e-01, 3.3381e-02, 5.5173e-01, 1.4089e-01,\n",
      "        4.0387e+00, 5.2940e-02, 5.8480e-01, 7.9245e-02, 1.5114e+00, 7.9236e-02,\n",
      "        4.2175e-01, 3.1185e-02, 3.2822e-01, 2.5646e-01, 2.2898e-02, 1.1867e-02,\n",
      "        2.7147e-05, 6.9088e-05, 2.0309e+00, 5.8196e-02, 9.1810e-01, 1.7427e-01,\n",
      "        2.7943e-02, 7.9144e+00, 1.0991e+00, 3.6686e-01, 1.3159e-02, 1.4139e-02,\n",
      "        1.8066e-04, 2.3949e-05, 2.5256e-07, 2.5619e-05, 1.0529e-04, 4.6006e+00,\n",
      "        7.2966e+00, 2.4496e+00, 9.4578e-01, 1.7770e+00, 6.8610e-01, 1.8030e+00,\n",
      "        4.9688e-01, 6.9324e-01, 1.0532e+01, 1.0134e-01, 1.2808e+00, 1.0359e-01,\n",
      "        1.1175e-01, 3.6287e-02, 8.1775e-02, 6.0613e-04, 1.8678e-03, 8.3202e-04,\n",
      "        1.2030e+00, 4.6705e+00, 1.2494e+00, 1.2951e+00, 4.8977e+00, 2.5030e-01,\n",
      "        1.2176e+00, 4.5378e-01, 1.1887e-01, 3.0892e-02, 1.2644e-01, 1.0611e-03,\n",
      "        3.9222e-04, 1.4900e+00, 3.4565e+00, 2.8602e-01, 1.5487e+00, 1.0204e+01,\n",
      "        9.8605e-01, 1.9862e-01, 9.7164e-01, 2.5306e-01, 4.8838e-02, 1.6665e-01,\n",
      "        4.3953e-01, 1.4043e-02, 7.3549e-02, 2.7818e-01, 1.6240e-01, 3.1318e-02,\n",
      "        1.0070e-01, 3.4658e-02, 1.2916e-02, 1.5906e-01, 1.2593e-02, 5.4858e-02,\n",
      "        1.1242e-03, 2.0358e-01, 8.1181e-01, 3.9455e+00, 2.7042e+00, 3.2188e+00,\n",
      "        1.7873e+00, 2.7863e-01, 3.1950e-01, 1.7826e-01, 2.8231e-01, 1.1482e+00,\n",
      "        1.4534e-01, 1.1582e-01, 3.7184e-01, 3.7783e-02, 1.9374e-02, 4.4077e-02,\n",
      "        2.0329e-02, 2.8215e-02, 1.0766e-02, 8.5608e-03, 1.7922e-03, 7.0941e-04,\n",
      "        2.1258e-04, 2.7871e-04, 2.4623e-03, 2.9052e-03, 1.2222e+00, 6.2876e-02,\n",
      "        4.9554e+01, 1.0441e+00, 9.4086e-02, 3.1412e-01, 1.6858e+00, 1.0971e-01,\n",
      "        1.7795e+00, 1.4968e+00, 9.3776e-02, 1.4415e+00, 2.3063e-02, 2.6516e-01,\n",
      "        1.7998e-02, 1.6665e-02, 4.1897e-04, 7.1072e-03, 2.2952e-03, 6.2018e-05,\n",
      "        2.2931e-01, 2.5331e-01, 2.8259e+00, 1.1213e-02, 1.6284e+00, 2.3494e-02,\n",
      "        7.9767e-01, 6.0665e-01, 6.4289e-02, 2.8646e-06, 4.6600e-06, 1.4945e-04,\n",
      "        1.8002e-04], device='cuda:0')\n",
      "Sig sum tensor(199.0000, device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 3993.2404422110553\n",
      "pruning 22 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     24\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    2\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   17\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :7 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    8\n",
      "     ║    ╠════╝\n",
      "     ║    24\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    21\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   21\n",
      "   ╠════╗\n",
      "   ║    12\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  32\n",
      "  ╠════╗\n",
      "  ║    10\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 34\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.2473e+05, 1.4897e+07, 6.8763e+05, 1.6277e+06, 6.4883e+05, 4.8909e+05,\n",
      "        4.9860e+04, 2.4642e+06, 1.9894e+05, 4.4534e+06, 3.2023e+05, 1.1639e+05,\n",
      "        3.8566e+05, 1.2716e+06, 6.2137e+05, 9.7751e+04, 4.9011e+06, 6.4002e+05,\n",
      "        1.3560e+07, 7.3944e+06, 1.3548e+06, 5.2027e+06, 3.5098e+05, 1.6515e+05,\n",
      "        1.9158e+06, 2.9490e+05, 1.0955e+05, 3.9708e+04, 1.7987e+05, 3.8708e+05,\n",
      "        1.9268e+05, 5.4468e+05, 1.4556e+01, 8.7876e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.7746, 0.6893, 0.7396, 0.4532, 0.1770, 0.4750, 0.5369, 0.5436, 0.2505,\n",
      "        0.8582, 0.4048, 0.3653, 0.8711, 0.4898, 0.6048, 0.2504, 0.5088, 0.2210,\n",
      "        0.9511, 0.6062, 0.6156, 0.7519, 0.6440, 0.4767, 0.5341, 0.3989, 0.4354,\n",
      "        0.7044, 0.5091, 0.3758, 0.1015, 0.1500, 0.0860, 0.0466],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.9280e+05, 1.8516e+07, 7.1635e+05, 3.5604e+06, 2.1360e+06, 1.0271e+06,\n",
      "        9.2367e+04, 4.4987e+06, 5.9645e+05, 2.5251e+06, 7.6246e+05, 2.9549e+05,\n",
      "        1.9878e+05, 2.5953e+06, 9.8239e+05, 2.9311e+05, 9.6300e+06, 1.9943e+06,\n",
      "        2.6522e+06, 1.1649e+07, 2.0833e+06, 5.1631e+06, 4.9974e+05, 3.4568e+05,\n",
      "        3.5706e+06, 7.0901e+05, 2.4740e+05, 4.6943e+04, 3.5322e+05, 9.6638e+05,\n",
      "        6.9250e+05, 1.8518e+06, 5.3220e+01, 3.3514e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.5254e+05, 5.4260e+06, 6.4519e+04, 1.4559e+06, 3.6406e+05, 2.4770e+05,\n",
      "        9.9276e+06, 3.5608e+06, 1.8064e+06, 1.4896e+05, 3.2458e+05, 1.2149e+06,\n",
      "        1.4102e+05, 3.3608e+06, 1.3461e+05, 9.9087e+05, 3.2940e+04, 9.0770e+06,\n",
      "        4.4332e+05, 6.1078e+06, 5.2862e+05, 8.8720e+05, 1.0697e+05, 2.1839e+06,\n",
      "        3.9640e+05, 2.0059e+06, 7.2946e+04, 7.9703e+05, 6.6034e+05, 5.9027e+04,\n",
      "        3.4109e+04, 2.4990e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8936, 0.8636, 0.5288, 0.9109, 0.8250, 0.7879, 0.8558, 0.5360, 0.7862,\n",
      "        0.8153, 0.6487, 0.2217, 0.8229, 0.4610, 0.6373, 0.8382, 0.4329, 0.9499,\n",
      "        0.7636, 0.4191, 0.8707, 0.4087, 0.4608, 0.4612, 0.5978, 0.6004, 0.4577,\n",
      "        0.4265, 0.4971, 0.1633, 0.2316, 0.1273], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.9264e+05, 2.9595e+06, 1.2161e+05, 5.1885e+05, 2.5487e+05, 2.1019e+05,\n",
      "        5.7268e+06, 6.6088e+06, 1.5449e+06, 1.1003e+05, 4.5608e+05, 3.7822e+06,\n",
      "        9.9906e+04, 7.2461e+06, 1.9530e+05, 6.4110e+05, 7.4725e+04, 1.8179e+06,\n",
      "        4.1915e+05, 1.4192e+07, 2.7338e+05, 2.0986e+06, 2.3073e+05, 4.7065e+06,\n",
      "        6.3767e+05, 3.2064e+06, 1.5824e+05, 1.8283e+06, 1.3283e+06, 1.9756e+05,\n",
      "        1.0484e+05, 8.7238e+01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.8554e+06, 1.3149e+05, 7.3853e+06, 2.6977e+05, 7.7849e+04, 1.6799e+07,\n",
      "        2.2442e+06, 6.0795e+05, 2.9305e+04, 5.9190e+04, 1.7512e+02, 3.3127e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5342, 0.4339, 0.8909, 0.2386, 0.6995, 0.4233, 0.2482, 0.3107, 0.1941,\n",
      "        0.2144, 0.0945, 0.1487], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([9.0473e+06, 2.9776e+05, 3.2228e+06, 8.2158e+05, 9.3581e+04, 3.8748e+07,\n",
      "        6.7486e+06, 1.6762e+06, 9.4469e+04, 1.8601e+05, 6.3432e+02, 1.1280e+03],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.1453e+07, 3.2445e+07, 7.5436e+06, 2.7709e+06, 8.0065e+06, 8.2328e+05,\n",
      "        5.1072e+06, 2.9336e+06, 6.8686e+06, 2.2930e+07, 4.8334e+05, 1.0092e+07,\n",
      "        1.8754e+05, 2.1473e+05, 2.2615e+05, 4.0860e+05, 2.6494e+04, 2.6633e+02,\n",
      "        8.0719e+00, 1.6695e+02, 7.1512e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8745, 0.7722, 0.7828, 0.6688, 0.6484, 0.6246, 0.5519, 0.8327, 0.8677,\n",
      "        0.5001, 0.5940, 0.8049, 0.3446, 0.5096, 0.5250, 0.2422, 0.3332, 0.0581,\n",
      "        0.0546, 0.1270, 0.1158], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.5784e+07, 2.9563e+07, 6.5527e+06, 3.6705e+06, 1.1259e+07, 1.2361e+06,\n",
      "        9.1540e+06, 1.9632e+06, 3.6352e+06, 4.5851e+07, 7.8499e+05, 7.8769e+06,\n",
      "        4.9166e+05, 4.2125e+05, 4.2965e+05, 1.2386e+06, 7.0660e+04, 1.0035e+03,\n",
      "        3.0523e+01, 5.8296e+02, 2.5294e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.0088e+06, 1.0082e+07, 5.0284e+06, 5.8671e+06, 1.0742e+07, 4.9465e+05,\n",
      "        2.6839e+06, 7.5157e+05, 1.2480e+05, 1.2019e+05, 5.9319e+05, 8.7740e+03,\n",
      "        1.4578e+03, 1.5490e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1673, 0.5755, 0.4467, 0.8175, 0.5116, 0.2666, 0.5043, 0.2578, 0.1010,\n",
      "        0.2284, 0.4467, 0.1028, 0.0699, 0.1084], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([6.6911e+06, 1.7119e+07, 1.1128e+07, 4.2825e+06, 2.0986e+07, 1.4511e+06,\n",
      "        5.3219e+06, 2.2314e+06, 4.4879e+05, 3.7094e+05, 1.3128e+06, 3.1488e+04,\n",
      "        5.4235e+03, 5.5244e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.7153e+08, 3.9192e+06, 4.5573e+05, 5.4441e+07, 1.1249e+07, 1.2439e+06,\n",
      "        1.2562e+07, 8.2022e+06, 4.1488e+05, 8.3115e+04, 8.2685e+05, 1.6335e+06,\n",
      "        4.9725e+04, 4.0670e+05, 5.3401e+05, 5.9928e+05, 1.7078e+05, 4.3137e+05,\n",
      "        6.9397e+04, 1.0139e+05, 3.8649e+05, 4.9184e+04, 7.2061e+04, 7.5780e+03,\n",
      "        5.0919e+03, 2.7028e+03, 1.0565e+02, 1.1205e+03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9861, 0.5201, 0.5024, 0.9679, 0.5038, 0.2948, 0.9887, 0.8711, 0.5490,\n",
      "        0.4531, 0.6289, 0.7064, 0.2180, 0.7934, 0.3785, 0.3841, 0.4377, 0.4529,\n",
      "        0.3801, 0.7812, 0.0917, 0.2022, 0.1772, 0.3880, 0.3562, 0.1098, 0.0679,\n",
      "        0.2527], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([9.5233e+06, 7.5232e+06, 9.0703e+05, 6.9864e+06, 2.2328e+07, 3.5089e+06,\n",
      "        5.6827e+05, 4.2293e+06, 7.4841e+05, 1.8183e+05, 1.2274e+06, 1.9186e+06,\n",
      "        1.5554e+05, 3.3616e+05, 1.3275e+06, 1.4764e+06, 3.8410e+05, 9.4396e+05,\n",
      "        1.7208e+05, 8.8719e+04, 1.4043e+06, 1.5696e+05, 2.3717e+05, 1.8552e+04,\n",
      "        1.3113e+04, 9.6244e+03, 3.9392e+02, 3.3493e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.7016e+06, 4.1603e+05, 1.6247e+07, 1.9603e+07, 4.9182e+06, 3.5154e+07,\n",
      "        8.2317e+05, 6.6635e+05, 3.8984e+05, 3.3554e+05, 2.4722e+06, 1.9534e+05,\n",
      "        4.7667e+05, 1.0915e+06, 8.8734e+04, 6.6191e+04, 1.3839e+05, 1.9046e+05,\n",
      "        1.1876e+05, 3.3108e+04, 3.0466e+04, 1.2270e+04, 7.5188e+01, 6.4870e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9151, 0.3944, 0.8296, 0.8663, 0.4421, 0.9324, 0.5999, 0.3131, 0.3897,\n",
      "        0.1077, 0.2568, 0.2973, 0.4496, 0.2998, 0.1002, 0.1468, 0.1943, 0.4639,\n",
      "        0.1330, 0.0887, 0.2304, 0.2724, 0.1257, 0.2534], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2570e+06, 1.0078e+06, 1.1073e+07, 1.0485e+07, 1.0975e+07, 9.5109e+06,\n",
      "        1.3173e+06, 1.8309e+06, 9.5167e+05, 1.1976e+06, 7.3490e+06, 5.4909e+05,\n",
      "        1.0494e+06, 3.0573e+06, 3.1939e+05, 2.2589e+05, 4.4598e+05, 4.0845e+05,\n",
      "        4.1184e+05, 1.2069e+05, 9.3789e+04, 3.5709e+04, 2.6295e+02, 1.9372e+03],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([15300.4980, 49213.5938,  1235.1935,  2704.0913,   796.4119,  1869.7808,\n",
      "          177.1178,   318.6448], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.0848, 0.1587, 0.1123, 0.1382, 0.1496, 0.0867, 0.0446, 0.0926],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([ 56014.1836, 165610.4375,   4385.7036,   9321.5303,   2709.1482,\n",
      "          6830.6904,    676.8940,   1156.5916], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.0698e+07, 2.1265e+06, 1.9788e+08, 2.3654e+06, 3.2967e+06, 1.1719e+07,\n",
      "        1.3792e+07, 5.4302e+05, 1.3041e+07, 2.5526e+06, 1.3609e+05, 5.5443e+06,\n",
      "        6.6407e+04, 1.0529e+06, 6.6337e+04, 5.0622e+04, 1.1346e+05, 1.9154e+04,\n",
      "        9.7838e+02, 1.3344e+02, 9.5252e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8219, 0.9611, 0.7011, 0.5413, 0.9752, 0.9763, 0.8046, 0.6035, 0.8610,\n",
      "        0.2959, 0.1569, 0.5878, 0.3478, 0.4628, 0.2105, 0.1590, 0.1099, 0.1171,\n",
      "        0.2131, 0.1737, 0.1038], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.4747e+07, 3.3075e+05, 2.3662e+08, 4.3402e+06, 3.2646e+05, 1.1133e+06,\n",
      "        1.0778e+07, 8.6133e+05, 7.2489e+06, 7.1891e+06, 4.5892e+05, 9.1416e+06,\n",
      "        1.7325e+05, 2.2623e+06, 2.0950e+05, 1.7030e+05, 4.0393e+05, 6.7642e+04,\n",
      "        3.0797e+03, 4.4106e+02, 3.4147e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([ 269.9511, 1228.2045, 2135.7507], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3252, 0.1347, 0.1705], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([ 728.6088, 4250.9565, 7086.8369], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.4254e+05, 7.0709e+05, 4.5564e+06, 1.5345e+04, 4.1344e+06, 2.0880e+04,\n",
      "        1.7070e+06, 1.1115e+06, 1.2708e+05, 4.6347e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3154, 0.5975, 0.5859, 0.4612, 0.4377, 0.4253, 0.3239, 0.3110, 0.2569,\n",
      "        0.0436], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([6.6414e+05, 1.1383e+06, 7.5480e+06, 3.3072e+04, 9.2999e+06, 4.7996e+04,\n",
      "        4.6164e+06, 3.0632e+06, 3.7772e+05, 1.7731e+02], device='cuda:0')\n",
      "All_sigs tensor([2.9280e+05, 1.8516e+07, 7.1635e+05, 3.5604e+06, 2.1360e+06, 1.0271e+06,\n",
      "        9.2367e+04, 4.4987e+06, 5.9645e+05, 2.5251e+06, 7.6246e+05, 2.9549e+05,\n",
      "        1.9878e+05, 2.5953e+06, 9.8239e+05, 2.9311e+05, 9.6300e+06, 1.9943e+06,\n",
      "        2.6522e+06, 1.1649e+07, 2.0833e+06, 5.1631e+06, 4.9974e+05, 3.4568e+05,\n",
      "        3.5706e+06, 7.0901e+05, 2.4740e+05, 4.6943e+04, 3.5322e+05, 9.6638e+05,\n",
      "        6.9250e+05, 1.8518e+06, 5.3220e+01, 3.3514e+00, 1.9264e+05, 2.9595e+06,\n",
      "        1.2161e+05, 5.1885e+05, 2.5487e+05, 2.1019e+05, 5.7268e+06, 6.6088e+06,\n",
      "        1.5449e+06, 1.1003e+05, 4.5608e+05, 3.7822e+06, 9.9906e+04, 7.2461e+06,\n",
      "        1.9530e+05, 6.4110e+05, 7.4725e+04, 1.8179e+06, 4.1915e+05, 1.4192e+07,\n",
      "        2.7338e+05, 2.0986e+06, 2.3073e+05, 4.7065e+06, 6.3767e+05, 3.2064e+06,\n",
      "        1.5824e+05, 1.8283e+06, 1.3283e+06, 1.9756e+05, 1.0484e+05, 8.7238e+01,\n",
      "        9.0473e+06, 2.9776e+05, 3.2228e+06, 8.2158e+05, 9.3581e+04, 3.8748e+07,\n",
      "        6.7486e+06, 1.6762e+06, 9.4469e+04, 1.8601e+05, 6.3432e+02, 1.1280e+03,\n",
      "        1.5784e+07, 2.9563e+07, 6.5527e+06, 3.6705e+06, 1.1259e+07, 1.2361e+06,\n",
      "        9.1540e+06, 1.9632e+06, 3.6352e+06, 4.5851e+07, 7.8499e+05, 7.8769e+06,\n",
      "        4.9166e+05, 4.2125e+05, 4.2965e+05, 1.2386e+06, 7.0660e+04, 1.0035e+03,\n",
      "        3.0523e+01, 5.8296e+02, 2.5294e+02, 6.6911e+06, 1.7119e+07, 1.1128e+07,\n",
      "        4.2825e+06, 2.0986e+07, 1.4511e+06, 5.3219e+06, 2.2314e+06, 4.4879e+05,\n",
      "        3.7094e+05, 1.3128e+06, 3.1488e+04, 5.4235e+03, 5.5244e+02, 9.5233e+06,\n",
      "        7.5232e+06, 9.0703e+05, 6.9864e+06, 2.2328e+07, 3.5089e+06, 5.6827e+05,\n",
      "        4.2293e+06, 7.4841e+05, 1.8183e+05, 1.2274e+06, 1.9186e+06, 1.5554e+05,\n",
      "        3.3616e+05, 1.3275e+06, 1.4764e+06, 3.8410e+05, 9.4396e+05, 1.7208e+05,\n",
      "        8.8719e+04, 1.4043e+06, 1.5696e+05, 2.3717e+05, 1.8552e+04, 1.3113e+04,\n",
      "        9.6244e+03, 3.9392e+02, 3.3493e+03, 1.2570e+06, 1.0078e+06, 1.1073e+07,\n",
      "        1.0485e+07, 1.0975e+07, 9.5109e+06, 1.3173e+06, 1.8309e+06, 9.5167e+05,\n",
      "        1.1976e+06, 7.3490e+06, 5.4909e+05, 1.0494e+06, 3.0573e+06, 3.1939e+05,\n",
      "        2.2589e+05, 4.4598e+05, 4.0845e+05, 4.1184e+05, 1.2069e+05, 9.3789e+04,\n",
      "        3.5709e+04, 2.6295e+02, 1.9372e+03, 5.6014e+04, 1.6561e+05, 4.3857e+03,\n",
      "        9.3215e+03, 2.7091e+03, 6.8307e+03, 6.7689e+02, 1.1566e+03, 1.4747e+07,\n",
      "        3.3075e+05, 2.3662e+08, 4.3402e+06, 3.2646e+05, 1.1133e+06, 1.0778e+07,\n",
      "        8.6133e+05, 7.2489e+06, 7.1891e+06, 4.5892e+05, 9.1416e+06, 1.7325e+05,\n",
      "        2.2623e+06, 2.0950e+05, 1.7030e+05, 4.0393e+05, 6.7642e+04, 3.0797e+03,\n",
      "        4.4106e+02, 3.4147e+03, 7.2861e+02, 4.2510e+03, 7.0868e+03, 6.6414e+05,\n",
      "        1.1383e+06, 7.5480e+06, 3.3072e+04, 9.2999e+06, 4.7996e+04, 4.6164e+06,\n",
      "        3.0632e+06, 3.7772e+05, 1.7731e+02], device='cuda:0')\n",
      "All_sig tensor([6.9720e-02, 4.4089e+00, 1.7057e-01, 8.4778e-01, 5.0861e-01, 2.4457e-01,\n",
      "        2.1994e-02, 1.0712e+00, 1.4202e-01, 6.0126e-01, 1.8155e-01, 7.0360e-02,\n",
      "        4.7331e-02, 6.1797e-01, 2.3392e-01, 6.9792e-02, 2.2930e+00, 4.7487e-01,\n",
      "        6.3153e-01, 2.7737e+00, 4.9606e-01, 1.2294e+00, 1.1899e-01, 8.2310e-02,\n",
      "        8.5019e-01, 1.6882e-01, 5.8910e-02, 1.1178e-02, 8.4105e-02, 2.3011e-01,\n",
      "        1.6489e-01, 4.4094e-01, 1.2672e-05, 7.9801e-07, 4.5869e-02, 7.0470e-01,\n",
      "        2.8957e-02, 1.2354e-01, 6.0688e-02, 5.0050e-02, 1.3636e+00, 1.5736e+00,\n",
      "        3.6785e-01, 2.6199e-02, 1.0860e-01, 9.0060e-01, 2.3789e-02, 1.7254e+00,\n",
      "        4.6502e-02, 1.5265e-01, 1.7793e-02, 4.3286e-01, 9.9804e-02, 3.3794e+00,\n",
      "        6.5096e-02, 4.9970e-01, 5.4939e-02, 1.1207e+00, 1.5184e-01, 7.6348e-01,\n",
      "        3.7679e-02, 4.3534e-01, 3.1629e-01, 4.7041e-02, 2.4963e-02, 2.0772e-05,\n",
      "        2.1543e+00, 7.0901e-02, 7.6739e-01, 1.9563e-01, 2.2283e-02, 9.2265e+00,\n",
      "        1.6069e+00, 3.9912e-01, 2.2494e-02, 4.4290e-02, 1.5104e-04, 2.6860e-04,\n",
      "        3.7583e+00, 7.0394e+00, 1.5603e+00, 8.7400e-01, 2.6809e+00, 2.9434e-01,\n",
      "        2.1797e+00, 4.6746e-01, 8.6558e-01, 1.0918e+01, 1.8692e-01, 1.8756e+00,\n",
      "        1.1707e-01, 1.0030e-01, 1.0230e-01, 2.9493e-01, 1.6825e-02, 2.3894e-04,\n",
      "        7.2680e-06, 1.3881e-04, 6.0227e-05, 1.5932e+00, 4.0762e+00, 2.6498e+00,\n",
      "        1.0197e+00, 4.9970e+00, 3.4552e-01, 1.2672e+00, 5.3132e-01, 1.0686e-01,\n",
      "        8.8325e-02, 3.1258e-01, 7.4977e-03, 1.2914e-03, 1.3154e-04, 2.2676e+00,\n",
      "        1.7914e+00, 2.1597e-01, 1.6635e+00, 5.3166e+00, 8.3552e-01, 1.3531e-01,\n",
      "        1.0071e+00, 1.7820e-01, 4.3295e-02, 2.9225e-01, 4.5684e-01, 3.7036e-02,\n",
      "        8.0044e-02, 3.1608e-01, 3.5154e-01, 9.1459e-02, 2.2477e-01, 4.0975e-02,\n",
      "        2.1125e-02, 3.3437e-01, 3.7375e-02, 5.6472e-02, 4.4174e-03, 3.1224e-03,\n",
      "        2.2917e-03, 9.3798e-05, 7.9751e-04, 2.9931e-01, 2.3997e-01, 2.6367e+00,\n",
      "        2.4966e+00, 2.6134e+00, 2.2647e+00, 3.1366e-01, 4.3595e-01, 2.2660e-01,\n",
      "        2.8516e-01, 1.7499e+00, 1.3075e-01, 2.4989e-01, 7.2798e-01, 7.6050e-02,\n",
      "        5.3787e-02, 1.0619e-01, 9.7257e-02, 9.8064e-02, 2.8738e-02, 2.2332e-02,\n",
      "        8.5027e-03, 6.2612e-05, 4.6127e-04, 1.3338e-02, 3.9434e-02, 1.0443e-03,\n",
      "        2.2196e-03, 6.4508e-04, 1.6265e-03, 1.6118e-04, 2.7540e-04, 3.5115e+00,\n",
      "        7.8756e-02, 5.6341e+01, 1.0334e+00, 7.7735e-02, 2.6509e-01, 2.5665e+00,\n",
      "        2.0509e-01, 1.7261e+00, 1.7118e+00, 1.0928e-01, 2.1767e+00, 4.1253e-02,\n",
      "        5.3869e-01, 4.9885e-02, 4.0551e-02, 9.6181e-02, 1.6106e-02, 7.3330e-04,\n",
      "        1.0502e-04, 8.1308e-04, 1.7349e-04, 1.0122e-03, 1.6875e-03, 1.5814e-01,\n",
      "        2.7104e-01, 1.7973e+00, 7.8748e-03, 2.2144e+00, 1.1428e-02, 1.0992e+00,\n",
      "        7.2937e-01, 8.9939e-02, 4.2220e-05], device='cuda:0')\n",
      "Sig sum tensor(207., device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 4199.706589371981\n",
      "pruning 22 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     26\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    2\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   17\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :8 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     34\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    8\n",
      "     ║    ╠════╝\n",
      "     ║    26\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   20\n",
      "   ╠════╗\n",
      "   ║    12\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  35\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 33\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.3507e+05, 1.3301e+07, 7.3276e+05, 1.7937e+06, 8.5682e+05, 4.5096e+05,\n",
      "        4.3201e+04, 2.0577e+06, 1.7315e+05, 5.0579e+06, 3.5836e+05, 1.0699e+05,\n",
      "        3.6225e+05, 1.3464e+06, 4.8609e+05, 1.3899e+05, 5.1764e+06, 6.5455e+05,\n",
      "        1.2543e+07, 8.0815e+06, 1.4379e+06, 5.4518e+06, 2.7799e+05, 1.7120e+05,\n",
      "        1.6333e+06, 4.0740e+05, 1.2180e+05, 5.5827e+04, 2.0404e+05, 3.8679e+05,\n",
      "        2.3061e+05, 8.0861e+05, 5.3301e-02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.7768, 0.6543, 0.7615, 0.4310, 0.1861, 0.4503, 0.4971, 0.5118, 0.2464,\n",
      "        0.8551, 0.3689, 0.3492, 0.8076, 0.4770, 0.5736, 0.3033, 0.5081, 0.2137,\n",
      "        0.9090, 0.6084, 0.5879, 0.7614, 0.5798, 0.4878, 0.5071, 0.4616, 0.3821,\n",
      "        0.6972, 0.5236, 0.3585, 0.1034, 0.2012, 0.0175], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.9922e+05, 1.8393e+07, 6.9902e+05, 4.0824e+06, 2.7896e+06, 9.9154e+05,\n",
      "        8.6906e+04, 4.0185e+06, 5.2192e+05, 2.9313e+06, 9.0468e+05, 2.7854e+05,\n",
      "        2.7885e+05, 2.8166e+06, 8.2908e+05, 3.8731e+05, 1.0186e+07, 2.0588e+06,\n",
      "        4.5653e+06, 1.2660e+07, 2.3702e+06, 5.2035e+06, 4.6730e+05, 3.5075e+05,\n",
      "        3.2202e+06, 8.7746e+05, 3.0102e+05, 6.7611e+04, 3.8882e+05, 9.9252e+05,\n",
      "        8.2709e+05, 2.5838e+06, 2.0948e-01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([5.1596e+05, 4.9662e+06, 4.9437e+04, 1.2653e+06, 2.4793e+05, 2.1599e+05,\n",
      "        1.1051e+07, 3.1666e+06, 2.0203e+06, 8.6553e+04, 3.7329e+05, 1.9152e+06,\n",
      "        1.3233e+05, 2.9924e+06, 1.2248e+05, 9.8858e+05, 3.4480e+04, 7.5749e+06,\n",
      "        4.1186e+05, 5.9707e+06, 5.5337e+05, 1.2220e+06, 1.2605e+05, 2.2581e+06,\n",
      "        6.4227e+05, 1.8013e+06, 9.4883e+04, 1.1550e+06, 7.1304e+05, 6.3373e+04,\n",
      "        5.5781e+04, 4.4977e+00, 1.1494e+01, 7.9771e+00, 5.0433e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8868, 0.8247, 0.5109, 0.8844, 0.7185, 0.7510, 0.8518, 0.5259, 0.7803,\n",
      "        0.7186, 0.7003, 0.2391, 0.8353, 0.4391, 0.6405, 0.7857, 0.4572, 0.8802,\n",
      "        0.7873, 0.4039, 0.8581, 0.4347, 0.4670, 0.4682, 0.6460, 0.5777, 0.4591,\n",
      "        0.4675, 0.4875, 0.1594, 0.2288, 0.0427, 0.0909, 0.0593, 0.0522],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.3373e+05, 3.4825e+06, 9.6714e+04, 5.8508e+05, 2.7916e+05, 2.1514e+05,\n",
      "        6.5490e+06, 6.0048e+06, 1.7755e+06, 9.7432e+04, 4.4749e+05, 5.8294e+06,\n",
      "        8.7163e+04, 6.7140e+06, 1.7612e+05, 8.4729e+05, 7.4861e+04, 3.6308e+06,\n",
      "        3.5035e+05, 1.4236e+07, 3.1408e+05, 2.7634e+06, 2.6872e+05, 4.8031e+06,\n",
      "        9.0934e+05, 3.0429e+06, 2.0530e+05, 2.4599e+06, 1.4616e+06, 2.1309e+05,\n",
      "        1.7207e+05, 1.7223e+01, 4.1798e+01, 3.0017e+01, 1.9120e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([7.5471e+06, 1.8765e+05, 7.4965e+06, 2.9049e+05, 7.5748e+04, 1.8296e+07,\n",
      "        2.2242e+06, 6.6719e+05, 5.7241e+04, 1.8016e+05, 1.8442e+01, 1.9785e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5738, 0.4244, 0.8914, 0.2165, 0.6888, 0.4158, 0.2146, 0.3041, 0.2212,\n",
      "        0.3349, 0.0712, 0.0848], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2865e+07, 4.3205e+05, 3.2571e+06, 9.1041e+05, 9.4302e+04, 4.2750e+07,\n",
      "        6.9877e+06, 1.8573e+06, 1.7833e+05, 4.7927e+05, 6.8516e+01, 7.2427e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.7007e+07, 4.2420e+07, 7.0065e+06, 3.7452e+06, 8.3868e+06, 4.3278e+05,\n",
      "        4.2487e+06, 3.2119e+06, 4.2842e+06, 3.1777e+07, 8.2570e+05, 8.7671e+06,\n",
      "        1.9431e+05, 2.5373e+05, 4.0831e+05, 6.1364e+05, 1.0824e+05, 1.2439e+02,\n",
      "        8.7200e-01, 3.1007e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8547, 0.7695, 0.7342, 0.6585, 0.6282, 0.6350, 0.5067, 0.7973, 0.8597,\n",
      "        0.5101, 0.5993, 0.8053, 0.4032, 0.4932, 0.5067, 0.2459, 0.4195, 0.0456,\n",
      "        0.1471, 0.1448], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.5694e+07, 3.9118e+07, 7.4502e+06, 5.1157e+06, 1.2474e+07, 6.3189e+05,\n",
      "        8.3839e+06, 2.6038e+06, 2.4036e+06, 6.2267e+07, 1.3233e+06, 6.8289e+06,\n",
      "        4.6386e+05, 5.1442e+05, 8.0565e+05, 1.8509e+06, 2.5136e+05, 4.7488e+02,\n",
      "        2.9748e+00, 1.0607e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.1415e+06, 8.2891e+06, 3.8476e+06, 5.7221e+06, 1.4927e+07, 9.6405e+05,\n",
      "        3.7676e+06, 9.1315e+05, 8.8860e+04, 2.1024e+05, 8.5825e+05, 1.3913e+04,\n",
      "        5.4127e+04, 9.9729e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1675, 0.5741, 0.4110, 0.7687, 0.4971, 0.2879, 0.5316, 0.2457, 0.0985,\n",
      "        0.2624, 0.4252, 0.1141, 0.0911, 0.0709], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([7.1312e+06, 1.4122e+07, 9.0652e+06, 5.2949e+06, 3.0026e+07, 2.7462e+06,\n",
      "        7.0588e+06, 2.7551e+06, 3.2043e+05, 6.2032e+05, 1.9733e+06, 4.9300e+04,\n",
      "        1.9679e+05, 3.7065e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.7823e+08, 4.1881e+06, 6.1528e+05, 5.2859e+07, 1.4264e+07, 7.1221e+05,\n",
      "        1.1623e+07, 7.3005e+06, 5.3646e+05, 1.0516e+05, 1.2059e+06, 1.8502e+06,\n",
      "        8.2429e+04, 2.8087e+05, 5.8151e+05, 1.0339e+06, 4.9911e+05, 1.3021e+06,\n",
      "        8.6612e+04, 1.3724e+05, 4.0062e+05, 7.1395e+04, 8.5066e+04, 2.0909e+04,\n",
      "        2.9963e+04, 1.6584e+04, 1.2959e+03, 7.5391e+02, 5.8070e+01, 4.1502e+03,\n",
      "        6.7297e+02, 1.7867e+03, 4.2277e+02, 1.1355e+03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9851, 0.5186, 0.5008, 0.9671, 0.5041, 0.3002, 0.9882, 0.8710, 0.5534,\n",
      "        0.4536, 0.6349, 0.7107, 0.2172, 0.7926, 0.3778, 0.3793, 0.4355, 0.4553,\n",
      "        0.3792, 0.7778, 0.0848, 0.2206, 0.1782, 0.3905, 0.3050, 0.1369, 0.2629,\n",
      "        0.2424, 0.0237, 0.0923, 0.1207, 0.0413, 0.3148, 0.3177],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.0593e+07, 8.0653e+06, 1.2286e+06, 6.9558e+06, 2.8296e+07, 1.9936e+06,\n",
      "        5.4783e+05, 3.7671e+06, 9.5835e+05, 2.2983e+05, 1.7612e+06, 2.1409e+06,\n",
      "        2.5809e+05, 2.3298e+05, 1.4473e+06, 2.5671e+06, 1.1270e+06, 2.8371e+06,\n",
      "        2.1508e+05, 1.2196e+05, 1.4666e+06, 2.2257e+05, 2.7962e+05, 5.0975e+04,\n",
      "        8.3291e+04, 5.7258e+04, 3.8208e+03, 2.2846e+03, 2.2678e+02, 1.5069e+04,\n",
      "        2.3670e+03, 6.8517e+03, 1.1588e+03, 3.0991e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.4636e+06, 6.0780e+05, 2.0595e+07, 1.4742e+07, 3.9280e+06, 3.2887e+07,\n",
      "        6.6213e+05, 4.9215e+05, 6.0464e+05, 4.6447e+05, 3.0422e+06, 1.9830e+05,\n",
      "        5.2619e+05, 1.2604e+06, 9.5243e+04, 1.2243e+05, 2.0582e+05, 4.3807e+05,\n",
      "        2.3857e+05, 5.1903e+04, 3.8498e+04, 4.8992e+04, 3.4579e+02, 9.9932e+01,\n",
      "        1.1131e+03, 6.1165e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8838, 0.3968, 0.8076, 0.8453, 0.4342, 0.9357, 0.6055, 0.3116, 0.3776,\n",
      "        0.1195, 0.2746, 0.3008, 0.4476, 0.2937, 0.1175, 0.1879, 0.1870, 0.5719,\n",
      "        0.1513, 0.0864, 0.2147, 0.2931, 0.0588, 0.0653, 0.0774, 0.0369],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.0742e+06, 1.4665e+06, 1.5846e+07, 9.1218e+06, 8.8904e+06, 8.4587e+06,\n",
      "        1.0449e+06, 1.3551e+06, 1.5053e+06, 1.6358e+06, 8.8275e+06, 5.5457e+05,\n",
      "        1.1626e+06, 3.5607e+06, 3.3622e+05, 3.9771e+05, 6.6931e+05, 7.5008e+05,\n",
      "        8.0985e+05, 1.8967e+05, 1.2093e+05, 1.3853e+05, 1.3018e+03, 3.7361e+02,\n",
      "        4.1078e+03, 2.3564e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([32062.0117, 93080.2734, 18518.7559, 45070.2031, 29712.8516,  5306.5439,\n",
      "         2547.8088,   437.5655], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.0976, 0.1759, 0.2405, 0.1378, 0.1159, 0.2922, 0.0908, 0.1500],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([115732.1094, 306816.4688,  56262.0273, 155436.9531, 105071.5625,\n",
      "         15024.5215,   9266.2705,   1487.7283], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.5364e+07, 2.4108e+06, 2.1610e+08, 2.9330e+06, 1.4093e+06, 1.0026e+07,\n",
      "        1.2933e+07, 6.5341e+05, 1.4822e+07, 2.5003e+06, 1.6982e+05, 7.9490e+06,\n",
      "        1.0493e+05, 1.3797e+06, 1.2199e+05, 1.0887e+05, 2.5558e+05, 3.7811e+04,\n",
      "        1.0251e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8728, 0.9621, 0.7031, 0.6375, 0.9638, 0.9737, 0.7868, 0.5723, 0.8528,\n",
      "        0.2933, 0.1781, 0.6257, 0.3630, 0.4771, 0.2365, 0.1434, 0.1057, 0.1807,\n",
      "        0.1217], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([7.8169e+06, 3.6549e+05, 2.5668e+08, 4.2524e+06, 2.0412e+05, 1.0527e+06,\n",
      "        1.1028e+07, 1.1179e+06, 8.7251e+06, 7.0675e+06, 5.5827e+05, 1.1901e+07,\n",
      "        2.6736e+05, 2.8857e+06, 3.7253e+05, 3.7303e+05, 9.1424e+05, 1.2391e+05,\n",
      "        3.6013e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([24026.2598,  9085.3281,   389.1693,  8302.2842,   174.2918],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1970, 0.2003, 0.2509, 0.1660, 0.1436], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([77171.3047, 29063.7988,  1166.1219, 27698.0449,   597.0275],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([ 224128.1562,  970219.6875, 4623581.0000,   26670.9355, 3956478.2500,\n",
      "          12101.1631, 2292797.7500,  944954.1250,  110828.9297],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3386, 0.6017, 0.5783, 0.4809, 0.4152, 0.4119, 0.3438, 0.3038, 0.2214],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([ 592949.6875, 1545900.3750, 7798805.0000,   55379.2461, 9254219.0000,\n",
      "          28465.5742, 6017742.5000, 2631617.2500,  345143.7500],\n",
      "       device='cuda:0')\n",
      "All_sigs tensor([2.9922e+05, 1.8393e+07, 6.9902e+05, 4.0824e+06, 2.7896e+06, 9.9154e+05,\n",
      "        8.6906e+04, 4.0185e+06, 5.2192e+05, 2.9313e+06, 9.0468e+05, 2.7854e+05,\n",
      "        2.7885e+05, 2.8166e+06, 8.2908e+05, 3.8731e+05, 1.0186e+07, 2.0588e+06,\n",
      "        4.5653e+06, 1.2660e+07, 2.3702e+06, 5.2035e+06, 4.6730e+05, 3.5075e+05,\n",
      "        3.2202e+06, 8.7746e+05, 3.0102e+05, 6.7611e+04, 3.8882e+05, 9.9252e+05,\n",
      "        8.2709e+05, 2.5838e+06, 2.0948e-01, 2.3373e+05, 3.4825e+06, 9.6714e+04,\n",
      "        5.8508e+05, 2.7916e+05, 2.1514e+05, 6.5490e+06, 6.0048e+06, 1.7755e+06,\n",
      "        9.7432e+04, 4.4749e+05, 5.8294e+06, 8.7163e+04, 6.7140e+06, 1.7612e+05,\n",
      "        8.4729e+05, 7.4861e+04, 3.6308e+06, 3.5035e+05, 1.4236e+07, 3.1408e+05,\n",
      "        2.7634e+06, 2.6872e+05, 4.8031e+06, 9.0934e+05, 3.0429e+06, 2.0530e+05,\n",
      "        2.4599e+06, 1.4616e+06, 2.1309e+05, 1.7207e+05, 1.7223e+01, 4.1798e+01,\n",
      "        3.0017e+01, 1.9120e+00, 1.2865e+07, 4.3205e+05, 3.2571e+06, 9.1041e+05,\n",
      "        9.4302e+04, 4.2750e+07, 6.9877e+06, 1.8573e+06, 1.7833e+05, 4.7927e+05,\n",
      "        6.8516e+01, 7.2427e+00, 1.5694e+07, 3.9118e+07, 7.4502e+06, 5.1157e+06,\n",
      "        1.2474e+07, 6.3189e+05, 8.3839e+06, 2.6038e+06, 2.4036e+06, 6.2267e+07,\n",
      "        1.3233e+06, 6.8289e+06, 4.6386e+05, 5.1442e+05, 8.0565e+05, 1.8509e+06,\n",
      "        2.5136e+05, 4.7488e+02, 2.9748e+00, 1.0607e+03, 7.1312e+06, 1.4122e+07,\n",
      "        9.0652e+06, 5.2949e+06, 3.0026e+07, 2.7462e+06, 7.0588e+06, 2.7551e+06,\n",
      "        3.2043e+05, 6.2032e+05, 1.9733e+06, 4.9300e+04, 1.9679e+05, 3.7065e+00,\n",
      "        1.0593e+07, 8.0653e+06, 1.2286e+06, 6.9558e+06, 2.8296e+07, 1.9936e+06,\n",
      "        5.4783e+05, 3.7671e+06, 9.5835e+05, 2.2983e+05, 1.7612e+06, 2.1409e+06,\n",
      "        2.5809e+05, 2.3298e+05, 1.4473e+06, 2.5671e+06, 1.1270e+06, 2.8371e+06,\n",
      "        2.1508e+05, 1.2196e+05, 1.4666e+06, 2.2257e+05, 2.7962e+05, 5.0975e+04,\n",
      "        8.3291e+04, 5.7258e+04, 3.8208e+03, 2.2846e+03, 2.2678e+02, 1.5069e+04,\n",
      "        2.3670e+03, 6.8517e+03, 1.1588e+03, 3.0991e+03, 2.0742e+06, 1.4665e+06,\n",
      "        1.5846e+07, 9.1218e+06, 8.8904e+06, 8.4587e+06, 1.0449e+06, 1.3551e+06,\n",
      "        1.5053e+06, 1.6358e+06, 8.8275e+06, 5.5457e+05, 1.1626e+06, 3.5607e+06,\n",
      "        3.3622e+05, 3.9771e+05, 6.6931e+05, 7.5008e+05, 8.0985e+05, 1.8967e+05,\n",
      "        1.2093e+05, 1.3853e+05, 1.3018e+03, 3.7361e+02, 4.1078e+03, 2.3564e+02,\n",
      "        1.1573e+05, 3.0682e+05, 5.6262e+04, 1.5544e+05, 1.0507e+05, 1.5025e+04,\n",
      "        9.2663e+03, 1.4877e+03, 7.8169e+06, 3.6549e+05, 2.5668e+08, 4.2524e+06,\n",
      "        2.0412e+05, 1.0527e+06, 1.1028e+07, 1.1179e+06, 8.7251e+06, 7.0675e+06,\n",
      "        5.5827e+05, 1.1901e+07, 2.6736e+05, 2.8857e+06, 3.7253e+05, 3.7303e+05,\n",
      "        9.1424e+05, 1.2391e+05, 3.6013e+02, 7.7171e+04, 2.9064e+04, 1.1661e+03,\n",
      "        2.7698e+04, 5.9703e+02, 5.9295e+05, 1.5459e+06, 7.7988e+06, 5.5379e+04,\n",
      "        9.2542e+06, 2.8466e+04, 6.0177e+06, 2.6316e+06, 3.4514e+05],\n",
      "       device='cuda:0')\n",
      "All_sig tensor([6.6565e-02, 4.0918e+00, 1.5551e-01, 9.0818e-01, 6.2058e-01, 2.2058e-01,\n",
      "        1.9333e-02, 8.9397e-01, 1.1611e-01, 6.5210e-01, 2.0126e-01, 6.1966e-02,\n",
      "        6.2034e-02, 6.2659e-01, 1.8444e-01, 8.6163e-02, 2.2659e+00, 4.5800e-01,\n",
      "        1.0156e+00, 2.8163e+00, 5.2728e-01, 1.1576e+00, 1.0396e-01, 7.8030e-02,\n",
      "        7.1637e-01, 1.9520e-01, 6.6966e-02, 1.5041e-02, 8.6497e-02, 2.2080e-01,\n",
      "        1.8400e-01, 5.7479e-01, 4.6602e-08, 5.1996e-02, 7.7473e-01, 2.1515e-02,\n",
      "        1.3016e-01, 6.2103e-02, 4.7862e-02, 1.4569e+00, 1.3358e+00, 3.9499e-01,\n",
      "        2.1675e-02, 9.9551e-02, 1.2968e+00, 1.9391e-02, 1.4936e+00, 3.9181e-02,\n",
      "        1.8849e-01, 1.6654e-02, 8.0772e-01, 7.7941e-02, 3.1670e+00, 6.9872e-02,\n",
      "        6.1475e-01, 5.9780e-02, 1.0685e+00, 2.0230e-01, 6.7693e-01, 4.5672e-02,\n",
      "        5.4724e-01, 3.2516e-01, 4.7405e-02, 3.8280e-02, 3.8316e-06, 9.2985e-06,\n",
      "        6.6777e-06, 4.2536e-07, 2.8620e+00, 9.6115e-02, 7.2459e-01, 2.0253e-01,\n",
      "        2.0979e-02, 9.5103e+00, 1.5545e+00, 4.1318e-01, 3.9672e-02, 1.0662e-01,\n",
      "        1.5242e-05, 1.6113e-06, 3.4913e+00, 8.7024e+00, 1.6574e+00, 1.1381e+00,\n",
      "        2.7750e+00, 1.4057e-01, 1.8651e+00, 5.7925e-01, 5.3472e-01, 1.3852e+01,\n",
      "        2.9440e-01, 1.5192e+00, 1.0319e-01, 1.1444e-01, 1.7923e-01, 4.1176e-01,\n",
      "        5.5918e-02, 1.0564e-04, 6.6180e-07, 2.3598e-04, 1.5864e+00, 3.1416e+00,\n",
      "        2.0167e+00, 1.1779e+00, 6.6798e+00, 6.1093e-01, 1.5703e+00, 6.1290e-01,\n",
      "        7.1285e-02, 1.3800e-01, 4.3899e-01, 1.0967e-02, 4.3778e-02, 8.2456e-07,\n",
      "        2.3565e+00, 1.7942e+00, 2.7332e-01, 1.5474e+00, 6.2948e+00, 4.4351e-01,\n",
      "        1.2187e-01, 8.3803e-01, 2.1320e-01, 5.1128e-02, 3.9179e-01, 4.7628e-01,\n",
      "        5.7416e-02, 5.1830e-02, 3.2198e-01, 5.7110e-01, 2.5072e-01, 6.3116e-01,\n",
      "        4.7847e-02, 2.7132e-02, 3.2627e-01, 4.9513e-02, 6.2206e-02, 1.1340e-02,\n",
      "        1.8529e-02, 1.2738e-02, 8.4998e-04, 5.0824e-04, 5.0451e-05, 3.3524e-03,\n",
      "        5.2657e-04, 1.5243e-03, 2.5780e-04, 6.8945e-04, 4.6143e-01, 3.2624e-01,\n",
      "        3.5252e+00, 2.0293e+00, 1.9778e+00, 1.8818e+00, 2.3245e-01, 3.0147e-01,\n",
      "        3.3488e-01, 3.6392e-01, 1.9638e+00, 1.2337e-01, 2.5865e-01, 7.9213e-01,\n",
      "        7.4798e-02, 8.8476e-02, 1.4890e-01, 1.6687e-01, 1.8016e-01, 4.2195e-02,\n",
      "        2.6902e-02, 3.0817e-02, 2.8961e-04, 8.3115e-05, 9.1383e-04, 5.2422e-05,\n",
      "        2.5746e-02, 6.8256e-02, 1.2516e-02, 3.4579e-02, 2.3375e-02, 3.3424e-03,\n",
      "        2.0614e-03, 3.3097e-04, 1.7390e+00, 8.1308e-02, 5.7103e+01, 9.4600e-01,\n",
      "        4.5410e-02, 2.3420e-01, 2.4533e+00, 2.4868e-01, 1.9410e+00, 1.5723e+00,\n",
      "        1.2420e-01, 2.6476e+00, 5.9478e-02, 6.4197e-01, 8.2875e-02, 8.2986e-02,\n",
      "        2.0339e-01, 2.7567e-02, 8.0117e-05, 1.7168e-02, 6.4657e-03, 2.5942e-04,\n",
      "        6.1618e-03, 1.3282e-04, 1.3191e-01, 3.4391e-01, 1.7350e+00, 1.2320e-02,\n",
      "        2.0587e+00, 6.3326e-03, 1.3387e+00, 5.8544e-01, 7.6782e-02],\n",
      "       device='cuda:0')\n",
      "Sig sum tensor(215.0000, device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 4495.105786046512\n",
      "pruning 25 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   17\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :9 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     30\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    9\n",
      "     ║    ╠════╝\n",
      "     ║    26\n",
      "     ║    ╠════╗\n",
      "     ║    ║    6\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    17\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   19\n",
      "   ╠════╗\n",
      "   ║    14\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  35\n",
      "  ╠════╗\n",
      "  ║    12\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 33\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.7037e+05, 1.2428e+07, 8.0417e+05, 1.7228e+06, 1.3029e+06, 5.6976e+05,\n",
      "        3.6703e+04, 1.8190e+06, 1.8514e+05, 5.8162e+06, 3.3890e+05, 1.1886e+05,\n",
      "        3.5992e+05, 1.7688e+06, 3.6323e+05, 1.2020e+05, 4.0028e+06, 6.3557e+05,\n",
      "        1.1288e+07, 8.1183e+06, 1.1930e+06, 5.6687e+06, 3.3237e+05, 1.5208e+05,\n",
      "        1.2486e+06, 3.3108e+05, 1.4063e+05, 5.9553e+04, 1.7210e+05, 5.1588e+05,\n",
      "        2.4279e+05, 8.5583e+05, 1.0320e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.7483, 0.6525, 0.7959, 0.4123, 0.2150, 0.4726, 0.4710, 0.5042, 0.2512,\n",
      "        0.8692, 0.3587, 0.3743, 0.7702, 0.5066, 0.5447, 0.3023, 0.4726, 0.2136,\n",
      "        0.8903, 0.6062, 0.5648, 0.7718, 0.5833, 0.4647, 0.4854, 0.4321, 0.3604,\n",
      "        0.6911, 0.5155, 0.3844, 0.0974, 0.1972, 0.1617], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.7216e+05, 1.7278e+07, 6.5656e+05, 4.0498e+06, 4.0910e+06, 1.2019e+06,\n",
      "        7.7665e+04, 3.6073e+06, 5.5451e+05, 3.0420e+06, 8.6936e+05, 2.9748e+05,\n",
      "        3.3078e+05, 3.4907e+06, 6.6146e+05, 3.3546e+05, 8.4435e+06, 1.9993e+06,\n",
      "        4.9548e+06, 1.2789e+07, 2.0769e+06, 5.1752e+06, 5.5396e+05, 3.2561e+05,\n",
      "        2.5701e+06, 7.5211e+05, 3.5979e+05, 7.3575e+04, 3.3354e+05, 1.2704e+06,\n",
      "        8.7658e+05, 2.7481e+06, 3.4603e+01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.6512e+05, 4.9285e+06, 8.6256e+04, 1.2635e+06, 2.3578e+05, 2.3282e+05,\n",
      "        1.1671e+07, 2.8139e+06, 2.2015e+06, 5.6663e+04, 2.9488e+05, 1.9466e+06,\n",
      "        1.1393e+05, 2.9810e+06, 1.2520e+05, 8.1268e+05, 2.4189e+04, 7.2598e+06,\n",
      "        2.9640e+05, 6.8130e+06, 4.8737e+05, 9.2919e+05, 1.3131e+05, 1.9650e+06,\n",
      "        8.0790e+05, 2.3961e+06, 9.5494e+04, 1.3571e+06, 7.0244e+05, 1.1899e+05,\n",
      "        1.1933e+05, 5.4901e+00, 5.0854e+00, 7.6608e-01, 8.4262e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8598, 0.8009, 0.4995, 0.8768, 0.6560, 0.7195, 0.8459, 0.5261, 0.7551,\n",
      "        0.6407, 0.6899, 0.2211, 0.7722, 0.4493, 0.6231, 0.7071, 0.4431, 0.8590,\n",
      "        0.7178, 0.4213, 0.8600, 0.3637, 0.4716, 0.4564, 0.6572, 0.6051, 0.4638,\n",
      "        0.4421, 0.4645, 0.1834, 0.2337, 0.0488, 0.0591, 0.0661, 0.0517],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.6085e+05, 3.9243e+06, 1.7269e+05, 6.2243e+05, 3.2447e+05, 2.6119e+05,\n",
      "        7.1962e+06, 5.3345e+06, 2.1564e+06, 8.1439e+04, 3.6575e+05, 6.0646e+06,\n",
      "        1.0383e+05, 6.5662e+06, 1.8877e+05, 9.5216e+05, 5.3883e+04, 4.0956e+06,\n",
      "        3.3460e+05, 1.5771e+07, 2.7299e+05, 2.3650e+06, 2.7752e+05, 4.2728e+06,\n",
      "        1.1076e+06, 3.7851e+06, 2.0483e+05, 3.0286e+06, 1.5046e+06, 3.8867e+05,\n",
      "        3.6579e+05, 2.0890e+01, 1.9140e+01, 2.8619e+00, 3.1963e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.6275e+06, 2.7597e+05, 5.5639e+06, 3.2748e+05, 8.2816e+04, 2.1908e+07,\n",
      "        3.2290e+06, 8.7076e+05, 5.9825e+04, 3.1632e+05, 7.0985e-02, 7.1951e+01,\n",
      "        2.1514e+00, 6.8649e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5697, 0.4339, 0.8467, 0.1991, 0.7034, 0.4281, 0.2223, 0.3182, 0.1948,\n",
      "        0.3411, 0.0181, 0.0453, 0.0562, 0.0838], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.1406e+07, 6.2488e+05, 3.4124e+06, 1.0492e+06, 9.8264e+04, 5.0115e+07,\n",
      "        1.0044e+07, 2.3748e+06, 1.9269e+05, 8.3368e+05, 2.7880e-01, 2.7475e+02,\n",
      "        8.1221e+00, 2.5160e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.7563e+07, 4.5753e+07, 7.2312e+06, 3.2505e+06, 7.8125e+06, 3.2216e+05,\n",
      "        3.5598e+06, 3.1927e+06, 2.8856e+06, 3.3990e+07, 8.6094e+05, 7.8771e+06,\n",
      "        3.2314e+05, 3.0080e+05, 5.8436e+05, 1.0608e+06, 1.6886e+05, 6.9171e+01,\n",
      "        1.5033e+03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8596, 0.7593, 0.7460, 0.6433, 0.6476, 0.6253, 0.5170, 0.7730, 0.8439,\n",
      "        0.5017, 0.5850, 0.8078, 0.4268, 0.5203, 0.5014, 0.2726, 0.4377, 0.1145,\n",
      "        0.1185], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.5476e+07, 4.4050e+07, 7.3478e+06, 4.6377e+06, 1.1013e+07, 4.8287e+05,\n",
      "        6.8780e+06, 2.8989e+06, 1.8023e+06, 6.7743e+07, 1.4291e+06, 6.0560e+06,\n",
      "        7.4085e+05, 5.7721e+05, 1.1654e+06, 3.0867e+06, 3.7980e+05, 2.4500e+02,\n",
      "        5.3005e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.4170e+06, 7.6536e+06, 3.3114e+06, 5.2743e+06, 1.5928e+07, 1.1357e+06,\n",
      "        4.1672e+06, 1.1791e+06, 9.1423e+04, 2.3242e+05, 1.4218e+06, 3.0320e+04,\n",
      "        1.5381e+05, 3.6492e+01, 1.7045e+01, 2.7046e+03, 2.8301e+01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1704, 0.5647, 0.4059, 0.7693, 0.4927, 0.2826, 0.5037, 0.2793, 0.1488,\n",
      "        0.2371, 0.4368, 0.1228, 0.0988, 0.0276, 0.0343, 0.0873, 0.1234],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.7018e+06, 1.3326e+07, 7.8695e+06, 4.8661e+06, 3.2320e+07, 3.2589e+06,\n",
      "        8.2729e+06, 3.3992e+06, 3.1127e+05, 7.0923e+05, 3.2029e+06, 1.0639e+05,\n",
      "        5.5447e+05, 1.4194e+02, 6.5840e+01, 9.8740e+03, 9.9235e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.8021e+08, 2.9560e+06, 9.4135e+05, 5.6742e+07, 1.0726e+07, 6.7876e+05,\n",
      "        1.4803e+07, 5.7492e+06, 7.7683e+05, 1.1316e+05, 1.4507e+06, 2.2172e+06,\n",
      "        1.1101e+05, 2.3565e+05, 6.2544e+05, 1.7847e+06, 1.1955e+06, 3.5315e+06,\n",
      "        7.9200e+04, 1.1578e+05, 4.8263e+05, 1.1067e+05, 5.9283e+04, 3.1841e+04,\n",
      "        8.6056e+04, 3.9538e+04, 9.4877e+03, 2.2246e+04, 4.2787e+02, 4.2255e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9833, 0.5191, 0.4995, 0.9653, 0.5039, 0.3206, 0.9878, 0.8708, 0.5583,\n",
      "        0.4529, 0.6394, 0.7151, 0.2261, 0.7874, 0.3814, 0.3735, 0.4334, 0.4573,\n",
      "        0.3725, 0.7811, 0.0816, 0.2395, 0.1867, 0.3937, 0.3099, 0.1443, 0.0908,\n",
      "        0.0463, 0.2834, 0.2857], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2038e+07, 5.6861e+06, 1.8846e+06, 7.8704e+06, 2.1283e+07, 1.8447e+06,\n",
      "        7.1996e+05, 2.9714e+06, 1.3724e+06, 2.4764e+05, 2.0926e+06, 2.5265e+06,\n",
      "        3.4364e+05, 2.0043e+05, 1.5475e+06, 4.4722e+06, 2.7095e+06, 7.6659e+06,\n",
      "        1.9878e+05, 1.0136e+05, 1.7730e+06, 3.3667e+05, 1.9287e+05, 7.7220e+04,\n",
      "        2.3754e+05, 1.3533e+05, 3.4503e+04, 8.4860e+04, 1.2265e+03, 1.2073e+03],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.8807e+06, 5.7209e+05, 1.8337e+07, 1.2226e+07, 3.1049e+06, 3.1576e+07,\n",
      "        6.4152e+05, 6.8074e+05, 4.6540e+05, 3.6680e+05, 3.1864e+06, 1.8942e+05,\n",
      "        8.7566e+05, 1.2701e+06, 8.1254e+04, 1.6095e+05, 3.4199e+05, 6.9528e+05,\n",
      "        4.3299e+05, 7.1761e+04, 5.9105e+04, 1.2837e+05, 1.7597e+02, 3.0473e+02,\n",
      "        5.0799e+01, 2.0218e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8653, 0.3920, 0.8072, 0.6402, 0.4312, 0.9146, 0.6245, 0.3344, 0.3275,\n",
      "        0.1125, 0.2794, 0.2934, 0.4369, 0.2991, 0.1200, 0.1837, 0.2317, 0.4493,\n",
      "        0.1671, 0.0887, 0.2402, 0.3554, 0.0473, 0.1079, 0.1773, 0.1083],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.6300e+06, 1.3914e+06, 1.4145e+07, 1.7598e+07, 7.0638e+06, 1.0788e+07,\n",
      "        9.6345e+05, 1.8124e+06, 1.2520e+06, 1.3022e+06, 9.1840e+06, 5.3541e+05,\n",
      "        1.9723e+06, 3.5608e+06, 2.8602e+05, 5.2553e+05, 1.0511e+06, 1.5315e+06,\n",
      "        1.4426e+06, 2.6158e+05, 1.7962e+05, 3.3100e+05, 6.7062e+02, 1.0874e+03,\n",
      "        1.6717e+02, 7.2113e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([9.4215e+04, 1.3129e+05, 3.7287e+04, 1.8668e+05, 8.4299e+04, 4.4510e+04,\n",
      "        2.4346e+04, 8.7049e+00, 1.0712e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1151, 0.1765, 0.2619, 0.2443, 0.1230, 0.3212, 0.1392, 0.0296, 0.0688],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.3347e+05, 4.3248e+05, 1.1009e+05, 5.6432e+05, 2.9571e+05, 1.2086e+05,\n",
      "        8.3825e+04, 3.3788e+01, 3.9900e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.4734e+07, 3.3951e+06, 2.0361e+08, 1.9354e+06, 1.3744e+06, 1.1943e+07,\n",
      "        1.0738e+07, 7.4181e+05, 1.4002e+07, 2.3757e+06, 1.8731e+05, 8.6325e+06,\n",
      "        1.2436e+05, 1.5249e+06, 1.9884e+05, 2.4627e+05, 3.8387e+05, 7.4081e+04,\n",
      "        1.4563e+03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8396, 0.9440, 0.7000, 0.5110, 0.9633, 0.9760, 0.7767, 0.5525, 0.8411,\n",
      "        0.2959, 0.1752, 0.6273, 0.3304, 0.5161, 0.2360, 0.1387, 0.1041, 0.2099,\n",
      "        0.1771], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.5868e+07, 7.5985e+05, 2.4436e+08, 3.7853e+06, 2.0180e+05, 1.1484e+06,\n",
      "        9.5927e+06, 1.3277e+06, 8.9001e+06, 6.6912e+06, 6.1799e+05, 1.2870e+07,\n",
      "        3.3310e+05, 2.9516e+06, 6.0767e+05, 8.4841e+05, 1.3756e+06, 2.3412e+05,\n",
      "        4.7937e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([67439.4844, 16225.4346, 68135.6172,   293.6839,  1185.2806,   184.7848],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1934, 0.1983, 0.1929, 0.0919, 0.0574, 0.0527], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([217592.0156,  52028.6914, 219968.4531,   1066.7711,   4468.9302,\n",
      "           700.1923], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.0176e+05, 8.3621e+05, 6.4814e+06, 2.5281e+04, 5.4535e+06, 1.8734e+04,\n",
      "        3.1517e+06, 1.0164e+06, 1.1372e+05, 9.5683e-02, 7.2399e-02, 1.4883e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3436, 0.5875, 0.6012, 0.4929, 0.4306, 0.3788, 0.3839, 0.3057, 0.1960,\n",
      "        0.0282, 0.0316, 0.0742], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.2978e+05, 1.3798e+06, 1.0338e+07, 5.1278e+04, 1.2422e+07, 4.6553e+04,\n",
      "        7.7677e+06, 2.8227e+06, 3.6572e+05, 3.7193e-01, 2.8045e-01, 5.5114e+02],\n",
      "       device='cuda:0')\n",
      "All_sigs tensor([2.7216e+05, 1.7278e+07, 6.5656e+05, 4.0498e+06, 4.0910e+06, 1.2019e+06,\n",
      "        7.7665e+04, 3.6073e+06, 5.5451e+05, 3.0420e+06, 8.6936e+05, 2.9748e+05,\n",
      "        3.3078e+05, 3.4907e+06, 6.6146e+05, 3.3546e+05, 8.4435e+06, 1.9993e+06,\n",
      "        4.9548e+06, 1.2789e+07, 2.0769e+06, 5.1752e+06, 5.5396e+05, 3.2561e+05,\n",
      "        2.5701e+06, 7.5211e+05, 3.5979e+05, 7.3575e+04, 3.3354e+05, 1.2704e+06,\n",
      "        8.7658e+05, 2.7481e+06, 3.4603e+01, 2.6085e+05, 3.9243e+06, 1.7269e+05,\n",
      "        6.2243e+05, 3.2447e+05, 2.6119e+05, 7.1962e+06, 5.3345e+06, 2.1564e+06,\n",
      "        8.1439e+04, 3.6575e+05, 6.0646e+06, 1.0383e+05, 6.5662e+06, 1.8877e+05,\n",
      "        9.5216e+05, 5.3883e+04, 4.0956e+06, 3.3460e+05, 1.5771e+07, 2.7299e+05,\n",
      "        2.3650e+06, 2.7752e+05, 4.2728e+06, 1.1076e+06, 3.7851e+06, 2.0483e+05,\n",
      "        3.0286e+06, 1.5046e+06, 3.8867e+05, 3.6579e+05, 2.0890e+01, 1.9140e+01,\n",
      "        2.8619e+00, 3.1963e+00, 1.1406e+07, 6.2488e+05, 3.4124e+06, 1.0492e+06,\n",
      "        9.8264e+04, 5.0115e+07, 1.0044e+07, 2.3748e+06, 1.9269e+05, 8.3368e+05,\n",
      "        2.7880e-01, 2.7475e+02, 8.1221e+00, 2.5160e+00, 1.5476e+07, 4.4050e+07,\n",
      "        7.3478e+06, 4.6377e+06, 1.1013e+07, 4.8287e+05, 6.8780e+06, 2.8989e+06,\n",
      "        1.8023e+06, 6.7743e+07, 1.4291e+06, 6.0560e+06, 7.4085e+05, 5.7721e+05,\n",
      "        1.1654e+06, 3.0867e+06, 3.7980e+05, 2.4500e+02, 5.3005e+03, 4.7018e+06,\n",
      "        1.3326e+07, 7.8695e+06, 4.8661e+06, 3.2320e+07, 3.2589e+06, 8.2729e+06,\n",
      "        3.3992e+06, 3.1127e+05, 7.0923e+05, 3.2029e+06, 1.0639e+05, 5.5447e+05,\n",
      "        1.4194e+02, 6.5840e+01, 9.8740e+03, 9.9235e+01, 1.2038e+07, 5.6861e+06,\n",
      "        1.8846e+06, 7.8704e+06, 2.1283e+07, 1.8447e+06, 7.1996e+05, 2.9714e+06,\n",
      "        1.3724e+06, 2.4764e+05, 2.0926e+06, 2.5265e+06, 3.4364e+05, 2.0043e+05,\n",
      "        1.5475e+06, 4.4722e+06, 2.7095e+06, 7.6659e+06, 1.9878e+05, 1.0136e+05,\n",
      "        1.7730e+06, 3.3667e+05, 1.9287e+05, 7.7220e+04, 2.3754e+05, 1.3533e+05,\n",
      "        3.4503e+04, 8.4860e+04, 1.2265e+03, 1.2073e+03, 2.6300e+06, 1.3914e+06,\n",
      "        1.4145e+07, 1.7598e+07, 7.0638e+06, 1.0788e+07, 9.6345e+05, 1.8124e+06,\n",
      "        1.2520e+06, 1.3022e+06, 9.1840e+06, 5.3541e+05, 1.9723e+06, 3.5608e+06,\n",
      "        2.8602e+05, 5.2553e+05, 1.0511e+06, 1.5315e+06, 1.4426e+06, 2.6158e+05,\n",
      "        1.7962e+05, 3.3100e+05, 6.7062e+02, 1.0874e+03, 1.6717e+02, 7.2113e+02,\n",
      "        3.3347e+05, 4.3248e+05, 1.1009e+05, 5.6432e+05, 2.9571e+05, 1.2086e+05,\n",
      "        8.3825e+04, 3.3788e+01, 3.9900e+02, 1.5868e+07, 7.5985e+05, 2.4436e+08,\n",
      "        3.7853e+06, 2.0180e+05, 1.1484e+06, 9.5927e+06, 1.3277e+06, 8.9001e+06,\n",
      "        6.6912e+06, 6.1799e+05, 1.2870e+07, 3.3310e+05, 2.9516e+06, 6.0767e+05,\n",
      "        8.4841e+05, 1.3756e+06, 2.3412e+05, 4.7937e+03, 2.1759e+05, 5.2029e+04,\n",
      "        2.1997e+05, 1.0668e+03, 4.4689e+03, 7.0019e+02, 5.2978e+05, 1.3798e+06,\n",
      "        1.0338e+07, 5.1278e+04, 1.2422e+07, 4.6553e+04, 7.7677e+06, 2.8227e+06,\n",
      "        3.6572e+05, 3.7193e-01, 2.8045e-01, 5.5114e+02], device='cuda:0')\n",
      "All_sig tensor([5.9372e-02, 3.7693e+00, 1.4323e-01, 8.8349e-01, 8.9246e-01, 2.6220e-01,\n",
      "        1.6943e-02, 7.8696e-01, 1.2097e-01, 6.6363e-01, 1.8966e-01, 6.4897e-02,\n",
      "        7.2161e-02, 7.6152e-01, 1.4430e-01, 7.3182e-02, 1.8420e+00, 4.3615e-01,\n",
      "        1.0809e+00, 2.7900e+00, 4.5308e-01, 1.1290e+00, 1.2085e-01, 7.1033e-02,\n",
      "        5.6069e-01, 1.6408e-01, 7.8491e-02, 1.6051e-02, 7.2763e-02, 2.7713e-01,\n",
      "        1.9123e-01, 5.9952e-01, 7.5489e-06, 5.6906e-02, 8.5611e-01, 3.7674e-02,\n",
      "        1.3579e-01, 7.0784e-02, 5.6980e-02, 1.5699e+00, 1.1637e+00, 4.7043e-01,\n",
      "        1.7766e-02, 7.9789e-02, 1.3230e+00, 2.2651e-02, 1.4325e+00, 4.1181e-02,\n",
      "        2.0772e-01, 1.1755e-02, 8.9348e-01, 7.2995e-02, 3.4405e+00, 5.9554e-02,\n",
      "        5.1594e-01, 6.0543e-02, 9.3214e-01, 2.4164e-01, 8.2574e-01, 4.4686e-02,\n",
      "        6.6070e-01, 3.2823e-01, 8.4790e-02, 7.9800e-02, 4.5572e-06, 4.1754e-06,\n",
      "        6.2433e-07, 6.9728e-07, 2.4883e+00, 1.3632e-01, 7.4443e-01, 2.2888e-01,\n",
      "        2.1437e-02, 1.0933e+01, 2.1912e+00, 5.1808e-01, 4.2036e-02, 1.8187e-01,\n",
      "        6.0822e-08, 5.9938e-05, 1.7719e-06, 5.4887e-07, 3.3762e+00, 9.6096e+00,\n",
      "        1.6030e+00, 1.0117e+00, 2.4026e+00, 1.0534e-01, 1.5005e+00, 6.3240e-01,\n",
      "        3.9318e-01, 1.4778e+01, 3.1177e-01, 1.3212e+00, 1.6162e-01, 1.2592e-01,\n",
      "        2.5424e-01, 6.7338e-01, 8.2855e-02, 5.3449e-05, 1.1563e-03, 1.0257e+00,\n",
      "        2.9072e+00, 1.7168e+00, 1.0616e+00, 7.0507e+00, 7.1095e-01, 1.8048e+00,\n",
      "        7.4155e-01, 6.7905e-02, 1.5472e-01, 6.9873e-01, 2.3210e-02, 1.2096e-01,\n",
      "        3.0966e-05, 1.4363e-05, 2.1541e-03, 2.1649e-05, 2.6261e+00, 1.2405e+00,\n",
      "        4.1113e-01, 1.7170e+00, 4.6431e+00, 4.0243e-01, 1.5706e-01, 6.4824e-01,\n",
      "        2.9939e-01, 5.4024e-02, 4.5650e-01, 5.5118e-01, 7.4966e-02, 4.3724e-02,\n",
      "        3.3759e-01, 9.7562e-01, 5.9109e-01, 1.6724e+00, 4.3366e-02, 2.2112e-02,\n",
      "        3.8678e-01, 7.3447e-02, 4.2076e-02, 1.6846e-02, 5.1820e-02, 2.9522e-02,\n",
      "        7.5270e-03, 1.8513e-02, 2.6756e-04, 2.6337e-04, 5.7376e-01, 3.0355e-01,\n",
      "        3.0857e+00, 3.8392e+00, 1.5410e+00, 2.3534e+00, 2.1018e-01, 3.9539e-01,\n",
      "        2.7313e-01, 2.8408e-01, 2.0035e+00, 1.1680e-01, 4.3027e-01, 7.7681e-01,\n",
      "        6.2396e-02, 1.1465e-01, 2.2930e-01, 3.3411e-01, 3.1472e-01, 5.7066e-02,\n",
      "        3.9186e-02, 7.2210e-02, 1.4630e-04, 2.3722e-04, 3.6469e-05, 1.5732e-04,\n",
      "        7.2748e-02, 9.4347e-02, 2.4017e-02, 1.2311e-01, 6.4512e-02, 2.6366e-02,\n",
      "        1.8287e-02, 7.3710e-06, 8.7045e-05, 3.4616e+00, 1.6577e-01, 5.3309e+01,\n",
      "        8.2578e-01, 4.4025e-02, 2.5054e-01, 2.0927e+00, 2.8965e-01, 1.9416e+00,\n",
      "        1.4597e+00, 1.3482e-01, 2.8076e+00, 7.2668e-02, 6.4391e-01, 1.3257e-01,\n",
      "        1.8508e-01, 3.0011e-01, 5.1075e-02, 1.0458e-03, 4.7469e-02, 1.1350e-02,\n",
      "        4.7987e-02, 2.3272e-04, 9.7492e-04, 1.5275e-04, 1.1557e-01, 3.0102e-01,\n",
      "        2.2553e+00, 1.1187e-02, 2.7098e+00, 1.0156e-02, 1.6946e+00, 6.1578e-01,\n",
      "        7.9783e-02, 8.1138e-08, 6.1182e-08, 1.2023e-04], device='cuda:0')\n",
      "Sig sum tensor(220.0000, device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 4583.889163636364\n",
      "pruning 27 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :10 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     31\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    11\n",
      "     ║    ╠════╝\n",
      "     ║    24\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    23\n",
      "    ╠════╗\n",
      "    ║    17\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   24\n",
      "   ╠════╗\n",
      "   ║    11\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  35\n",
      "  ╠════╗\n",
      "  ║    10\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 34\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.7727e+05, 1.2784e+07, 9.7409e+05, 1.3987e+06, 1.3215e+06, 5.7989e+05,\n",
      "        3.6290e+04, 2.1286e+06, 2.9007e+05, 5.3481e+06, 2.8419e+05, 1.1407e+05,\n",
      "        4.3225e+05, 1.8437e+06, 3.5591e+05, 1.0500e+05, 3.7954e+06, 4.2198e+05,\n",
      "        9.7622e+06, 9.0182e+06, 1.4124e+06, 6.0409e+06, 3.7819e+05, 1.6520e+05,\n",
      "        1.1717e+06, 3.4082e+05, 1.6701e+05, 6.5751e+04, 1.2823e+05, 5.9065e+05,\n",
      "        2.1102e+05, 8.8934e+05, 2.1044e-02, 6.1896e-03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.7676, 0.6442, 0.8224, 0.3825, 0.2204, 0.4647, 0.4604, 0.5253, 0.2940,\n",
      "        0.8738, 0.3292, 0.3594, 0.7743, 0.5084, 0.5393, 0.3042, 0.4656, 0.1841,\n",
      "        0.8625, 0.6291, 0.5770, 0.7783, 0.5882, 0.4904, 0.4791, 0.4449, 0.3738,\n",
      "        0.7102, 0.4633, 0.3932, 0.0923, 0.2010, 0.0355, 0.1200],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.5772e+05, 1.8195e+07, 6.9199e+05, 3.4551e+06, 4.1210e+06, 1.2418e+06,\n",
      "        7.8335e+04, 4.0419e+06, 8.1922e+05, 2.6997e+06, 7.6255e+05, 2.9229e+05,\n",
      "        3.9016e+05, 3.6256e+06, 6.5586e+05, 2.9225e+05, 8.1125e+06, 1.3771e+06,\n",
      "        5.3683e+06, 1.3379e+07, 2.3897e+06, 5.3579e+06, 6.2290e+05, 3.3676e+05,\n",
      "        2.4412e+06, 7.5681e+05, 4.1833e+05, 7.6209e+04, 2.7527e+05, 1.4336e+06,\n",
      "        7.6617e+05, 2.8423e+06, 8.1190e-02, 2.1788e-02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.8405e+05, 5.0730e+06, 7.2765e+04, 1.0407e+06, 2.1169e+05, 2.2025e+05,\n",
      "        1.2720e+07, 2.2422e+06, 2.6740e+06, 4.8743e+04, 2.9405e+05, 1.4619e+06,\n",
      "        1.1932e+05, 2.4233e+06, 9.8964e+04, 7.7970e+05, 2.1668e+04, 7.7426e+06,\n",
      "        3.0376e+05, 5.5881e+06, 4.3304e+05, 8.0597e+05, 1.4335e+05, 1.9361e+06,\n",
      "        8.5529e+05, 1.8496e+06, 6.9395e+04, 1.3847e+06, 7.5269e+05, 1.3501e+05,\n",
      "        1.8863e+05, 6.1849e-01, 1.7293e+00, 7.0237e-02, 4.9552e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8548, 0.8013, 0.4889, 0.8506, 0.6195, 0.7125, 0.8663, 0.5158, 0.7652,\n",
      "        0.6392, 0.6956, 0.2103, 0.7791, 0.4227, 0.6227, 0.7089, 0.4676, 0.8867,\n",
      "        0.7161, 0.3774, 0.8611, 0.3463, 0.4710, 0.4516, 0.6614, 0.5675, 0.4499,\n",
      "        0.4523, 0.4891, 0.1793, 0.2513, 0.0391, 0.0479, 0.0381, 0.0279],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.8104e+05, 4.0328e+06, 1.4877e+05, 6.2190e+05, 3.2217e+05, 2.5332e+05,\n",
      "        6.7999e+06, 4.3426e+06, 2.5117e+06, 7.0337e+04, 3.5801e+05, 4.6176e+06,\n",
      "        1.0543e+05, 5.5962e+06, 1.4934e+05, 9.0804e+05, 4.6141e+04, 3.5096e+06,\n",
      "        3.4499e+05, 1.3917e+07, 2.4054e+05, 2.1073e+06, 3.0331e+05, 4.2471e+06,\n",
      "        1.1583e+06, 3.2001e+06, 1.5269e+05, 3.0335e+06, 1.5383e+06, 4.4319e+05,\n",
      "        5.6488e+05, 2.3772e+00, 6.5862e+00, 2.7026e-01, 1.9268e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.5099e+06, 2.3817e+05, 5.6300e+06, 3.3055e+05, 1.0380e+05, 2.0772e+07,\n",
      "        3.1325e+06, 7.2460e+05, 8.8392e+04, 3.7394e+05, 2.2975e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5829, 0.4317, 0.8553, 0.1834, 0.7033, 0.4234, 0.2079, 0.3063, 0.2008,\n",
      "        0.3270, 0.0321], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.0861e+07, 5.4141e+05, 3.2583e+06, 1.0797e+06, 1.2318e+05, 4.7909e+07,\n",
      "        9.9256e+06, 2.0107e+06, 2.8256e+05, 1.0066e+06, 8.8948e-01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.9459e+07, 4.3389e+07, 9.3787e+06, 3.1988e+06, 6.8598e+06, 2.1396e+05,\n",
      "        3.0222e+06, 3.4086e+06, 3.2680e+06, 3.0697e+07, 5.2230e+05, 7.9956e+06,\n",
      "        4.0430e+05, 3.0409e+05, 8.0948e+05, 1.9153e+06, 1.7258e+05, 6.7275e+03,\n",
      "        3.4240e-01, 3.9910e-01, 8.6335e-01, 7.5929e+00, 4.5667e+00, 7.9146e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8613, 0.7640, 0.7656, 0.6379, 0.6152, 0.6285, 0.4928, 0.8038, 0.8483,\n",
      "        0.4963, 0.5853, 0.8291, 0.4460, 0.5200, 0.5312, 0.3229, 0.4240, 0.1641,\n",
      "        0.1076, 0.1532, 0.0184, 0.1141, 0.1637, 0.0412], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.6343e+07, 4.0966e+07, 8.7940e+06, 4.6332e+06, 1.0558e+07, 3.1798e+05,\n",
      "        6.1319e+06, 2.6748e+06, 1.9827e+06, 6.1843e+07, 8.6648e+05, 5.4653e+06,\n",
      "        8.9591e+05, 5.8387e+05, 1.5178e+06, 5.1874e+06, 3.9763e+05, 2.2494e+04,\n",
      "        1.2223e+00, 1.3519e+00, 3.3898e+00, 2.6906e+01, 1.5276e+01, 3.0355e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.5679e+06, 4.0940e+06, 3.3858e+06, 5.4029e+06, 1.6051e+07, 1.3624e+06,\n",
      "        4.0194e+06, 1.7103e+06, 1.2142e+05, 2.4829e+05, 1.7713e+06, 4.1208e+04,\n",
      "        2.3321e+05, 1.2500e+04, 3.1902e+01, 4.2534e-01, 3.8979e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1781, 0.5564, 0.4043, 0.7767, 0.4877, 0.2893, 0.4821, 0.2984, 0.1620,\n",
      "        0.2473, 0.4274, 0.1412, 0.1003, 0.1064, 0.0873, 0.1412, 0.0111],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.1546e+06, 7.2638e+06, 8.0680e+06, 4.8260e+06, 3.2891e+07, 3.8731e+06,\n",
      "        8.3262e+06, 4.7999e+06, 4.0701e+05, 7.4755e+05, 4.0570e+06, 1.4155e+05,\n",
      "        8.3932e+05, 4.4677e+04, 1.1647e+02, 1.4611e+00, 1.5418e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.5873e+08, 2.9173e+06, 9.8982e+05, 4.8737e+07, 9.3979e+06, 6.1456e+05,\n",
      "        1.9479e+07, 4.3752e+06, 8.4732e+05, 1.3144e+05, 1.5427e+06, 2.0412e+06,\n",
      "        1.1065e+05, 2.3625e+05, 7.1237e+05, 1.5956e+06, 1.4470e+06, 5.4001e+06,\n",
      "        8.2330e+04, 1.0888e+05, 4.6752e+05, 9.4549e+04, 6.2968e+04, 3.1766e+04,\n",
      "        1.2993e+05, 5.9035e+04, 1.5065e+04, 3.2236e+04, 5.4134e+01, 6.2208e+02,\n",
      "        1.1347e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9817, 0.5197, 0.4995, 0.9640, 0.5043, 0.3282, 0.9875, 0.8709, 0.5600,\n",
      "        0.4529, 0.6382, 0.7238, 0.2297, 0.7854, 0.3823, 0.3691, 0.4350, 0.4584,\n",
      "        0.3688, 0.7808, 0.0816, 0.2319, 0.1904, 0.3877, 0.3094, 0.1480, 0.0912,\n",
      "        0.0502, 0.3872, 0.4644, 0.0338], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.1593e+07, 5.6049e+06, 1.9817e+06, 7.0213e+06, 1.8636e+07, 1.6515e+06,\n",
      "        9.7546e+05, 2.2595e+06, 1.4912e+06, 2.8764e+05, 2.2324e+06, 2.2553e+06,\n",
      "        3.4094e+05, 2.0281e+05, 1.7603e+06, 4.0267e+06, 3.2701e+06, 1.1699e+07,\n",
      "        2.0785e+05, 9.5487e+04, 1.7174e+06, 2.9050e+05, 2.0391e+05, 7.7802e+04,\n",
      "        3.5889e+05, 2.0119e+05, 5.4766e+04, 1.2247e+05, 1.3270e+02, 1.3328e+03,\n",
      "        4.3854e+01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([5.1338e+06, 6.5721e+05, 1.8522e+07, 9.1999e+06, 3.5686e+06, 2.4824e+07,\n",
      "        5.6419e+05, 4.3625e+05, 4.5733e+05, 4.2182e+05, 2.9120e+06, 2.3788e+05,\n",
      "        7.2627e+05, 1.3338e+06, 8.2258e+04, 1.9001e+05, 3.6111e+05, 8.4653e+05,\n",
      "        4.0043e+05, 8.5260e+04, 6.0091e+04, 2.2317e+05, 9.3124e+00, 1.7534e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8690, 0.3963, 0.8003, 0.5656, 0.4274, 0.9046, 0.6227, 0.3425, 0.3515,\n",
      "        0.1287, 0.2780, 0.3095, 0.4089, 0.3212, 0.1257, 0.1907, 0.2271, 0.4268,\n",
      "        0.1761, 0.0950, 0.2462, 0.3457, 0.0442, 0.0544], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.6903e+06, 1.5870e+06, 1.4798e+07, 1.5987e+07, 8.1734e+06, 9.4754e+06,\n",
      "        8.5149e+05, 1.1473e+06, 1.1863e+06, 1.4702e+06, 8.4092e+06, 6.5697e+05,\n",
      "        1.7173e+06, 3.6216e+06, 2.8766e+05, 6.1514e+05, 1.1164e+06, 1.9409e+06,\n",
      "        1.3197e+06, 3.0865e+05, 1.8119e+05, 5.8409e+05, 3.5602e+01, 6.6321e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([7.4461e+04, 1.7678e+05, 7.3118e+04, 2.1027e+05, 1.1579e+05, 8.4183e+04,\n",
      "        4.3918e+04, 1.0133e+00, 3.4898e+00, 4.1280e+00, 8.0044e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1079, 0.1844, 0.2860, 0.2429, 0.1280, 0.3443, 0.1442, 0.0353, 0.0606,\n",
      "        0.0315, 0.0319], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.6572e+05, 5.7671e+05, 2.0881e+05, 6.3679e+05, 4.0390e+05, 2.2079e+05,\n",
      "        1.5034e+05, 3.9101e+00, 1.3114e+01, 1.5992e+01, 3.0995e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.9618e+07, 3.2750e+06, 1.9269e+08, 1.8777e+06, 1.5726e+06, 1.3330e+07,\n",
      "        9.6295e+06, 9.3029e+05, 1.3190e+07, 2.3141e+06, 3.8693e+05, 8.0757e+06,\n",
      "        1.5710e+05, 1.6028e+06, 2.8862e+05, 3.3113e+05, 4.6012e+05, 8.9895e+04,\n",
      "        6.5747e+03, 7.1350e+00, 1.8034e+00, 1.9080e+00, 2.0438e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8508, 0.9428, 0.7106, 0.5076, 0.9650, 0.9733, 0.7764, 0.5798, 0.8272,\n",
      "        0.3011, 0.2155, 0.6090, 0.3734, 0.5186, 0.2508, 0.1542, 0.1346, 0.2051,\n",
      "        0.2365, 0.0689, 0.0603, 0.0969, 0.0745], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.1705e+07, 7.4953e+05, 2.2304e+08, 3.6984e+06, 2.2019e+05, 1.4260e+06,\n",
      "        8.6142e+06, 1.5636e+06, 9.1160e+06, 6.4693e+06, 1.2143e+06, 1.2632e+07,\n",
      "        3.9373e+05, 3.0863e+06, 8.6490e+05, 1.1203e+06, 1.5927e+06, 2.8582e+05,\n",
      "        2.0078e+04, 2.6574e+01, 6.7787e+00, 6.8926e+00, 7.5664e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([111294.4141,  28211.8809, 111978.7734], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2067, 0.2159, 0.2040], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([353162.8438,  88484.2109, 356527.7500], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.4696e+05, 8.3236e+05, 7.8589e+06, 2.2183e+04, 4.5863e+06, 2.0264e+04,\n",
      "        3.5812e+06, 9.9436e+05, 1.0674e+05, 5.4392e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3303, 0.5840, 0.6042, 0.4679, 0.4236, 0.4306, 0.3825, 0.3389, 0.2132,\n",
      "        0.0348], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.9368e+05, 1.3850e+06, 1.2443e+07, 4.7217e+04, 1.0575e+07, 4.6155e+04,\n",
      "        8.8460e+06, 2.6294e+06, 3.3593e+05, 2.0999e+00], device='cuda:0')\n",
      "All_sigs tensor([2.5772e+05, 1.8195e+07, 6.9199e+05, 3.4551e+06, 4.1210e+06, 1.2418e+06,\n",
      "        7.8335e+04, 4.0419e+06, 8.1922e+05, 2.6997e+06, 7.6255e+05, 2.9229e+05,\n",
      "        3.9016e+05, 3.6256e+06, 6.5586e+05, 2.9225e+05, 8.1125e+06, 1.3771e+06,\n",
      "        5.3683e+06, 1.3379e+07, 2.3897e+06, 5.3579e+06, 6.2290e+05, 3.3676e+05,\n",
      "        2.4412e+06, 7.5681e+05, 4.1833e+05, 7.6209e+04, 2.7527e+05, 1.4336e+06,\n",
      "        7.6617e+05, 2.8423e+06, 8.1190e-02, 2.1788e-02, 2.8104e+05, 4.0328e+06,\n",
      "        1.4877e+05, 6.2190e+05, 3.2217e+05, 2.5332e+05, 6.7999e+06, 4.3426e+06,\n",
      "        2.5117e+06, 7.0337e+04, 3.5801e+05, 4.6176e+06, 1.0543e+05, 5.5962e+06,\n",
      "        1.4934e+05, 9.0804e+05, 4.6141e+04, 3.5096e+06, 3.4499e+05, 1.3917e+07,\n",
      "        2.4054e+05, 2.1073e+06, 3.0331e+05, 4.2471e+06, 1.1583e+06, 3.2001e+06,\n",
      "        1.5269e+05, 3.0335e+06, 1.5383e+06, 4.4319e+05, 5.6488e+05, 2.3772e+00,\n",
      "        6.5862e+00, 2.7026e-01, 1.9268e+00, 1.0861e+07, 5.4141e+05, 3.2583e+06,\n",
      "        1.0797e+06, 1.2318e+05, 4.7909e+07, 9.9256e+06, 2.0107e+06, 2.8256e+05,\n",
      "        1.0066e+06, 8.8948e-01, 1.6343e+07, 4.0966e+07, 8.7940e+06, 4.6332e+06,\n",
      "        1.0558e+07, 3.1798e+05, 6.1319e+06, 2.6748e+06, 1.9827e+06, 6.1843e+07,\n",
      "        8.6648e+05, 5.4653e+06, 8.9591e+05, 5.8387e+05, 1.5178e+06, 5.1874e+06,\n",
      "        3.9763e+05, 2.2494e+04, 1.2223e+00, 1.3519e+00, 3.3898e+00, 2.6906e+01,\n",
      "        1.5276e+01, 3.0355e+01, 5.1546e+06, 7.2638e+06, 8.0680e+06, 4.8260e+06,\n",
      "        3.2891e+07, 3.8731e+06, 8.3262e+06, 4.7999e+06, 4.0701e+05, 7.4755e+05,\n",
      "        4.0570e+06, 1.4155e+05, 8.3932e+05, 4.4677e+04, 1.1647e+02, 1.4611e+00,\n",
      "        1.5418e+00, 1.1593e+07, 5.6049e+06, 1.9817e+06, 7.0213e+06, 1.8636e+07,\n",
      "        1.6515e+06, 9.7546e+05, 2.2595e+06, 1.4912e+06, 2.8764e+05, 2.2324e+06,\n",
      "        2.2553e+06, 3.4094e+05, 2.0281e+05, 1.7603e+06, 4.0267e+06, 3.2701e+06,\n",
      "        1.1699e+07, 2.0785e+05, 9.5487e+04, 1.7174e+06, 2.9050e+05, 2.0391e+05,\n",
      "        7.7802e+04, 3.5889e+05, 2.0119e+05, 5.4766e+04, 1.2247e+05, 1.3270e+02,\n",
      "        1.3328e+03, 4.3854e+01, 2.6903e+06, 1.5870e+06, 1.4798e+07, 1.5987e+07,\n",
      "        8.1734e+06, 9.4754e+06, 8.5149e+05, 1.1473e+06, 1.1863e+06, 1.4702e+06,\n",
      "        8.4092e+06, 6.5697e+05, 1.7173e+06, 3.6216e+06, 2.8766e+05, 6.1514e+05,\n",
      "        1.1164e+06, 1.9409e+06, 1.3197e+06, 3.0865e+05, 1.8119e+05, 5.8409e+05,\n",
      "        3.5602e+01, 6.6321e+00, 2.6572e+05, 5.7671e+05, 2.0881e+05, 6.3679e+05,\n",
      "        4.0390e+05, 2.2079e+05, 1.5034e+05, 3.9101e+00, 1.3114e+01, 1.5992e+01,\n",
      "        3.0995e+00, 1.1705e+07, 7.4953e+05, 2.2304e+08, 3.6984e+06, 2.2019e+05,\n",
      "        1.4260e+06, 8.6142e+06, 1.5636e+06, 9.1160e+06, 6.4693e+06, 1.2143e+06,\n",
      "        1.2632e+07, 3.9373e+05, 3.0863e+06, 8.6490e+05, 1.1203e+06, 1.5927e+06,\n",
      "        2.8582e+05, 2.0078e+04, 2.6574e+01, 6.7787e+00, 6.8926e+00, 7.5664e+00,\n",
      "        3.5316e+05, 8.8484e+04, 3.5653e+05, 3.9368e+05, 1.3850e+06, 1.2443e+07,\n",
      "        4.7217e+04, 1.0575e+07, 4.6155e+04, 8.8460e+06, 2.6294e+06, 3.3593e+05,\n",
      "        2.0999e+00], device='cuda:0')\n",
      "All_sig tensor([5.9393e-02, 4.1932e+00, 1.5947e-01, 7.9625e-01, 9.4971e-01, 2.8617e-01,\n",
      "        1.8053e-02, 9.3148e-01, 1.8880e-01, 6.2217e-01, 1.7574e-01, 6.7361e-02,\n",
      "        8.9916e-02, 8.3554e-01, 1.5115e-01, 6.7350e-02, 1.8696e+00, 3.1737e-01,\n",
      "        1.2372e+00, 3.0833e+00, 5.5071e-01, 1.2348e+00, 1.4355e-01, 7.7610e-02,\n",
      "        5.6259e-01, 1.7441e-01, 9.6408e-02, 1.7563e-02, 6.3439e-02, 3.3039e-01,\n",
      "        1.7657e-01, 6.5502e-01, 1.8711e-08, 5.0213e-09, 6.4769e-02, 9.2939e-01,\n",
      "        3.4286e-02, 1.4332e-01, 7.4246e-02, 5.8380e-02, 1.5671e+00, 1.0008e+00,\n",
      "        5.7884e-01, 1.6210e-02, 8.2506e-02, 1.0642e+00, 2.4297e-02, 1.2897e+00,\n",
      "        3.4417e-02, 2.0926e-01, 1.0634e-02, 8.0881e-01, 7.9506e-02, 3.2073e+00,\n",
      "        5.5434e-02, 4.8565e-01, 6.9900e-02, 9.7878e-01, 2.6694e-01, 7.3749e-01,\n",
      "        3.5188e-02, 6.9909e-01, 3.5452e-01, 1.0214e-01, 1.3018e-01, 5.4785e-07,\n",
      "        1.5178e-06, 6.2283e-08, 4.4405e-07, 2.5030e+00, 1.2477e-01, 7.5091e-01,\n",
      "        2.4882e-01, 2.8389e-02, 1.1041e+01, 2.2874e+00, 4.6338e-01, 6.5117e-02,\n",
      "        2.3198e-01, 2.0499e-07, 3.7664e+00, 9.4409e+00, 2.0266e+00, 1.0677e+00,\n",
      "        2.4332e+00, 7.3281e-02, 1.4131e+00, 6.1643e-01, 4.5693e-01, 1.4252e+01,\n",
      "        1.9969e-01, 1.2595e+00, 2.0647e-01, 1.3456e-01, 3.4979e-01, 1.1955e+00,\n",
      "        9.1637e-02, 5.1838e-03, 2.8169e-07, 3.1155e-07, 7.8121e-07, 6.2006e-06,\n",
      "        3.5206e-06, 6.9955e-06, 1.1879e+00, 1.6740e+00, 1.8593e+00, 1.1122e+00,\n",
      "        7.5799e+00, 8.9258e-01, 1.9188e+00, 1.1062e+00, 9.3799e-02, 1.7228e-01,\n",
      "        9.3497e-01, 3.2622e-02, 1.9343e-01, 1.0296e-02, 2.6842e-05, 3.3671e-07,\n",
      "        3.5533e-07, 2.6717e+00, 1.2917e+00, 4.5670e-01, 1.6181e+00, 4.2948e+00,\n",
      "        3.8060e-01, 2.2480e-01, 5.2072e-01, 3.4366e-01, 6.6288e-02, 5.1448e-01,\n",
      "        5.1976e-01, 7.8573e-02, 4.6740e-02, 4.0566e-01, 9.2797e-01, 7.5361e-01,\n",
      "        2.6961e+00, 4.7901e-02, 2.2006e-02, 3.9579e-01, 6.6948e-02, 4.6994e-02,\n",
      "        1.7930e-02, 8.2710e-02, 4.6366e-02, 1.2621e-02, 2.8224e-02, 3.0581e-05,\n",
      "        3.0715e-04, 1.0106e-05, 6.1999e-01, 3.6574e-01, 3.4104e+00, 3.6843e+00,\n",
      "        1.8836e+00, 2.1837e+00, 1.9623e-01, 2.6440e-01, 2.7339e-01, 3.3881e-01,\n",
      "        1.9380e+00, 1.5140e-01, 3.9576e-01, 8.3462e-01, 6.6294e-02, 1.4176e-01,\n",
      "        2.5727e-01, 4.4728e-01, 3.0415e-01, 7.1132e-02, 4.1756e-02, 1.3461e-01,\n",
      "        8.2046e-06, 1.5284e-06, 6.1237e-02, 1.3291e-01, 4.8122e-02, 1.4675e-01,\n",
      "        9.3081e-02, 5.0882e-02, 3.4647e-02, 9.0112e-07, 3.0221e-06, 3.6854e-06,\n",
      "        7.1430e-07, 2.6975e+00, 1.7273e-01, 5.1401e+01, 8.5232e-01, 5.0743e-02,\n",
      "        3.2864e-01, 1.9852e+00, 3.6034e-01, 2.1009e+00, 1.4909e+00, 2.7983e-01,\n",
      "        2.9110e+00, 9.0739e-02, 7.1125e-01, 1.9932e-01, 2.5817e-01, 3.6705e-01,\n",
      "        6.5869e-02, 4.6272e-03, 6.1241e-06, 1.5622e-06, 1.5884e-06, 1.7437e-06,\n",
      "        8.1389e-02, 2.0392e-02, 8.2164e-02, 9.0727e-02, 3.1918e-01, 2.8676e+00,\n",
      "        1.0881e-02, 2.4370e+00, 1.0637e-02, 2.0386e+00, 6.0597e-01, 7.7417e-02,\n",
      "        4.8395e-07], device='cuda:0')\n",
      "Sig sum tensor(223., device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 4339.201147982063\n",
      "pruning 30 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :11 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     34\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    10\n",
      "     ║    ╠════╝\n",
      "     ║    24\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    20\n",
      "    ╠════╗\n",
      "    ║    20\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   21\n",
      "   ╠════╗\n",
      "   ║    12\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  32\n",
      "  ╠════╗\n",
      "  ║    11\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 34\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.6433e+05, 1.1841e+07, 7.1530e+05, 1.2740e+06, 1.0817e+06, 5.0485e+05,\n",
      "        2.8808e+04, 1.6627e+06, 2.0101e+05, 6.2954e+06, 4.6303e+05, 7.9574e+04,\n",
      "        3.6570e+05, 1.6965e+06, 2.4131e+05, 1.0367e+05, 3.6468e+06, 6.6478e+05,\n",
      "        1.0992e+07, 6.8665e+06, 1.1518e+06, 5.1715e+06, 2.2869e+05, 1.5166e+05,\n",
      "        1.0777e+06, 3.3896e+05, 1.6426e+05, 7.1902e+04, 1.5865e+05, 4.3090e+05,\n",
      "        1.7823e+05, 6.4292e+05, 1.0351e-01, 3.4194e-03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.7686, 0.6412, 0.7954, 0.3617, 0.1953, 0.4547, 0.4201, 0.5039, 0.2555,\n",
      "        0.8880, 0.3835, 0.3195, 0.7314, 0.4998, 0.5126, 0.3225, 0.4892, 0.2216,\n",
      "        0.8839, 0.5971, 0.5409, 0.7520, 0.4935, 0.4807, 0.4610, 0.4259, 0.3419,\n",
      "        0.6905, 0.5314, 0.3518, 0.0820, 0.1540, 0.0314, 0.0235],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.4465e+05, 1.6992e+07, 5.8545e+05, 3.2528e+06, 3.4820e+06, 1.1012e+06,\n",
      "        6.6828e+04, 3.2996e+06, 5.9858e+05, 2.8203e+06, 1.1418e+06, 2.1659e+05,\n",
      "        3.9297e+05, 3.3943e+06, 4.7041e+05, 2.8094e+05, 7.4505e+06, 2.0699e+06,\n",
      "        5.1025e+06, 1.1067e+07, 2.1153e+06, 5.1301e+06, 4.6333e+05, 3.1504e+05,\n",
      "        2.3237e+06, 7.7842e+05, 4.3243e+05, 8.9009e+04, 2.9737e+05, 1.1172e+06,\n",
      "        6.5443e+05, 2.1757e+06, 4.0107e-01, 1.3356e-02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.8253e+05, 3.2864e+06, 8.3064e+04, 8.5242e+05, 3.2471e+05, 2.6794e+05,\n",
      "        9.3688e+06, 2.5329e+06, 1.7308e+06, 3.8653e+04, 2.7750e+05, 2.2112e+06,\n",
      "        8.8641e+04, 2.5478e+06, 9.4783e+04, 9.2929e+05, 1.7558e+04, 5.4186e+06,\n",
      "        2.4382e+05, 5.9563e+06, 5.0048e+05, 9.0088e+05, 1.5696e+05, 1.9204e+06,\n",
      "        1.2819e+06, 3.7334e+06, 1.2087e+05, 1.1531e+06, 5.8798e+05, 1.3628e+05,\n",
      "        1.4721e+05, 3.5600e-02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8603, 0.7365, 0.4689, 0.8261, 0.6742, 0.7253, 0.8300, 0.5159, 0.6960,\n",
      "        0.5692, 0.6882, 0.2142, 0.7797, 0.4099, 0.6127, 0.6935, 0.4512, 0.8365,\n",
      "        0.7268, 0.3835, 0.8600, 0.3425, 0.4645, 0.4562, 0.6714, 0.6217, 0.4316,\n",
      "        0.3848, 0.4468, 0.1811, 0.2086, 0.1112], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.6971e+05, 3.4636e+06, 1.7647e+05, 5.9296e+05, 4.2311e+05, 2.9443e+05,\n",
      "        6.3719e+06, 4.9051e+06, 2.1050e+06, 6.6605e+04, 3.4608e+05, 6.9504e+06,\n",
      "        7.8098e+04, 6.0133e+06, 1.4684e+05, 1.1392e+06, 3.8542e+04, 3.5432e+06,\n",
      "        2.6644e+05, 1.4688e+07, 2.8020e+05, 2.3695e+06, 3.3622e+05, 4.1770e+06,\n",
      "        1.6848e+06, 5.6492e+06, 2.7480e+05, 2.8374e+06, 1.3012e+06, 4.4637e+05,\n",
      "        4.6602e+05, 1.2656e-01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.9951e+06, 1.8587e+05, 4.8851e+06, 3.6433e+05, 1.0554e+05, 1.4789e+07,\n",
      "        2.7496e+06, 1.0934e+06, 9.1103e+04, 5.2275e+05, 4.2212e-01, 1.8109e+01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5466, 0.4352, 0.8483, 0.2100, 0.7252, 0.3949, 0.2214, 0.3127, 0.1977,\n",
      "        0.3313, 0.0543, 0.0368], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2687e+07, 4.1994e+05, 2.9643e+06, 1.1512e+06, 1.1599e+05, 3.5797e+07,\n",
      "        8.5635e+06, 3.0060e+06, 2.9236e+05, 1.3983e+06, 1.5968e+00, 6.9767e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.8813e+07, 3.9752e+07, 5.4925e+06, 3.2709e+06, 1.0794e+07, 2.1822e+05,\n",
      "        4.8124e+06, 3.4450e+06, 1.9183e+06, 2.8264e+07, 1.7031e+06, 6.8112e+06,\n",
      "        2.3676e+05, 2.9368e+05, 6.8470e+05, 1.4531e+06, 1.7817e+05, 3.9662e+04,\n",
      "        2.5513e+01, 1.2984e+01, 4.0064e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8396, 0.7547, 0.7378, 0.6334, 0.6132, 0.6235, 0.5161, 0.7775, 0.8237,\n",
      "        0.4927, 0.5997, 0.7880, 0.4088, 0.5210, 0.5150, 0.2816, 0.4453, 0.2700,\n",
      "        0.0504, 0.0782, 0.1218], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2068e+07, 3.9010e+07, 5.7607e+06, 4.7960e+06, 1.6701e+07, 3.2868e+05,\n",
      "        9.3157e+06, 3.0659e+06, 1.3526e+06, 5.7353e+07, 2.7271e+06, 5.7771e+06,\n",
      "        5.5992e+05, 5.6269e+05, 1.3282e+06, 4.1760e+06, 3.9534e+05, 1.1581e+05,\n",
      "        9.6912e+01, 4.7873e+01, 1.4073e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.1757e+06, 6.8756e+06, 5.9631e+06, 3.2140e+06, 1.2854e+07, 1.3326e+06,\n",
      "        3.8006e+06, 1.3974e+06, 1.5895e+05, 1.7614e+05, 1.2455e+06, 7.7549e+04,\n",
      "        2.6476e+05, 3.0348e+04, 5.5742e+00, 1.8234e+01, 1.8785e+00, 1.4079e+00,\n",
      "        3.0194e-01, 2.5634e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1655, 0.5749, 0.4103, 0.7587, 0.4850, 0.2950, 0.5018, 0.3392, 0.1640,\n",
      "        0.2507, 0.3892, 0.1570, 0.1025, 0.0919, 0.0337, 0.0254, 0.0511, 0.0158,\n",
      "        0.0212, 0.0226], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([7.2625e+06, 1.1692e+07, 1.4065e+07, 3.1025e+06, 2.6478e+07, 3.7579e+06,\n",
      "        7.5741e+06, 3.6936e+06, 5.3155e+05, 5.2789e+05, 3.0427e+06, 2.6149e+05,\n",
      "        9.5052e+05, 1.1023e+05, 2.1546e+01, 7.1080e+01, 7.1296e+00, 5.5422e+00,\n",
      "        1.1821e+00, 1.0022e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.4521e+08, 2.6916e+06, 6.5460e+05, 4.8867e+07, 7.8616e+06, 5.2154e+05,\n",
      "        1.9445e+07, 5.5450e+06, 8.3356e+05, 1.5685e+05, 1.6236e+06, 2.6931e+06,\n",
      "        1.2093e+05, 2.4846e+05, 8.8256e+05, 1.8894e+06, 1.0079e+06, 4.4467e+06,\n",
      "        7.7705e+04, 9.3452e+04, 4.5274e+05, 1.1589e+05, 6.0686e+04, 4.8467e+04,\n",
      "        1.6502e+05, 1.1155e+05, 4.8747e+04, 6.8838e+04, 7.6101e+01, 3.1385e+02,\n",
      "        4.7451e+02, 6.1374e+02, 6.9932e+02, 3.3844e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9805, 0.5191, 0.4984, 0.9627, 0.5039, 0.3312, 0.9871, 0.8708, 0.5647,\n",
      "        0.4551, 0.6441, 0.7249, 0.2374, 0.7829, 0.3835, 0.3639, 0.4368, 0.4613,\n",
      "        0.3648, 0.7845, 0.0801, 0.2396, 0.1873, 0.3912, 0.3165, 0.1580, 0.0936,\n",
      "        0.0565, 0.0967, 0.1362, 0.0371, 0.0521, 0.1658, 0.0717],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.1314e+07, 5.1773e+06, 1.3133e+06, 7.2952e+06, 1.5601e+07, 1.3952e+06,\n",
      "        1.0000e+06, 2.8663e+06, 1.4513e+06, 3.4189e+05, 2.3115e+06, 2.9634e+06,\n",
      "        3.6885e+05, 2.1573e+05, 2.1764e+06, 4.8076e+06, 2.2705e+06, 9.5823e+06,\n",
      "        1.9745e+05, 8.0546e+04, 1.6659e+06, 3.5250e+05, 1.9729e+05, 1.1804e+05,\n",
      "        4.5120e+05, 3.7572e+05, 1.7673e+05, 2.5980e+05, 2.7497e+02, 1.0845e+03,\n",
      "        1.8276e+03, 2.3269e+03, 2.3334e+03, 1.2566e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.0952e+06, 3.9456e+05, 1.2020e+07, 7.8552e+06, 2.5831e+06, 1.7021e+07,\n",
      "        5.0012e+05, 5.9134e+05, 4.5594e+05, 3.0735e+05, 3.1760e+06, 2.2237e+05,\n",
      "        7.3951e+05, 1.6156e+06, 1.2555e+05, 1.9561e+05, 4.1624e+05, 8.1732e+05,\n",
      "        3.9064e+05, 1.4020e+05, 6.5707e+04, 1.5793e+05, 2.0582e+02, 5.8379e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8494, 0.3912, 0.7621, 0.5548, 0.4269, 0.8904, 0.6356, 0.3417, 0.3473,\n",
      "        0.1198, 0.3280, 0.3042, 0.4459, 0.2938, 0.1365, 0.1886, 0.2292, 0.4247,\n",
      "        0.1910, 0.1140, 0.1906, 0.2903, 0.0588, 0.1238], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.4673e+06, 9.6082e+05, 1.1437e+07, 1.3989e+07, 5.9215e+06, 7.4614e+06,\n",
      "        7.2891e+05, 1.5570e+06, 1.1904e+06, 1.0822e+06, 8.5375e+06, 6.1892e+05,\n",
      "        1.6392e+06, 4.5634e+06, 4.3365e+05, 6.3487e+05, 1.2834e+06, 1.8807e+06,\n",
      "        1.2640e+06, 4.9685e+05, 2.1273e+05, 4.4836e+05, 7.7487e+02, 2.0461e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.0457e+05, 2.0201e+05, 6.3695e+04, 3.5877e+05, 7.8459e+04, 1.4342e+05,\n",
      "        1.6490e+05, 3.1043e+01, 1.5750e+01, 1.0593e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1207, 0.1814, 0.2887, 0.2583, 0.1056, 0.3216, 0.1980, 0.0127, 0.0389,\n",
      "        0.0182], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.6778e+05, 6.6146e+05, 1.8123e+05, 1.0644e+06, 2.8069e+05, 3.8916e+05,\n",
      "        5.2898e+05, 1.2259e+02, 6.0552e+01, 4.1599e+01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.1196e+07, 2.2397e+06, 1.8379e+08, 2.6110e+06, 1.8753e+06, 1.0884e+07,\n",
      "        9.9457e+06, 8.8372e+05, 9.5151e+06, 2.1193e+06, 2.8718e+05, 8.9923e+06,\n",
      "        1.5521e+05, 1.6176e+06, 3.1727e+05, 1.5372e+05, 4.3877e+05, 1.0426e+05,\n",
      "        1.3663e+04, 3.3402e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8370, 0.9444, 0.7089, 0.5202, 0.9478, 0.9616, 0.7682, 0.5181, 0.8114,\n",
      "        0.2822, 0.2042, 0.6097, 0.3476, 0.5341, 0.2672, 0.1293, 0.1188, 0.2010,\n",
      "        0.1883, 0.0415], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.3823e+07, 4.9793e+05, 2.1399e+08, 5.0114e+06, 3.9119e+05, 1.6701e+06,\n",
      "        9.2231e+06, 1.7034e+06, 7.1800e+06, 6.0848e+06, 9.1421e+05, 1.4037e+07,\n",
      "        4.0506e+05, 3.0147e+06, 9.3002e+05, 5.3536e+05, 1.5466e+06, 3.3323e+05,\n",
      "        4.4363e+04, 1.2806e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.8983e+05, 3.7458e+04, 2.1710e+05, 2.5827e+01, 2.5357e+01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2259, 0.2218, 0.2123, 0.1582, 0.1046], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.8782e+05, 1.1660e+05, 6.8405e+05, 8.6961e+01, 9.0817e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.2516e+05, 9.1294e+05, 4.9123e+06, 5.6916e+04, 7.1640e+06, 2.1986e+04,\n",
      "        2.4034e+06, 8.0310e+05, 7.1638e+04, 5.5199e+01, 1.3483e+01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3632, 0.5951, 0.5853, 0.5186, 0.4437, 0.3833, 0.3467, 0.3240, 0.1753,\n",
      "        0.0856, 0.0422], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.7356e+05, 1.4785e+06, 8.1489e+06, 1.0961e+05, 1.5942e+07, 5.4235e+04,\n",
      "        6.2808e+06, 2.1714e+06, 2.3632e+05, 2.0190e+02, 5.1652e+01],\n",
      "       device='cuda:0')\n",
      "All_sigs tensor([2.4465e+05, 1.6992e+07, 5.8545e+05, 3.2528e+06, 3.4820e+06, 1.1012e+06,\n",
      "        6.6828e+04, 3.2996e+06, 5.9858e+05, 2.8203e+06, 1.1418e+06, 2.1659e+05,\n",
      "        3.9297e+05, 3.3943e+06, 4.7041e+05, 2.8094e+05, 7.4505e+06, 2.0699e+06,\n",
      "        5.1025e+06, 1.1067e+07, 2.1153e+06, 5.1301e+06, 4.6333e+05, 3.1504e+05,\n",
      "        2.3237e+06, 7.7842e+05, 4.3243e+05, 8.9009e+04, 2.9737e+05, 1.1172e+06,\n",
      "        6.5443e+05, 2.1757e+06, 4.0107e-01, 1.3356e-02, 2.6971e+05, 3.4636e+06,\n",
      "        1.7647e+05, 5.9296e+05, 4.2311e+05, 2.9443e+05, 6.3719e+06, 4.9051e+06,\n",
      "        2.1050e+06, 6.6605e+04, 3.4608e+05, 6.9504e+06, 7.8098e+04, 6.0133e+06,\n",
      "        1.4684e+05, 1.1392e+06, 3.8542e+04, 3.5432e+06, 2.6644e+05, 1.4688e+07,\n",
      "        2.8020e+05, 2.3695e+06, 3.3622e+05, 4.1770e+06, 1.6848e+06, 5.6492e+06,\n",
      "        2.7480e+05, 2.8374e+06, 1.3012e+06, 4.4637e+05, 4.6602e+05, 1.2656e-01,\n",
      "        1.2687e+07, 4.1994e+05, 2.9643e+06, 1.1512e+06, 1.1599e+05, 3.5797e+07,\n",
      "        8.5635e+06, 3.0060e+06, 2.9236e+05, 1.3983e+06, 1.5968e+00, 6.9767e+01,\n",
      "        1.2068e+07, 3.9010e+07, 5.7607e+06, 4.7960e+06, 1.6701e+07, 3.2868e+05,\n",
      "        9.3157e+06, 3.0659e+06, 1.3526e+06, 5.7353e+07, 2.7271e+06, 5.7771e+06,\n",
      "        5.5992e+05, 5.6269e+05, 1.3282e+06, 4.1760e+06, 3.9534e+05, 1.1581e+05,\n",
      "        9.6912e+01, 4.7873e+01, 1.4073e+03, 7.2625e+06, 1.1692e+07, 1.4065e+07,\n",
      "        3.1025e+06, 2.6478e+07, 3.7579e+06, 7.5741e+06, 3.6936e+06, 5.3155e+05,\n",
      "        5.2789e+05, 3.0427e+06, 2.6149e+05, 9.5052e+05, 1.1023e+05, 2.1546e+01,\n",
      "        7.1080e+01, 7.1296e+00, 5.5422e+00, 1.1821e+00, 1.0022e+02, 1.1314e+07,\n",
      "        5.1773e+06, 1.3133e+06, 7.2952e+06, 1.5601e+07, 1.3952e+06, 1.0000e+06,\n",
      "        2.8663e+06, 1.4513e+06, 3.4189e+05, 2.3115e+06, 2.9634e+06, 3.6885e+05,\n",
      "        2.1573e+05, 2.1764e+06, 4.8076e+06, 2.2705e+06, 9.5823e+06, 1.9745e+05,\n",
      "        8.0546e+04, 1.6659e+06, 3.5250e+05, 1.9729e+05, 1.1804e+05, 4.5120e+05,\n",
      "        3.7572e+05, 1.7673e+05, 2.5980e+05, 2.7497e+02, 1.0845e+03, 1.8276e+03,\n",
      "        2.3269e+03, 2.3334e+03, 1.2566e+03, 2.4673e+06, 9.6082e+05, 1.1437e+07,\n",
      "        1.3989e+07, 5.9215e+06, 7.4614e+06, 7.2891e+05, 1.5570e+06, 1.1904e+06,\n",
      "        1.0822e+06, 8.5375e+06, 6.1892e+05, 1.6392e+06, 4.5634e+06, 4.3365e+05,\n",
      "        6.3487e+05, 1.2834e+06, 1.8807e+06, 1.2640e+06, 4.9685e+05, 2.1273e+05,\n",
      "        4.4836e+05, 7.7487e+02, 2.0461e+01, 3.6778e+05, 6.6146e+05, 1.8123e+05,\n",
      "        1.0644e+06, 2.8069e+05, 3.8916e+05, 5.2898e+05, 1.2259e+02, 6.0552e+01,\n",
      "        4.1599e+01, 1.3823e+07, 4.9793e+05, 2.1399e+08, 5.0114e+06, 3.9119e+05,\n",
      "        1.6701e+06, 9.2231e+06, 1.7034e+06, 7.1800e+06, 6.0848e+06, 9.1421e+05,\n",
      "        1.4037e+07, 4.0506e+05, 3.0147e+06, 9.3002e+05, 5.3536e+05, 1.5466e+06,\n",
      "        3.3323e+05, 4.4363e+04, 1.2806e+02, 5.8782e+05, 1.1660e+05, 6.8405e+05,\n",
      "        8.6961e+01, 9.0817e+01, 5.7356e+05, 1.4785e+06, 8.1489e+06, 1.0961e+05,\n",
      "        1.5942e+07, 5.4235e+04, 6.2808e+06, 2.1714e+06, 2.3632e+05, 2.0190e+02,\n",
      "        5.1652e+01], device='cuda:0')\n",
      "All_sig tensor([5.8487e-02, 4.0623e+00, 1.3996e-01, 7.7764e-01, 8.3241e-01, 2.6325e-01,\n",
      "        1.5976e-02, 7.8882e-01, 1.4310e-01, 6.7423e-01, 2.7296e-01, 5.1778e-02,\n",
      "        9.3944e-02, 8.1145e-01, 1.1246e-01, 6.7163e-02, 1.7811e+00, 4.9484e-01,\n",
      "        1.2198e+00, 2.6458e+00, 5.0568e-01, 1.2264e+00, 1.1077e-01, 7.5315e-02,\n",
      "        5.5552e-01, 1.8609e-01, 1.0338e-01, 2.1279e-02, 7.1090e-02, 2.6709e-01,\n",
      "        1.5645e-01, 5.2013e-01, 9.5882e-08, 3.1930e-09, 6.4477e-02, 8.2802e-01,\n",
      "        4.2189e-02, 1.4176e-01, 1.0115e-01, 7.0388e-02, 1.5233e+00, 1.1726e+00,\n",
      "        5.0323e-01, 1.5923e-02, 8.2736e-02, 1.6616e+00, 1.8670e-02, 1.4376e+00,\n",
      "        3.5104e-02, 2.7233e-01, 9.2141e-03, 8.4705e-01, 6.3695e-02, 3.5115e+00,\n",
      "        6.6986e-02, 5.6646e-01, 8.0377e-02, 9.9857e-01, 4.0278e-01, 1.3505e+00,\n",
      "        6.5695e-02, 6.7832e-01, 3.1107e-01, 1.0671e-01, 1.1141e-01, 3.0256e-08,\n",
      "        3.0329e+00, 1.0039e-01, 7.0866e-01, 2.7522e-01, 2.7728e-02, 8.5577e+00,\n",
      "        2.0472e+00, 7.1863e-01, 6.9892e-02, 3.3428e-01, 3.8175e-07, 1.6679e-05,\n",
      "        2.8849e+00, 9.3259e+00, 1.3772e+00, 1.1465e+00, 3.9925e+00, 7.8575e-02,\n",
      "        2.2270e+00, 7.3295e-01, 3.2335e-01, 1.3711e+01, 6.5194e-01, 1.3811e+00,\n",
      "        1.3386e-01, 1.3452e-01, 3.1752e-01, 9.9833e-01, 9.4511e-02, 2.7686e-02,\n",
      "        2.3168e-05, 1.1445e-05, 3.3644e-04, 1.7362e+00, 2.7951e+00, 3.3624e+00,\n",
      "        7.4171e-01, 6.3300e+00, 8.9838e-01, 1.8107e+00, 8.8301e-01, 1.2707e-01,\n",
      "        1.2620e-01, 7.2740e-01, 6.2513e-02, 2.2723e-01, 2.6353e-02, 5.1509e-06,\n",
      "        1.6993e-05, 1.7044e-06, 1.3250e-06, 2.8261e-07, 2.3960e-05, 2.7047e+00,\n",
      "        1.2377e+00, 3.1397e-01, 1.7440e+00, 3.7296e+00, 3.3355e-01, 2.3907e-01,\n",
      "        6.8522e-01, 3.4695e-01, 8.1733e-02, 5.5259e-01, 7.0844e-01, 8.8179e-02,\n",
      "        5.1572e-02, 5.2030e-01, 1.1493e+00, 5.4279e-01, 2.2908e+00, 4.7202e-02,\n",
      "        1.9256e-02, 3.9827e-01, 8.4270e-02, 4.7165e-02, 2.8218e-02, 1.0787e-01,\n",
      "        8.9822e-02, 4.2251e-02, 6.2108e-02, 6.5735e-05, 2.5925e-04, 4.3690e-04,\n",
      "        5.5628e-04, 5.5784e-04, 3.0042e-04, 5.8983e-01, 2.2970e-01, 2.7341e+00,\n",
      "        3.3442e+00, 1.4156e+00, 1.7838e+00, 1.7426e-01, 3.7223e-01, 2.8457e-01,\n",
      "        2.5871e-01, 2.0410e+00, 1.4796e-01, 3.9187e-01, 1.0910e+00, 1.0367e-01,\n",
      "        1.5177e-01, 3.0680e-01, 4.4960e-01, 3.0219e-01, 1.1878e-01, 5.0855e-02,\n",
      "        1.0719e-01, 1.8524e-04, 4.8915e-06, 8.7922e-02, 1.5813e-01, 4.3325e-02,\n",
      "        2.5446e-01, 6.7104e-02, 9.3033e-02, 1.2646e-01, 2.9308e-05, 1.4476e-05,\n",
      "        9.9449e-06, 3.3045e+00, 1.1904e-01, 5.1158e+01, 1.1981e+00, 9.3519e-02,\n",
      "        3.9926e-01, 2.2049e+00, 4.0722e-01, 1.7165e+00, 1.4547e+00, 2.1855e-01,\n",
      "        3.3558e+00, 9.6834e-02, 7.2071e-01, 2.2233e-01, 1.2799e-01, 3.6974e-01,\n",
      "        7.9663e-02, 1.0606e-02, 3.0616e-05, 1.4053e-01, 2.7874e-02, 1.6353e-01,\n",
      "        2.0789e-05, 2.1711e-05, 1.3712e-01, 3.5347e-01, 1.9481e+00, 2.6203e-02,\n",
      "        3.8113e+00, 1.2966e-02, 1.5015e+00, 5.1911e-01, 5.6495e-02, 4.8266e-05,\n",
      "        1.2348e-05], device='cuda:0')\n",
      "Sig sum tensor(223.0000, device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 4182.98001793722\n",
      "pruning 30 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :12 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     31\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    11\n",
      "     ║    ╠════╝\n",
      "     ║    23\n",
      "     ║    ╠════╗\n",
      "     ║    ║    9\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    20\n",
      "    ╠════╗\n",
      "    ║    18\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   20\n",
      "   ╠════╗\n",
      "   ║    11\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  33\n",
      "  ╠════╗\n",
      "  ║    15\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([  237218.8594, 13279423.0000,   755059.0000,  1353535.5000,\n",
      "         1133372.7500,   549042.1250,    35752.6484,  1904686.7500,\n",
      "          209774.1562,  6536738.5000,   463214.8750,    85123.8516,\n",
      "          355338.8438,  1865916.0000,   241504.2812,   105032.7734,\n",
      "         3637758.7500,   602813.8125, 11045681.0000,  6912039.0000,\n",
      "         1417880.6250,  5142507.5000,   248863.3750,   141252.9688,\n",
      "         1093761.3750,   344529.8750,   218031.0000,    74711.0625,\n",
      "          154822.8750,   515667.2500,   215768.9688,   683861.3750],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.7563, 0.6480, 0.8000, 0.3499, 0.2166, 0.4518, 0.4331, 0.5185, 0.2713,\n",
      "        0.8894, 0.3756, 0.3171, 0.7110, 0.5011, 0.5189, 0.3288, 0.4842, 0.2145,\n",
      "        0.8743, 0.5911, 0.5585, 0.7428, 0.4660, 0.4654, 0.4604, 0.4195, 0.3694,\n",
      "        0.6753, 0.4912, 0.3661, 0.0836, 0.1456], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([  231202.9688, 18698546.0000,   603996.3750,  3519536.7500,\n",
      "         3551568.7500,  1203962.1250,    81072.4141,  3668127.2500,\n",
      "          611429.0625,  2891256.0000,  1156952.2500,   232517.4531,\n",
      "          410805.3750,  3723859.5000,   464713.8438,   281991.5000,\n",
      "         7505921.5000,  1893921.8750,  5551993.5000, 11306245.0000,\n",
      "         2503740.0000,  5290707.5000,   531579.8750,   302045.9062,\n",
      "         2360731.7500,   799997.6250,   550002.0000,    97049.3359,\n",
      "          315082.9688,  1307436.7500,   790932.0625,  2337124.0000],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([5.0800e+05, 3.4455e+06, 9.0271e+04, 7.0403e+05, 3.1214e+05, 2.7263e+05,\n",
      "        1.0057e+07, 2.6153e+06, 1.7880e+06, 3.4592e+04, 2.9389e+05, 2.2972e+06,\n",
      "        8.2947e+04, 2.5267e+06, 7.8266e+04, 9.7180e+05, 1.5964e+04, 6.1197e+06,\n",
      "        2.2783e+05, 5.5731e+06, 3.2824e+05, 9.4096e+05, 1.6724e+05, 1.5763e+06,\n",
      "        1.4249e+06, 3.4624e+06, 1.2096e+05, 1.4880e+06, 8.9730e+05, 1.7692e+05,\n",
      "        2.6507e+05, 4.2315e-01, 4.4914e-03], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8524, 0.7235, 0.4590, 0.7838, 0.6727, 0.7209, 0.8347, 0.5286, 0.6978,\n",
      "        0.5494, 0.6768, 0.2119, 0.7948, 0.4131, 0.6090, 0.7151, 0.4318, 0.8260,\n",
      "        0.7178, 0.3724, 0.8557, 0.3261, 0.4546, 0.4563, 0.6494, 0.6004, 0.4471,\n",
      "        0.3948, 0.4693, 0.1895, 0.2341, 0.1238, 0.0482], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.9984e+05, 3.8108e+06, 1.9534e+05, 6.0888e+05, 4.0870e+05, 3.0436e+05,\n",
      "        6.6516e+06, 4.9319e+06, 2.1615e+06, 6.2347e+04, 3.7999e+05, 7.2420e+06,\n",
      "        6.8069e+04, 5.9317e+06, 1.2241e+05, 1.1073e+06, 3.6281e+04, 4.2604e+06,\n",
      "        2.5717e+05, 1.3991e+07, 1.8948e+05, 2.5366e+06, 3.6482e+05, 3.4285e+06,\n",
      "        1.9985e+06, 5.5341e+06, 2.6754e+05, 3.6021e+06, 1.9048e+06, 5.7359e+05,\n",
      "        8.1202e+05, 1.4831e+00, 1.7100e-02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([7.3789e+06, 2.4179e+05, 4.7147e+06, 4.8700e+05, 1.1369e+05, 1.7654e+07,\n",
      "        3.0288e+06, 1.0880e+06, 1.7382e+05, 4.5749e+05, 2.8308e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5098, 0.4433, 0.8564, 0.2073, 0.7186, 0.4045, 0.2115, 0.3103, 0.2270,\n",
      "        0.3206, 0.0198], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.4469e+07, 5.3837e+05, 2.7074e+06, 1.5442e+06, 1.2797e+05, 4.2051e+07,\n",
      "        9.5523e+06, 3.0017e+06, 5.3742e+05, 1.2433e+06, 1.1099e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.9245e+07, 3.7967e+07, 7.3008e+06, 3.1912e+06, 1.1810e+07, 2.0118e+05,\n",
      "        5.0010e+06, 3.0995e+06, 1.9947e+06, 3.1530e+07, 1.5322e+06, 6.7433e+06,\n",
      "        3.5595e+05, 3.0942e+05, 8.9328e+05, 2.2308e+06, 2.3941e+05, 7.7092e+04,\n",
      "        2.7759e+01, 3.2167e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8391, 0.7451, 0.7329, 0.6307, 0.6306, 0.6070, 0.5158, 0.7824, 0.8171,\n",
      "        0.5081, 0.5773, 0.7907, 0.4489, 0.5227, 0.5210, 0.2646, 0.4816, 0.2995,\n",
      "        0.0880, 0.1294], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2389e+07, 3.8706e+07, 7.8001e+06, 4.7145e+06, 1.7450e+07, 3.1628e+05,\n",
      "        9.6854e+06, 2.6984e+06, 1.4597e+06, 6.2037e+07, 2.5906e+06, 5.6441e+06,\n",
      "        7.8460e+05, 5.9080e+05, 1.7114e+06, 6.5622e+06, 4.9646e+05, 2.1602e+05,\n",
      "        1.0126e+02, 1.1202e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.3376e+06, 5.2865e+06, 7.3016e+06, 3.4321e+06, 1.6548e+07, 1.5123e+06,\n",
      "        3.9998e+06, 1.4011e+06, 1.1609e+05, 1.8739e+05, 1.4821e+06, 1.0348e+05,\n",
      "        3.6661e+05, 6.1488e+04, 5.0996e+00, 3.5982e-01, 1.0393e+00, 2.8470e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1825, 0.5520, 0.4143, 0.7481, 0.5008, 0.2835, 0.5028, 0.2948, 0.1494,\n",
      "        0.2640, 0.3868, 0.1651, 0.0982, 0.1004, 0.0305, 0.0724, 0.0581, 0.1641],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([7.6445e+06, 9.4742e+06, 1.7107e+07, 3.4588e+06, 3.3042e+07, 4.3345e+06,\n",
      "        7.9550e+06, 3.9521e+06, 3.9497e+05, 5.5165e+05, 3.6354e+06, 3.4558e+05,\n",
      "        1.3225e+06, 2.2125e+05, 1.9776e+01, 1.3351e+00, 3.9156e+00, 9.5188e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.4910e+08, 2.9367e+06, 5.5134e+05, 4.8262e+07, 8.3234e+06, 5.0659e+05,\n",
      "        1.7631e+07, 5.1848e+06, 9.0295e+05, 1.6684e+05, 1.8589e+06, 2.4041e+06,\n",
      "        1.5769e+05, 3.2345e+05, 8.7738e+05, 2.1750e+06, 1.5323e+06, 5.2437e+06,\n",
      "        7.5658e+04, 1.1644e+05, 4.8701e+05, 1.2703e+05, 6.6774e+04, 5.9891e+04,\n",
      "        2.5644e+05, 1.5749e+05, 7.5156e+04, 7.5231e+04, 5.1602e+02, 3.9596e+02,\n",
      "        6.0868e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9797, 0.5192, 0.4981, 0.9615, 0.5040, 0.3343, 0.9868, 0.8708, 0.5688,\n",
      "        0.4585, 0.6411, 0.7283, 0.2409, 0.7872, 0.3812, 0.3601, 0.4375, 0.4624,\n",
      "        0.3589, 0.7840, 0.0816, 0.2347, 0.1902, 0.3954, 0.3208, 0.1667, 0.0961,\n",
      "        0.4117, 0.1105, 0.4487, 0.1290], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2113e+07, 5.6475e+06, 1.1068e+06, 7.4342e+06, 1.6514e+07, 1.3490e+06,\n",
      "        9.3022e+05, 2.6798e+06, 1.5573e+06, 3.6138e+05, 2.6687e+06, 2.6127e+06,\n",
      "        4.7881e+05, 2.7534e+05, 2.1718e+06, 5.5675e+06, 3.4475e+06, 1.1276e+07,\n",
      "        1.9403e+05, 1.0061e+05, 1.7891e+06, 3.8885e+05, 2.1628e+05, 1.4483e+05,\n",
      "        6.9668e+05, 5.2498e+05, 2.7175e+05, 1.7703e+05, 1.8359e+03, 8.7319e+02,\n",
      "        2.1207e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([5.0547e+06, 5.1837e+05, 1.2402e+07, 6.9084e+06, 2.2094e+06, 1.7002e+07,\n",
      "        5.8229e+05, 5.7530e+05, 4.8101e+05, 3.6174e+05, 3.1566e+06, 2.2115e+05,\n",
      "        9.1736e+05, 2.4641e+06, 1.4404e+05, 2.1362e+05, 5.0237e+05, 1.0483e+06,\n",
      "        5.3266e+05, 1.6072e+05, 7.9662e+04, 1.8058e+05, 1.1752e+01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8612, 0.3872, 0.7793, 0.5418, 0.4178, 0.8830, 0.6334, 0.5060, 0.3039,\n",
      "        0.1155, 0.3255, 0.2914, 0.4181, 0.2979, 0.1473, 0.1709, 0.2580, 0.4406,\n",
      "        0.2149, 0.1115, 0.1833, 0.2725, 0.1308], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.8068e+06, 1.2705e+06, 1.0947e+07, 1.2663e+07, 5.1450e+06, 7.9536e+06,\n",
      "        8.5391e+05, 1.1369e+06, 1.3393e+06, 1.2798e+06, 8.5170e+06, 6.2682e+05,\n",
      "        2.1354e+06, 6.9201e+06, 4.9131e+05, 7.0847e+05, 1.4911e+06, 2.3458e+06,\n",
      "        1.6727e+06, 5.7119e+05, 2.6023e+05, 5.2551e+05, 4.0860e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([8.3980e+04, 2.0745e+05, 1.2988e+05, 5.5593e+05, 1.3355e+05, 2.7287e+05,\n",
      "        1.2584e+05, 9.4481e-01, 2.3800e+00, 7.9855e-01, 2.5308e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1093, 0.2133, 0.2992, 0.2799, 0.1190, 0.3506, 0.1561, 0.0318, 0.0188,\n",
      "        0.0391, 0.0277], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.9920e+05, 6.5283e+05, 3.6408e+05, 1.6014e+06, 4.7066e+05, 7.0877e+05,\n",
      "        4.2477e+05, 3.6590e+00, 9.3412e+00, 3.0694e+00, 9.8424e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.5737e+07, 2.3395e+06, 1.8553e+08, 3.5997e+06, 2.9925e+06, 1.3740e+07,\n",
      "        1.2113e+07, 1.5528e+06, 1.0329e+07, 2.4016e+06, 3.8168e+05, 9.8840e+06,\n",
      "        1.6270e+05, 1.5953e+06, 4.5533e+05, 2.2736e+05, 4.9677e+05, 1.3083e+05,\n",
      "        2.9148e+04, 1.7068e+00], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8263, 0.9425, 0.6994, 0.5483, 0.9493, 0.9678, 0.7697, 0.5473, 0.8080,\n",
      "        0.3055, 0.2088, 0.6217, 0.3412, 0.5350, 0.2494, 0.1429, 0.1112, 0.1661,\n",
      "        0.2075, 0.0663], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.0936e+07, 5.3847e+05, 2.2305e+08, 6.5042e+06, 6.0702e+05, 1.7690e+06,\n",
      "        1.1159e+07, 2.8116e+06, 7.9312e+06, 6.6712e+06, 1.2079e+06, 1.4957e+07,\n",
      "        4.2873e+05, 2.9671e+06, 1.3670e+06, 7.7945e+05, 1.7662e+06, 4.3640e+05,\n",
      "        9.2398e+04, 6.3745e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.5228e+05, 7.2408e+04, 3.2605e+05, 1.5643e+01, 3.9165e+01, 2.4722e+01,\n",
      "        3.6140e+01, 8.4280e+00, 1.4166e+00], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2181, 0.2064, 0.2212, 0.0658, 0.0757, 0.0767, 0.0784, 0.1979, 0.0805],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([7.8904e+05, 2.2985e+05, 1.0157e+06, 5.8458e+01, 1.4480e+02, 9.1299e+01,\n",
      "        1.3322e+02, 2.7042e+01, 5.2101e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.8888e+05, 9.4197e+05, 6.1668e+06, 6.7412e+04, 7.1064e+06, 2.3838e+04,\n",
      "        2.8807e+06, 8.2226e+05, 8.5947e+04, 9.3469e-01, 3.0377e-02, 3.1701e-03,\n",
      "        5.1901e-01, 2.8336e-03, 1.2820e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3548, 0.5864, 0.5867, 0.5133, 0.4457, 0.3757, 0.3499, 0.3189, 0.1761,\n",
      "        0.0471, 0.0487, 0.0401, 0.0232, 0.0138, 0.0590], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.8746e+05, 1.5584e+06, 1.0196e+07, 1.3123e+05, 1.5756e+07, 5.9527e+04,\n",
      "        7.4907e+06, 2.2400e+06, 2.8325e+05, 3.5625e+00, 1.1559e-01, 1.2172e-02,\n",
      "        2.0278e+00, 1.1178e-02, 4.8254e-01], device='cuda:0')\n",
      "All_sigs tensor([2.3120e+05, 1.8699e+07, 6.0400e+05, 3.5195e+06, 3.5516e+06, 1.2040e+06,\n",
      "        8.1072e+04, 3.6681e+06, 6.1143e+05, 2.8913e+06, 1.1570e+06, 2.3252e+05,\n",
      "        4.1081e+05, 3.7239e+06, 4.6471e+05, 2.8199e+05, 7.5059e+06, 1.8939e+06,\n",
      "        5.5520e+06, 1.1306e+07, 2.5037e+06, 5.2907e+06, 5.3158e+05, 3.0205e+05,\n",
      "        2.3607e+06, 8.0000e+05, 5.5000e+05, 9.7049e+04, 3.1508e+05, 1.3074e+06,\n",
      "        7.9093e+05, 2.3371e+06, 2.9984e+05, 3.8108e+06, 1.9534e+05, 6.0888e+05,\n",
      "        4.0870e+05, 3.0436e+05, 6.6516e+06, 4.9319e+06, 2.1615e+06, 6.2347e+04,\n",
      "        3.7999e+05, 7.2420e+06, 6.8069e+04, 5.9317e+06, 1.2241e+05, 1.1073e+06,\n",
      "        3.6281e+04, 4.2604e+06, 2.5717e+05, 1.3991e+07, 1.8948e+05, 2.5366e+06,\n",
      "        3.6482e+05, 3.4285e+06, 1.9985e+06, 5.5341e+06, 2.6754e+05, 3.6021e+06,\n",
      "        1.9048e+06, 5.7359e+05, 8.1202e+05, 1.4831e+00, 1.7100e-02, 1.4469e+07,\n",
      "        5.3837e+05, 2.7074e+06, 1.5442e+06, 1.2797e+05, 4.2051e+07, 9.5523e+06,\n",
      "        3.0017e+06, 5.3742e+05, 1.2433e+06, 1.1099e+00, 1.2389e+07, 3.8706e+07,\n",
      "        7.8001e+06, 4.7145e+06, 1.7450e+07, 3.1628e+05, 9.6854e+06, 2.6984e+06,\n",
      "        1.4597e+06, 6.2037e+07, 2.5906e+06, 5.6441e+06, 7.8460e+05, 5.9080e+05,\n",
      "        1.7114e+06, 6.5622e+06, 4.9646e+05, 2.1602e+05, 1.0126e+02, 1.1202e+00,\n",
      "        7.6445e+06, 9.4742e+06, 1.7107e+07, 3.4588e+06, 3.3042e+07, 4.3345e+06,\n",
      "        7.9550e+06, 3.9521e+06, 3.9497e+05, 5.5165e+05, 3.6354e+06, 3.4558e+05,\n",
      "        1.3225e+06, 2.2125e+05, 1.9776e+01, 1.3351e+00, 3.9156e+00, 9.5188e+00,\n",
      "        1.2113e+07, 5.6475e+06, 1.1068e+06, 7.4342e+06, 1.6514e+07, 1.3490e+06,\n",
      "        9.3022e+05, 2.6798e+06, 1.5573e+06, 3.6138e+05, 2.6687e+06, 2.6127e+06,\n",
      "        4.7881e+05, 2.7534e+05, 2.1718e+06, 5.5675e+06, 3.4475e+06, 1.1276e+07,\n",
      "        1.9403e+05, 1.0061e+05, 1.7891e+06, 3.8885e+05, 2.1628e+05, 1.4483e+05,\n",
      "        6.9668e+05, 5.2498e+05, 2.7175e+05, 1.7703e+05, 1.8359e+03, 8.7319e+02,\n",
      "        2.1207e+02, 2.8068e+06, 1.2705e+06, 1.0947e+07, 1.2663e+07, 5.1450e+06,\n",
      "        7.9536e+06, 8.5391e+05, 1.1369e+06, 1.3393e+06, 1.2798e+06, 8.5170e+06,\n",
      "        6.2682e+05, 2.1354e+06, 6.9201e+06, 4.9131e+05, 7.0847e+05, 1.4911e+06,\n",
      "        2.3458e+06, 1.6727e+06, 5.7119e+05, 2.6023e+05, 5.2551e+05, 4.0860e+01,\n",
      "        2.9920e+05, 6.5283e+05, 3.6408e+05, 1.6014e+06, 4.7066e+05, 7.0877e+05,\n",
      "        4.2477e+05, 3.6590e+00, 9.3412e+00, 3.0694e+00, 9.8424e+00, 1.0936e+07,\n",
      "        5.3847e+05, 2.2305e+08, 6.5042e+06, 6.0702e+05, 1.7690e+06, 1.1159e+07,\n",
      "        2.8116e+06, 7.9312e+06, 6.6712e+06, 1.2079e+06, 1.4957e+07, 4.2873e+05,\n",
      "        2.9671e+06, 1.3670e+06, 7.7945e+05, 1.7662e+06, 4.3640e+05, 9.2398e+04,\n",
      "        6.3745e+00, 7.8904e+05, 2.2985e+05, 1.0157e+06, 5.8458e+01, 1.4480e+02,\n",
      "        9.1299e+01, 1.3322e+02, 2.7042e+01, 5.2101e+00, 4.8746e+05, 1.5584e+06,\n",
      "        1.0196e+07, 1.3123e+05, 1.5756e+07, 5.9527e+04, 7.4907e+06, 2.2400e+06,\n",
      "        2.8325e+05, 3.5625e+00, 1.1559e-01, 1.2172e-02, 2.0278e+00, 1.1178e-02,\n",
      "        4.8254e-01], device='cuda:0')\n",
      "All_sig tensor([5.1610e-02, 4.1740e+00, 1.3483e-01, 7.8565e-01, 7.9280e-01, 2.6876e-01,\n",
      "        1.8097e-02, 8.1882e-01, 1.3649e-01, 6.4540e-01, 2.5826e-01, 5.1904e-02,\n",
      "        9.1702e-02, 8.3126e-01, 1.0374e-01, 6.2948e-02, 1.6755e+00, 4.2277e-01,\n",
      "        1.2393e+00, 2.5238e+00, 5.5890e-01, 1.1810e+00, 1.1866e-01, 6.7424e-02,\n",
      "        5.2698e-01, 1.7858e-01, 1.2277e-01, 2.1664e-02, 7.0335e-02, 2.9185e-01,\n",
      "        1.7656e-01, 5.2171e-01, 6.6932e-02, 8.5066e-01, 4.3605e-02, 1.3592e-01,\n",
      "        9.1232e-02, 6.7941e-02, 1.4848e+00, 1.1009e+00, 4.8250e-01, 1.3917e-02,\n",
      "        8.4823e-02, 1.6166e+00, 1.5195e-02, 1.3241e+00, 2.7326e-02, 2.4718e-01,\n",
      "        8.0988e-03, 9.5104e-01, 5.7408e-02, 3.1233e+00, 4.2298e-02, 5.6624e-01,\n",
      "        8.1438e-02, 7.6533e-01, 4.4611e-01, 1.2354e+00, 5.9722e-02, 8.0409e-01,\n",
      "        4.2521e-01, 1.2804e-01, 1.8126e-01, 3.3106e-07, 3.8172e-09, 3.2299e+00,\n",
      "        1.2018e-01, 6.0437e-01, 3.4471e-01, 2.8567e-02, 9.3869e+00, 2.1323e+00,\n",
      "        6.7005e-01, 1.1997e-01, 2.7755e-01, 2.4777e-07, 2.7655e+00, 8.6402e+00,\n",
      "        1.7412e+00, 1.0524e+00, 3.8953e+00, 7.0602e-02, 2.1620e+00, 6.0235e-01,\n",
      "        3.2584e-01, 1.3848e+01, 5.7829e-01, 1.2599e+00, 1.7514e-01, 1.3188e-01,\n",
      "        3.8203e-01, 1.4648e+00, 1.1082e-01, 4.8221e-02, 2.2605e-05, 2.5007e-07,\n",
      "        1.7065e+00, 2.1149e+00, 3.8187e+00, 7.7208e-01, 7.3759e+00, 9.6757e-01,\n",
      "        1.7758e+00, 8.8222e-01, 8.8166e-02, 1.2314e-01, 8.1151e-01, 7.7142e-02,\n",
      "        2.9522e-01, 4.9389e-02, 4.4145e-06, 2.9802e-07, 8.7407e-07, 2.1248e-06,\n",
      "        2.7040e+00, 1.2607e+00, 2.4707e-01, 1.6595e+00, 3.6863e+00, 3.0114e-01,\n",
      "        2.0765e-01, 5.9819e-01, 3.4764e-01, 8.0669e-02, 5.9572e-01, 5.8323e-01,\n",
      "        1.0688e-01, 6.1463e-02, 4.8481e-01, 1.2428e+00, 7.6956e-01, 2.5171e+00,\n",
      "        4.3313e-02, 2.2459e-02, 3.9938e-01, 8.6801e-02, 4.8280e-02, 3.2330e-02,\n",
      "        1.5552e-01, 1.1719e-01, 6.0661e-02, 3.9518e-02, 4.0983e-04, 1.9492e-04,\n",
      "        4.7340e-05, 6.2654e-01, 2.8362e-01, 2.4437e+00, 2.8266e+00, 1.1485e+00,\n",
      "        1.7755e+00, 1.9062e-01, 2.5378e-01, 2.9897e-01, 2.8568e-01, 1.9012e+00,\n",
      "        1.3992e-01, 4.7668e-01, 1.5447e+00, 1.0967e-01, 1.5815e-01, 3.3285e-01,\n",
      "        5.2364e-01, 3.7339e-01, 1.2750e-01, 5.8089e-02, 1.1731e-01, 9.1209e-06,\n",
      "        6.6790e-02, 1.4573e-01, 8.1271e-02, 3.5747e-01, 1.0506e-01, 1.5821e-01,\n",
      "        9.4819e-02, 8.1679e-07, 2.0852e-06, 6.8518e-07, 2.1971e-06, 2.4411e+00,\n",
      "        1.2020e-01, 4.9792e+01, 1.4519e+00, 1.3550e-01, 3.9489e-01, 2.4909e+00,\n",
      "        6.2762e-01, 1.7704e+00, 1.4892e+00, 2.6964e-01, 3.3387e+00, 9.5703e-02,\n",
      "        6.6233e-01, 3.0515e-01, 1.7399e-01, 3.9425e-01, 9.7416e-02, 2.0626e-02,\n",
      "        1.4229e-06, 1.7613e-01, 5.1309e-02, 2.2674e-01, 1.3049e-05, 3.2324e-05,\n",
      "        2.0380e-05, 2.9738e-05, 6.0364e-06, 1.1630e-06, 1.0881e-01, 3.4787e-01,\n",
      "        2.2760e+00, 2.9293e-02, 3.5172e+00, 1.3288e-02, 1.6721e+00, 5.0004e-01,\n",
      "        6.3229e-02, 7.9524e-07, 2.5802e-08, 2.7171e-09, 4.5266e-07, 2.4953e-09,\n",
      "        1.0772e-07], device='cuda:0')\n",
      "Sig sum tensor(223., device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 4479.767533632287\n",
      "pruning 30 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :13 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     31\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    11\n",
      "     ║    ╠════╝\n",
      "     ║    25\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    25\n",
      "    ╠════╗\n",
      "    ║    16\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   20\n",
      "   ╠════╗\n",
      "   ║    12\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  33\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 36\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.5143e+05, 1.4223e+07, 8.5761e+05, 1.3127e+06, 1.1911e+06, 6.3929e+05,\n",
      "        4.2060e+04, 1.8139e+06, 2.1475e+05, 6.9522e+06, 5.1230e+05, 1.0396e+05,\n",
      "        4.1685e+05, 2.1885e+06, 2.6279e+05, 1.0234e+05, 3.9363e+06, 6.7137e+05,\n",
      "        1.1668e+07, 8.2001e+06, 1.4312e+06, 5.6037e+06, 2.5947e+05, 1.5565e+05,\n",
      "        1.1030e+06, 3.9590e+05, 2.5877e+05, 6.8492e+04, 1.4977e+05, 6.2350e+05,\n",
      "        2.4804e+05, 8.8154e+05, 3.4078e-02, 1.1502e-02, 5.3794e-04, 5.3283e-04],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.7375, 0.6565, 0.8065, 0.3306, 0.2222, 0.4534, 0.4404, 0.5064, 0.2733,\n",
      "        0.8801, 0.3709, 0.3509, 0.7141, 0.5070, 0.5273, 0.3047, 0.4636, 0.2128,\n",
      "        0.8459, 0.6079, 0.5609, 0.7489, 0.4576, 0.4498, 0.4550, 0.4196, 0.3669,\n",
      "        0.6683, 0.4541, 0.3832, 0.0891, 0.1832, 0.0599, 0.0381, 0.0252, 0.0261],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.6405e+05, 1.9542e+07, 6.6388e+05, 3.5151e+06, 3.7057e+06, 1.3978e+06,\n",
      "        9.4155e+04, 3.5818e+06, 6.2425e+05, 3.3337e+06, 1.2892e+06, 2.6990e+05,\n",
      "        4.7670e+05, 4.3156e+06, 4.9688e+05, 2.8460e+05, 8.4465e+06, 2.1139e+06,\n",
      "        7.1937e+06, 1.2861e+07, 2.5138e+06, 5.6275e+06, 5.6299e+05, 3.4254e+05,\n",
      "        2.4046e+06, 9.1913e+05, 6.5536e+05, 9.0882e+04, 3.2704e+05, 1.5384e+06,\n",
      "        9.0378e+05, 2.8801e+06, 1.2815e-01, 4.4258e-02, 2.0976e-03, 2.0758e-03],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([5.6754e+05, 3.9385e+06, 1.3921e+05, 6.8193e+05, 3.4104e+05, 2.9043e+05,\n",
      "        1.1877e+07, 2.8624e+06, 2.1359e+06, 4.2332e+04, 3.3833e+05, 2.6424e+06,\n",
      "        1.0980e+05, 2.8279e+06, 8.4564e+04, 1.0052e+06, 1.2691e+04, 6.6055e+06,\n",
      "        2.4547e+05, 6.6144e+06, 3.1570e+05, 1.1126e+06, 1.7494e+05, 1.8081e+06,\n",
      "        1.7347e+06, 3.0285e+06, 1.0390e+05, 1.4049e+06, 8.6634e+05, 2.0599e+05,\n",
      "        2.4719e+05, 9.9032e-02, 1.9932e-02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8323, 0.7384, 0.4676, 0.7659, 0.6359, 0.6893, 0.8389, 0.5220, 0.7180,\n",
      "        0.5820, 0.6755, 0.2180, 0.7731, 0.4166, 0.6102, 0.6838, 0.4406, 0.8490,\n",
      "        0.7164, 0.3836, 0.8385, 0.3336, 0.4573, 0.4580, 0.6766, 0.5688, 0.4210,\n",
      "        0.3846, 0.4714, 0.1985, 0.2265, 0.0454, 0.0652], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.8078e+05, 4.1215e+06, 2.9647e+05, 6.3849e+05, 4.9666e+05, 3.6091e+05,\n",
      "        7.6542e+06, 5.4730e+06, 2.4089e+06, 7.0773e+04, 4.3922e+05, 8.2659e+06,\n",
      "        9.9652e+04, 6.5993e+06, 1.3187e+05, 1.2716e+06, 2.8398e+04, 3.9910e+06,\n",
      "        2.7846e+05, 1.6308e+07, 2.0390e+05, 2.9658e+06, 3.7973e+05, 3.9203e+06,\n",
      "        2.2439e+06, 5.2239e+06, 2.4063e+05, 3.4583e+06, 1.8317e+06, 6.6042e+05,\n",
      "        7.6481e+05, 3.7816e-01, 7.4529e-02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([9.3984e+06, 2.9603e+05, 4.9376e+06, 4.4337e+05, 1.2987e+05, 1.9794e+07,\n",
      "        3.4374e+06, 1.0920e+06, 1.3628e+05, 5.8897e+05, 1.3745e-02, 2.5424e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5664, 0.4352, 0.8368, 0.1983, 0.7160, 0.4150, 0.2032, 0.2960, 0.2018,\n",
      "        0.3311, 0.0694, 0.0817], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.6300e+07, 6.6883e+05, 3.2236e+06, 1.4218e+06, 1.4753e+05, 4.6319e+07,\n",
      "        1.0955e+07, 3.0754e+06, 4.3508e+05, 1.5759e+06, 5.1167e-02, 9.3383e-01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.2651e+07, 4.1995e+07, 9.7805e+06, 3.9287e+06, 1.1325e+07, 1.9945e+05,\n",
      "        4.8979e+06, 3.5431e+06, 1.8651e+06, 3.7155e+07, 1.6197e+06, 7.4193e+06,\n",
      "        2.6790e+05, 3.4805e+05, 1.0309e+06, 2.6285e+06, 2.1459e+05, 1.1741e+05,\n",
      "        7.0751e-01, 3.9932e+00], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8472, 0.7518, 0.7617, 0.6262, 0.6354, 0.6119, 0.5037, 0.7827, 0.8141,\n",
      "        0.5036, 0.5758, 0.8095, 0.4528, 0.5368, 0.5210, 0.2929, 0.4427, 0.3229,\n",
      "        0.0984, 0.0711], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.3846e+07, 4.1690e+07, 9.3209e+06, 5.8735e+06, 1.6517e+07, 3.0960e+05,\n",
      "        9.7237e+06, 3.0794e+06, 1.3872e+06, 7.3771e+07, 2.7484e+06, 5.6536e+06,\n",
      "        5.8642e+05, 6.4488e+05, 1.9752e+06, 7.4346e+06, 4.7838e+05, 3.1799e+05,\n",
      "        2.5514e+00, 1.4837e+01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.9276e+06, 6.4153e+06, 5.6387e+06, 4.3874e+06, 1.8993e+07, 1.4238e+06,\n",
      "        3.6369e+06, 1.9679e+06, 1.0164e+05, 1.8798e+05, 2.4424e+06, 1.0934e+05,\n",
      "        4.3614e+05, 8.1914e+04, 1.3231e-01, 1.7678e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1925, 0.5514, 0.3954, 0.7519, 0.4979, 0.2842, 0.4869, 0.3091, 0.1367,\n",
      "        0.2543, 0.3999, 0.1690, 0.1010, 0.0939, 0.0165, 0.0526],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([9.4556e+06, 1.1512e+07, 1.3636e+07, 4.3545e+06, 3.8142e+07, 4.0766e+06,\n",
      "        7.4645e+06, 5.4381e+06, 3.5098e+05, 5.6073e+05, 5.8625e+06, 3.6343e+05,\n",
      "        1.5684e+06, 2.9688e+05, 5.2050e-01, 6.6992e-01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.6351e+08, 3.5490e+06, 6.5934e+05, 5.4408e+07, 1.0494e+07, 4.9278e+05,\n",
      "        2.7964e+07, 6.0888e+06, 9.4070e+05, 1.7627e+05, 1.9851e+06, 2.7067e+06,\n",
      "        1.9265e+05, 3.2954e+05, 1.0033e+06, 2.4359e+06, 1.5532e+06, 9.4906e+06,\n",
      "        7.5863e+04, 1.2696e+05, 5.5343e+05, 1.2938e+05, 7.6139e+04, 6.8473e+04,\n",
      "        3.5863e+05, 2.2370e+05, 7.9231e+04, 1.0490e+05, 9.3547e+01, 1.7882e+02,\n",
      "        1.4456e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9795, 0.5190, 0.4972, 0.9609, 0.5038, 0.3289, 0.9865, 0.8703, 0.5716,\n",
      "        0.4590, 0.6403, 0.7206, 0.6007, 0.7817, 0.3744, 0.3531, 0.4381, 0.4631,\n",
      "        0.3573, 0.7853, 0.0809, 0.2271, 0.1955, 0.3920, 0.3182, 0.1712, 0.0967,\n",
      "        0.4130, 0.3962, 0.2797, 0.1764], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.3402e+07, 6.8278e+06, 1.3262e+06, 8.5173e+06, 2.0829e+07, 1.3227e+06,\n",
      "        1.5061e+06, 3.1589e+06, 1.6119e+06, 3.8148e+05, 2.8558e+06, 3.0247e+06,\n",
      "        3.0769e+05, 2.8777e+05, 2.5106e+06, 6.3035e+06, 3.4910e+06, 2.0384e+07,\n",
      "        1.9503e+05, 1.0902e+05, 2.0346e+06, 4.0000e+05, 2.4502e+05, 1.6652e+05,\n",
      "        9.7807e+05, 7.4162e+05, 2.8628e+05, 2.4633e+05, 2.2593e+02, 5.1523e+02,\n",
      "        4.7627e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.2471e+06, 6.6814e+05, 1.6347e+07, 8.0230e+06, 2.2495e+06, 1.5815e+07,\n",
      "        7.3682e+05, 5.3025e+05, 3.8592e+05, 4.3081e+05, 3.6602e+06, 2.5452e+05,\n",
      "        9.1611e+05, 2.7472e+06, 1.2800e+05, 2.6735e+05, 4.8799e+05, 1.5717e+06,\n",
      "        4.9688e+05, 1.9387e+05, 1.0257e+05, 3.0152e+05, 4.8366e-01, 5.0119e+00,\n",
      "        1.3739e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8696, 0.3895, 0.7876, 0.5442, 0.4017, 0.8796, 0.6285, 0.4914, 0.3023,\n",
      "        0.1200, 0.3218, 0.2891, 0.4201, 0.2898, 0.1393, 0.1974, 0.2400, 0.6193,\n",
      "        0.2138, 0.1147, 0.1929, 0.3351, 0.0830, 0.0483, 0.0554],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.2584e+06, 1.6315e+06, 1.3892e+07, 1.4626e+07, 5.3836e+06, 7.6174e+06,\n",
      "        1.0950e+06, 1.0788e+06, 1.0770e+06, 1.5165e+06, 9.9291e+06, 7.2377e+05,\n",
      "        2.1249e+06, 7.8042e+06, 4.4064e+05, 8.5833e+05, 1.4836e+06, 2.3937e+06,\n",
      "        1.5626e+06, 6.8651e+05, 3.3116e+05, 8.0189e+05, 1.7740e+00, 1.9079e+01,\n",
      "        5.1909e+01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.0831e+05, 2.5353e+05, 1.5123e+05, 6.5405e+05, 1.7813e+05, 4.3037e+05,\n",
      "        1.0616e+05, 5.6284e+00, 4.2892e+01, 7.3421e-01, 1.3939e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1223, 0.2248, 0.3155, 0.2738, 0.1250, 0.3647, 0.1313, 0.0169, 0.0560,\n",
      "        0.0814, 0.0810], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.8025e+05, 7.8610e+05, 4.1405e+05, 1.8998e+06, 6.2346e+05, 1.0937e+06,\n",
      "        3.6888e+05, 2.2133e+01, 1.6197e+02, 2.6977e+00, 5.1241e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.6041e+07, 2.6328e+06, 2.0342e+08, 4.7106e+06, 2.3914e+06, 1.3232e+07,\n",
      "        1.2059e+07, 1.4674e+06, 1.1366e+07, 2.5783e+06, 3.3906e+05, 1.0568e+07,\n",
      "        1.3304e+05, 2.0730e+06, 5.0640e+05, 2.7898e+05, 4.2747e+05, 1.1790e+05,\n",
      "        4.7440e+04, 8.2949e-01, 1.4699e+00, 1.8249e+01, 6.7329e-01, 1.6626e+01,\n",
      "        1.0205e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8542, 0.9359, 0.7105, 0.5521, 0.9572, 0.9670, 0.7711, 0.5308, 0.8058,\n",
      "        0.3117, 0.1966, 0.6267, 0.3282, 0.5542, 0.2733, 0.1491, 0.0989, 0.1945,\n",
      "        0.2142, 0.0288, 0.0356, 0.0250, 0.0131, 0.0284, 0.0633],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([9.3561e+06, 6.7542e+05, 2.3560e+08, 8.4399e+06, 4.0978e+05, 1.7472e+06,\n",
      "        1.1041e+07, 2.7542e+06, 8.8309e+06, 7.0986e+06, 1.0896e+06, 1.5779e+07,\n",
      "        3.5752e+05, 3.6963e+06, 1.4720e+06, 9.4951e+05, 1.5407e+06, 3.7986e+05,\n",
      "        1.4912e+05, 3.2225e+00, 5.6702e+00, 7.1171e+01, 2.6578e+00, 6.4620e+01,\n",
      "        3.8237e+01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.0620e+05, 1.0078e+05, 4.4117e+05, 2.6695e+00, 1.1572e+01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2348, 0.2317, 0.2341, 0.0828, 0.0226], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([9.3724e+05, 3.0969e+05, 1.3516e+06, 9.7936e+00, 4.5243e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([ 208181.2188, 1022593.4375, 7542454.5000,   77873.7266, 6604379.5000,\n",
      "          33873.2812, 3245601.7500,  771957.2500,   93498.2500],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3371, 0.5746, 0.5961, 0.5036, 0.4227, 0.3891, 0.3704, 0.2883, 0.1850],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([  552003.9375,  1739934.3750, 12184862.0000,   154632.4375,\n",
      "        15249706.0000,    82772.1172,  8174132.5000,  2197588.7500,\n",
      "          304815.1875], device='cuda:0')\n",
      "All_sigs tensor([2.6405e+05, 1.9542e+07, 6.6388e+05, 3.5151e+06, 3.7057e+06, 1.3978e+06,\n",
      "        9.4155e+04, 3.5818e+06, 6.2425e+05, 3.3337e+06, 1.2892e+06, 2.6990e+05,\n",
      "        4.7670e+05, 4.3156e+06, 4.9688e+05, 2.8460e+05, 8.4465e+06, 2.1139e+06,\n",
      "        7.1937e+06, 1.2861e+07, 2.5138e+06, 5.6275e+06, 5.6299e+05, 3.4254e+05,\n",
      "        2.4046e+06, 9.1913e+05, 6.5536e+05, 9.0882e+04, 3.2704e+05, 1.5384e+06,\n",
      "        9.0378e+05, 2.8801e+06, 1.2815e-01, 4.4258e-02, 2.0976e-03, 2.0758e-03,\n",
      "        3.8078e+05, 4.1215e+06, 2.9647e+05, 6.3849e+05, 4.9666e+05, 3.6091e+05,\n",
      "        7.6542e+06, 5.4730e+06, 2.4089e+06, 7.0773e+04, 4.3922e+05, 8.2659e+06,\n",
      "        9.9652e+04, 6.5993e+06, 1.3187e+05, 1.2716e+06, 2.8398e+04, 3.9910e+06,\n",
      "        2.7846e+05, 1.6308e+07, 2.0390e+05, 2.9658e+06, 3.7973e+05, 3.9203e+06,\n",
      "        2.2439e+06, 5.2239e+06, 2.4063e+05, 3.4583e+06, 1.8317e+06, 6.6042e+05,\n",
      "        7.6481e+05, 3.7816e-01, 7.4529e-02, 1.6300e+07, 6.6883e+05, 3.2236e+06,\n",
      "        1.4218e+06, 1.4753e+05, 4.6319e+07, 1.0955e+07, 3.0754e+06, 4.3508e+05,\n",
      "        1.5759e+06, 5.1167e-02, 9.3383e-01, 1.3846e+07, 4.1690e+07, 9.3209e+06,\n",
      "        5.8735e+06, 1.6517e+07, 3.0960e+05, 9.7237e+06, 3.0794e+06, 1.3872e+06,\n",
      "        7.3771e+07, 2.7484e+06, 5.6536e+06, 5.8642e+05, 6.4488e+05, 1.9752e+06,\n",
      "        7.4346e+06, 4.7838e+05, 3.1799e+05, 2.5514e+00, 1.4837e+01, 9.4556e+06,\n",
      "        1.1512e+07, 1.3636e+07, 4.3545e+06, 3.8142e+07, 4.0766e+06, 7.4645e+06,\n",
      "        5.4381e+06, 3.5098e+05, 5.6073e+05, 5.8625e+06, 3.6343e+05, 1.5684e+06,\n",
      "        2.9688e+05, 5.2050e-01, 6.6992e-01, 1.3402e+07, 6.8278e+06, 1.3262e+06,\n",
      "        8.5173e+06, 2.0829e+07, 1.3227e+06, 1.5061e+06, 3.1589e+06, 1.6119e+06,\n",
      "        3.8148e+05, 2.8558e+06, 3.0247e+06, 3.0769e+05, 2.8777e+05, 2.5106e+06,\n",
      "        6.3035e+06, 3.4910e+06, 2.0384e+07, 1.9503e+05, 1.0902e+05, 2.0346e+06,\n",
      "        4.0000e+05, 2.4502e+05, 1.6652e+05, 9.7807e+05, 7.4162e+05, 2.8628e+05,\n",
      "        2.4633e+05, 2.2593e+02, 5.1523e+02, 4.7627e+02, 3.2584e+06, 1.6315e+06,\n",
      "        1.3892e+07, 1.4626e+07, 5.3836e+06, 7.6174e+06, 1.0950e+06, 1.0788e+06,\n",
      "        1.0770e+06, 1.5165e+06, 9.9291e+06, 7.2377e+05, 2.1249e+06, 7.8042e+06,\n",
      "        4.4064e+05, 8.5833e+05, 1.4836e+06, 2.3937e+06, 1.5626e+06, 6.8651e+05,\n",
      "        3.3116e+05, 8.0189e+05, 1.7740e+00, 1.9079e+01, 5.1909e+01, 3.8025e+05,\n",
      "        7.8610e+05, 4.1405e+05, 1.8998e+06, 6.2346e+05, 1.0937e+06, 3.6888e+05,\n",
      "        2.2133e+01, 1.6197e+02, 2.6977e+00, 5.1241e+00, 9.3561e+06, 6.7542e+05,\n",
      "        2.3560e+08, 8.4399e+06, 4.0978e+05, 1.7472e+06, 1.1041e+07, 2.7542e+06,\n",
      "        8.8309e+06, 7.0986e+06, 1.0896e+06, 1.5779e+07, 3.5752e+05, 3.6963e+06,\n",
      "        1.4720e+06, 9.4951e+05, 1.5407e+06, 3.7986e+05, 1.4912e+05, 3.2225e+00,\n",
      "        5.6702e+00, 7.1171e+01, 2.6578e+00, 6.4620e+01, 3.8237e+01, 9.3724e+05,\n",
      "        3.0969e+05, 1.3516e+06, 9.7936e+00, 4.5243e+01, 5.5200e+05, 1.7399e+06,\n",
      "        1.2185e+07, 1.5463e+05, 1.5250e+07, 8.2772e+04, 8.1741e+06, 2.1976e+06,\n",
      "        3.0482e+05], device='cuda:0')\n",
      "All_sig tensor([5.3484e-02, 3.9582e+00, 1.3447e-01, 7.1199e-01, 7.5060e-01, 2.8312e-01,\n",
      "        1.9071e-02, 7.2549e-01, 1.2644e-01, 6.7525e-01, 2.6113e-01, 5.4669e-02,\n",
      "        9.6556e-02, 8.7413e-01, 1.0064e-01, 5.7646e-02, 1.7109e+00, 4.2818e-01,\n",
      "        1.4571e+00, 2.6049e+00, 5.0918e-01, 1.1399e+00, 1.1403e-01, 6.9383e-02,\n",
      "        4.8707e-01, 1.8617e-01, 1.3274e-01, 1.8408e-02, 6.6243e-02, 3.1161e-01,\n",
      "        1.8306e-01, 5.8337e-01, 2.5956e-08, 8.9647e-09, 4.2487e-10, 4.2046e-10,\n",
      "        7.7128e-02, 8.3482e-01, 6.0050e-02, 1.2933e-01, 1.0060e-01, 7.3102e-02,\n",
      "        1.5504e+00, 1.1086e+00, 4.8793e-01, 1.4335e-02, 8.8964e-02, 1.6743e+00,\n",
      "        2.0185e-02, 1.3367e+00, 2.6710e-02, 2.5756e-01, 5.7520e-03, 8.0838e-01,\n",
      "        5.6403e-02, 3.3033e+00, 4.1300e-02, 6.0074e-01, 7.6915e-02, 7.9407e-01,\n",
      "        4.5451e-01, 1.0581e+00, 4.8740e-02, 7.0048e-01, 3.7102e-01, 1.3377e-01,\n",
      "        1.5491e-01, 7.6596e-08, 1.5096e-08, 3.3016e+00, 1.3547e-01, 6.5295e-01,\n",
      "        2.8798e-01, 2.9883e-02, 9.3821e+00, 2.2190e+00, 6.2292e-01, 8.8127e-02,\n",
      "        3.1921e-01, 1.0364e-08, 1.8915e-07, 2.8044e+00, 8.4445e+00, 1.8880e+00,\n",
      "        1.1897e+00, 3.3456e+00, 6.2709e-02, 1.9696e+00, 6.2375e-01, 2.8099e-01,\n",
      "        1.4942e+01, 5.5670e-01, 1.1451e+00, 1.1878e-01, 1.3062e-01, 4.0009e-01,\n",
      "        1.5059e+00, 9.6897e-02, 6.4409e-02, 5.1680e-07, 3.0053e-06, 1.9153e+00,\n",
      "        2.3318e+00, 2.7621e+00, 8.8202e-01, 7.7258e+00, 8.2572e-01, 1.5120e+00,\n",
      "        1.1015e+00, 7.1092e-02, 1.1358e-01, 1.1875e+00, 7.3613e-02, 3.1769e-01,\n",
      "        6.0133e-02, 1.0543e-07, 1.3569e-07, 2.7147e+00, 1.3830e+00, 2.6862e-01,\n",
      "        1.7252e+00, 4.2190e+00, 2.6792e-01, 3.0507e-01, 6.3984e-01, 3.2649e-01,\n",
      "        7.7269e-02, 5.7845e-01, 6.1267e-01, 6.2323e-02, 5.8288e-02, 5.0854e-01,\n",
      "        1.2768e+00, 7.0711e-01, 4.1288e+00, 3.9504e-02, 2.2082e-02, 4.1211e-01,\n",
      "        8.1021e-02, 4.9630e-02, 3.3728e-02, 1.9811e-01, 1.5022e-01, 5.7986e-02,\n",
      "        4.9894e-02, 4.5764e-05, 1.0436e-04, 9.6470e-05, 6.6000e-01, 3.3047e-01,\n",
      "        2.8138e+00, 2.9626e+00, 1.0905e+00, 1.5429e+00, 2.2180e-01, 2.1852e-01,\n",
      "        2.1814e-01, 3.0717e-01, 2.0112e+00, 1.4660e-01, 4.3040e-01, 1.5808e+00,\n",
      "        8.9253e-02, 1.7386e-01, 3.0050e-01, 4.8484e-01, 3.1651e-01, 1.3906e-01,\n",
      "        6.7076e-02, 1.6243e-01, 3.5932e-07, 3.8645e-06, 1.0514e-05, 7.7020e-02,\n",
      "        1.5923e-01, 8.3867e-02, 3.8481e-01, 1.2628e-01, 2.2154e-01, 7.4717e-02,\n",
      "        4.4830e-06, 3.2806e-05, 5.4642e-07, 1.0379e-06, 1.8951e+00, 1.3681e-01,\n",
      "        4.7721e+01, 1.7095e+00, 8.3002e-02, 3.5391e-01, 2.2364e+00, 5.5787e-01,\n",
      "        1.7887e+00, 1.4378e+00, 2.2070e-01, 3.1960e+00, 7.2417e-02, 7.4870e-01,\n",
      "        2.9815e-01, 1.9233e-01, 3.1207e-01, 7.6942e-02, 3.0205e-02, 6.5272e-07,\n",
      "        1.1485e-06, 1.4416e-05, 5.3834e-07, 1.3089e-05, 7.7449e-06, 1.8984e-01,\n",
      "        6.2729e-02, 2.7378e-01, 1.9837e-06, 9.1641e-06, 1.1181e-01, 3.5243e-01,\n",
      "        2.4681e+00, 3.1321e-02, 3.0889e+00, 1.6766e-02, 1.6557e+00, 4.4513e-01,\n",
      "        6.1741e-02], device='cuda:0')\n",
      "Sig sum tensor(223.0000, device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 4936.99329147982\n",
      "pruning 30 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :14 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     35\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    26\n",
      "     ║    ╠════╗\n",
      "     ║    ║    6\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    15\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   23\n",
      "   ╠════╗\n",
      "   ║    11\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  35\n",
      "  ╠════╗\n",
      "  ║    12\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 34\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.1871e+05, 1.5546e+07, 8.4414e+05, 1.3418e+06, 1.1741e+06, 5.8323e+05,\n",
      "        4.1096e+04, 2.3317e+06, 2.1618e+05, 6.8712e+06, 4.9475e+05, 1.0020e+05,\n",
      "        3.9799e+05, 2.1740e+06, 2.0861e+05, 7.9672e+04, 3.4996e+06, 4.5146e+05,\n",
      "        1.1210e+07, 7.8768e+06, 1.6397e+06, 5.2139e+06, 2.5775e+05, 1.4596e+05,\n",
      "        8.9811e+05, 3.6604e+05, 3.0831e+05, 7.8576e+04, 1.3093e+05, 6.5117e+05,\n",
      "        3.7219e+05, 1.1539e+06, 4.6772e-03, 4.7053e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.7256, 0.6720, 0.8166, 0.3206, 0.2120, 0.4195, 0.4328, 0.5276, 0.2625,\n",
      "        0.8881, 0.3608, 0.3757, 0.6927, 0.4968, 0.5122, 0.2984, 0.4538, 0.1937,\n",
      "        0.8479, 0.6035, 0.5737, 0.7483, 0.4273, 0.4440, 0.4372, 0.4145, 0.3771,\n",
      "        0.6964, 0.4389, 0.3716, 0.0954, 0.1832, 0.0188, 0.0350],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.4004e+05, 2.0396e+07, 6.1935e+05, 3.6464e+06, 3.7010e+06, 1.3543e+06,\n",
      "        9.3243e+04, 4.4059e+06, 6.3768e+05, 3.0749e+06, 1.2650e+06, 2.5022e+05,\n",
      "        4.8920e+05, 4.3760e+06, 4.0706e+05, 2.2360e+05, 7.6456e+06, 1.4561e+06,\n",
      "        6.8204e+06, 1.2492e+07, 2.7961e+06, 5.2494e+06, 5.9049e+05, 3.2464e+05,\n",
      "        2.0220e+06, 8.5727e+05, 7.6815e+05, 9.5429e+04, 2.9385e+05, 1.6368e+06,\n",
      "        1.3468e+06, 3.7703e+06, 1.8357e-02, 1.8163e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.0112e+05, 3.9778e+06, 1.3434e+05, 5.3198e+05, 3.3512e+05, 3.1551e+05,\n",
      "        1.3344e+07, 2.5499e+06, 1.9817e+06, 4.1189e+04, 3.0063e+05, 2.2779e+06,\n",
      "        9.8741e+04, 2.4856e+06, 8.1722e+04, 1.0554e+06, 8.7279e+03, 6.9197e+06,\n",
      "        2.0263e+05, 5.2576e+06, 3.4450e+05, 9.5612e+05, 1.7084e+05, 1.6854e+06,\n",
      "        2.0738e+06, 2.8610e+06, 1.0093e+05, 2.2214e+06, 1.2722e+06, 2.7642e+05,\n",
      "        5.1716e+05, 1.3132e+01, 4.5843e+00, 1.3787e+01, 3.6504e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8342, 0.7395, 0.4494, 0.7217, 0.6313, 0.7079, 0.8446, 0.5118, 0.7218,\n",
      "        0.5335, 0.6790, 0.2016, 0.7538, 0.4048, 0.6182, 0.6987, 0.4466, 0.8463,\n",
      "        0.7193, 0.3529, 0.8448, 0.3064, 0.4395, 0.4640, 0.6768, 0.5579, 0.4194,\n",
      "        0.3896, 0.5122, 0.2013, 0.2500, 0.0923, 0.1151, 0.0321, 0.0826],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.9873e+05, 4.1449e+06, 2.9584e+05, 5.9219e+05, 4.9419e+05, 3.6863e+05,\n",
      "        8.2943e+06, 4.9793e+06, 2.2054e+06, 7.6851e+04, 3.8600e+05, 7.2746e+06,\n",
      "        9.7237e+04, 5.9179e+06, 1.2480e+05, 1.2719e+06, 1.9321e+04, 4.2528e+06,\n",
      "        2.2748e+05, 1.3608e+07, 2.1385e+05, 2.6525e+06, 3.8304e+05, 3.6133e+06,\n",
      "        2.6809e+06, 5.0597e+06, 2.3440e+05, 5.4238e+06, 2.4822e+06, 8.8307e+05,\n",
      "        1.5515e+06, 4.7680e+01, 1.6227e+01, 5.3380e+01, 1.3395e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([9.8216e+06, 2.8322e+05, 4.7718e+06, 4.9945e+05, 1.8633e+05, 2.1498e+07,\n",
      "        3.2266e+06, 1.0302e+06, 2.1742e+05, 6.5314e+05, 5.0662e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5613, 0.4416, 0.8577, 0.1855, 0.6931, 0.4111, 0.1936, 0.2923, 0.2047,\n",
      "        0.3122, 0.0238], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.7236e+07, 6.3256e+05, 2.7153e+06, 1.6273e+06, 2.2875e+05, 5.0641e+07,\n",
      "        1.0408e+07, 2.9165e+06, 6.9167e+05, 1.7970e+06, 1.9782e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.1478e+07, 3.5778e+07, 1.2128e+07, 3.7369e+06, 9.9520e+06, 1.7346e+05,\n",
      "        4.2253e+06, 4.2542e+06, 1.9578e+06, 4.0239e+07, 1.2850e+06, 8.8148e+06,\n",
      "        4.4361e+05, 4.9674e+05, 1.7398e+06, 5.1739e+06, 2.0426e+05, 1.9892e+05,\n",
      "        3.8627e+01, 1.3747e+00, 2.5798e+00, 2.3780e+01, 4.9658e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8471, 0.7451, 0.7645, 0.6223, 0.6271, 0.5911, 0.4698, 0.7908, 0.8085,\n",
      "        0.5079, 0.5485, 0.8321, 0.4890, 0.5367, 0.5426, 0.3640, 0.4487, 0.3104,\n",
      "        0.0557, 0.0596, 0.0682, 0.0718, 0.0477], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.3132e+07, 3.6484e+07, 1.1426e+07, 5.6463e+06, 1.4845e+07, 2.8374e+05,\n",
      "        8.9608e+06, 3.5605e+06, 1.4998e+06, 7.9200e+07, 2.3209e+06, 5.9187e+06,\n",
      "        9.0664e+05, 9.2053e+05, 3.1833e+06, 1.3161e+07, 4.5048e+05, 5.4867e+05,\n",
      "        1.4590e+02, 5.1712e+00, 9.6157e+00, 8.8286e+01, 1.8915e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.8836e+06, 4.7652e+06, 5.3274e+06, 5.1865e+06, 2.0275e+07, 9.4238e+05,\n",
      "        3.1096e+06, 1.8076e+06, 1.7829e+05, 2.0322e+05, 2.8335e+06, 1.5312e+05,\n",
      "        5.6850e+05, 2.4174e+05, 1.1373e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1731, 0.5339, 0.3946, 0.7620, 0.4923, 0.2480, 0.4367, 0.3046, 0.1736,\n",
      "        0.2457, 0.4002, 0.1649, 0.0943, 0.1032, 0.0436], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([9.5383e+06, 8.8842e+06, 1.2902e+07, 4.9373e+06, 4.1173e+07, 2.8347e+06,\n",
      "        7.0061e+06, 5.0278e+06, 5.8934e+05, 6.1315e+05, 6.7977e+06, 5.1151e+05,\n",
      "        2.0596e+06, 8.6714e+05, 4.3507e+01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.4169e+08, 4.0777e+06, 7.5018e+05, 4.8708e+07, 1.0502e+07, 6.8153e+05,\n",
      "        3.2690e+07, 3.9512e+06, 1.1100e+06, 1.9657e+05, 2.3638e+06, 2.6048e+06,\n",
      "        2.3906e+05, 3.7675e+05, 8.7891e+05, 2.1652e+06, 2.4996e+06, 1.1504e+07,\n",
      "        7.2251e+04, 1.6659e+05, 5.1645e+05, 1.1392e+05, 8.1405e+04, 7.5382e+04,\n",
      "        4.9813e+05, 2.3198e+05, 1.0158e+05, 1.5554e+05, 1.8349e+01, 1.1746e+03,\n",
      "        3.6501e+01, 1.0662e+02, 6.2477e+01, 3.8860e+02, 1.4756e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9784, 0.5197, 0.4967, 0.9593, 0.5038, 0.3551, 0.9859, 0.8701, 0.5755,\n",
      "        0.4606, 0.6516, 0.7206, 0.2567, 0.7769, 0.3774, 0.3468, 0.4444, 0.4654,\n",
      "        0.3506, 0.7866, 0.0813, 0.2284, 0.1904, 0.3930, 0.3249, 0.1680, 0.0989,\n",
      "        0.4174, 0.0220, 0.1249, 0.0598, 0.1016, 0.0560, 0.1419, 0.0468],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2263e+07, 7.8347e+06, 1.5101e+06, 7.9332e+06, 2.0845e+07, 1.7581e+06,\n",
      "        1.8418e+06, 2.0535e+06, 1.8847e+06, 4.2411e+05, 3.2940e+06, 2.9111e+06,\n",
      "        7.1076e+05, 3.3615e+05, 2.1889e+06, 5.6576e+06, 5.5547e+06, 2.4599e+07,\n",
      "        1.8768e+05, 1.4221e+05, 1.8979e+06, 3.5159e+05, 2.6362e+05, 1.8302e+05,\n",
      "        1.3451e+06, 7.7202e+05, 3.6613e+05, 3.6245e+05, 7.1777e+01, 4.1115e+03,\n",
      "        1.3728e+02, 3.8316e+02, 2.3592e+02, 1.3339e+03, 5.6263e+02],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.9320e+06, 1.1436e+06, 1.7594e+07, 6.0082e+06, 2.3343e+06, 1.1780e+07,\n",
      "        6.1641e+05, 5.8425e+05, 3.3451e+05, 5.1812e+05, 3.8150e+06, 2.4996e+05,\n",
      "        1.2239e+06, 3.0586e+06, 1.2493e+05, 2.7255e+05, 5.0635e+05, 9.4226e+05,\n",
      "        7.1962e+05, 3.0456e+05, 1.3968e+05, 4.2120e+05, 1.7049e+02, 4.9996e-01,\n",
      "        3.9468e+00, 2.7889e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8598, 0.3935, 0.7710, 0.5331, 0.4069, 0.8681, 0.6626, 0.3532, 0.3116,\n",
      "        0.1175, 0.3105, 0.2920, 0.4147, 0.2950, 0.1474, 0.1650, 0.2192, 0.4224,\n",
      "        0.2531, 0.1181, 0.1908, 0.3673, 0.0306, 0.0127, 0.0219, 0.0638],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.8883e+06, 2.7743e+06, 1.6116e+07, 1.1222e+07, 5.5382e+06, 6.2128e+06,\n",
      "        8.3194e+05, 1.5115e+06, 9.2106e+05, 1.8291e+06, 1.0522e+07, 7.0788e+05,\n",
      "        2.8653e+06, 8.6257e+06, 4.2606e+05, 9.1035e+05, 1.5814e+06, 2.1770e+06,\n",
      "        2.1499e+06, 1.0744e+06, 4.5210e+05, 1.0659e+06, 6.6110e+02, 1.9744e+00,\n",
      "        1.5441e+01, 1.0444e+03], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([122392.0156, 335770.4688, 310940.6250, 787565.1875, 176313.1250,\n",
      "        352252.6250, 213863.8594], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1313, 0.2117, 0.3357, 0.2782, 0.1146, 0.3338, 0.1516],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([ 425307.7500, 1058785.8750,  826181.5625, 2273987.7500,  624408.8125,\n",
      "         938750.1875,  725749.0000], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.5288e+07, 2.2373e+06, 1.7091e+08, 3.6181e+06, 3.8770e+06, 1.7700e+07,\n",
      "        1.1902e+07, 2.2673e+06, 9.7035e+06, 2.7412e+06, 6.9384e+05, 1.0468e+07,\n",
      "        1.4760e+05, 2.3287e+06, 7.4035e+05, 2.1057e+05, 6.6534e+05, 2.1927e+05,\n",
      "        8.3605e+04], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9381, 0.9313, 0.7119, 0.5087, 0.9518, 0.9692, 0.7706, 0.5681, 0.7940,\n",
      "        0.3100, 0.2286, 0.6020, 0.3110, 0.5604, 0.2725, 0.1382, 0.1074, 0.2466,\n",
      "        0.2013], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.7825e+06, 6.1455e+05, 1.9698e+08, 7.1100e+06, 7.4804e+05, 2.1826e+06,\n",
      "        1.0923e+07, 3.9173e+06, 7.9948e+06, 7.5659e+06, 2.1409e+06, 1.6664e+07,\n",
      "        4.0680e+05, 4.0950e+06, 2.1543e+06, 7.2590e+05, 2.3754e+06, 6.6080e+05,\n",
      "        2.6712e+05], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.9890e+05, 1.5991e+05, 5.2544e+05, 1.9419e+02, 8.7078e+01, 8.6274e+01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2320, 0.2410, 0.2083, 0.0893, 0.0912, 0.0874], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2254e+06, 4.8550e+05, 1.6639e+06, 7.0739e+02, 3.1654e+02, 3.1492e+02],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.9782e+05, 1.1988e+06, 7.8364e+06, 7.8418e+04, 7.3526e+06, 6.3796e+04,\n",
      "        4.3812e+06, 8.8175e+05, 1.2636e+05, 1.2238e+00, 2.8920e+00, 4.5100e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3340, 0.5823, 0.5939, 0.5023, 0.4363, 0.4025, 0.3598, 0.2942, 0.1776,\n",
      "        0.0115, 0.0450, 0.0316], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.2701e+05, 2.0029e+06, 1.2729e+07, 1.5613e+05, 1.6578e+07, 1.5247e+05,\n",
      "        1.1220e+07, 2.4894e+06, 4.1569e+05, 4.8390e+00, 1.1047e+01, 1.7470e+00],\n",
      "       device='cuda:0')\n",
      "All_sigs tensor([2.4004e+05, 2.0396e+07, 6.1935e+05, 3.6464e+06, 3.7010e+06, 1.3543e+06,\n",
      "        9.3243e+04, 4.4059e+06, 6.3768e+05, 3.0749e+06, 1.2650e+06, 2.5022e+05,\n",
      "        4.8920e+05, 4.3760e+06, 4.0706e+05, 2.2360e+05, 7.6456e+06, 1.4561e+06,\n",
      "        6.8204e+06, 1.2492e+07, 2.7961e+06, 5.2494e+06, 5.9049e+05, 3.2464e+05,\n",
      "        2.0220e+06, 8.5727e+05, 7.6815e+05, 9.5429e+04, 2.9385e+05, 1.6368e+06,\n",
      "        1.3468e+06, 3.7703e+06, 1.8357e-02, 1.8163e+00, 3.9873e+05, 4.1449e+06,\n",
      "        2.9584e+05, 5.9219e+05, 4.9419e+05, 3.6863e+05, 8.2943e+06, 4.9793e+06,\n",
      "        2.2054e+06, 7.6851e+04, 3.8600e+05, 7.2746e+06, 9.7237e+04, 5.9179e+06,\n",
      "        1.2480e+05, 1.2719e+06, 1.9321e+04, 4.2528e+06, 2.2748e+05, 1.3608e+07,\n",
      "        2.1385e+05, 2.6525e+06, 3.8304e+05, 3.6133e+06, 2.6809e+06, 5.0597e+06,\n",
      "        2.3440e+05, 5.4238e+06, 2.4822e+06, 8.8307e+05, 1.5515e+06, 4.7680e+01,\n",
      "        1.6227e+01, 5.3380e+01, 1.3395e+01, 1.7236e+07, 6.3256e+05, 2.7153e+06,\n",
      "        1.6273e+06, 2.2875e+05, 5.0641e+07, 1.0408e+07, 2.9165e+06, 6.9167e+05,\n",
      "        1.7970e+06, 1.9782e+01, 1.3132e+07, 3.6484e+07, 1.1426e+07, 5.6463e+06,\n",
      "        1.4845e+07, 2.8374e+05, 8.9608e+06, 3.5605e+06, 1.4998e+06, 7.9200e+07,\n",
      "        2.3209e+06, 5.9187e+06, 9.0664e+05, 9.2053e+05, 3.1833e+06, 1.3161e+07,\n",
      "        4.5048e+05, 5.4867e+05, 1.4590e+02, 5.1712e+00, 9.6157e+00, 8.8286e+01,\n",
      "        1.8915e+01, 9.5383e+06, 8.8842e+06, 1.2902e+07, 4.9373e+06, 4.1173e+07,\n",
      "        2.8347e+06, 7.0061e+06, 5.0278e+06, 5.8934e+05, 6.1315e+05, 6.7977e+06,\n",
      "        5.1151e+05, 2.0596e+06, 8.6714e+05, 4.3507e+01, 1.2263e+07, 7.8347e+06,\n",
      "        1.5101e+06, 7.9332e+06, 2.0845e+07, 1.7581e+06, 1.8418e+06, 2.0535e+06,\n",
      "        1.8847e+06, 4.2411e+05, 3.2940e+06, 2.9111e+06, 7.1076e+05, 3.3615e+05,\n",
      "        2.1889e+06, 5.6576e+06, 5.5547e+06, 2.4599e+07, 1.8768e+05, 1.4221e+05,\n",
      "        1.8979e+06, 3.5159e+05, 2.6362e+05, 1.8302e+05, 1.3451e+06, 7.7202e+05,\n",
      "        3.6613e+05, 3.6245e+05, 7.1777e+01, 4.1115e+03, 1.3728e+02, 3.8316e+02,\n",
      "        2.3592e+02, 1.3339e+03, 5.6263e+02, 3.8883e+06, 2.7743e+06, 1.6116e+07,\n",
      "        1.1222e+07, 5.5382e+06, 6.2128e+06, 8.3194e+05, 1.5115e+06, 9.2106e+05,\n",
      "        1.8291e+06, 1.0522e+07, 7.0788e+05, 2.8653e+06, 8.6257e+06, 4.2606e+05,\n",
      "        9.1035e+05, 1.5814e+06, 2.1770e+06, 2.1499e+06, 1.0744e+06, 4.5210e+05,\n",
      "        1.0659e+06, 6.6110e+02, 1.9744e+00, 1.5441e+01, 1.0444e+03, 4.2531e+05,\n",
      "        1.0588e+06, 8.2618e+05, 2.2740e+06, 6.2441e+05, 9.3875e+05, 7.2575e+05,\n",
      "        3.7825e+06, 6.1455e+05, 1.9698e+08, 7.1100e+06, 7.4804e+05, 2.1826e+06,\n",
      "        1.0923e+07, 3.9173e+06, 7.9948e+06, 7.5659e+06, 2.1409e+06, 1.6664e+07,\n",
      "        4.0680e+05, 4.0950e+06, 2.1543e+06, 7.2590e+05, 2.3754e+06, 6.6080e+05,\n",
      "        2.6712e+05, 1.2254e+06, 4.8550e+05, 1.6639e+06, 7.0739e+02, 3.1654e+02,\n",
      "        3.1492e+02, 5.2701e+05, 2.0029e+06, 1.2729e+07, 1.5613e+05, 1.6578e+07,\n",
      "        1.5247e+05, 1.1220e+07, 2.4894e+06, 4.1569e+05, 4.8390e+00, 1.1047e+01,\n",
      "        1.7470e+00], device='cuda:0')\n",
      "All_sig tensor([4.9139e-02, 4.1752e+00, 1.2679e-01, 7.4646e-01, 7.5762e-01, 2.7724e-01,\n",
      "        1.9088e-02, 9.0192e-01, 1.3054e-01, 6.2946e-01, 2.5896e-01, 5.1221e-02,\n",
      "        1.0014e-01, 8.9581e-01, 8.3330e-02, 4.5774e-02, 1.5651e+00, 2.9808e-01,\n",
      "        1.3962e+00, 2.5573e+00, 5.7240e-01, 1.0746e+00, 1.2088e-01, 6.6457e-02,\n",
      "        4.1392e-01, 1.7549e-01, 1.5725e-01, 1.9535e-02, 6.0154e-02, 3.3506e-01,\n",
      "        2.7570e-01, 7.7182e-01, 3.7578e-09, 3.7181e-07, 8.1624e-02, 8.4850e-01,\n",
      "        6.0562e-02, 1.2123e-01, 1.0117e-01, 7.5461e-02, 1.6979e+00, 1.0193e+00,\n",
      "        4.5146e-01, 1.5732e-02, 7.9018e-02, 1.4892e+00, 1.9905e-02, 1.2115e+00,\n",
      "        2.5548e-02, 2.6037e-01, 3.9553e-03, 8.7060e-01, 4.6568e-02, 2.7858e+00,\n",
      "        4.3777e-02, 5.4299e-01, 7.8412e-02, 7.3968e-01, 5.4880e-01, 1.0358e+00,\n",
      "        4.7984e-02, 1.1103e+00, 5.0812e-01, 1.8077e-01, 3.1761e-01, 9.7606e-06,\n",
      "        3.3218e-06, 1.0927e-05, 2.7422e-06, 3.5284e+00, 1.2949e-01, 5.5585e-01,\n",
      "        3.3312e-01, 4.6828e-02, 1.0367e+01, 2.1306e+00, 5.9703e-01, 1.4159e-01,\n",
      "        3.6786e-01, 4.0495e-06, 2.6883e+00, 7.4686e+00, 2.3390e+00, 1.1559e+00,\n",
      "        3.0389e+00, 5.8084e-02, 1.8344e+00, 7.2888e-01, 3.0702e-01, 1.6213e+01,\n",
      "        4.7512e-01, 1.2116e+00, 1.8560e-01, 1.8844e-01, 6.5166e-01, 2.6943e+00,\n",
      "        9.2217e-02, 1.1232e-01, 2.9868e-05, 1.0586e-06, 1.9684e-06, 1.8073e-05,\n",
      "        3.8720e-06, 1.9526e+00, 1.8187e+00, 2.6411e+00, 1.0107e+00, 8.4284e+00,\n",
      "        5.8030e-01, 1.4342e+00, 1.0292e+00, 1.2064e-01, 1.2552e-01, 1.3916e+00,\n",
      "        1.0471e-01, 4.2162e-01, 1.7751e-01, 8.9062e-06, 2.5103e+00, 1.6038e+00,\n",
      "        3.0914e-01, 1.6240e+00, 4.2671e+00, 3.5989e-01, 3.7704e-01, 4.2037e-01,\n",
      "        3.8582e-01, 8.6820e-02, 6.7432e-01, 5.9593e-01, 1.4550e-01, 6.8814e-02,\n",
      "        4.4808e-01, 1.1582e+00, 1.1371e+00, 5.0356e+00, 3.8420e-02, 2.9112e-02,\n",
      "        3.8852e-01, 7.1975e-02, 5.3966e-02, 3.7465e-02, 2.7535e-01, 1.5804e-01,\n",
      "        7.4951e-02, 7.4197e-02, 1.4693e-05, 8.4167e-04, 2.8102e-05, 7.8436e-05,\n",
      "        4.8296e-05, 2.7306e-04, 1.1518e-04, 7.9597e-01, 5.6793e-01, 3.2991e+00,\n",
      "        2.2972e+00, 1.1337e+00, 1.2718e+00, 1.7031e-01, 3.0941e-01, 1.8855e-01,\n",
      "        3.7443e-01, 2.1539e+00, 1.4491e-01, 5.8656e-01, 1.7658e+00, 8.7219e-02,\n",
      "        1.8636e-01, 3.2372e-01, 4.4565e-01, 4.4011e-01, 2.1995e-01, 9.2549e-02,\n",
      "        2.1821e-01, 1.3533e-04, 4.0418e-07, 3.1609e-06, 2.1380e-04, 8.7065e-02,\n",
      "        2.1674e-01, 1.6913e-01, 4.6551e-01, 1.2782e-01, 1.9217e-01, 1.4857e-01,\n",
      "        7.7431e-01, 1.2580e-01, 4.0323e+01, 1.4555e+00, 1.5313e-01, 4.4681e-01,\n",
      "        2.2360e+00, 8.0190e-01, 1.6366e+00, 1.5488e+00, 4.3825e-01, 3.4113e+00,\n",
      "        8.3275e-02, 8.3828e-01, 4.4100e-01, 1.4860e-01, 4.8628e-01, 1.3527e-01,\n",
      "        5.4681e-02, 2.5085e-01, 9.9386e-02, 3.4061e-01, 1.4481e-04, 6.4800e-05,\n",
      "        6.4467e-05, 1.0788e-01, 4.1002e-01, 2.6058e+00, 3.1960e-02, 3.3937e+00,\n",
      "        3.1211e-02, 2.2968e+00, 5.0960e-01, 8.5096e-02, 9.9060e-07, 2.2615e-06,\n",
      "        3.5764e-07], device='cuda:0')\n",
      "Sig sum tensor(223., device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 4884.967892376681\n",
      "pruning 30 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :15 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     33\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    11\n",
      "     ║    ╠════╝\n",
      "     ║    24\n",
      "     ║    ╠════╗\n",
      "     ║    ║    6\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    23\n",
      "    ╠════╗\n",
      "    ║    16\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   19\n",
      "   ╠════╗\n",
      "   ║    12\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  32\n",
      "  ╠════╗\n",
      "  ║    12\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 35\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.6846e+05, 1.3476e+07, 7.7588e+05, 1.2445e+06, 1.4100e+06, 6.4156e+05,\n",
      "        3.1226e+04, 1.6069e+06, 2.1513e+05, 7.4832e+06, 5.8039e+05, 9.5804e+04,\n",
      "        3.5810e+05, 2.4921e+06, 1.5303e+05, 6.4311e+04, 3.4853e+06, 5.4827e+05,\n",
      "        1.0705e+07, 6.2160e+06, 1.3679e+06, 5.2750e+06, 1.8986e+05, 1.2433e+05,\n",
      "        8.2838e+05, 3.0281e+05, 2.9840e+05, 7.7756e+04, 1.4904e+05, 6.9770e+05,\n",
      "        2.5361e+05, 6.6162e+05, 2.1635e-02, 3.4439e-03, 8.3040e-05],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.6665, 0.6418, 0.8148, 0.3246, 0.2549, 0.4577, 0.3890, 0.4946, 0.2888,\n",
      "        0.8965, 0.4008, 0.3579, 0.6688, 0.5232, 0.4906, 0.3006, 0.4617, 0.2077,\n",
      "        0.8363, 0.5839, 0.5410, 0.7361, 0.3674, 0.4272, 0.4264, 0.3732, 0.3730,\n",
      "        0.6542, 0.4489, 0.4107, 0.0794, 0.1313, 0.0326, 0.0161, 0.0355],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.2470e+05, 1.9308e+07, 5.7467e+05, 3.3625e+06, 4.2022e+06, 1.3917e+06,\n",
      "        7.6323e+04, 3.2483e+06, 6.1203e+05, 3.0995e+06, 1.3910e+06, 2.4606e+05,\n",
      "        4.7444e+05, 4.7535e+06, 3.1182e+05, 1.7991e+05, 7.5047e+06, 1.7375e+06,\n",
      "        7.0109e+06, 1.0347e+07, 2.5117e+06, 5.5678e+06, 4.8046e+05, 2.8487e+05,\n",
      "        1.9005e+06, 7.5926e+05, 7.4836e+05, 1.0755e+05, 3.2854e+05, 1.6447e+06,\n",
      "        9.3392e+05, 2.2991e+06, 8.3721e-02, 1.3554e-02, 3.2035e-04],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([5.0877e+05, 3.3116e+06, 1.3335e+05, 6.5282e+05, 3.7701e+05, 3.5065e+05,\n",
      "        9.9984e+06, 2.4619e+06, 1.8989e+06, 3.2327e+04, 2.5966e+05, 2.5532e+06,\n",
      "        6.9341e+04, 2.3774e+06, 8.3230e+04, 1.0375e+06, 7.9220e+03, 5.7672e+06,\n",
      "        1.5706e+05, 5.6668e+06, 2.1867e+05, 9.0268e+05, 1.7372e+05, 1.2104e+06,\n",
      "        1.9204e+06, 4.5014e+06, 1.3426e+05, 1.9016e+06, 1.1304e+06, 3.1787e+05,\n",
      "        4.7838e+05, 5.4786e-02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8204, 0.7222, 0.4489, 0.7146, 0.6408, 0.7256, 0.8071, 0.5227, 0.6801,\n",
      "        0.5205, 0.6452, 0.1977, 0.6956, 0.4128, 0.5894, 0.6720, 0.4079, 0.8253,\n",
      "        0.6888, 0.3766, 0.8509, 0.3015, 0.4437, 0.4478, 0.6364, 0.5953, 0.4421,\n",
      "        0.3791, 0.4811, 0.2220, 0.2296, 0.0234], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.6545e+05, 3.6792e+06, 2.9396e+05, 7.4524e+05, 5.4176e+05, 3.8491e+05,\n",
      "        7.7146e+06, 4.6999e+06, 2.4295e+06, 6.2007e+04, 3.6851e+05, 8.1935e+06,\n",
      "        8.4436e+04, 5.5844e+06, 1.3670e+05, 1.3610e+06, 1.8762e+04, 4.0292e+06,\n",
      "        1.9552e+05, 1.4131e+07, 1.3039e+05, 2.5222e+06, 3.8657e+05, 2.6738e+06,\n",
      "        2.7933e+06, 7.2868e+06, 2.9963e+05, 4.7232e+06, 2.3463e+06, 9.8918e+05,\n",
      "        1.4741e+06, 2.1401e-01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.5722e+06, 2.8611e+05, 3.6565e+06, 6.4950e+05, 1.6898e+05, 1.9098e+07,\n",
      "        3.6933e+06, 1.4790e+06, 2.3323e+05, 7.9534e+05, 4.8656e-02, 3.2869e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.4792, 0.4467, 0.8423, 0.2098, 0.7165, 0.4178, 0.2246, 0.3416, 0.2107,\n",
      "        0.3307, 0.0548, 0.0399], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.3690e+07, 6.3325e+05, 2.3067e+06, 2.0530e+06, 1.9165e+05, 4.4480e+07,\n",
      "        1.1456e+07, 3.8951e+06, 7.3641e+05, 2.1294e+06, 1.8395e-01, 1.2623e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.7815e+07, 3.6863e+07, 9.5167e+06, 2.7011e+06, 1.3311e+07, 1.3605e+05,\n",
      "        5.8227e+06, 3.3675e+06, 1.3554e+06, 3.1617e+07, 1.8903e+06, 5.9402e+06,\n",
      "        5.1776e+05, 4.2607e+05, 9.4900e+05, 4.0675e+06, 2.8120e+05, 2.6574e+05,\n",
      "        5.5098e+00], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8390, 0.7343, 0.7464, 0.5992, 0.6236, 0.5815, 0.5341, 0.7765, 0.7774,\n",
      "        0.5115, 0.5385, 0.7974, 0.5131, 0.5373, 0.5173, 0.3062, 0.4641, 0.3305,\n",
      "        0.1027], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.1470e+07, 3.9173e+07, 9.6527e+06, 4.3301e+06, 2.0044e+07, 2.2775e+05,\n",
      "        1.0851e+07, 3.0101e+06, 1.2071e+06, 6.1786e+07, 3.4898e+06, 4.8129e+06,\n",
      "        1.0085e+06, 7.8858e+05, 1.8322e+06, 1.1288e+07, 6.0276e+05, 7.1167e+05,\n",
      "        1.9775e+01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.4399e+06, 4.2715e+06, 9.9220e+06, 3.0835e+06, 1.3453e+07, 1.0445e+06,\n",
      "        3.2427e+06, 1.2266e+06, 1.7135e+05, 1.8578e+05, 2.0716e+06, 1.9965e+05,\n",
      "        5.5017e+05, 3.2843e+05, 2.4909e-01, 2.1644e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1859, 0.5236, 0.4052, 0.7479, 0.4963, 0.2559, 0.4634, 0.2980, 0.1510,\n",
      "        0.2536, 0.3773, 0.1720, 0.0993, 0.1202, 0.0127, 0.0069],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.1201e+07, 8.1401e+06, 2.3608e+07, 3.1095e+06, 2.7105e+07, 3.1088e+06,\n",
      "        6.9598e+06, 3.4446e+06, 5.8189e+05, 5.5464e+05, 5.1602e+06, 6.6127e+05,\n",
      "        1.9822e+06, 1.1557e+06, 9.8369e-01, 8.5983e-01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.4226e+08, 2.8059e+06, 8.1306e+05, 5.1809e+07, 6.3371e+06, 5.9594e+05,\n",
      "        2.7302e+07, 4.5312e+06, 1.1467e+06, 2.2186e+05, 2.2418e+06, 3.2032e+06,\n",
      "        2.8827e+05, 3.8149e+05, 9.1177e+05, 3.4543e+06, 2.1461e+06, 1.0265e+07,\n",
      "        7.2820e+04, 1.6083e+05, 6.7846e+05, 1.1133e+05, 9.6582e+04, 6.9155e+04,\n",
      "        5.0070e+05, 3.2315e+05, 1.6793e+05, 2.2430e+05, 4.7883e+02, 6.8482e-01,\n",
      "        1.6973e+02, 4.9778e+02, 5.0176e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9778, 0.5205, 0.4965, 0.9582, 0.5040, 0.3390, 0.9858, 0.8693, 0.5818,\n",
      "        0.4621, 0.6576, 0.7278, 0.2768, 0.7756, 0.3737, 0.3417, 0.4457, 0.4663,\n",
      "        0.3427, 0.7905, 0.0837, 0.2270, 0.1924, 0.3983, 0.3205, 0.1724, 0.0998,\n",
      "        0.4196, 0.2840, 0.0082, 0.0458, 0.1214, 0.3383], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2634e+07, 5.3817e+06, 1.6374e+06, 8.6713e+06, 1.2573e+07, 1.5756e+06,\n",
      "        1.5558e+06, 2.3689e+06, 1.9181e+06, 4.7738e+05, 3.0706e+06, 3.4876e+06,\n",
      "        8.3390e+05, 3.4238e+05, 2.2840e+06, 9.0951e+06, 4.7587e+06, 2.1914e+07,\n",
      "        1.9145e+05, 1.3476e+05, 2.4866e+06, 3.4422e+05, 3.1199e+05, 1.6643e+05,\n",
      "        1.3609e+06, 1.0698e+06, 6.0470e+05, 5.2072e+05, 1.3715e+03, 2.7168e+00,\n",
      "        6.4783e+02, 1.7494e+03, 1.3280e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.3722e+06, 5.5510e+05, 1.0558e+07, 6.4511e+06, 1.7095e+06, 1.0275e+07,\n",
      "        7.2380e+05, 6.1431e+05, 3.2259e+05, 4.3217e+05, 3.9172e+06, 2.7902e+05,\n",
      "        1.5523e+06, 3.7510e+06, 1.5218e+05, 4.0861e+05, 6.9887e+05, 1.1212e+06,\n",
      "        6.7018e+05, 3.0249e+05, 1.5738e+05, 3.2667e+05, 3.4387e-01, 2.3331e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8514, 0.3866, 0.7896, 0.5318, 0.4047, 0.8586, 0.6609, 0.4947, 0.3049,\n",
      "        0.1164, 0.3330, 0.3127, 0.4339, 0.3152, 0.1529, 0.1727, 0.2506, 0.4317,\n",
      "        0.2858, 0.1223, 0.1871, 0.3127, 0.0740, 0.0131], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([3.7871e+06, 1.3620e+06, 8.8843e+06, 1.2081e+07, 4.0706e+06, 5.8124e+06,\n",
      "        9.8191e+05, 1.2416e+06, 8.9697e+05, 1.5274e+06, 1.0452e+07, 7.6711e+05,\n",
      "        3.5153e+06, 1.0275e+07, 5.1567e+05, 1.3521e+06, 2.0948e+06, 2.5487e+06,\n",
      "        1.9146e+06, 1.0619e+06, 5.1173e+05, 8.9805e+05, 1.2737e+00, 9.2101e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.3190e+05, 3.1280e+05, 3.0837e+05, 9.2918e+05, 2.1952e+05, 4.5041e+05,\n",
      "        2.6356e+05, 1.0902e+01, 1.7967e+00, 1.0606e+00, 4.4018e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1234, 0.2281, 0.3025, 0.3105, 0.1166, 0.3521, 0.1638, 0.0838, 0.0074,\n",
      "        0.0211, 0.0294], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.6250e+05, 9.6574e+05, 8.6040e+05, 2.5629e+06, 7.7572e+05, 1.1672e+06,\n",
      "        8.8162e+05, 3.9952e+01, 7.1338e+00, 4.1529e+00, 1.7089e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.6399e+07, 2.2143e+06, 1.7236e+08, 4.7143e+06, 8.6314e+06, 1.3600e+07,\n",
      "        1.0055e+07, 2.2428e+06, 7.9167e+06, 2.3796e+06, 5.2729e+05, 1.2937e+07,\n",
      "        1.8044e+05, 2.2564e+06, 7.0322e+05, 2.2014e+05, 5.8739e+05, 2.6878e+05,\n",
      "        6.6983e+04, 1.7779e-01, 1.1414e+01, 9.6887e-01, 8.9572e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8209, 0.9150, 0.6949, 0.5129, 0.9508, 0.9650, 0.7535, 0.5458, 0.8078,\n",
      "        0.3049, 0.2175, 0.6508, 0.3250, 0.5836, 0.2717, 0.1363, 0.1089, 0.2623,\n",
      "        0.1905, 0.0454, 0.0567, 0.0817, 0.0169], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.1747e+07, 7.5285e+05, 2.1032e+08, 9.1847e+06, 1.6996e+06, 1.9042e+06,\n",
      "        9.9150e+06, 4.0745e+06, 6.0856e+06, 6.6162e+06, 1.6503e+06, 1.8071e+07,\n",
      "        4.8716e+05, 3.7585e+06, 2.0487e+06, 7.6052e+05, 2.0937e+06, 7.9310e+05,\n",
      "        2.1688e+05, 6.7889e-01, 4.3070e+01, 3.5588e+00, 3.5224e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([4.0927e+05, 1.8221e+05, 6.7036e+05, 1.4389e+01, 5.0474e+00, 7.6242e+01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2360, 0.2267, 0.2147, 0.0169, 0.0129, 0.1526], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2507e+06, 5.6364e+05, 2.1059e+06, 5.6584e+01, 1.9929e+01, 2.5844e+02],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.0230e+05, 9.3213e+05, 8.3999e+06, 9.1113e+04, 1.0066e+07, 3.9563e+04,\n",
      "        3.6474e+06, 7.4412e+05, 1.0634e+05, 7.7758e-03, 1.1595e-01, 1.8221e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3503, 0.5578, 0.5999, 0.5177, 0.4696, 0.3578, 0.3688, 0.3289, 0.1655,\n",
      "        0.0485, 0.0569, 0.0815], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.2572e+05, 1.6487e+06, 1.3445e+07, 1.7579e+05, 2.1358e+07, 1.0163e+05,\n",
      "        9.2094e+06, 1.9974e+06, 3.5498e+05, 2.9595e-02, 4.3741e-01, 6.6945e-01],\n",
      "       device='cuda:0')\n",
      "All_sigs tensor([2.2470e+05, 1.9308e+07, 5.7467e+05, 3.3625e+06, 4.2022e+06, 1.3917e+06,\n",
      "        7.6323e+04, 3.2483e+06, 6.1203e+05, 3.0995e+06, 1.3910e+06, 2.4606e+05,\n",
      "        4.7444e+05, 4.7535e+06, 3.1182e+05, 1.7991e+05, 7.5047e+06, 1.7375e+06,\n",
      "        7.0109e+06, 1.0347e+07, 2.5117e+06, 5.5678e+06, 4.8046e+05, 2.8487e+05,\n",
      "        1.9005e+06, 7.5926e+05, 7.4836e+05, 1.0755e+05, 3.2854e+05, 1.6447e+06,\n",
      "        9.3392e+05, 2.2991e+06, 8.3721e-02, 1.3554e-02, 3.2035e-04, 3.6545e+05,\n",
      "        3.6792e+06, 2.9396e+05, 7.4524e+05, 5.4176e+05, 3.8491e+05, 7.7146e+06,\n",
      "        4.6999e+06, 2.4295e+06, 6.2007e+04, 3.6851e+05, 8.1935e+06, 8.4436e+04,\n",
      "        5.5844e+06, 1.3670e+05, 1.3610e+06, 1.8762e+04, 4.0292e+06, 1.9552e+05,\n",
      "        1.4131e+07, 1.3039e+05, 2.5222e+06, 3.8657e+05, 2.6738e+06, 2.7933e+06,\n",
      "        7.2868e+06, 2.9963e+05, 4.7232e+06, 2.3463e+06, 9.8918e+05, 1.4741e+06,\n",
      "        2.1401e-01, 1.3690e+07, 6.3325e+05, 2.3067e+06, 2.0530e+06, 1.9165e+05,\n",
      "        4.4480e+07, 1.1456e+07, 3.8951e+06, 7.3641e+05, 2.1294e+06, 1.8395e-01,\n",
      "        1.2623e+00, 1.1470e+07, 3.9173e+07, 9.6527e+06, 4.3301e+06, 2.0044e+07,\n",
      "        2.2775e+05, 1.0851e+07, 3.0101e+06, 1.2071e+06, 6.1786e+07, 3.4898e+06,\n",
      "        4.8129e+06, 1.0085e+06, 7.8858e+05, 1.8322e+06, 1.1288e+07, 6.0276e+05,\n",
      "        7.1167e+05, 1.9775e+01, 1.1201e+07, 8.1401e+06, 2.3608e+07, 3.1095e+06,\n",
      "        2.7105e+07, 3.1088e+06, 6.9598e+06, 3.4446e+06, 5.8189e+05, 5.5464e+05,\n",
      "        5.1602e+06, 6.6127e+05, 1.9822e+06, 1.1557e+06, 9.8369e-01, 8.5983e-01,\n",
      "        1.2634e+07, 5.3817e+06, 1.6374e+06, 8.6713e+06, 1.2573e+07, 1.5756e+06,\n",
      "        1.5558e+06, 2.3689e+06, 1.9181e+06, 4.7738e+05, 3.0706e+06, 3.4876e+06,\n",
      "        8.3390e+05, 3.4238e+05, 2.2840e+06, 9.0951e+06, 4.7587e+06, 2.1914e+07,\n",
      "        1.9145e+05, 1.3476e+05, 2.4866e+06, 3.4422e+05, 3.1199e+05, 1.6643e+05,\n",
      "        1.3609e+06, 1.0698e+06, 6.0470e+05, 5.2072e+05, 1.3715e+03, 2.7168e+00,\n",
      "        6.4783e+02, 1.7494e+03, 1.3280e+02, 3.7871e+06, 1.3620e+06, 8.8843e+06,\n",
      "        1.2081e+07, 4.0706e+06, 5.8124e+06, 9.8191e+05, 1.2416e+06, 8.9697e+05,\n",
      "        1.5274e+06, 1.0452e+07, 7.6711e+05, 3.5153e+06, 1.0275e+07, 5.1567e+05,\n",
      "        1.3521e+06, 2.0948e+06, 2.5487e+06, 1.9146e+06, 1.0619e+06, 5.1173e+05,\n",
      "        8.9805e+05, 1.2737e+00, 9.2101e+00, 4.6250e+05, 9.6574e+05, 8.6040e+05,\n",
      "        2.5629e+06, 7.7572e+05, 1.1672e+06, 8.8162e+05, 3.9952e+01, 7.1338e+00,\n",
      "        4.1529e+00, 1.7089e+00, 1.1747e+07, 7.5285e+05, 2.1032e+08, 9.1847e+06,\n",
      "        1.6996e+06, 1.9042e+06, 9.9150e+06, 4.0745e+06, 6.0856e+06, 6.6162e+06,\n",
      "        1.6503e+06, 1.8071e+07, 4.8716e+05, 3.7585e+06, 2.0487e+06, 7.6052e+05,\n",
      "        2.0937e+06, 7.9310e+05, 2.1688e+05, 6.7889e-01, 4.3070e+01, 3.5588e+00,\n",
      "        3.5224e+01, 1.2507e+06, 5.6364e+05, 2.1059e+06, 5.6584e+01, 1.9929e+01,\n",
      "        2.5844e+02, 5.2572e+05, 1.6487e+06, 1.3445e+07, 1.7579e+05, 2.1358e+07,\n",
      "        1.0163e+05, 9.2094e+06, 1.9974e+06, 3.5498e+05, 2.9595e-02, 4.3741e-01,\n",
      "        6.6945e-01], device='cuda:0')\n",
      "All_sig tensor([4.7101e-02, 4.0474e+00, 1.2046e-01, 7.0484e-01, 8.8087e-01, 2.9172e-01,\n",
      "        1.5999e-02, 6.8091e-01, 1.2830e-01, 6.4971e-01, 2.9158e-01, 5.1580e-02,\n",
      "        9.9453e-02, 9.9643e-01, 6.5364e-02, 3.7712e-02, 1.5731e+00, 3.6422e-01,\n",
      "        1.4696e+00, 2.1690e+00, 5.2650e-01, 1.1671e+00, 1.0071e-01, 5.9716e-02,\n",
      "        3.9838e-01, 1.5916e-01, 1.5687e-01, 2.2544e-02, 6.8869e-02, 3.4477e-01,\n",
      "        1.9577e-01, 4.8195e-01, 1.7550e-08, 2.8411e-09, 6.7153e-11, 7.6606e-02,\n",
      "        7.7124e-01, 6.1619e-02, 1.5622e-01, 1.1356e-01, 8.0685e-02, 1.6171e+00,\n",
      "        9.8521e-01, 5.0927e-01, 1.2998e-02, 7.7247e-02, 1.7175e+00, 1.7700e-02,\n",
      "        1.1706e+00, 2.8654e-02, 2.8530e-01, 3.9328e-03, 8.4461e-01, 4.0985e-02,\n",
      "        2.9622e+00, 2.7332e-02, 5.2870e-01, 8.1034e-02, 5.6048e-01, 5.8555e-01,\n",
      "        1.5275e+00, 6.2810e-02, 9.9009e-01, 4.9184e-01, 2.0735e-01, 3.0901e-01,\n",
      "        4.4862e-08, 2.8697e+00, 1.3274e-01, 4.8353e-01, 4.3035e-01, 4.0173e-02,\n",
      "        9.3239e+00, 2.4013e+00, 8.1650e-01, 1.5437e-01, 4.4637e-01, 3.8560e-08,\n",
      "        2.6461e-07, 2.4044e+00, 8.2116e+00, 2.0234e+00, 9.0769e-01, 4.2017e+00,\n",
      "        4.7741e-02, 2.2746e+00, 6.3099e-01, 2.5304e-01, 1.2952e+01, 7.3154e-01,\n",
      "        1.0089e+00, 2.1140e-01, 1.6530e-01, 3.8408e-01, 2.3662e+00, 1.2635e-01,\n",
      "        1.4918e-01, 4.1454e-06, 2.3481e+00, 1.7063e+00, 4.9487e+00, 6.5182e-01,\n",
      "        5.6819e+00, 6.5167e-01, 1.4589e+00, 7.2207e-01, 1.2198e-01, 1.1626e-01,\n",
      "        1.0817e+00, 1.3862e-01, 4.1552e-01, 2.4227e-01, 2.0620e-07, 1.8024e-07,\n",
      "        2.6485e+00, 1.1281e+00, 3.4323e-01, 1.8177e+00, 2.6357e+00, 3.3028e-01,\n",
      "        3.2614e-01, 4.9658e-01, 4.0208e-01, 1.0007e-01, 6.4367e-01, 7.3107e-01,\n",
      "        1.7480e-01, 7.1771e-02, 4.7877e-01, 1.9065e+00, 9.9753e-01, 4.5936e+00,\n",
      "        4.0132e-02, 2.8249e-02, 5.2124e-01, 7.2157e-02, 6.5399e-02, 3.4887e-02,\n",
      "        2.8528e-01, 2.2425e-01, 1.2676e-01, 1.0915e-01, 2.8749e-04, 5.6949e-07,\n",
      "        1.3580e-04, 3.6672e-04, 2.7838e-05, 7.9386e-01, 2.8550e-01, 1.8623e+00,\n",
      "        2.5324e+00, 8.5329e-01, 1.2184e+00, 2.0583e-01, 2.6026e-01, 1.8803e-01,\n",
      "        3.2017e-01, 2.1909e+00, 1.6080e-01, 7.3688e-01, 2.1538e+00, 1.0810e-01,\n",
      "        2.8344e-01, 4.3912e-01, 5.3427e-01, 4.0133e-01, 2.2261e-01, 1.0727e-01,\n",
      "        1.8825e-01, 2.6699e-07, 1.9306e-06, 9.6951e-02, 2.0244e-01, 1.8036e-01,\n",
      "        5.3723e-01, 1.6261e-01, 2.4467e-01, 1.8481e-01, 8.3748e-06, 1.4954e-06,\n",
      "        8.7053e-07, 3.5822e-07, 2.4623e+00, 1.5781e-01, 4.4088e+01, 1.9253e+00,\n",
      "        3.5626e-01, 3.9917e-01, 2.0784e+00, 8.5411e-01, 1.2757e+00, 1.3869e+00,\n",
      "        3.4595e-01, 3.7881e+00, 1.0212e-01, 7.8785e-01, 4.2945e-01, 1.5942e-01,\n",
      "        4.3889e-01, 1.6625e-01, 4.5463e-02, 1.4231e-07, 9.0285e-06, 7.4601e-07,\n",
      "        7.3836e-06, 2.6218e-01, 1.1815e-01, 4.4143e-01, 1.1861e-05, 4.1775e-06,\n",
      "        5.4174e-05, 1.1020e-01, 3.4560e-01, 2.8183e+00, 3.6850e-02, 4.4770e+00,\n",
      "        2.1303e-02, 1.9305e+00, 4.1870e-01, 7.4412e-02, 6.2038e-09, 9.1691e-08,\n",
      "        1.4033e-07], device='cuda:0')\n",
      "Sig sum tensor(223., device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 4770.496860986547\n",
      "pruning 30 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :16 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     35\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    9\n",
      "     ║    ╠════╝\n",
      "     ║    25\n",
      "     ║    ╠════╗\n",
      "     ║    ║    6\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    21\n",
      "    ╠════╗\n",
      "    ║    15\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   20\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  35\n",
      "  ╠════╗\n",
      "  ║    12\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 35\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.7111e+05, 1.4732e+07, 8.6418e+05, 1.2013e+06, 1.3447e+06, 5.7249e+05,\n",
      "        3.0420e+04, 1.6898e+06, 2.0421e+05, 8.4721e+06, 4.9984e+05, 9.8061e+04,\n",
      "        3.8738e+05, 2.3751e+06, 1.6435e+05, 8.3970e+04, 4.1937e+06, 6.1340e+05,\n",
      "        1.0761e+07, 6.9728e+06, 1.5130e+06, 4.8829e+06, 1.7594e+05, 1.3508e+05,\n",
      "        7.2660e+05, 4.1216e+05, 2.7679e+05, 8.3454e+04, 1.5173e+05, 7.0999e+05,\n",
      "        3.1082e+05, 1.0256e+06, 1.0272e-02, 3.1495e-03, 9.8695e-03],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.6949, 0.6492, 0.8297, 0.3090, 0.2177, 0.4129, 0.3664, 0.4942, 0.2631,\n",
      "        0.8953, 0.3520, 0.3615, 0.6760, 0.5021, 0.4864, 0.3334, 0.4701, 0.2086,\n",
      "        0.8110, 0.5940, 0.5428, 0.7349, 0.3625, 0.4411, 0.4046, 0.4052, 0.3305,\n",
      "        0.6805, 0.4295, 0.3861, 0.0874, 0.1710, 0.0421, 0.0572, 0.1255],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.0880e+05, 2.0674e+07, 5.8876e+05, 3.3207e+06, 4.2078e+06, 1.3444e+06,\n",
      "        7.7091e+04, 3.4191e+06, 6.0192e+05, 3.5468e+06, 1.2956e+06, 2.5044e+05,\n",
      "        5.0203e+05, 4.7304e+06, 3.3763e+05, 2.2390e+05, 8.8889e+06, 1.9418e+06,\n",
      "        8.1344e+06, 1.1323e+07, 2.7669e+06, 5.1785e+06, 4.4866e+05, 3.0199e+05,\n",
      "        1.7304e+06, 9.8058e+05, 7.4124e+05, 1.0667e+05, 3.4623e+05, 1.7435e+06,\n",
      "        1.1346e+06, 3.4009e+06, 3.9358e-02, 1.1878e-02, 3.4524e-02],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.6504e+05, 3.6164e+06, 1.7779e+05, 5.5129e+05, 3.8483e+05, 2.8004e+05,\n",
      "        1.1442e+07, 2.7457e+06, 1.8432e+06, 4.2351e+04, 3.6177e+05, 2.4576e+06,\n",
      "        1.0236e+05, 2.4178e+06, 7.3883e+04, 1.0047e+06, 9.7333e+03, 5.2649e+06,\n",
      "        2.2659e+05, 5.4753e+06, 2.9740e+05, 1.1685e+06, 2.0056e+05, 1.8544e+06,\n",
      "        2.7419e+06, 4.2404e+06, 1.3072e+05, 2.1358e+06, 9.8136e+05, 2.7292e+05,\n",
      "        5.2499e+05, 7.8221e-02, 1.7356e-03, 1.6655e-01, 1.4780e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8090, 0.7006, 0.4589, 0.7051, 0.6191, 0.6975, 0.7863, 0.5238, 0.6916,\n",
      "        0.5466, 0.6619, 0.2026, 0.6869, 0.4002, 0.6114, 0.6377, 0.4450, 0.7797,\n",
      "        0.6831, 0.3620, 0.8217, 0.3134, 0.4543, 0.4572, 0.6761, 0.5606, 0.4354,\n",
      "        0.3733, 0.4889, 0.2153, 0.2247, 0.0610, 0.0300, 0.0255, 0.0794],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.0821e+05, 4.3310e+06, 3.8484e+05, 6.5029e+05, 5.8638e+05, 3.3884e+05,\n",
      "        9.7811e+06, 5.2297e+06, 2.2734e+06, 7.6807e+04, 4.8927e+05, 7.8382e+06,\n",
      "        1.2819e+05, 5.8003e+06, 1.1484e+05, 1.4560e+06, 2.1607e+04, 4.6403e+06,\n",
      "        2.8719e+05, 1.3973e+07, 2.1206e+05, 3.2095e+06, 4.3775e+05, 4.0259e+06,\n",
      "        3.5525e+06, 7.4522e+06, 2.9524e+05, 5.3542e+06, 2.0065e+06, 8.5666e+05,\n",
      "        1.6281e+06, 2.9380e-01, 6.7339e-03, 6.4921e-01, 5.4428e-01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([10016466.0000,   287773.5312,  4566220.5000,   595986.6250,\n",
      "          210827.9844, 20034796.0000,  3841043.0000,  1578012.5000,\n",
      "          319030.1250,   616190.0625], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5430, 0.4352, 0.8322, 0.1892, 0.7135, 0.4138, 0.2029, 0.3209, 0.2182,\n",
      "        0.3070], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([18308890.0000,   650180.1250,  3065522.2500,  1932979.7500,\n",
      "          241610.6094, 46976564.0000, 12246422.0000,  4286287.0000,\n",
      "          997670.0000,  1708015.2500], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.5811e+07, 3.9678e+07, 1.0675e+07, 3.7379e+06, 1.3340e+07, 1.7556e+05,\n",
      "        5.0709e+06, 4.3496e+06, 1.5290e+06, 3.7257e+07, 1.9845e+06, 7.1580e+06,\n",
      "        3.5521e+05, 4.2690e+05, 1.6781e+06, 4.6105e+06, 2.4562e+05, 4.0952e+05,\n",
      "        4.6796e-01, 6.9210e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8202, 0.7467, 0.7287, 0.5988, 0.6257, 0.5863, 0.4502, 0.7644, 0.7759,\n",
      "        0.4968, 0.5274, 0.8248, 0.5171, 0.5320, 0.5469, 0.3291, 0.4928, 0.3556,\n",
      "        0.0277, 0.0686], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.1370e+07, 4.0206e+07, 1.1584e+07, 5.9986e+06, 1.9971e+07, 2.9049e+05,\n",
      "        1.1151e+07, 4.0988e+06, 1.3709e+06, 7.4994e+07, 3.7514e+06, 5.0151e+06,\n",
      "        6.8605e+05, 7.9910e+05, 3.0417e+06, 1.2372e+07, 4.9828e+05, 1.0555e+06,\n",
      "        1.8201e+00, 2.5785e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.9958e+06, 4.8194e+06, 6.3614e+06, 3.4684e+06, 1.6972e+07, 1.4296e+06,\n",
      "        3.6508e+06, 1.3217e+06, 1.6454e+05, 2.0688e+05, 2.2871e+06, 2.6549e+05,\n",
      "        5.7896e+05, 3.4297e+05, 1.5795e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1951, 0.5274, 0.3863, 0.7310, 0.4886, 0.2756, 0.4687, 0.2493, 0.1348,\n",
      "        0.2513, 0.3765, 0.1863, 0.1181, 0.1089, 0.0502], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.2865e+07, 9.1102e+06, 1.5616e+07, 3.7316e+06, 3.4721e+07, 4.1425e+06,\n",
      "        7.7594e+06, 3.9688e+06, 5.6941e+05, 6.1958e+05, 5.7041e+06, 8.6413e+05,\n",
      "        2.0423e+06, 1.2224e+06, 6.0007e+01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.4738e+08, 4.1700e+06, 8.8091e+05, 5.0465e+07, 8.9115e+06, 7.0778e+05,\n",
      "        3.1750e+07, 4.4196e+06, 1.1745e+06, 3.0502e+05, 2.4344e+06, 3.3482e+06,\n",
      "        3.5604e+05, 3.9804e+05, 1.0924e+06, 3.8039e+06, 1.3537e+06, 1.1005e+07,\n",
      "        7.9370e+04, 2.2057e+05, 5.1947e+05, 1.4456e+05, 1.0603e+05, 9.4776e+04,\n",
      "        5.1472e+05, 4.9443e+05, 1.5049e+05, 3.0875e+05, 2.6156e+01, 1.6530e+02,\n",
      "        4.4922e+02, 2.9013e+02, 4.7852e+01, 7.2988e+01, 9.6258e+01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9776, 0.5206, 0.4961, 0.9572, 0.5037, 0.3541, 0.9852, 0.8684, 0.5862,\n",
      "        0.4620, 0.6581, 0.7252, 0.2765, 0.7809, 0.3748, 0.3412, 0.4478, 0.4668,\n",
      "        0.3397, 0.7895, 0.0806, 0.2249, 0.1910, 0.4005, 0.3289, 0.1760, 0.1003,\n",
      "        0.4200, 0.0292, 0.1363, 0.4696, 0.0565, 0.0511, 0.3034, 0.4842],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.3206e+07, 7.9963e+06, 1.7756e+06, 8.6413e+06, 1.7692e+07, 1.8286e+06,\n",
      "        1.8737e+06, 2.3267e+06, 1.9439e+06, 6.5646e+05, 3.3296e+06, 3.6809e+06,\n",
      "        1.0304e+06, 3.4882e+05, 2.7320e+06, 1.0024e+07, 2.9902e+06, 2.3474e+07,\n",
      "        2.0965e+05, 1.8570e+05, 1.9103e+06, 4.4820e+05, 3.4311e+05, 2.2729e+05,\n",
      "        1.3817e+06, 1.6297e+06, 5.4160e+05, 7.1630e+05, 1.0158e+02, 5.7111e+02,\n",
      "        9.5301e+02, 1.0949e+03, 1.8163e+02, 2.0336e+02, 1.9860e+02],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.9553e+06, 1.4168e+06, 1.6912e+07, 7.3187e+06, 1.7889e+06, 9.5577e+06,\n",
      "        9.5469e+05, 7.6275e+05, 3.7410e+05, 4.8570e+05, 4.4364e+06, 2.5771e+05,\n",
      "        1.1565e+06, 3.8500e+06, 1.4967e+05, 3.7759e+05, 7.9099e+05, 1.4329e+06,\n",
      "        5.2647e+05, 3.7028e+05, 2.0633e+05, 3.0508e+05, 1.7167e+01, 2.2239e+01,\n",
      "        6.7273e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8415, 0.3928, 0.7563, 0.5331, 0.3980, 0.8632, 0.6564, 0.3380, 0.3479,\n",
      "        0.1259, 0.3394, 0.2826, 0.4373, 0.2941, 0.1584, 0.1677, 0.2655, 0.6374,\n",
      "        0.2595, 0.1235, 0.1903, 0.3038, 0.0334, 0.0421, 0.0432],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.4089e+06, 3.4411e+06, 1.6486e+07, 1.3667e+07, 4.3078e+06, 5.2300e+06,\n",
      "        1.3121e+06, 2.0199e+06, 9.7587e+05, 1.6983e+06, 1.1724e+07, 7.3948e+05,\n",
      "        2.6031e+06, 1.0871e+07, 5.0383e+05, 1.2572e+06, 2.3241e+06, 2.0780e+06,\n",
      "        1.5594e+06, 1.2982e+06, 6.6831e+05, 8.4956e+05, 6.6374e+01, 8.5210e+01,\n",
      "        2.5747e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.7328e+05, 3.9736e+05, 3.1683e+05, 9.0838e+05, 3.3921e+05, 5.1990e+05,\n",
      "        4.1076e+05, 4.9815e-01, 2.0971e+00], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1450, 0.2150, 0.3145, 0.2898, 0.1271, 0.3565, 0.1863, 0.0309, 0.0491],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.9261e+05, 1.2476e+06, 8.6875e+05, 2.5804e+06, 1.1844e+06, 1.3382e+06,\n",
      "        1.3369e+06, 1.9310e+00, 7.9769e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.2987e+07, 2.2904e+06, 1.8518e+08, 6.6248e+06, 4.6853e+06, 1.5457e+07,\n",
      "        9.9305e+06, 2.3269e+06, 9.4996e+06, 2.6111e+06, 6.3747e+05, 1.4176e+07,\n",
      "        1.8312e+05, 2.3856e+06, 7.2667e+05, 3.0085e+05, 4.8548e+05, 3.1291e+05,\n",
      "        2.0447e+05, 1.1589e+01, 3.8956e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9218, 0.9116, 0.7112, 0.5380, 0.9347, 0.9638, 0.7424, 0.5283, 0.7977,\n",
      "        0.3220, 0.2263, 0.6765, 0.3408, 0.5654, 0.3045, 0.1455, 0.1003, 0.2716,\n",
      "        0.2327, 0.0535, 0.0371], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.0646e+06, 8.1012e+05, 2.1395e+08, 1.2243e+07, 1.2246e+06, 2.2354e+06,\n",
      "        1.0232e+07, 4.3902e+06, 7.6854e+06, 7.0813e+06, 1.9728e+06, 1.8341e+07,\n",
      "        4.8288e+05, 4.1469e+06, 2.0217e+06, 1.0283e+06, 1.7471e+06, 9.1171e+05,\n",
      "        6.2755e+05, 4.3874e+01, 1.5004e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([5.0740e+05, 1.8378e+05, 8.4128e+05, 2.9273e+00, 5.5511e+00, 3.1742e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2326, 0.2125, 0.2980, 0.0397, 0.0523, 0.0343], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.5574e+06, 5.7888e+05, 2.3623e+06, 1.1244e+01, 2.1043e+01, 1.2261e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.4184e+05, 1.4391e+06, 7.5974e+06, 1.3470e+05, 8.9734e+06, 5.3267e+04,\n",
      "        4.2127e+06, 6.3985e+05, 1.1630e+05, 4.8784e-03, 1.0915e+00, 5.6432e-03],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3640, 0.5868, 0.5946, 0.5229, 0.4326, 0.3924, 0.3653, 0.3111, 0.1721,\n",
      "        0.0092, 0.0253, 0.0307], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([6.1523e+05, 2.3786e+06, 1.2320e+07, 2.5706e+05, 2.0365e+07, 1.2947e+05,\n",
      "        1.0694e+07, 1.7632e+06, 3.8513e+05, 1.9334e-02, 4.2558e+00, 2.1881e-02],\n",
      "       device='cuda:0')\n",
      "All_sigs tensor([2.0880e+05, 2.0674e+07, 5.8876e+05, 3.3207e+06, 4.2078e+06, 1.3444e+06,\n",
      "        7.7091e+04, 3.4191e+06, 6.0192e+05, 3.5468e+06, 1.2956e+06, 2.5044e+05,\n",
      "        5.0203e+05, 4.7304e+06, 3.3763e+05, 2.2390e+05, 8.8889e+06, 1.9418e+06,\n",
      "        8.1344e+06, 1.1323e+07, 2.7669e+06, 5.1785e+06, 4.4866e+05, 3.0199e+05,\n",
      "        1.7304e+06, 9.8058e+05, 7.4124e+05, 1.0667e+05, 3.4623e+05, 1.7435e+06,\n",
      "        1.1346e+06, 3.4009e+06, 3.9358e-02, 1.1878e-02, 3.4524e-02, 5.0821e+05,\n",
      "        4.3310e+06, 3.8484e+05, 6.5029e+05, 5.8638e+05, 3.3884e+05, 9.7811e+06,\n",
      "        5.2297e+06, 2.2734e+06, 7.6807e+04, 4.8927e+05, 7.8382e+06, 1.2819e+05,\n",
      "        5.8003e+06, 1.1484e+05, 1.4560e+06, 2.1607e+04, 4.6403e+06, 2.8719e+05,\n",
      "        1.3973e+07, 2.1206e+05, 3.2095e+06, 4.3775e+05, 4.0259e+06, 3.5525e+06,\n",
      "        7.4522e+06, 2.9524e+05, 5.3542e+06, 2.0065e+06, 8.5666e+05, 1.6281e+06,\n",
      "        2.9380e-01, 6.7339e-03, 6.4921e-01, 5.4428e-01, 1.8309e+07, 6.5018e+05,\n",
      "        3.0655e+06, 1.9330e+06, 2.4161e+05, 4.6977e+07, 1.2246e+07, 4.2863e+06,\n",
      "        9.9767e+05, 1.7080e+06, 1.1370e+07, 4.0206e+07, 1.1584e+07, 5.9986e+06,\n",
      "        1.9971e+07, 2.9049e+05, 1.1151e+07, 4.0988e+06, 1.3709e+06, 7.4994e+07,\n",
      "        3.7514e+06, 5.0151e+06, 6.8605e+05, 7.9910e+05, 3.0417e+06, 1.2372e+07,\n",
      "        4.9828e+05, 1.0555e+06, 1.8201e+00, 2.5785e+00, 1.2865e+07, 9.1102e+06,\n",
      "        1.5616e+07, 3.7316e+06, 3.4721e+07, 4.1425e+06, 7.7594e+06, 3.9688e+06,\n",
      "        5.6941e+05, 6.1958e+05, 5.7041e+06, 8.6413e+05, 2.0423e+06, 1.2224e+06,\n",
      "        6.0007e+01, 1.3206e+07, 7.9963e+06, 1.7756e+06, 8.6413e+06, 1.7692e+07,\n",
      "        1.8286e+06, 1.8737e+06, 2.3267e+06, 1.9439e+06, 6.5646e+05, 3.3296e+06,\n",
      "        3.6809e+06, 1.0304e+06, 3.4882e+05, 2.7320e+06, 1.0024e+07, 2.9902e+06,\n",
      "        2.3474e+07, 2.0965e+05, 1.8570e+05, 1.9103e+06, 4.4820e+05, 3.4311e+05,\n",
      "        2.2729e+05, 1.3817e+06, 1.6297e+06, 5.4160e+05, 7.1630e+05, 1.0158e+02,\n",
      "        5.7111e+02, 9.5301e+02, 1.0949e+03, 1.8163e+02, 2.0336e+02, 1.9860e+02,\n",
      "        4.4089e+06, 3.4411e+06, 1.6486e+07, 1.3667e+07, 4.3078e+06, 5.2300e+06,\n",
      "        1.3121e+06, 2.0199e+06, 9.7587e+05, 1.6983e+06, 1.1724e+07, 7.3948e+05,\n",
      "        2.6031e+06, 1.0871e+07, 5.0383e+05, 1.2572e+06, 2.3241e+06, 2.0780e+06,\n",
      "        1.5594e+06, 1.2982e+06, 6.6831e+05, 8.4956e+05, 6.6374e+01, 8.5210e+01,\n",
      "        2.5747e+00, 5.9261e+05, 1.2476e+06, 8.6875e+05, 2.5804e+06, 1.1844e+06,\n",
      "        1.3382e+06, 1.3369e+06, 1.9310e+00, 7.9769e+00, 4.0646e+06, 8.1012e+05,\n",
      "        2.1395e+08, 1.2243e+07, 1.2246e+06, 2.2354e+06, 1.0232e+07, 4.3902e+06,\n",
      "        7.6854e+06, 7.0813e+06, 1.9728e+06, 1.8341e+07, 4.8288e+05, 4.1469e+06,\n",
      "        2.0217e+06, 1.0283e+06, 1.7471e+06, 9.1171e+05, 6.2755e+05, 4.3874e+01,\n",
      "        1.5004e+02, 1.5574e+06, 5.7888e+05, 2.3623e+06, 1.1244e+01, 2.1043e+01,\n",
      "        1.2261e+01, 6.1523e+05, 2.3786e+06, 1.2320e+07, 2.5706e+05, 2.0365e+07,\n",
      "        1.2947e+05, 1.0694e+07, 1.7632e+06, 3.8513e+05, 1.9334e-02, 4.2558e+00,\n",
      "        2.1881e-02], device='cuda:0')\n",
      "All_sig tensor([4.0668e-02, 4.0265e+00, 1.1467e-01, 6.4676e-01, 8.1954e-01, 2.6185e-01,\n",
      "        1.5015e-02, 6.6593e-01, 1.1723e-01, 6.9079e-01, 2.5234e-01, 4.8777e-02,\n",
      "        9.7778e-02, 9.2133e-01, 6.5758e-02, 4.3608e-02, 1.7313e+00, 3.7820e-01,\n",
      "        1.5843e+00, 2.2054e+00, 5.3890e-01, 1.0086e+00, 8.7384e-02, 5.8817e-02,\n",
      "        3.3703e-01, 1.9098e-01, 1.4437e-01, 2.0776e-02, 6.7434e-02, 3.3958e-01,\n",
      "        2.2099e-01, 6.6238e-01, 7.6657e-09, 2.3134e-09, 6.7242e-09, 9.8983e-02,\n",
      "        8.4354e-01, 7.4953e-02, 1.2666e-01, 1.1421e-01, 6.5995e-02, 1.9050e+00,\n",
      "        1.0186e+00, 4.4279e-01, 1.4959e-02, 9.5293e-02, 1.5266e+00, 2.4967e-02,\n",
      "        1.1297e+00, 2.2367e-02, 2.8358e-01, 4.2083e-03, 9.0378e-01, 5.5934e-02,\n",
      "        2.7215e+00, 4.1301e-02, 6.2510e-01, 8.5260e-02, 7.8411e-01, 6.9191e-01,\n",
      "        1.4514e+00, 5.7503e-02, 1.0428e+00, 3.9079e-01, 1.6685e-01, 3.1710e-01,\n",
      "        5.7223e-08, 1.3115e-09, 1.2644e-07, 1.0601e-07, 3.5660e+00, 1.2663e-01,\n",
      "        5.9706e-01, 3.7648e-01, 4.7058e-02, 9.1495e+00, 2.3852e+00, 8.3482e-01,\n",
      "        1.9431e-01, 3.3266e-01, 2.2145e+00, 7.8307e+00, 2.2563e+00, 1.1683e+00,\n",
      "        3.8897e+00, 5.6578e-02, 2.1719e+00, 7.9831e-01, 2.6700e-01, 1.4606e+01,\n",
      "        7.3064e-01, 9.7677e-01, 1.3362e-01, 1.5564e-01, 5.9243e-01, 2.4096e+00,\n",
      "        9.7048e-02, 2.0558e-01, 3.5449e-07, 5.0221e-07, 2.5056e+00, 1.7744e+00,\n",
      "        3.0414e+00, 7.2680e-01, 6.7624e+00, 8.0682e-01, 1.5113e+00, 7.7298e-01,\n",
      "        1.1090e-01, 1.2067e-01, 1.1110e+00, 1.6830e-01, 3.9777e-01, 2.3809e-01,\n",
      "        1.1687e-05, 2.5720e+00, 1.5574e+00, 3.4583e-01, 1.6830e+00, 3.4459e+00,\n",
      "        3.5616e-01, 3.6492e-01, 4.5317e-01, 3.7861e-01, 1.2786e-01, 6.4850e-01,\n",
      "        7.1691e-01, 2.0068e-01, 6.7938e-02, 5.3210e-01, 1.9523e+00, 5.8238e-01,\n",
      "        4.5719e+00, 4.0832e-02, 3.6167e-02, 3.7207e-01, 8.7294e-02, 6.6827e-02,\n",
      "        4.4268e-02, 2.6910e-01, 3.1742e-01, 1.0549e-01, 1.3951e-01, 1.9783e-05,\n",
      "        1.1123e-04, 1.8561e-04, 2.1326e-04, 3.5375e-05, 3.9608e-05, 3.8680e-05,\n",
      "        8.5870e-01, 6.7022e-01, 3.2109e+00, 2.6620e+00, 8.3901e-01, 1.0186e+00,\n",
      "        2.5555e-01, 3.9340e-01, 1.9007e-01, 3.3076e-01, 2.2833e+00, 1.4403e-01,\n",
      "        5.0699e-01, 2.1172e+00, 9.8130e-02, 2.4485e-01, 4.5265e-01, 4.0472e-01,\n",
      "        3.0373e-01, 2.5284e-01, 1.3016e-01, 1.6547e-01, 1.2927e-05, 1.6596e-05,\n",
      "        5.0146e-07, 1.1542e-01, 2.4300e-01, 1.6920e-01, 5.0258e-01, 2.3068e-01,\n",
      "        2.6064e-01, 2.6038e-01, 3.7609e-07, 1.5536e-06, 7.9165e-01, 1.5778e-01,\n",
      "        4.1670e+01, 2.3845e+00, 2.3852e-01, 4.3538e-01, 1.9928e+00, 8.5506e-01,\n",
      "        1.4969e+00, 1.3792e+00, 3.8424e-01, 3.5722e+00, 9.4049e-02, 8.0768e-01,\n",
      "        3.9375e-01, 2.0028e-01, 3.4028e-01, 1.7757e-01, 1.2223e-01, 8.5451e-06,\n",
      "        2.9222e-05, 3.0334e-01, 1.1275e-01, 4.6010e-01, 2.1899e-06, 4.0984e-06,\n",
      "        2.3880e-06, 1.1983e-01, 4.6327e-01, 2.3995e+00, 5.0066e-02, 3.9664e+00,\n",
      "        2.5216e-02, 2.0829e+00, 3.4340e-01, 7.5010e-02, 3.7656e-09, 8.2889e-07,\n",
      "        4.2616e-09], device='cuda:0')\n",
      "Sig sum tensor(223.0000, device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 5134.356089686099\n",
      "pruning 30 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :17 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     31\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    9\n",
      "     ║    ╠════╝\n",
      "     ║    23\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    21\n",
      "    ╠════╗\n",
      "    ║    17\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   21\n",
      "   ╠════╗\n",
      "   ║    14\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  34\n",
      "  ╠════╗\n",
      "  ║    11\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 37\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.5907e+05, 1.3094e+07, 9.0404e+05, 1.5468e+06, 1.3165e+06, 6.9118e+05,\n",
      "        2.5350e+04, 1.8793e+06, 2.0454e+05, 8.3530e+06, 6.3462e+05, 1.0816e+05,\n",
      "        4.1941e+05, 2.8949e+06, 1.5142e+05, 7.3476e+04, 4.1065e+06, 5.5628e+05,\n",
      "        1.0681e+07, 8.3252e+06, 1.5552e+06, 5.4068e+06, 2.1474e+05, 1.3558e+05,\n",
      "        9.7660e+05, 3.2972e+05, 2.6840e+05, 7.9229e+04, 1.9178e+05, 8.1780e+05,\n",
      "        5.0886e+05, 1.4063e+06, 1.3015e-02, 8.3214e-03, 1.6509e-02, 9.0188e-04,\n",
      "        2.3986e-04], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.6524, 0.6384, 0.8134, 0.3108, 0.2457, 0.4415, 0.3380, 0.4822, 0.2713,\n",
      "        0.8828, 0.3781, 0.4139, 0.6745, 0.5273, 0.4838, 0.3107, 0.4525, 0.2058,\n",
      "        0.7917, 0.6032, 0.5397, 0.7431, 0.3338, 0.4265, 0.4325, 0.3698, 0.3287,\n",
      "        0.6319, 0.4681, 0.4079, 0.0963, 0.1580, 0.0815, 0.0281, 0.0479, 0.0084,\n",
      "        0.0448], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.2117e+05, 1.8940e+07, 6.7494e+05, 4.2640e+06, 3.9723e+06, 1.5441e+06,\n",
      "        6.7124e+04, 3.8923e+06, 5.9619e+05, 3.9164e+06, 1.5787e+06, 2.5360e+05,\n",
      "        5.4600e+05, 5.4731e+06, 3.1266e+05, 2.0258e+05, 8.9934e+06, 1.7672e+06,\n",
      "        8.9013e+06, 1.3213e+07, 2.8634e+06, 5.5569e+06, 5.7225e+05, 3.1101e+05,\n",
      "        2.2168e+06, 8.3121e+05, 7.2068e+05, 1.1664e+05, 4.0802e+05, 1.9369e+06,\n",
      "        1.8394e+06, 4.7363e+06, 4.7816e-02, 3.2348e-02, 6.2872e-02, 3.5771e-03,\n",
      "        9.1648e-04], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([5.8001e+05, 4.7250e+06, 1.6737e+05, 6.7958e+05, 3.1446e+05, 2.9542e+05,\n",
      "        1.4512e+07, 2.2932e+06, 2.1341e+06, 4.6480e+04, 2.6162e+05, 3.6375e+06,\n",
      "        7.4381e+04, 2.3790e+06, 1.0239e+05, 8.7559e+05, 7.7301e+03, 5.7168e+06,\n",
      "        1.6016e+05, 5.6795e+06, 1.9160e+05, 1.2759e+06, 1.9916e+05, 1.5204e+06,\n",
      "        2.6487e+06, 4.6990e+06, 1.2517e+05, 3.3714e+06, 1.3811e+06, 2.9800e+05,\n",
      "        4.9660e+05, 4.2572e-03, 5.2325e-02, 1.8964e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8125, 0.7382, 0.4852, 0.7185, 0.5849, 0.6944, 0.8037, 0.5278, 0.6707,\n",
      "        0.5164, 0.6330, 0.2118, 0.6701, 0.4128, 0.5850, 0.5950, 0.4205, 0.7688,\n",
      "        0.6876, 0.3883, 0.8118, 0.3393, 0.4387, 0.4730, 0.6422, 0.5613, 0.4097,\n",
      "        0.3938, 0.4510, 0.2084, 0.2160, 0.0101, 0.0420, 0.0118],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.3496e+05, 4.9484e+06, 3.4462e+05, 7.6534e+05, 5.2211e+05, 3.6115e+05,\n",
      "        1.1395e+07, 4.3316e+06, 2.8113e+06, 8.9914e+04, 3.8404e+05, 1.1468e+07,\n",
      "        9.8141e+04, 5.5878e+06, 1.6998e+05, 1.4184e+06, 1.7919e+04, 5.2860e+06,\n",
      "        2.0014e+05, 1.3897e+07, 1.4422e+05, 3.3719e+06, 4.4712e+05, 3.2050e+06,\n",
      "        3.7903e+06, 8.2464e+06, 2.9555e+05, 8.1747e+06, 3.0328e+06, 9.4360e+05,\n",
      "        1.5574e+06, 1.6857e-02, 2.0050e-01, 7.4956e-01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([9.3814e+06, 4.0965e+05, 3.8237e+06, 8.3408e+05, 2.7026e+05, 2.9196e+07,\n",
      "        4.5324e+06, 1.7247e+06, 4.7000e+05, 9.5999e+05, 2.3175e-01, 2.8787e-02,\n",
      "        8.3994e-02, 2.0465e-02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5053, 0.4510, 0.8331, 0.2250, 0.7236, 0.4327, 0.2404, 0.2938, 0.2609,\n",
      "        0.3353, 0.0678, 0.0226, 0.0706, 0.0520], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.8565e+07, 8.9952e+05, 2.5531e+06, 2.5857e+06, 2.9885e+05, 6.6254e+07,\n",
      "        1.3772e+07, 4.8718e+06, 1.3894e+06, 2.5524e+06, 8.6414e-01, 1.1255e-01,\n",
      "        3.1227e-01, 7.7605e-02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.5651e+07, 4.5531e+07, 1.0239e+07, 2.8381e+06, 1.2991e+07, 2.1092e+05,\n",
      "        5.0697e+06, 3.4021e+06, 1.2410e+06, 5.3149e+07, 3.1677e+06, 6.0275e+06,\n",
      "        4.0095e+05, 6.7737e+05, 1.5468e+06, 5.2004e+06, 2.7543e+05, 3.8186e+05,\n",
      "        2.7095e+00, 2.8185e+00, 5.3663e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8268, 0.7191, 0.7423, 0.5670, 0.6379, 0.5786, 0.5321, 0.7362, 0.7735,\n",
      "        0.5216, 0.5525, 0.7184, 0.4683, 0.5133, 0.5128, 0.2557, 0.4744, 0.3365,\n",
      "        0.0856, 0.0614, 0.0357], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.0841e+07, 5.1159e+07, 1.0556e+07, 4.9154e+06, 1.8815e+07, 3.5550e+05,\n",
      "        9.4876e+06, 3.5905e+06, 1.1246e+06, 1.0171e+08, 5.6705e+06, 6.7891e+06,\n",
      "        8.5280e+05, 1.3187e+06, 3.0142e+06, 1.5483e+07, 5.7909e+05, 1.0134e+06,\n",
      "        9.9106e+00, 1.0582e+01, 2.0699e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.5587e+06, 6.6783e+06, 9.7260e+06, 3.8875e+06, 1.8416e+07, 1.2449e+06,\n",
      "        3.8960e+06, 2.1542e+06, 2.2176e+05, 2.0735e+05, 2.7293e+06, 2.2508e+05,\n",
      "        6.1100e+05, 5.1892e+05, 2.3842e-01, 8.8693e-01, 2.4506e+01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1798, 0.5386, 0.4039, 0.7178, 0.5049, 0.2825, 0.5127, 0.2660, 0.1516,\n",
      "        0.2468, 0.3831, 0.1850, 0.0913, 0.1039, 0.0110, 0.1192, 0.0524],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.1675e+07, 1.2324e+07, 2.3189e+07, 4.3878e+06, 3.6469e+07, 3.5729e+06,\n",
      "        7.5945e+06, 6.3251e+06, 7.5257e+05, 6.2472e+05, 6.7343e+06, 7.3380e+05,\n",
      "        2.2207e+06, 1.8600e+06, 9.4321e-01, 3.1249e+00, 9.2889e+01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.6174e+08, 2.2149e+06, 1.3528e+06, 4.9215e+07, 6.1907e+06, 7.0935e+05,\n",
      "        3.7508e+07, 4.2338e+06, 1.1744e+06, 3.4512e+05, 2.7711e+06, 3.1179e+06,\n",
      "        3.7624e+05, 4.8499e+05, 8.5493e+05, 4.4559e+06, 2.4558e+06, 1.4205e+07,\n",
      "        8.9269e+04, 2.3727e+05, 5.3436e+05, 1.4283e+05, 1.1718e+05, 1.2156e+05,\n",
      "        6.7796e+05, 4.3120e+05, 1.9692e+05, 3.8188e+05, 6.7764e+01, 4.0724e+01,\n",
      "        6.5312e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9775, 0.5207, 0.4959, 0.9564, 0.5037, 0.3520, 0.9851, 0.8672, 0.5932,\n",
      "        0.4629, 0.6524, 0.7198, 0.2736, 0.7738, 0.3733, 0.3411, 0.4467, 0.4671,\n",
      "        0.3373, 0.7907, 0.0818, 0.2216, 0.1892, 0.4086, 0.3283, 0.1735, 0.1011,\n",
      "        0.4202, 0.0931, 0.0664, 0.1972], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.4560e+07, 4.2461e+06, 2.7280e+06, 8.5769e+06, 1.2290e+07, 1.8386e+06,\n",
      "        2.2336e+06, 2.2488e+06, 1.9112e+06, 7.4149e+05, 3.8527e+06, 3.4945e+06,\n",
      "        1.0932e+06, 4.3886e+05, 2.1430e+06, 1.1744e+07, 5.4354e+06, 3.0277e+07,\n",
      "        2.3664e+05, 1.9864e+05, 1.9627e+06, 4.4475e+05, 3.8000e+05, 2.8757e+05,\n",
      "        1.8216e+06, 1.4255e+06, 7.0807e+05, 8.8562e+05, 2.4583e+02, 1.5208e+02,\n",
      "        2.0972e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([8.9673e+06, 4.8009e+05, 1.5489e+07, 6.0680e+06, 2.2841e+06, 1.0770e+07,\n",
      "        8.8808e+05, 7.5411e+05, 4.5121e+05, 4.7602e+05, 4.5201e+06, 3.5771e+05,\n",
      "        1.2690e+06, 4.4753e+06, 2.0334e+05, 3.4193e+05, 7.1651e+05, 1.3437e+06,\n",
      "        8.9858e+05, 4.6432e+05, 2.2991e+05, 3.7114e+05, 8.7403e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8379, 0.3857, 0.7599, 0.5366, 0.4017, 0.8550, 0.6625, 0.3261, 0.2989,\n",
      "        0.1160, 0.3306, 0.2986, 0.4259, 0.2948, 0.1504, 0.1640, 0.2592, 0.4228,\n",
      "        0.2591, 0.1269, 0.1668, 0.3256, 0.0404], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.8150e+06, 1.1796e+06, 1.4878e+07, 1.1248e+07, 5.4665e+06, 6.2447e+06,\n",
      "        1.1989e+06, 2.0329e+06, 1.2654e+06, 1.6831e+06, 1.2103e+07, 1.0036e+06,\n",
      "        2.9142e+06, 1.2624e+07, 6.9102e+05, 1.1434e+06, 2.1231e+06, 3.1023e+06,\n",
      "        2.6632e+06, 1.6215e+06, 7.6625e+05, 1.0012e+06, 3.3549e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.7590e+05, 5.1534e+05, 3.5885e+05, 9.9449e+05, 3.7656e+05, 6.2616e+05,\n",
      "        4.8148e+05, 1.2857e+01, 1.8344e-01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1295, 0.2132, 0.3072, 0.2702, 0.1253, 0.3589, 0.1931, 0.0427, 0.0118],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([6.1246e+05, 1.6218e+06, 9.9453e+05, 2.9031e+06, 1.3176e+06, 1.6058e+06,\n",
      "        1.5541e+06, 4.9234e+01, 7.2508e-01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.6194e+07, 2.6616e+06, 1.9104e+08, 6.3399e+06, 3.3737e+06, 1.4009e+07,\n",
      "        9.2832e+06, 2.9096e+06, 9.7971e+06, 3.0779e+06, 1.0369e+06, 1.3857e+07,\n",
      "        1.7039e+05, 2.6186e+06, 8.4480e+05, 3.3481e+05, 6.2148e+05, 2.9281e+05,\n",
      "        2.1145e+05, 9.3476e-01, 1.7330e+00], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.7947, 0.9199, 0.7080, 0.5363, 0.9430, 0.9603, 0.7430, 0.5450, 0.8066,\n",
      "        0.3010, 0.2308, 0.6572, 0.2830, 0.5948, 0.2621, 0.1424, 0.1212, 0.2440,\n",
      "        0.2158, 0.0489, 0.0804], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.3299e+07, 8.5330e+05, 2.2316e+08, 1.1760e+07, 7.6876e+05, 2.2245e+06,\n",
      "        9.5423e+06, 5.2954e+06, 7.5783e+06, 8.6057e+06, 3.1902e+06, 1.9000e+07,\n",
      "        4.8868e+05, 4.2440e+06, 2.4934e+06, 1.1485e+06, 2.1846e+06, 8.8542e+05,\n",
      "        6.6330e+05, 3.5560e+00, 6.3743e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([5.6188e+05, 2.9612e+05, 9.0134e+05, 3.0695e+01, 2.7533e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2312, 0.2258, 0.2154, 0.0468, 0.0121], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.7278e+06, 9.1707e+05, 2.8289e+06, 1.1704e+02, 1.0880e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.0261e+05, 1.4137e+06, 8.6131e+06, 1.5406e+05, 9.5632e+06, 4.5835e+04,\n",
      "        5.9931e+06, 6.8618e+05, 1.8298e+05, 3.9690e-03, 4.8359e-02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3237, 0.5764, 0.6056, 0.5394, 0.4159, 0.3389, 0.3948, 0.2752, 0.1774,\n",
      "        0.0269, 0.0406], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.4809e+05, 2.3954e+06, 1.3589e+07, 2.8384e+05, 2.2345e+07, 1.2120e+05,\n",
      "        1.4509e+07, 1.9893e+06, 6.0209e+05, 1.5450e-02, 1.8558e-01],\n",
      "       device='cuda:0')\n",
      "All_sigs tensor([2.2117e+05, 1.8940e+07, 6.7494e+05, 4.2640e+06, 3.9723e+06, 1.5441e+06,\n",
      "        6.7124e+04, 3.8923e+06, 5.9619e+05, 3.9164e+06, 1.5787e+06, 2.5360e+05,\n",
      "        5.4600e+05, 5.4731e+06, 3.1266e+05, 2.0258e+05, 8.9934e+06, 1.7672e+06,\n",
      "        8.9013e+06, 1.3213e+07, 2.8634e+06, 5.5569e+06, 5.7225e+05, 3.1101e+05,\n",
      "        2.2168e+06, 8.3121e+05, 7.2068e+05, 1.1664e+05, 4.0802e+05, 1.9369e+06,\n",
      "        1.8394e+06, 4.7363e+06, 4.7816e-02, 3.2348e-02, 6.2872e-02, 3.5771e-03,\n",
      "        9.1648e-04, 4.3496e+05, 4.9484e+06, 3.4462e+05, 7.6534e+05, 5.2211e+05,\n",
      "        3.6115e+05, 1.1395e+07, 4.3316e+06, 2.8113e+06, 8.9914e+04, 3.8404e+05,\n",
      "        1.1468e+07, 9.8141e+04, 5.5878e+06, 1.6998e+05, 1.4184e+06, 1.7919e+04,\n",
      "        5.2860e+06, 2.0014e+05, 1.3897e+07, 1.4422e+05, 3.3719e+06, 4.4712e+05,\n",
      "        3.2050e+06, 3.7903e+06, 8.2464e+06, 2.9555e+05, 8.1747e+06, 3.0328e+06,\n",
      "        9.4360e+05, 1.5574e+06, 1.6857e-02, 2.0050e-01, 7.4956e-01, 1.8565e+07,\n",
      "        8.9952e+05, 2.5531e+06, 2.5857e+06, 2.9885e+05, 6.6254e+07, 1.3772e+07,\n",
      "        4.8718e+06, 1.3894e+06, 2.5524e+06, 8.6414e-01, 1.1255e-01, 3.1227e-01,\n",
      "        7.7605e-02, 1.0841e+07, 5.1159e+07, 1.0556e+07, 4.9154e+06, 1.8815e+07,\n",
      "        3.5550e+05, 9.4876e+06, 3.5905e+06, 1.1246e+06, 1.0171e+08, 5.6705e+06,\n",
      "        6.7891e+06, 8.5280e+05, 1.3187e+06, 3.0142e+06, 1.5483e+07, 5.7909e+05,\n",
      "        1.0134e+06, 9.9106e+00, 1.0582e+01, 2.0699e+00, 1.1675e+07, 1.2324e+07,\n",
      "        2.3189e+07, 4.3878e+06, 3.6469e+07, 3.5729e+06, 7.5945e+06, 6.3251e+06,\n",
      "        7.5257e+05, 6.2472e+05, 6.7343e+06, 7.3380e+05, 2.2207e+06, 1.8600e+06,\n",
      "        9.4321e-01, 3.1249e+00, 9.2889e+01, 1.4560e+07, 4.2461e+06, 2.7280e+06,\n",
      "        8.5769e+06, 1.2290e+07, 1.8386e+06, 2.2336e+06, 2.2488e+06, 1.9112e+06,\n",
      "        7.4149e+05, 3.8527e+06, 3.4945e+06, 1.0932e+06, 4.3886e+05, 2.1430e+06,\n",
      "        1.1744e+07, 5.4354e+06, 3.0277e+07, 2.3664e+05, 1.9864e+05, 1.9627e+06,\n",
      "        4.4475e+05, 3.8000e+05, 2.8757e+05, 1.8216e+06, 1.4255e+06, 7.0807e+05,\n",
      "        8.8562e+05, 2.4583e+02, 1.5208e+02, 2.0972e+02, 5.8150e+06, 1.1796e+06,\n",
      "        1.4878e+07, 1.1248e+07, 5.4665e+06, 6.2447e+06, 1.1989e+06, 2.0329e+06,\n",
      "        1.2654e+06, 1.6831e+06, 1.2103e+07, 1.0036e+06, 2.9142e+06, 1.2624e+07,\n",
      "        6.9102e+05, 1.1434e+06, 2.1231e+06, 3.1023e+06, 2.6632e+06, 1.6215e+06,\n",
      "        7.6625e+05, 1.0012e+06, 3.3549e+00, 6.1246e+05, 1.6218e+06, 9.9453e+05,\n",
      "        2.9031e+06, 1.3176e+06, 1.6058e+06, 1.5541e+06, 4.9234e+01, 7.2508e-01,\n",
      "        1.3299e+07, 8.5330e+05, 2.2316e+08, 1.1760e+07, 7.6876e+05, 2.2245e+06,\n",
      "        9.5423e+06, 5.2954e+06, 7.5783e+06, 8.6057e+06, 3.1902e+06, 1.9000e+07,\n",
      "        4.8868e+05, 4.2440e+06, 2.4934e+06, 1.1485e+06, 2.1846e+06, 8.8542e+05,\n",
      "        6.6330e+05, 3.5560e+00, 6.3743e+00, 1.7278e+06, 9.1707e+05, 2.8289e+06,\n",
      "        1.1704e+02, 1.0880e+00, 5.4809e+05, 2.3954e+06, 1.3589e+07, 2.8384e+05,\n",
      "        2.2345e+07, 1.2120e+05, 1.4509e+07, 1.9893e+06, 6.0209e+05, 1.5450e-02,\n",
      "        1.8558e-01], device='cuda:0')\n",
      "All_sig tensor([3.8549e-02, 3.3010e+00, 1.1764e-01, 7.4317e-01, 6.9233e-01, 2.6912e-01,\n",
      "        1.1699e-02, 6.7839e-01, 1.0391e-01, 6.8259e-01, 2.7515e-01, 4.4199e-02,\n",
      "        9.5162e-02, 9.5390e-01, 5.4494e-02, 3.5307e-02, 1.5675e+00, 3.0801e-01,\n",
      "        1.5514e+00, 2.3029e+00, 4.9907e-01, 9.6850e-01, 9.9737e-02, 5.4206e-02,\n",
      "        3.8637e-01, 1.4487e-01, 1.2561e-01, 2.0329e-02, 7.1113e-02, 3.3759e-01,\n",
      "        3.2059e-01, 8.2548e-01, 8.3338e-09, 5.6380e-09, 1.0958e-08, 6.2345e-10,\n",
      "        1.5973e-10, 7.5808e-02, 8.6245e-01, 6.0064e-02, 1.3339e-01, 9.0998e-02,\n",
      "        6.2945e-02, 1.9860e+00, 7.5495e-01, 4.8998e-01, 1.5671e-02, 6.6935e-02,\n",
      "        1.9988e+00, 1.7105e-02, 9.7389e-01, 2.9626e-02, 2.4722e-01, 3.1231e-03,\n",
      "        9.2130e-01, 3.4883e-02, 2.4221e+00, 2.5136e-02, 5.8769e-01, 7.7928e-02,\n",
      "        5.5860e-01, 6.6062e-01, 1.4373e+00, 5.1512e-02, 1.4248e+00, 5.2859e-01,\n",
      "        1.6446e-01, 2.7143e-01, 2.9380e-09, 3.4945e-08, 1.3064e-07, 3.2357e+00,\n",
      "        1.5678e-01, 4.4498e-01, 4.5067e-01, 5.2086e-02, 1.1547e+01, 2.4003e+00,\n",
      "        8.4911e-01, 2.4217e-01, 4.4486e-01, 1.5061e-07, 1.9616e-08, 5.4425e-08,\n",
      "        1.3526e-08, 1.8895e+00, 8.9164e+00, 1.8398e+00, 8.5671e-01, 3.2793e+00,\n",
      "        6.1960e-02, 1.6536e+00, 6.2579e-01, 1.9600e-01, 1.7726e+01, 9.8832e-01,\n",
      "        1.1833e+00, 1.4863e-01, 2.2983e-01, 5.2534e-01, 2.6985e+00, 1.0093e-01,\n",
      "        1.7663e-01, 1.7273e-06, 1.8443e-06, 3.6075e-07, 2.0349e+00, 2.1480e+00,\n",
      "        4.0417e+00, 7.6475e-01, 6.3562e+00, 6.2272e-01, 1.3236e+00, 1.1024e+00,\n",
      "        1.3117e-01, 1.0888e-01, 1.1737e+00, 1.2789e-01, 3.8705e-01, 3.2417e-01,\n",
      "        1.6439e-07, 5.4464e-07, 1.6190e-05, 2.5377e+00, 7.4006e-01, 4.7546e-01,\n",
      "        1.4949e+00, 2.1421e+00, 3.2045e-01, 3.8929e-01, 3.9195e-01, 3.3310e-01,\n",
      "        1.2923e-01, 6.7149e-01, 6.0906e-01, 1.9054e-01, 7.6489e-02, 3.7351e-01,\n",
      "        2.0468e+00, 9.4733e-01, 5.2770e+00, 4.1244e-02, 3.4621e-02, 3.4207e-01,\n",
      "        7.7516e-02, 6.6230e-02, 5.0121e-02, 3.1748e-01, 2.4844e-01, 1.2341e-01,\n",
      "        1.5435e-01, 4.2846e-05, 2.6505e-05, 3.6552e-05, 1.0135e+00, 2.0560e-01,\n",
      "        2.5931e+00, 1.9604e+00, 9.5276e-01, 1.0884e+00, 2.0896e-01, 3.5432e-01,\n",
      "        2.2055e-01, 2.9335e-01, 2.1095e+00, 1.7492e-01, 5.0792e-01, 2.2003e+00,\n",
      "        1.2044e-01, 1.9929e-01, 3.7003e-01, 5.4070e-01, 4.6416e-01, 2.8262e-01,\n",
      "        1.3355e-01, 1.7449e-01, 5.8472e-07, 1.0675e-01, 2.8267e-01, 1.7334e-01,\n",
      "        5.0597e-01, 2.2964e-01, 2.7987e-01, 2.7087e-01, 8.5810e-06, 1.2637e-07,\n",
      "        2.3179e+00, 1.4872e-01, 3.8894e+01, 2.0496e+00, 1.3399e-01, 3.8771e-01,\n",
      "        1.6631e+00, 9.2293e-01, 1.3208e+00, 1.4999e+00, 5.5602e-01, 3.3115e+00,\n",
      "        8.5172e-02, 7.3969e-01, 4.3458e-01, 2.0018e-01, 3.8075e-01, 1.5432e-01,\n",
      "        1.1561e-01, 6.1977e-07, 1.1110e-06, 3.0114e-01, 1.5984e-01, 4.9305e-01,\n",
      "        2.0399e-05, 1.8963e-07, 9.5526e-02, 4.1749e-01, 2.3684e+00, 4.9470e-02,\n",
      "        3.8946e+00, 2.1124e-02, 2.5287e+00, 3.4671e-01, 1.0494e-01, 2.6927e-09,\n",
      "        3.2345e-08], device='cuda:0')\n",
      "Sig sum tensor(223., device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 5737.574170403588\n",
      "pruning 30 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :18 ===\n",
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     30\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    11\n",
      "     ║    ╠════╝\n",
      "     ║    27\n",
      "     ║    ╠════╗\n",
      "     ║    ║    4\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    17\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   21\n",
      "   ╠════╗\n",
      "   ║    13\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  35\n",
      "  ╠════╗\n",
      "  ║    11\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 35\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.4470e+05, 1.2898e+07, 8.1768e+05, 1.6302e+06, 1.4889e+06, 6.6646e+05,\n",
      "        2.8641e+04, 1.6366e+06, 2.5282e+05, 8.7022e+06, 6.2765e+05, 9.6922e+04,\n",
      "        4.1488e+05, 2.8835e+06, 1.5251e+05, 6.1045e+04, 4.3035e+06, 5.3468e+05,\n",
      "        1.0352e+07, 7.3059e+06, 1.3048e+06, 4.9720e+06, 1.9046e+05, 1.3256e+05,\n",
      "        8.2029e+05, 3.6753e+05, 3.8447e+05, 8.0582e+04, 1.5154e+05, 8.2908e+05,\n",
      "        4.3359e+05, 1.2597e+06, 4.2878e-04, 6.2403e-03, 1.1523e+00],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.6542, 0.6151, 0.8053, 0.3162, 0.2452, 0.4133, 0.3360, 0.4696, 0.2868,\n",
      "        0.8873, 0.3728, 0.3630, 0.6672, 0.5078, 0.4943, 0.3104, 0.4613, 0.2002,\n",
      "        0.7986, 0.5848, 0.5123, 0.7246, 0.3083, 0.4241, 0.3982, 0.3631, 0.3573,\n",
      "        0.6407, 0.4063, 0.3940, 0.0884, 0.1449, 0.0171, 0.0297, 0.0422],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.0017e+05, 1.9860e+07, 6.3665e+05, 4.4590e+06, 4.4950e+06, 1.5641e+06,\n",
      "        7.6073e+04, 3.4721e+06, 7.2129e+05, 3.9246e+06, 1.5746e+06, 2.4697e+05,\n",
      "        5.5222e+05, 5.6769e+06, 3.0848e+05, 1.6839e+05, 9.2739e+06, 1.7105e+06,\n",
      "        8.3393e+06, 1.2133e+07, 2.5453e+06, 5.4772e+06, 5.2696e+05, 3.0538e+05,\n",
      "        1.9745e+06, 9.3627e+05, 9.8840e+05, 1.1582e+05, 3.5989e+05, 2.0096e+06,\n",
      "        1.5811e+06, 4.3089e+06, 1.6858e-03, 2.4220e-02, 4.4144e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([6.2298e+05, 3.8073e+06, 1.6603e+05, 6.7674e+05, 3.4823e+05, 3.4750e+05,\n",
      "        1.2539e+07, 2.1577e+06, 2.1261e+06, 5.2475e+04, 3.1006e+05, 3.3034e+06,\n",
      "        9.5077e+04, 2.1936e+06, 8.4877e+04, 1.0693e+06, 6.3145e+03, 5.6416e+06,\n",
      "        1.7428e+05, 5.3770e+06, 2.0081e+05, 1.2285e+06, 2.2026e+05, 1.4808e+06,\n",
      "        2.9819e+06, 4.7084e+06, 1.4717e+05, 2.7580e+06, 1.4629e+06, 3.8138e+05,\n",
      "        5.9372e+05, 2.7128e+01, 9.1405e+00, 3.1037e-02, 1.0822e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8044, 0.7068, 0.4526, 0.7094, 0.6041, 0.7109, 0.7805, 0.5190, 0.6808,\n",
      "        0.5297, 0.6462, 0.1997, 0.6654, 0.3983, 0.6007, 0.6044, 0.4257, 0.7996,\n",
      "        0.6693, 0.3580, 0.8176, 0.3098, 0.4424, 0.4591, 0.6584, 0.5647, 0.4352,\n",
      "        0.3706, 0.4970, 0.2254, 0.2133, 0.0631, 0.0300, 0.0129, 0.0439],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.8753e+05, 4.4658e+06, 3.6355e+05, 7.8666e+05, 5.5148e+05, 4.0182e+05,\n",
      "        1.1010e+07, 4.1516e+06, 2.7145e+06, 9.8721e+04, 4.3882e+05, 1.0575e+07,\n",
      "        1.2724e+05, 5.2792e+06, 1.3555e+05, 1.6921e+06, 1.4507e+04, 4.5233e+06,\n",
      "        2.3056e+05, 1.3809e+07, 1.4653e+05, 3.3913e+06, 4.9127e+05, 3.2035e+06,\n",
      "        4.0739e+06, 8.1983e+06, 3.3250e+05, 6.9433e+06, 2.9436e+06, 1.1817e+06,\n",
      "        1.8683e+06, 1.0166e+02, 3.5465e+01, 1.2254e-01, 4.1390e-01],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.1208e+07, 3.1767e+05, 4.2498e+06, 6.7361e+05, 2.9812e+05, 2.3560e+07,\n",
      "        4.0302e+06, 1.7692e+06, 3.4309e+05, 1.1706e+06, 2.4121e-01, 1.1724e+00,\n",
      "        2.2722e+00], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.5076, 0.4372, 0.8379, 0.1996, 0.7098, 0.4162, 0.2084, 0.3153, 0.2143,\n",
      "        0.3465, 0.0403, 0.0254, 0.0270], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([2.2075e+07, 7.1512e+05, 2.7559e+06, 2.1568e+06, 3.4600e+05, 5.5017e+07,\n",
      "        1.2761e+07, 4.8457e+06, 1.0783e+06, 3.0600e+06, 9.2594e-01, 4.5705e+00,\n",
      "        8.8431e+00], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.5907e+07, 4.3386e+07, 1.0919e+07, 3.9375e+06, 1.2391e+07, 2.2609e+05,\n",
      "        4.9436e+06, 4.1923e+06, 1.2811e+06, 4.1780e+07, 2.4758e+06, 6.7478e+06,\n",
      "        4.5264e+05, 6.3379e+05, 1.4102e+06, 6.8231e+06, 2.2701e+05, 4.4564e+05,\n",
      "        3.0810e+00, 3.3204e+00, 3.7172e+00], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8219, 0.7370, 0.7334, 0.6004, 0.6155, 0.5771, 0.5110, 0.7519, 0.7606,\n",
      "        0.5060, 0.5307, 0.7991, 0.5236, 0.5280, 0.5120, 0.3098, 0.4736, 0.3355,\n",
      "        0.0191, 0.0588, 0.0258], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.1335e+07, 4.5638e+07, 1.1646e+07, 6.2931e+06, 1.9056e+07, 3.8243e+05,\n",
      "        9.6697e+06, 4.1599e+06, 1.2265e+06, 8.2562e+07, 4.6472e+06, 5.4214e+06,\n",
      "        8.6260e+05, 1.1966e+06, 2.7529e+06, 1.8836e+07, 4.7804e+05, 1.1846e+06,\n",
      "        1.2089e+01, 1.2501e+01, 1.4486e+01], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([3.2109e+06, 5.0117e+06, 8.8444e+06, 3.6722e+06, 1.7130e+07, 9.1894e+05,\n",
      "        3.3491e+06, 1.8568e+06, 1.9029e+05, 2.2303e+05, 2.8267e+06, 2.8361e+05,\n",
      "        7.3465e+05, 5.1952e+05, 3.9708e+01, 3.8982e+00, 5.4357e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1837, 0.5203, 0.3909, 0.7431, 0.4900, 0.2615, 0.4953, 0.2650, 0.1486,\n",
      "        0.2550, 0.3731, 0.1758, 0.1130, 0.1106, 0.0516, 0.0161, 0.0196],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.0485e+07, 9.6168e+06, 2.1547e+07, 3.7729e+06, 3.4944e+07, 2.7144e+06,\n",
      "        6.7613e+06, 5.4589e+06, 6.4806e+05, 6.6459e+05, 7.0882e+06, 9.3496e+05,\n",
      "        2.6064e+06, 1.8482e+06, 1.5063e+02, 1.5342e+01, 2.1316e+00],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.4149e+08, 2.9448e+06, 1.0933e+06, 4.7608e+07, 5.4095e+06, 8.5962e+05,\n",
      "        4.4072e+07, 3.6422e+06, 1.2000e+06, 3.9203e+05, 3.0535e+06, 3.1048e+06,\n",
      "        4.2632e+05, 5.2140e+05, 8.3506e+05, 4.2637e+06, 2.7894e+06, 1.7084e+07,\n",
      "        1.0365e+05, 3.2466e+05, 5.8009e+05, 1.4535e+05, 1.4249e+05, 1.2162e+05,\n",
      "        6.8468e+05, 4.9085e+05, 2.2845e+05, 4.3430e+05, 5.9659e+02, 3.3856e+02],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.9763, 0.5210, 0.4956, 0.9554, 0.5030, 0.3543, 0.9844, 0.8655, 0.5926,\n",
      "        0.4647, 0.6702, 0.7267, 0.2772, 0.7698, 0.3688, 0.3352, 0.4516, 0.4677,\n",
      "        0.3337, 0.7953, 0.0830, 0.2109, 0.1958, 0.4040, 0.3287, 0.1742, 0.1013,\n",
      "        0.4203, 0.0495, 0.1061], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.3424e+07, 5.6423e+06, 2.2062e+06, 8.4911e+06, 1.0755e+07, 2.2202e+06,\n",
      "        2.7430e+06, 1.9592e+06, 1.9558e+06, 8.3948e+05, 4.0286e+06, 3.3943e+06,\n",
      "        1.2326e+06, 4.8017e+05, 2.1083e+06, 1.1339e+07, 6.1188e+06, 3.6375e+07,\n",
      "        2.7624e+05, 2.6578e+05, 2.1277e+06, 4.5879e+05, 4.5836e+05, 2.8994e+05,\n",
      "        1.8385e+06, 1.6213e+06, 8.2121e+05, 1.0071e+06, 2.2683e+03, 1.2106e+03],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([7.5329e+06, 9.7226e+05, 1.2947e+07, 6.9169e+06, 2.0562e+06, 9.6362e+06,\n",
      "        1.0636e+06, 7.4107e+05, 5.2320e+05, 5.1320e+05, 4.2584e+06, 3.7868e+05,\n",
      "        1.0131e+06, 5.4816e+06, 1.7930e+05, 4.4843e+05, 7.7278e+05, 1.3915e+06,\n",
      "        7.4019e+05, 4.8803e+05, 3.1585e+05, 5.0543e+05, 4.3962e+01, 1.8100e+01,\n",
      "        2.7156e+01, 5.6793e+00, 9.7348e+01], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8437, 0.3896, 0.7518, 0.5397, 0.4033, 0.8490, 0.6525, 0.3343, 0.3164,\n",
      "        0.1220, 0.3252, 0.3043, 0.4042, 0.3059, 0.1542, 0.1725, 0.2615, 0.4609,\n",
      "        0.2539, 0.1253, 0.1732, 0.3636, 0.0500, 0.0180, 0.0153, 0.0252, 0.0565],\n",
      "       device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([4.7107e+06, 2.3737e+06, 1.2856e+07, 1.2736e+07, 4.9078e+06, 5.8199e+06,\n",
      "        1.4782e+06, 1.9732e+06, 1.4307e+06, 1.8024e+06, 1.1495e+07, 1.0538e+06,\n",
      "        2.4146e+06, 1.5219e+07, 6.0663e+05, 1.4842e+06, 2.2828e+06, 3.0007e+06,\n",
      "        2.2091e+06, 1.7075e+06, 1.0445e+06, 1.2866e+06, 1.6705e+02, 7.1102e+01,\n",
      "        1.0696e+02, 2.2144e+01, 3.6740e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.8532e+05, 5.5357e+05, 4.4227e+05, 1.1932e+06, 4.6470e+05, 5.1580e+05,\n",
      "        5.7090e+05, 6.5334e+01, 3.5819e+01, 5.1200e+00, 2.8714e+01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.1250, 0.2202, 0.3141, 0.3315, 0.1269, 0.3392, 0.1987, 0.0358, 0.0424,\n",
      "        0.0234, 0.0389], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([6.4862e+05, 1.7266e+06, 1.2134e+06, 3.1908e+06, 1.6229e+06, 1.3634e+06,\n",
      "        1.8299e+06, 2.5199e+02, 1.3720e+02, 2.0000e+01, 1.1039e+02],\n",
      "       device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([1.3667e+07, 2.8270e+06, 1.8484e+08, 6.5469e+06, 8.7208e+06, 1.5998e+07,\n",
      "        9.3961e+06, 2.8312e+06, 7.4832e+06, 2.8139e+06, 9.8269e+05, 1.4002e+07,\n",
      "        1.9106e+05, 2.7471e+06, 9.3483e+05, 3.9162e+05, 5.9857e+05, 3.6854e+05,\n",
      "        3.1249e+05], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.8194, 0.9118, 0.7066, 0.5222, 0.9408, 0.9651, 0.7453, 0.5402, 0.7866,\n",
      "        0.3042, 0.2276, 0.6658, 0.3050, 0.5859, 0.2926, 0.1543, 0.1045, 0.2394,\n",
      "        0.2310], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([9.8750e+06, 9.9777e+05, 2.1694e+08, 1.2514e+07, 2.0640e+06, 2.2321e+06,\n",
      "        9.5717e+06, 5.2077e+06, 6.3876e+06, 7.8314e+06, 3.0361e+06, 1.8716e+07,\n",
      "        5.3114e+05, 4.5502e+06, 2.6451e+06, 1.3248e+06, 2.1440e+06, 1.1213e+06,\n",
      "        9.6120e+05], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([5.1488e+05, 3.5226e+05, 1.2449e+06, 1.0512e+02], device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.2231, 0.2332, 0.2431, 0.1657], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([1.6001e+06, 1.0805e+06, 3.7693e+06, 3.5081e+02], device='cuda:0')\n",
      "Significance before rethinking(apnz)\n",
      "tensor([2.2879e+05, 1.2620e+06, 8.4785e+06, 1.2395e+05, 1.0713e+07, 8.2688e+04,\n",
      "        4.9110e+06, 7.4397e+05, 1.7823e+05, 9.3483e-02, 7.4775e-01],\n",
      "       device='cuda:0')\n",
      "Apnz\n",
      "tensor([0.3573, 0.5551, 0.5960, 0.5051, 0.4384, 0.3805, 0.3731, 0.3074, 0.1778,\n",
      "        0.0389, 0.0407], device='cuda:0')\n",
      "Significance after rethinking(apnz)\n",
      "tensor([5.8814e+05, 2.2460e+06, 1.3701e+07, 2.4536e+05, 2.4065e+07, 2.0491e+05,\n",
      "        1.2315e+07, 2.0612e+06, 5.8617e+05, 3.5938e-01, 2.8692e+00],\n",
      "       device='cuda:0')\n",
      "All_sigs tensor([2.0017e+05, 1.9860e+07, 6.3665e+05, 4.4590e+06, 4.4950e+06, 1.5641e+06,\n",
      "        7.6073e+04, 3.4721e+06, 7.2129e+05, 3.9246e+06, 1.5746e+06, 2.4697e+05,\n",
      "        5.5222e+05, 5.6769e+06, 3.0848e+05, 1.6839e+05, 9.2739e+06, 1.7105e+06,\n",
      "        8.3393e+06, 1.2133e+07, 2.5453e+06, 5.4772e+06, 5.2696e+05, 3.0538e+05,\n",
      "        1.9745e+06, 9.3627e+05, 9.8840e+05, 1.1582e+05, 3.5989e+05, 2.0096e+06,\n",
      "        1.5811e+06, 4.3089e+06, 1.6858e-03, 2.4220e-02, 4.4144e+00, 4.8753e+05,\n",
      "        4.4658e+06, 3.6355e+05, 7.8666e+05, 5.5148e+05, 4.0182e+05, 1.1010e+07,\n",
      "        4.1516e+06, 2.7145e+06, 9.8721e+04, 4.3882e+05, 1.0575e+07, 1.2724e+05,\n",
      "        5.2792e+06, 1.3555e+05, 1.6921e+06, 1.4507e+04, 4.5233e+06, 2.3056e+05,\n",
      "        1.3809e+07, 1.4653e+05, 3.3913e+06, 4.9127e+05, 3.2035e+06, 4.0739e+06,\n",
      "        8.1983e+06, 3.3250e+05, 6.9433e+06, 2.9436e+06, 1.1817e+06, 1.8683e+06,\n",
      "        1.0166e+02, 3.5465e+01, 1.2254e-01, 4.1390e-01, 2.2075e+07, 7.1512e+05,\n",
      "        2.7559e+06, 2.1568e+06, 3.4600e+05, 5.5017e+07, 1.2761e+07, 4.8457e+06,\n",
      "        1.0783e+06, 3.0600e+06, 9.2594e-01, 4.5705e+00, 8.8431e+00, 1.1335e+07,\n",
      "        4.5638e+07, 1.1646e+07, 6.2931e+06, 1.9056e+07, 3.8243e+05, 9.6697e+06,\n",
      "        4.1599e+06, 1.2265e+06, 8.2562e+07, 4.6472e+06, 5.4214e+06, 8.6260e+05,\n",
      "        1.1966e+06, 2.7529e+06, 1.8836e+07, 4.7804e+05, 1.1846e+06, 1.2089e+01,\n",
      "        1.2501e+01, 1.4486e+01, 1.0485e+07, 9.6168e+06, 2.1547e+07, 3.7729e+06,\n",
      "        3.4944e+07, 2.7144e+06, 6.7613e+06, 5.4589e+06, 6.4806e+05, 6.6459e+05,\n",
      "        7.0882e+06, 9.3496e+05, 2.6064e+06, 1.8482e+06, 1.5063e+02, 1.5342e+01,\n",
      "        2.1316e+00, 1.3424e+07, 5.6423e+06, 2.2062e+06, 8.4911e+06, 1.0755e+07,\n",
      "        2.2202e+06, 2.7430e+06, 1.9592e+06, 1.9558e+06, 8.3948e+05, 4.0286e+06,\n",
      "        3.3943e+06, 1.2326e+06, 4.8017e+05, 2.1083e+06, 1.1339e+07, 6.1188e+06,\n",
      "        3.6375e+07, 2.7624e+05, 2.6578e+05, 2.1277e+06, 4.5879e+05, 4.5836e+05,\n",
      "        2.8994e+05, 1.8385e+06, 1.6213e+06, 8.2121e+05, 1.0071e+06, 2.2683e+03,\n",
      "        1.2106e+03, 4.7107e+06, 2.3737e+06, 1.2856e+07, 1.2736e+07, 4.9078e+06,\n",
      "        5.8199e+06, 1.4782e+06, 1.9732e+06, 1.4307e+06, 1.8024e+06, 1.1495e+07,\n",
      "        1.0538e+06, 2.4146e+06, 1.5219e+07, 6.0663e+05, 1.4842e+06, 2.2828e+06,\n",
      "        3.0007e+06, 2.2091e+06, 1.7075e+06, 1.0445e+06, 1.2866e+06, 1.6705e+02,\n",
      "        7.1102e+01, 1.0696e+02, 2.2144e+01, 3.6740e+02, 6.4862e+05, 1.7266e+06,\n",
      "        1.2134e+06, 3.1908e+06, 1.6229e+06, 1.3634e+06, 1.8299e+06, 2.5199e+02,\n",
      "        1.3720e+02, 2.0000e+01, 1.1039e+02, 9.8750e+06, 9.9777e+05, 2.1694e+08,\n",
      "        1.2514e+07, 2.0640e+06, 2.2321e+06, 9.5717e+06, 5.2077e+06, 6.3876e+06,\n",
      "        7.8314e+06, 3.0361e+06, 1.8716e+07, 5.3114e+05, 4.5502e+06, 2.6451e+06,\n",
      "        1.3248e+06, 2.1440e+06, 1.1213e+06, 9.6120e+05, 1.6001e+06, 1.0805e+06,\n",
      "        3.7693e+06, 3.5081e+02, 5.8814e+05, 2.2460e+06, 1.3701e+07, 2.4536e+05,\n",
      "        2.4065e+07, 2.0491e+05, 1.2315e+07, 2.0612e+06, 5.8617e+05, 3.5938e-01,\n",
      "        2.8692e+00], device='cuda:0')\n",
      "All_sig tensor([3.6071e-02, 3.5789e+00, 1.1473e-01, 8.0352e-01, 8.1001e-01, 2.8186e-01,\n",
      "        1.3709e-02, 6.2569e-01, 1.2998e-01, 7.0723e-01, 2.8375e-01, 4.4504e-02,\n",
      "        9.9512e-02, 1.0230e+00, 5.5589e-02, 3.0344e-02, 1.6712e+00, 3.0825e-01,\n",
      "        1.5028e+00, 2.1865e+00, 4.5867e-01, 9.8701e-01, 9.4960e-02, 5.5031e-02,\n",
      "        3.5581e-01, 1.6872e-01, 1.7811e-01, 2.0871e-02, 6.4854e-02, 3.6213e-01,\n",
      "        2.8492e-01, 7.7648e-01, 3.0379e-10, 4.3646e-09, 7.9550e-07, 8.7854e-02,\n",
      "        8.0475e-01, 6.5513e-02, 1.4176e-01, 9.9379e-02, 7.2409e-02, 1.9840e+00,\n",
      "        7.4813e-01, 4.8916e-01, 1.7790e-02, 7.9078e-02, 1.9056e+00, 2.2930e-02,\n",
      "        9.5133e-01, 2.4426e-02, 3.0493e-01, 2.6141e-03, 8.1512e-01, 4.1548e-02,\n",
      "        2.4885e+00, 2.6405e-02, 6.1112e-01, 8.8528e-02, 5.7728e-01, 7.3414e-01,\n",
      "        1.4774e+00, 5.9918e-02, 1.2512e+00, 5.3045e-01, 2.1295e-01, 3.3668e-01,\n",
      "        1.8320e-05, 6.3909e-06, 2.2082e-08, 7.4587e-08, 3.9780e+00, 1.2887e-01,\n",
      "        4.9662e-01, 3.8866e-01, 6.2351e-02, 9.9142e+00, 2.2996e+00, 8.7322e-01,\n",
      "        1.9431e-01, 5.5143e-01, 1.6686e-07, 8.2362e-07, 1.5936e-06, 2.0426e+00,\n",
      "        8.2241e+00, 2.0987e+00, 1.1340e+00, 3.4340e+00, 6.8915e-02, 1.7425e+00,\n",
      "        7.4963e-01, 2.2103e-01, 1.4878e+01, 8.3744e-01, 9.7696e-01, 1.5544e-01,\n",
      "        2.1563e-01, 4.9608e-01, 3.3943e+00, 8.6144e-02, 2.1346e-01, 2.1784e-06,\n",
      "        2.2528e-06, 2.6104e-06, 1.8894e+00, 1.7330e+00, 3.8829e+00, 6.7988e-01,\n",
      "        6.2971e+00, 4.8915e-01, 1.2184e+00, 9.8371e-01, 1.1678e-01, 1.1976e-01,\n",
      "        1.2773e+00, 1.6848e-01, 4.6968e-01, 3.3305e-01, 2.7145e-05, 2.7646e-06,\n",
      "        3.8413e-07, 2.4191e+00, 1.0168e+00, 3.9756e-01, 1.5301e+00, 1.9381e+00,\n",
      "        4.0010e-01, 4.9429e-01, 3.5305e-01, 3.5244e-01, 1.5128e-01, 7.2598e-01,\n",
      "        6.1167e-01, 2.2212e-01, 8.6529e-02, 3.7992e-01, 2.0433e+00, 1.1026e+00,\n",
      "        6.5548e+00, 4.9780e-02, 4.7895e-02, 3.8342e-01, 8.2675e-02, 8.2599e-02,\n",
      "        5.2249e-02, 3.3131e-01, 2.9216e-01, 1.4798e-01, 1.8148e-01, 4.0875e-04,\n",
      "        2.1816e-04, 8.4888e-01, 4.2776e-01, 2.3167e+00, 2.2951e+00, 8.8440e-01,\n",
      "        1.0488e+00, 2.6638e-01, 3.5558e-01, 2.5781e-01, 3.2480e-01, 2.0714e+00,\n",
      "        1.8990e-01, 4.3513e-01, 2.7425e+00, 1.0932e-01, 2.6746e-01, 4.1137e-01,\n",
      "        5.4074e-01, 3.9809e-01, 3.0770e-01, 1.8823e-01, 2.3184e-01, 3.0104e-05,\n",
      "        1.2813e-05, 1.9275e-05, 3.9904e-06, 6.6207e-05, 1.1688e-01, 3.1115e-01,\n",
      "        2.1866e-01, 5.7500e-01, 2.9245e-01, 2.4570e-01, 3.2976e-01, 4.5410e-05,\n",
      "        2.4725e-05, 3.6041e-06, 1.9893e-05, 1.7795e+00, 1.7980e-01, 3.9093e+01,\n",
      "        2.2550e+00, 3.7194e-01, 4.0224e-01, 1.7249e+00, 9.3845e-01, 1.1511e+00,\n",
      "        1.4113e+00, 5.4711e-01, 3.3727e+00, 9.5713e-02, 8.1996e-01, 4.7667e-01,\n",
      "        2.3873e-01, 3.8636e-01, 2.0206e-01, 1.7321e-01, 2.8834e-01, 1.9471e-01,\n",
      "        6.7925e-01, 6.3217e-05, 1.0599e-01, 4.0474e-01, 2.4690e+00, 4.4215e-02,\n",
      "        4.3367e+00, 3.6925e-02, 2.2191e+00, 3.7144e-01, 1.0563e-01, 6.4761e-08,\n",
      "        5.1704e-07], device='cuda:0')\n",
      "Sig sum tensor(223., device='cuda:0')\n",
      "remove_below 0.001\n",
      "remove_below 5549.2729686098655\n",
      "pruning 30 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :19 ===\n"
     ]
    }
   ],
   "source": [
    "trainer.loop(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynet.tree.beta_del_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i, hr in enumerate(dynet.tree.DYNAMIC_LIST):\n",
    "    if hr.residual is None: continue\n",
    "#     print(i, \" ==>\", type(hr))\n",
    "#     print(type(hr.residual))\n",
    "#     print(hr.residual.tree.beta_del_neuron)\n",
    "#     print(hr.residual.neurons_added, hr.residual.del_neurons)\n",
    "    \n",
    "#     (1-dynet.tree.beta_del_neuron)*hr.residual.neurons_added + dynet.tree.beta_del_neuron*hr.residual.del_neurons\n",
    "    \n",
    "#     hr.residual.compute_del_neurons()\n",
    "    print(hr.residual.tree is dynet.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{HierarchicalResidual_Connector(\n",
       "   (residual): Residual_Conv_Connector(\n",
       "     (fc0): HierarchicalResidual_Connector(\n",
       "       (residual): Residual_Conv_Connector(\n",
       "         (fc0): HierarchicalResidual_Connector(\n",
       "           (residual): Residual_Conv_Connector(\n",
       "             (fc0): HierarchicalResidual_Connector(\n",
       "               (residual): Residual_Conv_Connector(\n",
       "                 (fc0): HierarchicalResidual_Connector(\n",
       "                   (residual): Residual_Conv_Connector(\n",
       "                     (fc0): HierarchicalResidual_Shortcut(\n",
       "                       (shortcut): Shortcut_Conv()\n",
       "                     )\n",
       "                     (non_linearity): NonLinearity_Conv(\n",
       "                       (actf): ReLU()\n",
       "                     )\n",
       "                     (fc1): HierarchicalResidual_Conv(\n",
       "                       (shortcut): Shortcut_Conv()\n",
       "                       (residual): Residual_Conv(\n",
       "                         (fc0): HierarchicalResidual_Conv(\n",
       "                           (shortcut): Shortcut_Conv()\n",
       "                           (residual): Residual_Conv(\n",
       "                             (fc0): HierarchicalResidual_Conv(\n",
       "                               (shortcut): Shortcut_Conv()\n",
       "                             )\n",
       "                             (non_linearity): NonLinearity_Conv(\n",
       "                               (actf): ReLU()\n",
       "                             )\n",
       "                             (fc1): HierarchicalResidual_Conv(\n",
       "                               (shortcut): Shortcut_Conv()\n",
       "                             )\n",
       "                           )\n",
       "                         )\n",
       "                         (non_linearity): NonLinearity_Conv(\n",
       "                           (actf): ReLU()\n",
       "                         )\n",
       "                         (fc1): HierarchicalResidual_Conv(\n",
       "                           (shortcut): Shortcut_Conv()\n",
       "                           (residual): Residual_Conv(\n",
       "                             (fc0): HierarchicalResidual_Conv(\n",
       "                               (shortcut): Shortcut_Conv()\n",
       "                             )\n",
       "                             (non_linearity): NonLinearity_Conv(\n",
       "                               (actf): ReLU()\n",
       "                             )\n",
       "                             (fc1): HierarchicalResidual_Conv(\n",
       "                               (shortcut): Shortcut_Conv()\n",
       "                             )\n",
       "                           )\n",
       "                         )\n",
       "                       )\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (non_linearity): NonLinearity_Conv(\n",
       "                   (actf): ReLU()\n",
       "                 )\n",
       "                 (fc1): HierarchicalResidual_Conv(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                   (residual): Residual_Conv(\n",
       "                     (fc0): HierarchicalResidual_Conv(\n",
       "                       (shortcut): Shortcut_Conv()\n",
       "                     )\n",
       "                     (non_linearity): NonLinearity_Conv(\n",
       "                       (actf): ReLU()\n",
       "                     )\n",
       "                     (fc1): HierarchicalResidual_Conv(\n",
       "                       (shortcut): Shortcut_Conv()\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (non_linearity): NonLinearity_Conv(\n",
       "               (actf): ReLU()\n",
       "             )\n",
       "             (fc1): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "               (residual): Residual_Conv(\n",
       "                 (fc0): HierarchicalResidual_Conv(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                 )\n",
       "                 (non_linearity): NonLinearity_Conv(\n",
       "                   (actf): ReLU()\n",
       "                 )\n",
       "                 (fc1): HierarchicalResidual_Conv(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (non_linearity): NonLinearity_Conv(\n",
       "           (actf): ReLU()\n",
       "         )\n",
       "         (fc1): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "           (residual): Residual_Conv(\n",
       "             (fc0): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "             )\n",
       "             (non_linearity): NonLinearity_Conv(\n",
       "               (actf): ReLU()\n",
       "             )\n",
       "             (fc1): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (non_linearity): NonLinearity_Conv(\n",
       "       (actf): ReLU()\n",
       "     )\n",
       "     (fc1): HierarchicalResidual_Shortcut(\n",
       "       (shortcut): Shortcut()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " HierarchicalResidual_Connector(\n",
       "   (residual): Residual_Conv_Connector(\n",
       "     (fc0): HierarchicalResidual_Connector(\n",
       "       (residual): Residual_Conv_Connector(\n",
       "         (fc0): HierarchicalResidual_Connector(\n",
       "           (residual): Residual_Conv_Connector(\n",
       "             (fc0): HierarchicalResidual_Connector(\n",
       "               (residual): Residual_Conv_Connector(\n",
       "                 (fc0): HierarchicalResidual_Shortcut(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                 )\n",
       "                 (non_linearity): NonLinearity_Conv(\n",
       "                   (actf): ReLU()\n",
       "                 )\n",
       "                 (fc1): HierarchicalResidual_Conv(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                   (residual): Residual_Conv(\n",
       "                     (fc0): HierarchicalResidual_Conv(\n",
       "                       (shortcut): Shortcut_Conv()\n",
       "                       (residual): Residual_Conv(\n",
       "                         (fc0): HierarchicalResidual_Conv(\n",
       "                           (shortcut): Shortcut_Conv()\n",
       "                         )\n",
       "                         (non_linearity): NonLinearity_Conv(\n",
       "                           (actf): ReLU()\n",
       "                         )\n",
       "                         (fc1): HierarchicalResidual_Conv(\n",
       "                           (shortcut): Shortcut_Conv()\n",
       "                         )\n",
       "                       )\n",
       "                     )\n",
       "                     (non_linearity): NonLinearity_Conv(\n",
       "                       (actf): ReLU()\n",
       "                     )\n",
       "                     (fc1): HierarchicalResidual_Conv(\n",
       "                       (shortcut): Shortcut_Conv()\n",
       "                       (residual): Residual_Conv(\n",
       "                         (fc0): HierarchicalResidual_Conv(\n",
       "                           (shortcut): Shortcut_Conv()\n",
       "                         )\n",
       "                         (non_linearity): NonLinearity_Conv(\n",
       "                           (actf): ReLU()\n",
       "                         )\n",
       "                         (fc1): HierarchicalResidual_Conv(\n",
       "                           (shortcut): Shortcut_Conv()\n",
       "                         )\n",
       "                       )\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (non_linearity): NonLinearity_Conv(\n",
       "               (actf): ReLU()\n",
       "             )\n",
       "             (fc1): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "               (residual): Residual_Conv(\n",
       "                 (fc0): HierarchicalResidual_Conv(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                 )\n",
       "                 (non_linearity): NonLinearity_Conv(\n",
       "                   (actf): ReLU()\n",
       "                 )\n",
       "                 (fc1): HierarchicalResidual_Conv(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (non_linearity): NonLinearity_Conv(\n",
       "           (actf): ReLU()\n",
       "         )\n",
       "         (fc1): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "           (residual): Residual_Conv(\n",
       "             (fc0): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "             )\n",
       "             (non_linearity): NonLinearity_Conv(\n",
       "               (actf): ReLU()\n",
       "             )\n",
       "             (fc1): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (non_linearity): NonLinearity_Conv(\n",
       "       (actf): ReLU()\n",
       "     )\n",
       "     (fc1): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "       (residual): Residual_Conv(\n",
       "         (fc0): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "         )\n",
       "         (non_linearity): NonLinearity_Conv(\n",
       "           (actf): ReLU()\n",
       "         )\n",
       "         (fc1): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " HierarchicalResidual_Connector(\n",
       "   (residual): Residual_Conv_Connector(\n",
       "     (fc0): HierarchicalResidual_Connector(\n",
       "       (residual): Residual_Conv_Connector(\n",
       "         (fc0): HierarchicalResidual_Connector(\n",
       "           (residual): Residual_Conv_Connector(\n",
       "             (fc0): HierarchicalResidual_Shortcut(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "             )\n",
       "             (non_linearity): NonLinearity_Conv(\n",
       "               (actf): ReLU()\n",
       "             )\n",
       "             (fc1): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "               (residual): Residual_Conv(\n",
       "                 (fc0): HierarchicalResidual_Conv(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                   (residual): Residual_Conv(\n",
       "                     (fc0): HierarchicalResidual_Conv(\n",
       "                       (shortcut): Shortcut_Conv()\n",
       "                     )\n",
       "                     (non_linearity): NonLinearity_Conv(\n",
       "                       (actf): ReLU()\n",
       "                     )\n",
       "                     (fc1): HierarchicalResidual_Conv(\n",
       "                       (shortcut): Shortcut_Conv()\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (non_linearity): NonLinearity_Conv(\n",
       "                   (actf): ReLU()\n",
       "                 )\n",
       "                 (fc1): HierarchicalResidual_Conv(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                   (residual): Residual_Conv(\n",
       "                     (fc0): HierarchicalResidual_Conv(\n",
       "                       (shortcut): Shortcut_Conv()\n",
       "                     )\n",
       "                     (non_linearity): NonLinearity_Conv(\n",
       "                       (actf): ReLU()\n",
       "                     )\n",
       "                     (fc1): HierarchicalResidual_Conv(\n",
       "                       (shortcut): Shortcut_Conv()\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (non_linearity): NonLinearity_Conv(\n",
       "           (actf): ReLU()\n",
       "         )\n",
       "         (fc1): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "           (residual): Residual_Conv(\n",
       "             (fc0): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "             )\n",
       "             (non_linearity): NonLinearity_Conv(\n",
       "               (actf): ReLU()\n",
       "             )\n",
       "             (fc1): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (non_linearity): NonLinearity_Conv(\n",
       "       (actf): ReLU()\n",
       "     )\n",
       "     (fc1): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "       (residual): Residual_Conv(\n",
       "         (fc0): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "         )\n",
       "         (non_linearity): NonLinearity_Conv(\n",
       "           (actf): ReLU()\n",
       "         )\n",
       "         (fc1): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " HierarchicalResidual_Connector(\n",
       "   (residual): Residual_Conv_Connector(\n",
       "     (fc0): HierarchicalResidual_Connector(\n",
       "       (residual): Residual_Conv_Connector(\n",
       "         (fc0): HierarchicalResidual_Shortcut(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "         )\n",
       "         (non_linearity): NonLinearity_Conv(\n",
       "           (actf): ReLU()\n",
       "         )\n",
       "         (fc1): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "           (residual): Residual_Conv(\n",
       "             (fc0): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "               (residual): Residual_Conv(\n",
       "                 (fc0): HierarchicalResidual_Conv(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                 )\n",
       "                 (non_linearity): NonLinearity_Conv(\n",
       "                   (actf): ReLU()\n",
       "                 )\n",
       "                 (fc1): HierarchicalResidual_Conv(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "             (non_linearity): NonLinearity_Conv(\n",
       "               (actf): ReLU()\n",
       "             )\n",
       "             (fc1): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "               (residual): Residual_Conv(\n",
       "                 (fc0): HierarchicalResidual_Conv(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                 )\n",
       "                 (non_linearity): NonLinearity_Conv(\n",
       "                   (actf): ReLU()\n",
       "                 )\n",
       "                 (fc1): HierarchicalResidual_Conv(\n",
       "                   (shortcut): Shortcut_Conv()\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (non_linearity): NonLinearity_Conv(\n",
       "       (actf): ReLU()\n",
       "     )\n",
       "     (fc1): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "       (residual): Residual_Conv(\n",
       "         (fc0): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "         )\n",
       "         (non_linearity): NonLinearity_Conv(\n",
       "           (actf): ReLU()\n",
       "         )\n",
       "         (fc1): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " HierarchicalResidual_Connector(\n",
       "   (residual): Residual_Conv_Connector(\n",
       "     (fc0): HierarchicalResidual_Shortcut(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "     )\n",
       "     (non_linearity): NonLinearity_Conv(\n",
       "       (actf): ReLU()\n",
       "     )\n",
       "     (fc1): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "       (residual): Residual_Conv(\n",
       "         (fc0): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "           (residual): Residual_Conv(\n",
       "             (fc0): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "             )\n",
       "             (non_linearity): NonLinearity_Conv(\n",
       "               (actf): ReLU()\n",
       "             )\n",
       "             (fc1): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (non_linearity): NonLinearity_Conv(\n",
       "           (actf): ReLU()\n",
       "         )\n",
       "         (fc1): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "           (residual): Residual_Conv(\n",
       "             (fc0): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "             )\n",
       "             (non_linearity): NonLinearity_Conv(\n",
       "               (actf): ReLU()\n",
       "             )\n",
       "             (fc1): HierarchicalResidual_Conv(\n",
       "               (shortcut): Shortcut_Conv()\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " HierarchicalResidual_Conv(\n",
       "   (shortcut): Shortcut_Conv()\n",
       "   (residual): Residual_Conv(\n",
       "     (fc0): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "       (residual): Residual_Conv(\n",
       "         (fc0): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "         )\n",
       "         (non_linearity): NonLinearity_Conv(\n",
       "           (actf): ReLU()\n",
       "         )\n",
       "         (fc1): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (non_linearity): NonLinearity_Conv(\n",
       "       (actf): ReLU()\n",
       "     )\n",
       "     (fc1): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "       (residual): Residual_Conv(\n",
       "         (fc0): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "         )\n",
       "         (non_linearity): NonLinearity_Conv(\n",
       "           (actf): ReLU()\n",
       "         )\n",
       "         (fc1): HierarchicalResidual_Conv(\n",
       "           (shortcut): Shortcut_Conv()\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " HierarchicalResidual_Conv(\n",
       "   (shortcut): Shortcut_Conv()\n",
       "   (residual): Residual_Conv(\n",
       "     (fc0): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "     )\n",
       "     (non_linearity): NonLinearity_Conv(\n",
       "       (actf): ReLU()\n",
       "     )\n",
       "     (fc1): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " HierarchicalResidual_Conv(\n",
       "   (shortcut): Shortcut_Conv()\n",
       "   (residual): Residual_Conv(\n",
       "     (fc0): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "     )\n",
       "     (non_linearity): NonLinearity_Conv(\n",
       "       (actf): ReLU()\n",
       "     )\n",
       "     (fc1): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " HierarchicalResidual_Conv(\n",
       "   (shortcut): Shortcut_Conv()\n",
       "   (residual): Residual_Conv(\n",
       "     (fc0): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "     )\n",
       "     (non_linearity): NonLinearity_Conv(\n",
       "       (actf): ReLU()\n",
       "     )\n",
       "     (fc1): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " HierarchicalResidual_Conv(\n",
       "   (shortcut): Shortcut_Conv()\n",
       "   (residual): Residual_Conv(\n",
       "     (fc0): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "     )\n",
       "     (non_linearity): NonLinearity_Conv(\n",
       "       (actf): ReLU()\n",
       "     )\n",
       "     (fc1): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " HierarchicalResidual_Conv(\n",
       "   (shortcut): Shortcut_Conv()\n",
       "   (residual): Residual_Conv(\n",
       "     (fc0): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "     )\n",
       "     (non_linearity): NonLinearity_Conv(\n",
       "       (actf): ReLU()\n",
       "     )\n",
       "     (fc1): HierarchicalResidual_Conv(\n",
       "       (shortcut): Shortcut_Conv()\n",
       "     )\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynet.tree.DYNAMIC_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     28\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f93a010c5b0>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accs_all, label=\"train\")\n",
    "plt.plot(accs_test, label=\"test\")\n",
    "ymin, ymax = plt.gca().get_ylim()\n",
    "plt.text(0, 0.8*ymin+0.2*ymax, f\"Train-> max:{max(accs_all):.3f} end:{accs_all[-1]:.3f}\")\n",
    "plt.text(0, 0.9*ymin+0.1*ymax, f\"Test-> max:{max(accs_test):.3f} end:{accs_test[-1]:.3f}\")\n",
    "                    \n",
    "plt.legend()\n",
    "plt.savefig(\"files/03_noisynas_v0_softmax_9.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.60000000000001"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(accs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1679,  0.0724, -0.5550,  0.0567,  0.2950,  0.0881, -0.0326,  0.1185,\n",
       "        -0.3093, -0.0916], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynet.non_linearity.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
