{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "import time\n",
    "import sys, io\n",
    "\n",
    "import mylibrary.datasets as datasets\n",
    "import mylibrary.nnlib as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hidden_neuron_number(i, o):\n",
    "    return (max(i,o)*(min(i,o)**2))**(1/3)\n",
    "\n",
    "\n",
    "class Shortcut_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=(3,3), stride=1):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self._kernel = np.array(kernel, dtype=int)\n",
    "        self._padding = tuple(((self._kernel-1)/2).astype(int))\n",
    "        self._stride = stride\n",
    "        _wd = nn.Conv2d(input_dim, output_dim, self._kernel, stride=self._stride,\n",
    "                        padding=self._padding, bias=False).weight.data\n",
    "        ## Shape = OutputDim, InputDim, Kernel0, Kernel1\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "        del _wd\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.shape[1] > 0 and self.weight.shape[0] > 0:\n",
    "            return F.conv2d(x, self.weight, stride=self._stride, padding=self._padding)\n",
    "        ### output dim is 0\n",
    "        elif self.weight.shape[0] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###       #out_dim #inp_dim            #kernel\n",
    "            w = torch.zeros(1, 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return torch.zeros(o.shape[0], 0, o.shape[2], o.shape[3], dtype=x.dtype, device=x.device)\n",
    "        ### input dim is 0\n",
    "        elif x.shape[1] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###             #out_dim            #inp_dim            #kernel\n",
    "            w = torch.zeros(self.weight.shape[0], 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return o.data\n",
    "        else:\n",
    "            raise(f\"Unknown shape of input {x.shape} or weight {self.weight.shape}\")\n",
    "\n",
    "#     def decay_std_ratio(self, factor):\n",
    "#         self.weight.data = self.weight.data - self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "        \n",
    "#     def decay_std_ratio_grad(self, factor):\n",
    "#         self.weight.grad = self.weight.grad + self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "    \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        pass\n",
    "    \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "        pass\n",
    "     \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "        _w = self.weight.data[remaining, :]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "        _w = self.weight.data[:, remaining]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        _w = torch.cat((self.weight.data, torch.zeros(o, num, k0, k1, dtype=self.weight.data.dtype,\n",
    "                                                      device=self.weight.data.device)), dim=1)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i)\n",
    "        _new = torch.empty(num, i, k0, k1, dtype=self.weight.data.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        _w = torch.cat((self.weight.data, _new), dim=0)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)        \n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}S▚:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NonLinearity_Conv(nn.Module):\n",
    "\n",
    "#     def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "#         super().__init__()\n",
    "#         self.tree = tree\n",
    "#         self.bias = nn.Parameter(torch.zeros(io_dim))\n",
    "#         self.actf = actf_obj\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.actf(x+self.bias.view(1,-1,1,1))\n",
    "\n",
    "#     def add_neuron(self, num):\n",
    "#         _b = torch.cat((self.bias.data, torch.zeros(num, dtype=self.bias.data.dtype,\n",
    "#                                                     device=self.bias.data.device)))\n",
    "#         del self.bias\n",
    "#         self.bias = nn.Parameter(_b)\n",
    "        \n",
    "#     def remove_neuron(self, remaining):\n",
    "#         _b = self.bias.data[remaining]\n",
    "#         del self.bias\n",
    "#         self.bias = nn.Parameter(_b)\n",
    "\n",
    "class NonLinearity_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.bn = nn.BatchNorm2d(io_dim)\n",
    "        self.actf = actf_obj\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## if empty tensor forward that to next layer.\n",
    "        if x.shape[1] < 1:\n",
    "            return x\n",
    "        return self.actf(self.bn(x))\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        ####https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d\n",
    "        ## running_mean\n",
    "        _rm = self.bn.running_mean\n",
    "        _rm = torch.cat((_rm, torch.zeros(num, dtype=_rm.dtype, device=_rm.device)))\n",
    "        self.bn.running_mean = _rm\n",
    "        \n",
    "        ## running_var\n",
    "        _rv = self.bn.running_var\n",
    "        _rv = torch.cat((_rv, torch.ones(num, dtype=_rv.dtype, device=_rv.device)))\n",
    "        self.bn.running_var = _rv\n",
    "        \n",
    "        ## weight\n",
    "        _w = self.bn.weight.data\n",
    "        _w = torch.cat((_w, torch.ones(num, dtype=_w.dtype, device=_w.device)*0.01))\n",
    "        del self.bn.weight\n",
    "        self.bn.weight = nn.Parameter(_w)\n",
    "        \n",
    "        ## bias\n",
    "        _b = self.bn.bias.data\n",
    "        _b = torch.cat((_b, torch.zeros(num, dtype=_b.dtype, device=_b.device)))\n",
    "        del self.bn.bias\n",
    "        self.bn.bias = nn.Parameter(_b)\n",
    "        \n",
    "        self.bn.num_features += num\n",
    "        return\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        ## running_mean\n",
    "        _rm = self.bn.running_mean[remaining]\n",
    "        self.bn.running_mean = _rm\n",
    "        \n",
    "        ## running_var\n",
    "        _rv = self.bn.running_var[remaining]\n",
    "        self.bn.running_var = _rv\n",
    "        \n",
    "        ## weight\n",
    "        _w = self.bn.weight.data[remaining]\n",
    "        del self.bn.weight\n",
    "        self.bn.weight = nn.Parameter(_w)\n",
    "        \n",
    "        ## bias\n",
    "        _b = self.bn.bias.data[remaining]\n",
    "        del self.bn.bias\n",
    "        self.bn.bias = nn.Parameter(_b)\n",
    "        \n",
    "        self.bn.num_features = len(remaining)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearity(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.bias = nn.Parameter(torch.zeros(io_dim))\n",
    "        self.actf = actf_obj\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actf(x+self.bias)\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        _b = torch.cat((self.bias.data, torch.zeros(num, dtype=self.bias.data.dtype,\n",
    "                                                    device=self.bias.data.device)))\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        _b = self.bias.data[remaining]\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, hidden_dim, output_dim, stride=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "#         self.stride = stride\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = HierarchicalResidual_Conv(self.tree, input_dim, hidden_dim, stride=stride, activation=activation) \n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = HierarchicalResidual_Conv(self.tree, hidden_dim, output_dim, activation=activation)\n",
    "        self.fc1.shortcut.weight.data *= 0.\n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "            self.apnz = self.apnz/self.count\n",
    "            self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "#         self.count = None\n",
    "\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "#             self.significance += z.pow(2).sum(dim=0)\n",
    "            self.significance += z.abs().sum(dim=0)\n",
    "#             self.significance += z.abs().pow(0.8).sum(dim=0)\n",
    "#             print(f\"SIG ACT:\\n{float(self.activations.abs().mean())}\")\n",
    "#             print(f\"GRAD Mean, Std:\\n{float(grad_output[0].data.abs().mean()), float(grad_output[0].data.std())}\")\n",
    "\n",
    "            if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "                self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "        #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "                self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below=None, above=None, mask=None):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(self.significance.numel(), dtype=torch.bool)\n",
    "        if below:\n",
    "            mask = torch.logical_or(mask,self.significance<=below)\n",
    "        if above:\n",
    "            mask = torch.logical_or(mask,self.significance>above)\n",
    "            \n",
    "        print(f\"Significance:\\n{self.significance}\\nPrune:\\n{mask}\")\n",
    "            \n",
    "        self.to_remove = torch.nonzero(mask).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "#         max_dim = np.ceil((self.tree.parent_dict[self].input_dim+\\\n",
    "#             self.tree.parent_dict[self].output_dim)/2)\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None:\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10, 3)*0.\n",
    "a.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, stride=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.stride = 1\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, stride=self.stride).to(self.tree.device)\n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None ## this can be Residual Layer or None\n",
    "        ##### only one of shortcut or residual can be None at a time\n",
    "        self.forward = self.forward_shortcut\n",
    "        \n",
    "        self.std_ratio = 0. ## 0-> all variation due to shortcut, 1-> residual\n",
    "        self.target_std_ratio = 0. ##\n",
    "    \n",
    "    def forward_both(self, r):\n",
    "        s = self.shortcut(r)\n",
    "        r = self.residual(r)\n",
    "\n",
    "        if self.residual.hook is None: ### dont execute when computing significance\n",
    "            s_std = torch.std(s, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            r_std = torch.std(r, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            stdr = r_std/(s_std+r_std)\n",
    "\n",
    "            self.std_ratio = self.tree.beta_std_ratio*self.std_ratio + (1-self.tree.beta_std_ratio)*stdr.data\n",
    "            if r_std.min() > 1e-9:\n",
    "                ## recover for the fact that when decaying neurons, target ratio should also be reducing\n",
    "                if self.tree.total_decay_steps:\n",
    "                    i, o = self.shortcut.weight.shape[1],self.shortcut.weight.shape[0]\n",
    "                    if self.shortcut.to_remove is not None:\n",
    "                        i -= len(self.shortcut.to_remove)\n",
    "                    if self.shortcut.to_freeze is not None:\n",
    "                        o -= len(self.shortcut.to_freeze)\n",
    "                    h = self.residual.hidden_dim\n",
    "                    if self.residual.to_remove is not None:\n",
    "                        h -= len(self.residual.to_remove)\n",
    "                    \n",
    "#                     tr = h/np.ceil((i+o)/2 +1)\n",
    "                    tr = h/_get_hidden_neuron_number(i, o)\n",
    "                    self.compute_target_std_ratio(tr)\n",
    "                else:\n",
    "                    self.compute_target_std_ratio()\n",
    "                self.get_std_loss(stdr)\n",
    "        return s+r\n",
    "    \n",
    "    def forward_shortcut(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def forward_residual(self, x):\n",
    "        self.compute_target_std_ratio()\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def compute_target_std_ratio(self, tr = None):\n",
    "        if tr is None:\n",
    "#             tr = self.residual.hidden_dim/np.ceil((self.input_dim+self.output_dim)/2 +1)\n",
    "            tr = self.residual.hidden_dim/_get_hidden_neuron_number(self.input_dim, self.output_dim)\n",
    "#             tr = self.residual.hidden_dim/np.ceil(self.output_dim/2 +1)\n",
    "\n",
    "        tr = np.clip(tr, 0., 1.)\n",
    "        self.target_std_ratio = self.tree.beta_std_ratio*self.target_std_ratio +\\\n",
    "                                (1-self.tree.beta_std_ratio)*tr\n",
    "        pass        \n",
    "    \n",
    "    def get_std_loss(self, stdr):\n",
    "        del_std = self.target_std_ratio-stdr\n",
    "        del_std_loss = (del_std**2 + torch.abs(del_std)).mean()\n",
    "#         del_std_loss = (del_std**2).mean()\n",
    "        self.tree.std_loss += del_std_loss\n",
    "        return\n",
    "            \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_freezing_connection(to_freeze)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_decaying_connection(to_remove)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_freezed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.remove_freezed_connection(remaining)\n",
    "            if self.shortcut: self.std_ratio = self.std_ratio[:, remaining]\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_decayed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_input_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_output_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.add_output_connection(num)\n",
    "            # if torch.is_tensor(self.std_ratio):\n",
    "            if self.shortcut:\n",
    "                self.std_ratio = torch.cat((self.std_ratio, torch.zeros(1, num, device=self.tree.device)), dim=1)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        \n",
    "        if self.residual is None:\n",
    "            # print(f\"Adding {num} hidden units.. in new residual_layer\")\n",
    "            self.residual = Residual_Conv(self.tree, self.input_dim,\n",
    "                                          num, self.output_dim, stride=self.stride,\n",
    "                                          activation=self.activation).to(self.tree.device)\n",
    "            \n",
    "            self.tree.parent_dict[self.residual] = self\n",
    "            if self.shortcut is None:\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            else:\n",
    "                self.forward = self.forward_both\n",
    "                self.std_ratio = torch.zeros(1, self.output_dim, device=self.tree.device)\n",
    "                \n",
    "        else:\n",
    "            # print(f\"Adding {num} hidden units..\")\n",
    "            self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            if self.std_ratio.min()>0.98 and self.target_std_ratio>0.98:\n",
    "                del self.tree.parent_dict[self.shortcut]\n",
    "                del self.shortcut\n",
    "                self.shortcut = None\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            \n",
    "        elif self.target_std_ratio<0.95:\n",
    "            self.shortcut = Shortcut_Conv(self.tree, self.input_dim, self.output_dim, stride=self.stride)\n",
    "            self.shortcut.weight.data *= 0.\n",
    "            self.forward = self.forward_both\n",
    "            \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.residual.hidden_dim < 1:\n",
    "            del self.tree.parent_dict[self.residual]\n",
    "            del self.residual\n",
    "            ### its parent (Residual_Conv) removes it from dynamic list if possible\n",
    "            self.residual = None\n",
    "            self.forward = self.forward_shortcut\n",
    "            self.std_ratio = 0.\n",
    "            return\n",
    "        \n",
    "#         max_dim = np.ceil((self.input_dim+self.output_dim)/2)\n",
    "        # max_dim = min((self.input_dim, self.output_dim))+1\n",
    "        max_dim = _get_hidden_neuron_number(self.input_dim, self.output_dim) + 1 \n",
    "        # print(\"MaxDIM\", max_dim, self.residual.hidden_dim)\n",
    "        if self.residual.hidden_dim > max_dim:\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc0)\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc1)\n",
    "            # print(\"Added\", self.residual)\n",
    "            \n",
    "        # self.residual.fc0.morph_network()\n",
    "        # self.residual.fc1.morph_network()\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        stdr = self.std_ratio\n",
    "        if torch.is_tensor(self.std_ratio):\n",
    "            stdr = self.std_ratio.min()\n",
    "            \n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{self.target_std_ratio}, s:{stdr}\")\n",
    "        if self.shortcut:\n",
    "            self.shortcut.print_network_debug(depth+1)\n",
    "        if self.residual:\n",
    "            self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        if self.residual is None:\n",
    "            return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            print(f\"{pre_string}╠════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}║    \")\n",
    "            print(f\"{pre_string}╠════╝\")\n",
    "        else:\n",
    "            print(f\"{pre_string}╚════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}     \")\n",
    "            print(f\"{pre_string}╔════╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Conv Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation, hidden_dim, post_activation=None):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "        self.post_activation = post_activation\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = hrnet0\n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = hrnet1\n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        if self.post_activation:\n",
    "            x = self.post_activation(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "            self.apnz = self.apnz/self.count\n",
    "            self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "#         self.count = None\n",
    "\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "#             self.significance += z.pow(2).sum(dim=0)\n",
    "            self.significance += z.abs().sum(dim=0)\n",
    "#             self.significance += z.abs().pow(0.8).sum(dim=0)\n",
    "#             print(\"Current Significance \\n\", self.significance)\n",
    "#             print(f\"SIG ACT:\\n{float(self.activations.abs().mean())}\")\n",
    "#             print(f\"GRAD Mean, Std:\\n{float(grad_output[0].data.abs().mean()), float(grad_output[0].data.std())}\")\n",
    "\n",
    "            if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "                self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "        #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "                self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below=None, above=None, mask=None):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(self.significance.numel(), dtype=torch.bool)\n",
    "        if below:\n",
    "            mask = torch.logical_or(mask,self.significance<=below)\n",
    "        if above:\n",
    "            mask = torch.logical_or(mask,self.significance>above)\n",
    "            \n",
    "        print(f\"Significance:\\n{self.significance}\\nPrune:\\n{mask}\")\n",
    "        \n",
    "        self.to_remove = torch.nonzero(mask).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None:\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-96a8be50163c>:3: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  torch.nonzero(torch.logical_and(a,b))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10)<0 \n",
    "b = torch.randn(10) > 0.5\n",
    "torch.nonzero(torch.logical_and(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation=nn.ReLU(), post_activation=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = hrnet0.input_dim\n",
    "        self.output_dim = hrnet1.output_dim\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = None\n",
    "        self.residual = Residual_Conv_Connector(self.tree, hrnet0, hrnet1,\n",
    "                                                activation, hrnet0.output_dim, post_activation)\n",
    "        self.tree.parent_dict[self.residual] = self\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.residual.fc1.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.residual.fc1.add_output_connection(num)\n",
    "        \n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):  \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        print(f\"{pre_string}╚╗\")\n",
    "        self.residual.print_network(f\"{pre_string} \")\n",
    "        print(f\"{pre_string}╔╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut only Hierarchical Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        _wd = nn.Linear(input_dim, output_dim, bias=False).weight.data\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## input_dim        ## output_dim\n",
    "        if x.shape[1] + self.weight.shape[1] > 0:\n",
    "            return x.matmul(self.weight.t())\n",
    "        else:\n",
    "            # print(x.shape, self.weight.shape)\n",
    "            # print(x.matmul(self.weight.t()))\n",
    "            if x.shape[1] + self.weight.shape[1] == 0:\n",
    "                return torch.zeros(x.shape[0], self.weight.shape[0], dtype=x.dtype, device=x.device)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        pass\n",
    "    \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "        pass\n",
    "            \n",
    "     \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "        _w = self.weight.data[remaining, :]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "        _w = self.weight.data[:, remaining]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        _w = torch.zeros(o, num, dtype=self.weight.data.dtype, device=self.weight.data.device)\n",
    "        _w += torch.randn_like(_w)*0.01\n",
    "        _w = torch.cat((self.weight.data, _w), dim=1)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i)\n",
    "        _new = torch.empty(num, i, dtype=self.bias.weight.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        _w = torch.cat((self.weight.data, _new), dim=0)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)        \n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}S:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n",
    "\n",
    "\n",
    "class HierarchicalResidual_Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=None, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        ## this can be Shortcut Layer or None\n",
    "        if kernel is None:\n",
    "            self.shortcut = Shortcut(tree, self.input_dim, self.output_dim) \n",
    "        else:\n",
    "            self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, kernel, stride) \n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.shortcut.start_freezing_connection(to_freeze)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.shortcut.start_decaying_connection(to_remove)\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.shortcut.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.shortcut.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.shortcut.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.shortcut.add_output_connection(num)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        print(\"Cannot Add Hidden neuron to Shortcut Only Layer\")\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        pass\n",
    "        \n",
    "    def morph_network(self):\n",
    "        pass\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.shortcut.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree and Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree_State():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.DYNAMIC_LIST = set() ## residual parent is added, to make code effecient.\n",
    "        ## the parents which is not intended to have residual connection should not be added.\n",
    "        self.beta_std_ratio = None\n",
    "        self.beta_del_neuron = None\n",
    "        self.device = 'cpu'\n",
    "    \n",
    "        self.parent_dict = {}\n",
    "    \n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual:set = None\n",
    "        self.freeze_connection_shortcut:set = None\n",
    "        self.decay_connection_shortcut:set = None\n",
    "\n",
    "        self.decay_rate_std = 0.001\n",
    "\n",
    "        self.add_to_remove_ratio = 2.\n",
    "        pass\n",
    "    \n",
    "    def get_decay_factor(self):\n",
    "        ratio = self.current_decay_step/self.total_decay_steps\n",
    "#         self.decay_factor = np.exp(-2*ratio)*(1-ratio)\n",
    "        self.decay_factor = (1-ratio)**2\n",
    "        pass\n",
    "    \n",
    "    def clear_decay_variables(self):\n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual = None\n",
    "        self.freeze_connection_shortcut = None\n",
    "        self.decay_connection_shortcut = None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constructing Hierarchical Residual CNN (Resnet Inspired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutActivation(nn.Module):\n",
    "    \n",
    "    def __init__(self, p=0.3, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout2d(p)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dropout(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, device, input_dim = 1, hidden_dims = [8, 16, 32, 64], output_dim = 10, final_activation=None,\n",
    "                 num_stat=5, num_std=100, decay_rate_std=0.001):\n",
    "        super().__init__()\n",
    "        self.tree = Tree_State()\n",
    "        self.tree.beta_del_neuron = (num_stat-1)/num_stat\n",
    "        self.tree.beta_std_ratio = (num_std-1)/num_std\n",
    "        self.tree.decay_rate_std = decay_rate_std\n",
    "        self.tree.device = device\n",
    "        \n",
    "        self.root_net = None\n",
    "        self._construct_root_net(input_dim, hidden_dims, output_dim)\n",
    "        \n",
    "#         self.tree.DYNAMIC_LIST.add(self.root_net)\n",
    "        self.tree.parent_dict[self.root_net] = None\n",
    "        \n",
    "        if final_activation is None:\n",
    "            final_activation = lambda x: x\n",
    "        self.non_linearity = NonLinearity(\"Root\", output_dim, final_activation)\n",
    "        \n",
    "        self.neurons_added = 0\n",
    "\n",
    "        self._remove_below = None ## temporary variable\n",
    "        \n",
    "    def _construct_root_net(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        actf = DropoutActivation()\n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 8, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 8, 8, activation=actf)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 8, 16, stride=2, activation=actf)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2, activation=actf)\n",
    "        hrn3 = HierarchicalResidual_Conv(self.tree, 32, 32, stride=2, activation=actf)\n",
    "\n",
    "    \n",
    "        actf = lambda x: x\n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0, actf)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1, actf)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2, actf)\n",
    "        hrnR0123 = HierarchicalResidual_Connector(self.tree, hrnR012, hrn3, actf)\n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 32, 10)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR0123fc = HierarchicalResidual_Connector(self.tree, hrnR0123, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR0123fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "        morphables = [self.root_net, hrnR0123, hrnR012, hrnR01, hrnR0, hrn3, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def _construct_root_net2(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        \n",
    "        \n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 16, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 16, 16)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 32, 64, stride=2)\n",
    "\n",
    "        actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "    \n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1, actf)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2, actf)\n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 64, 10)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR012fc = HierarchicalResidual_Connector(self.tree, hrnR012, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR012fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "        morphables = [hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrnR012, hrnR01, hrnR0, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.non_linearity(self.root_net(x))\n",
    "\n",
    "    def add_neurons(self, num):\n",
    "        num_stat = num//2\n",
    "        num_random = num - num_stat\n",
    "        \n",
    "        DL = list(self.tree.DYNAMIC_LIST)\n",
    "        if num_random>0:\n",
    "            rands = torch.randint(high=len(DL), size=(num_random,))\n",
    "            index, count = torch.unique(rands, sorted=False, return_counts=True)\n",
    "            for i, idx in enumerate(index):\n",
    "                DL[idx].add_hidden_neuron(int(count[i]))\n",
    "\n",
    "        if num_stat>0:\n",
    "            del_neurons = []\n",
    "            for hr in DL:\n",
    "                if hr.residual:\n",
    "                    del_neurons.append(hr.residual.del_neurons)#+1e-7)\n",
    "                else:\n",
    "                    del_neurons.append(0.)#1e-7) ## residual layer yet not created \n",
    "            \n",
    "            prob_stat = torch.tensor(del_neurons)\n",
    "            prob_stat = torch.log(torch.exp(prob_stat)+1.)\n",
    "            m = torch.distributions.multinomial.Multinomial(total_count=num_stat,\n",
    "                                                            probs= prob_stat)\n",
    "            count = m.sample()#.type(torch.long)\n",
    "            for i, hr in enumerate(DL):\n",
    "                if count[i] < 1: continue\n",
    "                hr.add_hidden_neuron(int(count[i]))\n",
    "        \n",
    "        self.neurons_added += num \n",
    "        pass\n",
    "\n",
    "    def identify_removable_neurons(self, num=None, threshold_min=0., threshold_max=1.):\n",
    "        \n",
    "        all_sig = []\n",
    "        self.all_sig_ = []\n",
    "        \n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                all_sig.append(hr.residual.significance)\n",
    "                \n",
    "        all_sigs = torch.cat(all_sig)\n",
    "        del all_sig\n",
    "        \n",
    "#         print(\"All_sigs\", all_sigs)\n",
    "        \n",
    "#         print(\"Normalization\", (all_sigs/all_sigs.sum()).sum())\n",
    "        \n",
    "        ### Normalizes such that importance 1 is average importance\n",
    "        normalizer = float(torch.sum(all_sigs))/len(all_sigs)\n",
    "        all_sig = all_sigs/normalizer\n",
    "\n",
    "        ### Normalizes to range [0, 1]\n",
    "#         max_sig = all_sigs.max()\n",
    "#         all_sig = all_sigs/(max_sig+1e-9)\n",
    "#         print(\"All_sig\", all_sig)\n",
    "#         print(\"Sig sum\", all_sig.sum())\n",
    "        print(f\"Significance Stat:\\nMin, Max: {float(all_sig.min()), float(all_sig.max())}\")\n",
    "        print(f\"Mean, Std: {float(all_sig.mean()), float(all_sig.std())}\")\n",
    "        all_sig = all_sig[all_sig<threshold_max]\n",
    "        if len(all_sig)<1: ## if all significance is above threshold max \n",
    "            return 0, None, all_sigs\n",
    "        all_sig = torch.sort(all_sig)[0] ### sorted significance scores\n",
    "        \n",
    "        self.all_sig_ = all_sig\n",
    "        \n",
    "        if not num:num = int(np.ceil(self.neurons_added/self.tree.add_to_remove_ratio))\n",
    "        ## reset the neurons_added number if decay is started\n",
    "\n",
    "        remove_below = threshold_min\n",
    "        if num>len(all_sig):\n",
    "            remove_below = float(all_sig[-1])\n",
    "        elif num>0:\n",
    "            remove_below = float(all_sig[num-1])\n",
    "        \n",
    "        ### sig < threshold_min is always removed; whatsoever\n",
    "        if remove_below < threshold_min:\n",
    "            remove_below = threshold_min\n",
    "            \n",
    "        print(\"remove_below\", remove_below)\n",
    "        remove_below *= normalizer\n",
    "#         remove_below *= max_sig\n",
    "#         print(\"remove_below\", remove_below)\n",
    "\n",
    "        self._remove_below = remove_below\n",
    "#         self._remove_above = remove_above*normalizer\n",
    "        self._remove_above = None\n",
    "\n",
    "        return remove_below, all_sigs\n",
    "\n",
    "    def decay_neuron_start(self, decay_steps=1000):\n",
    "        if self._remove_below is None: return 0\n",
    "        \n",
    "        self.neurons_added = 0 ## resetting this variable\n",
    "        \n",
    "        self.tree.total_decay_steps = decay_steps\n",
    "        self.tree.current_decay_step = 0\n",
    "        self.tree.remove_neuron_residual = set()\n",
    "        self.tree.freeze_connection_shortcut = set()\n",
    "        self.tree.decay_connection_shortcut = set()\n",
    "        \n",
    "        count_remove = 0\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                ### always prune 1 % of the neurons randomly. It might overlap with less significant neurons\n",
    "                mask = torch.bernoulli(torch.ones_like(hr.residual.significance)*0.05).type(torch.bool)\n",
    "                count_remove += hr.residual.identify_removable_neurons(below=self._remove_below,\n",
    "                                                                       above=self._remove_above,\n",
    "                                                                       mask = mask\n",
    "                                                                      )\n",
    "        if count_remove<1:\n",
    "            self.tree.clear_decay_variables()\n",
    "        return count_remove\n",
    "    \n",
    "    def decay_neuron_step(self):\n",
    "        if self.tree.total_decay_steps is None:\n",
    "            return\n",
    "        \n",
    "        self.tree.current_decay_step += 1\n",
    "        \n",
    "        if self.tree.current_decay_step < self.tree.total_decay_steps:\n",
    "            self.tree.get_decay_factor()\n",
    "            for sh in self.tree.decay_connection_shortcut:\n",
    "                sh.decay_connection_step()\n",
    "            for sh in self.tree.freeze_connection_shortcut:\n",
    "                sh.freeze_connection_step()\n",
    "        else:\n",
    "            for rs in self.tree.remove_neuron_residual:\n",
    "                rs.remove_decayed_neurons()\n",
    "            \n",
    "            self.tree.clear_decay_variables()\n",
    "            \n",
    "    def compute_del_neurons(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.compute_del_neurons()\n",
    "    \n",
    "    def maintain_network(self):\n",
    "        self.root_net.maintain_shortcut_connection()\n",
    "        self.root_net.morph_network()\n",
    "        \n",
    "    def start_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.start_computing_significance()\n",
    "\n",
    "    def finish_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.finish_computing_significance()\n",
    "            \n",
    "    def print_network_debug(self):\n",
    "        self.root_net.print_network_debug(0)\n",
    "        \n",
    "    def print_network(self):\n",
    "        print(self.root_net.input_dim)\n",
    "        self.root_net.print_network()\n",
    "        print(\"│\")\n",
    "        print(self.root_net.output_dim)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.binomial(1, 0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.bernoulli(torch.ones(10)*0.01).type(torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dycnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = transforms.Compose([\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "cifar_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "# learning_rate = 0.00003\n",
    "\n",
    "dynet = Dynamic_CNN(device).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563, 313)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameters\n",
    "num_add_neuron = 50#25#10\n",
    "num_decay_steps = int(len(train_loader)*3)#3\n",
    "\n",
    "remove_above = 10\n",
    "threshold_max = 1\n",
    "threshold_min = 0.01\n",
    "\n",
    "train_epoch_min = 1 #1\n",
    "train_epoch_max = 6 #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.add_to_remove_ratio = 1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4689"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decay_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoTrainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_func = None\n",
    "        self.adding_func = None\n",
    "        self.pruning_func = None\n",
    "        self.maintainance_func = None\n",
    "        self.extra_func = None\n",
    "        \n",
    "        self.log_func = None\n",
    "        \n",
    "    def loop(self, count = 15):\n",
    "        cb = count\n",
    "        for i in range(count):\n",
    "            if i>-0.1:\n",
    "                self.adding_func()\n",
    "            else:\n",
    "                global optimizer, warmup\n",
    "                dynet.print_network()    \n",
    "                optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#                 optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "                warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader))\n",
    "            \n",
    "            self.training_func()\n",
    "            \n",
    "            if i>-0.1:\n",
    "                self.pruning_func()\n",
    "            \n",
    "            self.maintainance_func()\n",
    "            \n",
    "            self.log_func(i)\n",
    "            \n",
    "            if self.extra_func:\n",
    "                self.extra_func()\n",
    "            \n",
    "            print(f\"=====================\")\n",
    "            print(f\"===LOOPS FINISHED :{i} ===\")\n",
    "            print(f\"Pausing for 2 second to give user time to STOP PROCESS\")\n",
    "            time.sleep(2)\n",
    "        self.training_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when to stop training functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeff(num_iter, coeff0, coeff1, coeff2, coeff_opt, loss_list):\n",
    "    if len(loss_list)<10: return np.array([0]), np.array([0]), float(coeff0.data.cpu()[0])\n",
    "    \n",
    "    _t = torch.tensor(loss_list)\n",
    "    _t = (_t - _t[-1])/(_t[0]-_t.min()) ## normalize to make first point at 1 and last at 0 \n",
    "    _t = torch.clamp(_t, -1.1, 1.1)\n",
    "    _x = torch.linspace(0, 1, steps=len(_t))\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        coeff_opt.zero_grad()\n",
    "        _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "        _loss = ((_y - _t)**2).mean()\n",
    "        _loss.backward()\n",
    "        coeff_opt.step()\n",
    "\n",
    "        coeff0.data = torch.clamp(coeff0.data, -20., 20.)\n",
    "        coeff1.data = torch.clamp(coeff1.data, 0.7, 2.)\n",
    "        coeff2.data = torch.clamp(coeff2.data, -0.2,0.1)\n",
    "        \n",
    "    if torch.isnan(coeff0.data[0]):\n",
    "        coeff0.data[0] = 0.\n",
    "        coeff1.data[0] = 0.\n",
    "        coeff2.data[0] = 1. ## this gives signal\n",
    "        \n",
    "    _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "    return _x.numpy(), _t.numpy(), _y.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global variables\n",
    "optimizer = None\n",
    "warmup = None\n",
    "coeff_opt = None\n",
    "\n",
    "loss_all = []\n",
    "accs_all = []\n",
    "accs_test = []\n",
    "\n",
    "## for adam optimizer = \n",
    "learning_rate *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupLR_Polynomial():\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_epoch, num_batch_in_epoch, power=5):\n",
    "        self.warmup_epoch = warmup_epoch\n",
    "        self.optimizer = optimizer\n",
    "        self.num_batch = num_batch_in_epoch\n",
    "        self.steps = 0\n",
    "        self.power = power\n",
    "        \n",
    "    def step(self):\n",
    "        steps = self.steps/self.num_batch\n",
    "        self.steps += 1\n",
    "        \n",
    "        factor = 1\n",
    "        warming = False\n",
    "        if steps<self.warmup_epoch:\n",
    "            factor = (steps/self.warmup_epoch)**self.power\n",
    "            warming = True\n",
    "            \n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] *= factor\n",
    "        \n",
    "        return warming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neurons_func():\n",
    "    global optimizer, warmup, added\n",
    "    ### number of neurons\n",
    "    count = 0\n",
    "    for hr in dynet.tree.DYNAMIC_LIST:\n",
    "        if hr.residual:\n",
    "            count += hr.residual.hidden_dim\n",
    "    ## add more neurons relatively (+x%)\n",
    "    adding = num_add_neuron+int(0.07*count)\n",
    "    dynet.add_neurons(adding)\n",
    "    print(f\"Adding {adding} Neurons\")\n",
    "    added = adding\n",
    "    dynet.print_network()    \n",
    "    optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader), power=1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_network_func():\n",
    "    global optimizer, warmup, loss_all, accs_all\n",
    "    \n",
    "    coeff0 = torch.zeros(1, requires_grad=True)\n",
    "    coeff1 = torch.zeros(1, requires_grad=True)\n",
    "    coeff2 = torch.zeros(1, requires_grad=True)\n",
    "    coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "    loss_list = []\n",
    "    prev_loss = None\n",
    "    beta_loss = (1000-1)/1000\n",
    "    loss_ = []\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    breakall=False\n",
    "    \n",
    "\n",
    "    steps_ = -1\n",
    "    for epoch in range(train_epoch_max):\n",
    "        \n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "        for train_x, train_y in train_loader:\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "            steps_ += 1\n",
    "            \n",
    "            dynet.decay_neuron_step()\n",
    "            dynet.tree.std_loss = 0.    \n",
    "\n",
    "            yout = dynet(train_x)\n",
    "            loss = criterion(yout, train_y) + dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "            \n",
    "            if steps_>100:\n",
    "                prev_loss = (1-beta_loss)*float(loss)+beta_loss*prev_loss\n",
    "                loss_list.append(prev_loss)\n",
    "            elif steps_ == 100:\n",
    "                loss_.append(float(loss))\n",
    "                prev_loss = np.mean(loss_)\n",
    "                loss_ = []\n",
    "            else:\n",
    "                loss_.append(float(loss))\n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=False)\n",
    "            warmup.step()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "            targets = train_y.data.cpu().numpy()\n",
    "\n",
    "            correct = (outputs == targets).sum()\n",
    "            train_acc += correct\n",
    "            train_count += len(outputs)\n",
    "\n",
    "            if steps_%100 == 0 and steps_>0:\n",
    "                if len(loss_list)>0:\n",
    "                    max_indx = np.argmax(loss_list)\n",
    "                    loss_list = loss_list[max_indx:]\n",
    "    #                 loss_all.append(float(loss))\n",
    "                \n",
    "                _x, _t, _y = update_coeff(50, coeff0, coeff1, coeff2, coeff_opt, loss_list)\n",
    "                _c = float(coeff0.data.cpu()[0])\n",
    "    #             if coeff2.data[0] > 0.5: ## this is a signal to reset optimizer\n",
    "                coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "                _info = f'ES: {epoch}:{steps_}, coeff:{_c:.3f}/{-5}, \\nLoss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "\n",
    "                ax.clear()\n",
    "                if len(_x)>0:\n",
    "                    ax.plot(_x, _t, c='c')\n",
    "                    ax.plot(_x, _y, c='m')\n",
    "                xmin, xmax = ax.get_xlim()\n",
    "                ymin, ymax = ax.get_ylim()\n",
    "                ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                ax2.clear()\n",
    "                if len(accs_all)>0:\n",
    "                    acc_tr = accs_all\n",
    "                    acc_te = accs_test\n",
    "                    if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                    if len(acc_te)>20: acc_te = acc_te[-20:]\n",
    "                    ax2.plot(acc_tr, label=\"train\")\n",
    "                    ax2.plot(acc_te, label=\"test\")\n",
    "                    ax2.legend(loc=\"lower right\")\n",
    "                    \n",
    "                    ymin, ymax = ax2.get_ylim()\n",
    "                    ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                    ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_te):.3f} end{acc_te[-1]:.3f}\")\n",
    "\n",
    "                \n",
    "                fig.canvas.draw()\n",
    "                plt.savefig(\"./files/_temp_train_plot2.png\")\n",
    "\n",
    "                if _c < -5 and epoch>train_epoch_min: \n",
    "                    breakall=True\n",
    "                    break\n",
    "                    \n",
    "        if not breakall:\n",
    "            accs_all.append(train_acc/train_count*100.)\n",
    "            with torch.no_grad():\n",
    "                corrects = 0\n",
    "                dynet.eval()\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x  = test_x.to(device)\n",
    "                    yout = dynet.forward(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                    correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                    corrects += correct\n",
    "                dynet.train()\n",
    "                accs_test.append(corrects/len(test_dataset)*100)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_func():\n",
    "    global optimizer, warmup\n",
    "    optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "#     optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader), power=0.5)\n",
    "    \n",
    "    \n",
    "    dynet.start_computing_significance()\n",
    "\n",
    "    for train_x, train_y in train_loader:\n",
    "        train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "        dynet.tree.std_loss = 0.    \n",
    "        yout = dynet(train_x)\n",
    "#         yout.backward(gradient=torch.ones_like(yout))\n",
    "        loss = criterion(yout, train_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=False)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    dynet.finish_computing_significance()\n",
    "    dynet.identify_removable_neurons(num=None,\n",
    "                                 threshold_min = threshold_min,\n",
    "                                 threshold_max = threshold_max)\n",
    "    num_remove = dynet.decay_neuron_start(decay_steps=num_decay_steps)\n",
    "    if num_remove > 0:\n",
    "        print(f\"pruning {num_remove} neurons.\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,4))\n",
    "        ax = fig.add_subplot(121)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        \n",
    "        loss_list = []\n",
    "        steps_ = -1\n",
    "        breakall=False\n",
    "        for epoch in range(train_epoch_max):\n",
    "            loss_ = []\n",
    "            train_acc = 0\n",
    "            train_count = 0\n",
    "            for train_x, train_y in train_loader:\n",
    "                train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "                steps_ += 1\n",
    "\n",
    "                dynet.decay_neuron_step()\n",
    "                dynet.tree.std_loss = 0.    \n",
    "\n",
    "                yout = dynet(train_x)\n",
    "                loss = criterion(yout, train_y) + dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "\n",
    "                loss_.append(float(loss))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=False)\n",
    "                warmup.step()\n",
    "                optimizer.step()\n",
    "\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                targets = train_y.data.cpu().numpy()\n",
    "                correct = (outputs == targets).sum()\n",
    "                train_acc += correct\n",
    "                train_count += len(outputs)\n",
    "\n",
    "                dynet.decay_neuron_step()\n",
    "                \n",
    "                if steps_%50 == 0 and steps_>0:\n",
    "                    loss = np.mean(loss_)\n",
    "                    loss_ = []\n",
    "                    loss_list.append(loss)\n",
    "                \n",
    "                if steps_%100 == 0 and steps_>0:\n",
    "                    \n",
    "                    _info = f'ES: {epoch}:{steps_}, Loss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "#                     print(_info)\n",
    "                    ax.clear()\n",
    "                    out = (yout.data.cpu().numpy()>0.5).astype(int)\n",
    "                    ax.plot(loss_list)\n",
    "                    \n",
    "                    xmin, xmax = ax.get_xlim()\n",
    "                    ymin, ymax = ax.get_ylim()\n",
    "                    ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                    ax2.clear()\n",
    "                    if len(accs_all)>0:\n",
    "                        acc_tr = accs_all\n",
    "                        acc_te = accs_test\n",
    "                        if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                        if len(acc_te)>20: acc_te = acc_te[-20:]\n",
    "                        ax2.plot(acc_tr, label=\"train\")\n",
    "                        ax2.plot(acc_te, label=\"test\")\n",
    "                        ax2.legend(loc=\"lower right\")\n",
    "\n",
    "                        ymin, ymax = ax2.get_ylim()\n",
    "                        ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                        ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_te):.3f} end{acc_te[-1]:.3f}\")\n",
    "\n",
    "                    \n",
    "                    fig.canvas.draw()\n",
    "                    plt.savefig(\"./files/_temp_prune_plot2.png\")\n",
    "#                     plt.pause(0.01)\n",
    "#                     print(\"\\n\")\n",
    "                    \n",
    "                if steps_>num_decay_steps+int(num_decay_steps/2): breakall=True\n",
    "                if breakall: break\n",
    "            if breakall: break\n",
    "                \n",
    "        if not breakall:\n",
    "            accs_all.append(train_acc/train_count*100.)\n",
    "            with torch.no_grad():\n",
    "                corrects = 0\n",
    "                dynet.eval()\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x  = test_x.to(device)\n",
    "                    yout = dynet.forward(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                    correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                    corrects += correct\n",
    "                dynet.train()\n",
    "                accs_test.append(corrects/len(test_dataset)*100)\n",
    "        plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintain_network():\n",
    "    dynet.compute_del_neurons()\n",
    "    dynet.maintain_network()\n",
    "    dynet.print_network()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network_stat(loop_indx):\n",
    "    stdout = sys.stdout\n",
    "    s = io.StringIO(newline=\"\")\n",
    "    sys.stdout = s\n",
    "    dynet.print_network()\n",
    "    sys.stdout = stdout\n",
    "    s.seek(0)\n",
    "    # prints = s.read()\n",
    "    architecture = s.getvalue()\n",
    "    s.close()\n",
    "    \n",
    "    ### number of neurons\n",
    "    count = 0\n",
    "    for hr in dynet.tree.DYNAMIC_LIST:\n",
    "        if hr.residual:\n",
    "            count += hr.residual.hidden_dim\n",
    "    \n",
    "    with open(\"05_dynamic_CNN_log_2_9.3.txt\", \"a+\") as f:\n",
    "        if loop_indx == 0:\n",
    "            ### Print the configuration at top.\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "            \n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%B %d, %Y @ %H:%M:%S\")\n",
    "            f.write(f\"DateTime: {dt_string}\")\n",
    "            \n",
    "            f.write(f\"num_add_neuron :{num_add_neuron}\\n add_to_remove_ratio :{dynet.tree.add_to_remove_ratio}\\n\")\n",
    "            f.write(f\"learning_rate :{learning_rate}\\n num_decay_steps :{num_decay_steps}\\n\")\n",
    "            f.write(f\"threshold_max :{threshold_max}\\n threshold_min :{threshold_min}\\n\")\n",
    "            f.write(f\"train_epoch_min :{train_epoch_min}\\n threshold_max :{train_epoch_max}\\n\")\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "        \n",
    "        f.write(f\"####################| Loop:{loop_indx} | Epoch: {len(accs_all)} \\n\")\n",
    "        num_params = sum(p.numel() for p in dynet.parameters())\n",
    "        num_trainable = sum(p.numel() for p in dynet.parameters() if p.requires_grad)\n",
    "        f.write(f\"| Dynamic Neurons:{count} | Total Parameters: {num_params} | Trainable Parameters: {num_trainable}\\n\")\n",
    "        f.write(f\"| Train Acc:{accs_all[-1]:.3f} | Test Acc: {accs_test[-1]:.3f}\\n\")\n",
    "        f.write(architecture)\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set all functions and begin automated loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AutoTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.adding_func = add_neurons_func\n",
    "trainer.training_func = training_network_func\n",
    "trainer.pruning_func = pruning_func\n",
    "trainer.maintainance_func = maintain_network\n",
    "trainer.log_func = save_network_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_neurons_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     8\n",
      "    ╔╝\n",
      "    8\n",
      "   ╔╝\n",
      "   16\n",
      "  ╔╝\n",
      "  32\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, p in enumerate(list(dynet.parameters())):\n",
    "#     print(i, p.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = list(dynet.parameters())[3]\n",
    "# print(p.shape)\n",
    "# p[8:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = list(dynet.parameters())[4]\n",
    "# print(p.shape)\n",
    "# p[0][8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = list(dynet.parameters())[7]\n",
    "# print(p.shape)\n",
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.add_hidden_neuron(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.residual.fc0.shortcut.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.residual.fc1.shortcut.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.residual.fc0.shortcut.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.residual.fc1.shortcut.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.add_hidden_neuron(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.residual.fc0.shortcut.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.residual.fc0.shortcut.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc0.residual.fc1.shortcut.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc0.residual.fc1.shortcut.weight[32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.shortcut.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.shortcut.weight[:, 32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOG\n",
    "# adding neuron on the hr_conv layer is okay.\n",
    "# adding new neuron on a hr_connector layer, makes outgoing weights zero; and incoming weights non-zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tx, ty = iter(train_loader).next()\n",
    "# tx, ty = tx.to(device), ty.to(device)\n",
    "# yy = dynet(tx)\n",
    "# criterion(yy, ty).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 56 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    9\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    12\n",
      "    ╠════╗\n",
      "    ║    5\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   19\n",
      "   ╠════╗\n",
      "   ║    9\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  33\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 42\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 7.0068583488464355)\n",
      "Mean, Std: (1.0, 1.1059379577636719)\n",
      "remove_below 0.30925285816192627\n",
      "Significance:\n",
      "tensor([93.9589, 93.2424, 81.4630, 87.5843, 86.7019, 97.0075, 75.2271, 51.2063,\n",
      "        93.3809], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([102.3470,  86.7170, 154.5639, 143.3364, 186.1050, 131.5760, 169.7406,\n",
      "         63.9263, 130.3700], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True,  True, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 213.3415,  262.2608,  122.9744,  416.5245,  175.0338,  408.7890,\n",
      "         667.5245,  199.7718,  316.6879,  472.0445,  203.2314,   87.7233,\n",
      "         306.1482,  216.2869,  249.8037,  498.2434, 1474.9861,  483.9875,\n",
      "         664.4467], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 402.3865,  397.1442,  706.1766,  299.6644,  438.4112, 1028.8096,\n",
      "         802.5443,  377.6004,  945.2522,  753.1306,  418.7605,  593.5031],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False,  True, False, False,\n",
      "        False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([203.4657, 171.8850,  84.9774, 137.6974, 104.6841, 217.9296, 219.3748,\n",
      "        117.6257, 263.5023,  89.3533,  87.8866, 124.0482, 119.3263, 131.4561,\n",
      "         70.8485, 244.3362, 189.9970, 140.8089,  98.4599,  81.9924, 194.1131,\n",
      "         68.5897,  60.8668,  55.9916, 151.3007, 132.9065, 208.5492,  97.1777,\n",
      "         88.0977, 210.5622,  82.7469, 246.4052, 476.2298], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True, False, False, False, False, False, False,\n",
      "        False,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 81.1881,  56.0078,  95.9747,  70.9152,   0.0000,  78.4050,  75.7377,\n",
      "        137.3290], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False,  True, False, False,  True, False, False, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([667.2465, 355.7237, 465.3157, 248.4837, 334.9793, 348.1297, 286.9812,\n",
      "        483.7744, 574.1857, 408.3979, 184.4294, 727.8558, 229.6492, 819.3560,\n",
      "        787.2595], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([203.7411, 162.4046, 119.6436,   0.0000, 149.9920], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([73.3951, 68.7886, 74.6192, 81.5742, 41.5033, 74.9797, 77.8913, 43.5207,\n",
      "        62.8738, 53.0109, 51.6693, 67.5251, 72.4868, 54.6976, 55.5954, 99.7654,\n",
      "        61.1379, 75.3032, 49.2589, 55.4667, 59.7334, 65.0996, 79.7890, 89.6406,\n",
      "        97.6697, 60.6196, 57.9066, 51.2108, 80.9775, 71.2173, 46.9311, 95.3261,\n",
      "        16.7503, 16.4107, 12.9258, 17.3878, 14.4830, 26.6987, 18.8803, 20.0734,\n",
      "        17.0295, 27.1507], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False,  True,  True,  True,\n",
      "         True, False, False,  True,  True, False,  True, False,  True,  True,\n",
      "         True,  True, False, False, False,  True,  True,  True, False, False,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True], device='cuda:1')\n",
      "pruning 37 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    7\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    10\n",
      "    ╠════╗\n",
      "    ║    4\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   19\n",
      "   ╠════╗\n",
      "   ║    8\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  30\n",
      "  ╠════╗\n",
      "  ║    6\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 16\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :0 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 58 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     25\n",
      "     ╠════╗\n",
      "     ║    18\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    14\n",
      "    ╠════╗\n",
      "    ║    10\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   25\n",
      "   ╠════╗\n",
      "   ║    11\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  36\n",
      "  ╠════╗\n",
      "  ║    14\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 20\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 9.308046340942383)\n",
      "Mean, Std: (0.9999998807907104, 1.4372552633285522)\n",
      "remove_below 0.03254006803035736\n",
      "Significance:\n",
      "tensor([105.0026, 102.9978, 106.8273,  93.6427, 120.9766,  99.5062,  77.0870,\n",
      "        121.1010,   0.0000,  69.1678,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "         True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 96.8137,  88.6694, 155.6776, 146.7902, 219.0369, 117.3297,  87.6304,\n",
      "         80.7462,   0.0000,   0.0000,   0.0000,   0.0000,  95.5091, 155.8086,\n",
      "        102.2395, 103.0603, 130.9632, 113.4162], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True, False, False,  True, False, False, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 225.9658,  423.5943,  211.6618,  460.4115,  330.9045, 1042.8412,\n",
      "        1359.7833,  351.4061,  706.2391,  546.2817,  423.2707,  201.4339,\n",
      "         562.1779,  357.9730,  223.7311,  391.2685, 2632.3459, 1042.4714,\n",
      "        1083.2467,    4.7129,    4.1673,    3.3898,   21.3018,    8.9769,\n",
      "           9.2024], device='cuda:1')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True, False,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 717.9032,  763.3140, 1692.5654,  337.9973,  786.2753, 1341.4445,\n",
      "        1833.8258, 2039.1676, 1631.8782,  452.8870,    5.9242,   15.3721,\n",
      "           2.6889,  117.9948], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 530.9007,  181.7601,   84.1327,  191.0381,  266.3617,  740.9777,\n",
      "         479.4820,  257.2080,  444.0337,  132.2240,   51.2772,  162.7671,\n",
      "         261.2080,  271.4390,  115.6759,  387.1084,  266.0611,  171.3832,\n",
      "          84.5485,  285.0426,  306.6677,  180.0333,  210.7640,  164.9089,\n",
      "         167.2521,  185.1991,  107.1171,  324.6411,  344.8679, 1063.7107,\n",
      "           9.0602,   91.8409,  110.2314,   49.3734,  184.7220,   51.3558],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False,\n",
      "         True, False, False, False, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([135.7808,  80.8401, 106.0215,  92.5218,  61.8011,  90.6177,   0.0000,\n",
      "         76.9718, 111.1378,   0.0000,   0.0000,   0.0000,   0.0000,  39.8778],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False,  True,\n",
      "         True,  True,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([4.7591e+02, 5.5005e+02, 5.0223e+02, 3.0276e+02, 4.2495e+02, 5.7335e+02,\n",
      "        6.4428e+02, 2.8391e+02, 1.0381e+03, 2.4847e+02, 3.9899e+02, 6.6259e+02,\n",
      "        4.1396e+02, 9.6209e+02, 1.2054e+03, 7.2107e+00, 4.5372e-01, 1.8133e+00,\n",
      "        1.1098e+00, 9.4387e-01, 4.0576e+00, 9.8790e-01, 1.1452e+00, 7.1872e-01,\n",
      "        3.0917e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([183.1160, 131.1446, 136.5717, 146.2057,   0.0000,   0.0000,  96.8411,\n",
      "          0.0000,   0.0000, 149.3287], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True,  True,  True,  True,  True, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.5691e+02, 1.6202e+02, 2.3679e+02, 1.9237e+02, 1.5377e+02, 2.2300e+02,\n",
      "        1.5976e+02, 1.3806e+02, 1.4651e+02, 1.7948e+02, 1.7327e+02, 1.4901e+02,\n",
      "        1.9527e+02, 1.2161e+02, 1.4867e+02, 1.4268e+02, 9.8976e+00, 1.3291e+01,\n",
      "        7.3826e-01, 1.1859e-01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True],\n",
      "       device='cuda:1')\n",
      "pruning 40 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    13\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    12\n",
      "    ╠════╗\n",
      "    ║    5\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   20\n",
      "   ╠════╗\n",
      "   ║    9\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  32\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 18\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :1 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 59 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     22\n",
      "     ╠════╗\n",
      "     ║    23\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    15\n",
      "    ╠════╗\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   27\n",
      "   ╠════╗\n",
      "   ║    13\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  37\n",
      "  ╠════╗\n",
      "  ║    19\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 23\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 10.429962158203125)\n",
      "Mean, Std: (0.9999999403953552, 1.544403076171875)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([142.6819,  95.7907,  97.1135,  56.4335,  84.8902, 104.3425,  92.8710,\n",
      "        152.7958,  65.3272,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([108.0182,  99.6794, 121.6066, 132.3314, 208.5174, 144.8675,  91.1365,\n",
      "         72.8290,  85.1889, 154.4247,  93.8383,  83.3323,  89.6363,   0.0000,\n",
      "          0.0000,  96.3597,  99.5111,  73.7900,   0.0000,  72.3067,  88.4725,\n",
      "        121.6087,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True, False, False, False,  True, False,\n",
      "        False, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([9.4099e+02, 2.3270e+02, 8.0825e+02, 3.9510e+02, 1.3088e+03, 1.5322e+03,\n",
      "        6.3581e+02, 9.4334e+02, 6.9156e+02, 2.9860e+02, 3.2371e+02, 8.1438e+02,\n",
      "        3.8943e+02, 5.2129e+02, 4.7109e+02, 3.3317e+03, 1.1909e+03, 2.2978e+03,\n",
      "        6.4372e+01, 8.8458e+00, 4.0812e+00, 3.6562e+00, 8.2998e-01, 7.0902e+00,\n",
      "        7.7256e-01, 3.1594e+01, 3.0130e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False,  True,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False,  True, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 658.7479,  802.2493, 1826.2460,  577.4189,  830.8635, 1487.7749,\n",
      "        1993.1454, 2014.6299, 1918.9890,  607.4039,  248.0109,  321.6134,\n",
      "           7.2385,   14.9477,    5.7030], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.2279e+02, 1.6243e+02, 3.7864e+02, 6.6760e+02, 1.3028e+03, 5.7539e+02,\n",
      "        3.9777e+02, 3.8742e+02, 3.7231e+02, 1.2130e+02, 1.8408e+02, 3.9663e+02,\n",
      "        3.2050e+02, 1.1886e+02, 3.2291e+02, 1.9531e+02, 2.5171e+02, 5.1027e+02,\n",
      "        2.9974e+02, 3.2904e+02, 3.6996e+02, 2.1649e+02, 3.1673e+02, 1.4466e+02,\n",
      "        6.5988e+02, 5.9685e+02, 1.7789e+03, 4.3376e+02, 1.4822e+02, 1.0587e+02,\n",
      "        5.6793e+02, 3.0095e+02, 1.4079e+00, 1.8033e+01, 7.8005e+00, 1.7491e+01,\n",
      "        2.9338e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False,  True,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([110.2917,  99.2967,  81.5178, 112.5752,  74.2482,  99.2496,  75.6202,\n",
      "         60.7704,  78.7761,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "         65.9126,   0.0000,   0.0000,   0.0000,  66.0766], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True,  True,  True, False,  True,  True,  True, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([4.3748e+02, 4.7548e+02, 6.0153e+02, 2.6540e+02, 5.0674e+02, 6.9766e+02,\n",
      "        7.6879e+02, 3.2825e+02, 1.6815e+03, 2.6289e+02, 5.9253e+02, 3.1726e+02,\n",
      "        4.0494e+02, 9.4311e+02, 1.5692e+03, 4.9386e-01, 1.0366e+00, 6.4866e+00,\n",
      "        3.2699e+00, 3.6218e+00, 5.6271e+00, 8.0513e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False,  True,  True, False, False, False,\n",
      "        False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([161.3101, 128.3789, 138.7532, 149.1641, 154.3660,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000, 121.1960,   0.0000,   0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.5821e+02, 1.8709e+02, 3.0760e+02, 2.0229e+02, 1.7801e+02, 2.8884e+02,\n",
      "        1.7701e+02, 1.2797e+02, 1.7965e+02, 1.9304e+02, 2.0324e+02, 1.4537e+02,\n",
      "        2.1926e+02, 1.4987e+02, 1.4390e+02, 1.4810e+02, 2.3640e+01, 3.6461e+01,\n",
      "        1.0556e-01, 5.0775e-01, 2.1091e-02, 6.2807e-02, 4.2875e-02],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True], device='cuda:1')\n",
      "pruning 43 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     19\n",
      "     ╠════╗\n",
      "     ║    19\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    15\n",
      "    ╠════╗\n",
      "    ║    6\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   21\n",
      "   ╠════╗\n",
      "   ║    9\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    11\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 18\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :2 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 60 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     27\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    4\n",
      "     ║    ╠════╝\n",
      "     ║    25\n",
      "     ║    ╠════╗\n",
      "     ║    ║    1\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    21\n",
      "    ╠════╗\n",
      "    ║    9\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   29\n",
      "   ╠════╗\n",
      "   ║    14\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  34\n",
      "  ╠════╗\n",
      "  ║    19\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 26\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 11.006329536437988)\n",
      "Mean, Std: (0.9999998211860657, 1.8338661193847656)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([ 77.5857,  95.3581, 133.8897,  89.2413, 142.7247, 103.9948,  93.5158,\n",
      "        223.9827,  69.2625,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 90.0243,  98.0869, 120.0408, 149.3229, 172.5392, 118.8995,  87.8669,\n",
      "         76.5426, 104.0600, 137.8962,  79.8837, 108.5755,  85.7224,  70.9980,\n",
      "         87.9216, 111.2850, 111.1973,  84.0488, 176.0403,   0.0000,   0.0000,\n",
      "         55.8321,   0.0000,   0.0000,  79.8471], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False,  True, False,  True,\n",
      "         True, False,  True,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.5051e+03, 3.1020e+02, 7.5542e+02, 8.0233e+02, 1.2334e+03, 7.6239e+02,\n",
      "        1.3214e+03, 3.4503e+02, 1.1767e+03, 9.3959e+02, 1.2363e+03, 9.6104e+02,\n",
      "        3.5281e+03, 3.2835e+03, 3.8751e+03, 1.2770e+02, 6.1590e+01, 9.7672e+00,\n",
      "        2.2826e+00, 2.7264e+00, 1.1842e+01, 4.1509e+00, 1.3254e+00, 3.2726e+00,\n",
      "        1.0412e+01, 1.1458e+01, 1.1423e+01, 6.4799e+00, 6.0843e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "        False, False,  True,  True, False,  True, False, False, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([5.7625e+02, 6.6944e+02, 2.6698e+03, 5.7506e+02, 1.2983e+03, 1.5539e+03,\n",
      "        2.3374e+03, 2.5056e+03, 3.0835e+03, 5.0582e+02, 1.8297e+02, 3.0082e+02,\n",
      "        2.7077e+01, 7.8758e+00, 8.1196e+01, 3.5824e+00, 7.3179e+00, 6.5396e+00,\n",
      "        2.3770e+00, 3.3987e+00, 1.8892e+01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False,  True,  True,\n",
      "        False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 109.9417,  351.2026,  522.2474,  785.9807, 1867.8341,  636.8044,\n",
      "         373.4614,  644.8718,  325.0788,  659.9024,  240.2602,  566.0508,\n",
      "         216.6883,  519.7582,  598.8727,  389.0751,  400.3606,  674.4106,\n",
      "         352.4484,  172.9851,  951.2836,  718.9933, 2423.5129,  463.7068,\n",
      "         196.4940,  206.6717,  358.7341,  543.8143,   11.3570,    4.2376,\n",
      "           5.3743,    8.6630,    8.0289,    4.4828], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.5117e+02, 1.3658e+02, 1.3131e+02, 7.6538e+01, 7.5958e+01, 7.0370e+01,\n",
      "        7.3646e+01, 8.8998e+01, 8.7318e+01, 7.9046e+01, 5.9952e+01, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 8.8435e+01, 0.0000e+00, 0.0000e+00,\n",
      "        3.2357e-09], device='cuda:1')\n",
      "Prune:\n",
      "tensor([ True, False,  True, False, False,  True,  True, False, False, False,\n",
      "        False,  True,  True,  True,  True, False,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([5.1677e+02, 5.2833e+02, 7.3110e+02, 3.4694e+02, 4.5661e+02, 5.8084e+02,\n",
      "        7.5383e+02, 3.7452e+02, 1.9466e+03, 3.1851e+02, 6.5037e+02, 4.3026e+02,\n",
      "        3.4954e+02, 1.9411e+03, 6.2140e-01, 1.4834e+00, 7.6812e+00, 6.1181e-01,\n",
      "        2.9252e+00, 8.0316e-01, 1.6074e+00, 7.2967e+00, 3.3370e+00, 2.4427e+00,\n",
      "        1.9892e+00, 2.1347e+00, 1.5917e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True, False,  True,  True,  True,\n",
      "         True, False,  True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([64.7957, 56.1512, 54.0456,  0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([ True, False, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([67.3349], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([151.6479,  84.7397, 124.6794, 124.6501, 100.3283, 143.2437,   0.0000,\n",
      "          0.0000,  51.9073], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True,  True, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.6283e+02, 2.1828e+02, 2.8784e+02, 2.2297e+02, 1.8024e+02, 2.6953e+02,\n",
      "        2.1744e+02, 1.2755e+02, 1.9850e+02, 2.0401e+02, 2.2710e+02, 1.5232e+02,\n",
      "        2.1557e+02, 1.5016e+02, 1.4637e+02, 1.5927e+02, 2.2714e+01, 4.4713e+01,\n",
      "        1.5347e-01, 2.2351e-01, 1.4833e-02, 3.3630e-02, 3.1231e-02, 1.4015e-01,\n",
      "        3.3405e-01, 6.4237e-02], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True], device='cuda:1')\n",
      "pruning 56 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     16\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    2\n",
      "     ║    ╠════╝\n",
      "     ║    19\n",
      "     ║    ╠════╗\n",
      "     ║    ║    1\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   23\n",
      "   ╠════╗\n",
      "   ║    9\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  32\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 18\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :3 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 60 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     21\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    25\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    18\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   29\n",
      "   ╠════╗\n",
      "   ║    13\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  39\n",
      "  ╠════╗\n",
      "  ║    10\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 26\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 19.983375549316406)\n",
      "Mean, Std: (1.0, 2.1115057468414307)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([102.2043, 109.9413, 131.2099,  89.3107,  97.9165, 117.6590, 106.7042,\n",
      "        192.7233,  86.1856,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 89.7647, 104.5520, 117.3128, 133.4393, 153.0008, 119.9646, 106.7709,\n",
      "         54.5015, 105.9531, 122.1173,  87.7598,  92.3777,  97.3386,  87.6592,\n",
      "        111.6183,  78.5023, 123.5561,  68.7961,  86.6489,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,  69.9453,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True,  True, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.9175e+03, 4.8137e+02, 7.5374e+02, 6.8485e+02, 1.3204e+03, 9.8952e+02,\n",
      "        3.9062e+02, 1.1069e+03, 9.8081e+02, 1.9231e+03, 7.2717e+02, 3.7953e+03,\n",
      "        2.9451e+03, 7.5510e+03, 1.7873e+02, 1.5641e+02, 3.8561e+01, 4.7730e+01,\n",
      "        1.3944e+01, 3.4624e+01, 1.1341e+01, 7.2640e+01, 6.6476e+01, 1.8162e+01,\n",
      "        3.2804e+00, 1.9252e+01, 1.1771e+01, 3.1194e+01, 1.3680e+01],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False,  True,  True, False, False, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([5.7351e+02, 7.7388e+02, 2.3198e+03, 5.2760e+02, 1.5992e+03, 1.8932e+03,\n",
      "        2.1016e+03, 2.2666e+03, 3.7038e+03, 6.2022e+02, 2.6416e+02, 1.2034e+02,\n",
      "        3.4029e+02, 4.3066e+02, 6.2805e+01, 3.8995e+02, 8.2101e+00, 3.1877e+01,\n",
      "        1.0420e+01, 1.9274e+00, 5.2936e+00, 5.3890e-01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.7499e+02, 5.8486e+02, 6.7815e+02, 1.2540e+03, 2.4768e+03, 6.7965e+02,\n",
      "        4.1970e+02, 3.8528e+02, 3.4942e+02, 3.0403e+02, 4.1688e+02, 2.0984e+02,\n",
      "        4.9881e+02, 6.2569e+02, 2.3327e+02, 6.6574e+02, 5.1538e+02, 5.0028e+02,\n",
      "        1.3445e+03, 9.7527e+02, 2.3891e+03, 5.7058e+02, 1.9833e+02, 3.6845e+02,\n",
      "        2.3274e+02, 7.3023e+02, 3.5346e+01, 3.6315e+01, 1.0378e+01, 6.9270e+00,\n",
      "        5.1548e+00, 2.9248e+01, 7.9005e+00, 1.6118e+00, 6.8518e+00, 1.0586e+01,\n",
      "        1.5082e+00, 1.5171e+01, 4.4673e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False,  True, False, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([123.2507, 106.3194,  82.4275,  60.3473,  80.9004,  96.0459, 131.2048,\n",
      "         81.9891,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([4.3328e+02, 5.2812e+02, 6.9475e+02, 3.6109e+02, 5.2431e+02, 6.1129e+02,\n",
      "        7.2690e+02, 3.2864e+02, 1.8924e+03, 2.8815e+02, 4.8311e+02, 4.8095e+02,\n",
      "        3.6105e+02, 2.2891e+03, 9.5356e-01, 2.8443e+00, 3.2369e+00, 1.6719e+00,\n",
      "        3.1236e+00, 9.8793e-01, 5.7188e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
      "        False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([90.2411, 75.8383,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False,  True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([85.2314,  0.0000,  0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([156.9417, 106.0667, 132.4510, 101.7482, 121.5115, 195.4825,  56.6769,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "         52.9042,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.5514e+02, 1.9526e+02, 2.8614e+02, 2.0800e+02, 1.9458e+02, 2.7459e+02,\n",
      "        2.0124e+02, 1.2641e+02, 2.0665e+02, 2.0778e+02, 1.9491e+02, 1.4976e+02,\n",
      "        2.2956e+02, 1.4899e+02, 1.7519e+02, 1.5952e+02, 3.2316e+01, 4.9380e+01,\n",
      "        1.5414e-02, 4.7484e-03, 1.1667e-02, 3.8373e-03, 3.1790e-03, 2.1057e-03,\n",
      "        2.2519e-02, 5.3360e-03], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True], device='cuda:1')\n",
      "pruning 52 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    2\n",
      "     ║    ╠════╝\n",
      "     ║    20\n",
      "     ║    ╠════╗\n",
      "     ║    ║    1\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   26\n",
      "   ╠════╗\n",
      "   ║    9\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  36\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 18\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :4 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 61 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     24\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ║    25\n",
      "     ║    ╠════╗\n",
      "     ║    ║    4\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    28\n",
      "    ╠════╗\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   35\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  42\n",
      "  ╠════╗\n",
      "  ║    16\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 21\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 18.536304473876953)\n",
      "Mean, Std: (1.0, 2.0268194675445557)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([ 97.5532,  75.0503, 146.0431, 119.0155,  76.2684, 126.4461,  83.1275,\n",
      "        222.6728, 140.5864,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 80.8876,  89.3676, 103.9422, 131.8019, 172.7439, 133.8793,  95.5246,\n",
      "         68.4541, 114.3749, 152.1329,  83.5651,  97.5836, 108.3817, 123.4642,\n",
      "         79.9696,  96.2706, 123.0837,  88.6853,  84.0985,  83.6094,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.5918e+03, 7.4082e+02, 8.8595e+02, 7.6824e+02, 1.3837e+03, 9.9316e+02,\n",
      "        4.2555e+02, 1.1423e+03, 1.0865e+03, 2.3469e+03, 1.0284e+03, 3.7229e+03,\n",
      "        3.3082e+03, 7.3800e+03, 4.5666e+02, 3.2520e+02, 2.8526e+01, 1.7647e+01,\n",
      "        8.5595e+01, 5.2504e+01, 1.0208e+02, 1.6991e+02, 2.7885e+01, 8.8395e+00,\n",
      "        1.0083e+01, 2.7669e+01, 1.4634e+00, 8.1360e+00, 4.7473e+00, 5.9553e+00,\n",
      "        6.8717e+00, 1.2148e+00, 6.7654e-01, 6.9189e+00, 1.3299e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False,  True,  True, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([9.0027e+02, 7.5041e+02, 1.1559e+03, 1.8961e+03, 2.1385e+03, 2.4789e+03,\n",
      "        2.5622e+03, 2.9235e+03, 6.8626e+02, 3.3775e+02, 4.5792e+02, 1.8875e+03,\n",
      "        7.4531e+02, 3.5170e+02, 1.0575e+03, 1.0350e+02, 7.3650e+01, 1.5233e+00,\n",
      "        1.8026e+01, 2.9799e+00, 2.1158e+00, 1.0465e+01, 3.0679e+00, 4.4282e+00,\n",
      "        1.1778e+01, 2.9580e+00, 1.4587e+01, 3.4874e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False,  True,\n",
      "         True, False,  True, False, False,  True, False,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.3962e+02, 5.3750e+02, 1.1758e+03, 1.3596e+03, 2.2853e+03, 7.7609e+02,\n",
      "        3.9802e+02, 5.2967e+02, 5.2558e+02, 5.3964e+02, 5.3595e+02, 2.2075e+02,\n",
      "        6.6554e+02, 3.0838e+02, 5.5546e+02, 5.9990e+02, 4.8184e+02, 1.1105e+03,\n",
      "        1.3524e+03, 3.1339e+03, 8.3083e+02, 1.3398e+02, 6.4520e+02, 1.9441e+02,\n",
      "        7.1623e+02, 2.3148e+00, 2.4143e+01, 3.6243e+01, 3.2953e+01, 1.8432e+01,\n",
      "        7.3586e+00, 1.4432e+01, 6.9602e+00, 1.2017e+01, 3.1202e+00, 7.4239e+00,\n",
      "        1.0226e+01, 2.0921e+00, 1.8769e+00, 7.0254e+00, 1.7017e+01, 1.7230e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False,  True,\n",
      "        False, False, False, False,  True, False, False,  True,  True, False,\n",
      "        False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.0840e+02, 1.4442e+02, 7.2923e+01, 6.0596e+01, 9.1243e+01, 1.0533e+02,\n",
      "        8.9266e+01, 7.0184e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.3479e-09, 0.0000e+00, 1.0378e-05], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([4.5442e+02, 6.3398e+02, 7.0363e+02, 3.7994e+02, 5.2716e+02, 5.4589e+02,\n",
      "        8.1189e+02, 3.2947e+02, 2.0365e+03, 3.0893e+02, 4.1431e+02, 4.0853e+02,\n",
      "        3.7272e+02, 2.2491e+03, 1.3411e+00, 6.6993e+00, 9.8562e+00, 1.1531e+00,\n",
      "        2.9607e+00, 1.6710e+00, 1.0185e+00, 2.0223e-01, 1.0294e+01, 1.0993e+01],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True, False,  True,  True,  True,\n",
      "         True,  True, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([97.9688, 90.5171,  0.0000,  0.0000, 49.1366], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False,  True,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([110.0923,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([107.9659, 123.7490, 118.0432, 112.8729, 168.1825,  52.0030, 103.4276,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.2401e+02, 1.9726e+02, 3.0964e+02, 2.1924e+02, 1.7704e+02, 2.8084e+02,\n",
      "        2.1292e+02, 1.2422e+02, 2.1214e+02, 2.0125e+02, 2.3104e+02, 1.6377e+02,\n",
      "        2.3810e+02, 1.7815e+02, 1.5013e+02, 1.6777e+02, 4.5556e+01, 6.2053e+01,\n",
      "        3.8930e-02, 4.9795e-02, 6.5106e-02], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False,  True,  True,\n",
      "         True], device='cuda:1')\n",
      "pruning 57 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     16\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ║    20\n",
      "     ║    ╠════╗\n",
      "     ║    ║    1\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    20\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   31\n",
      "   ╠════╗\n",
      "   ║    9\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  34\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 16\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :5 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 61 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     21\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    8\n",
      "     ║    ╠════╝\n",
      "     ║    25\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    28\n",
      "    ╠════╗\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   43\n",
      "   ╠════╗\n",
      "   ║    13\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  37\n",
      "  ╠════╗\n",
      "  ║    14\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 19\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 19.06827163696289)\n",
      "Mean, Std: (1.0, 2.093602180480957)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([ 75.6052,  83.9695, 152.2096, 106.2608,  96.0002, 120.7795,  79.0409,\n",
      "        164.9705, 173.2279,   0.0000,   0.0000,   0.0000,  48.5100],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 84.6287,  78.0750,  93.4917, 128.1095, 162.5654, 130.6832,  90.8296,\n",
      "         58.3606, 104.4964, 160.1310, 108.1092,  90.3502, 131.5398, 105.6916,\n",
      "         88.4423,  84.4971, 144.5880,  83.2795,  93.7848,  89.0812,   0.0000,\n",
      "          0.0000,   0.0000,  51.0814,  77.2793], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True,  True, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.6465e+03, 1.0703e+03, 1.0641e+03, 7.1939e+02, 9.9519e+02, 1.0025e+03,\n",
      "        3.9697e+02, 8.7664e+02, 9.2467e+02, 2.7062e+03, 9.0593e+02, 3.9615e+03,\n",
      "        3.4922e+03, 7.3003e+03, 5.1440e+02, 2.4605e+02, 6.5361e+01, 1.1125e+01,\n",
      "        8.3495e+01, 1.4968e+02, 3.5265e+02, 2.3221e+02, 2.7825e+01, 8.2550e+00,\n",
      "        3.0912e+00, 1.6358e+01, 6.7295e-01, 4.1031e+00, 3.4589e+00, 5.2562e-01,\n",
      "        2.4819e+00, 6.9638e+00, 5.9330e+00, 1.2584e+00, 7.8291e-01, 9.6206e+00,\n",
      "        3.5139e+00, 1.5632e+00, 1.3484e+01, 5.7414e+00, 4.3519e+00, 1.3411e+00,\n",
      "        1.8033e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False,  True,  True,  True,  True,\n",
      "         True, False, False,  True,  True, False,  True,  True, False, False,\n",
      "        False,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.0945e+03, 7.5219e+02, 1.2305e+03, 1.2777e+03, 2.7543e+03, 3.0870e+03,\n",
      "        5.2920e+02, 4.6404e+02, 5.8976e+02, 1.6483e+03, 1.0242e+03, 1.8425e+02,\n",
      "        1.1980e+03, 4.9342e+02, 6.1417e+02, 4.2361e+01, 1.0421e+02, 2.2057e+01,\n",
      "        4.1992e+00, 1.4953e+01, 7.8337e+00, 7.8097e+00, 1.1827e+01, 1.4222e+01,\n",
      "        1.7554e+01, 3.6284e+00, 1.0037e+00, 2.3854e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([4.4426e+02, 1.2281e+03, 9.9347e+02, 1.6556e+03, 1.0785e+03, 3.6203e+02,\n",
      "        7.3147e+02, 4.1888e+02, 8.2861e+02, 7.9183e+02, 1.7513e+02, 6.2061e+02,\n",
      "        2.6022e+02, 4.4421e+02, 5.3034e+02, 4.6385e+02, 2.3311e+03, 3.1129e+03,\n",
      "        8.7652e+02, 1.9895e+02, 9.8811e+02, 2.5501e+02, 9.5979e+02, 1.7273e+02,\n",
      "        8.1543e+00, 3.9890e+00, 2.6523e+00, 6.7367e+00, 1.9151e+01, 8.1874e-01,\n",
      "        5.4674e+00, 2.0852e+01, 3.8518e+00, 1.1348e+01, 1.0882e+01, 2.5359e+00,\n",
      "        3.2536e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False, False, False,  True, False, False,  True,\n",
      "        False, False, False, False, False,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([8.0507e+01, 9.3405e+01, 7.1792e+01, 5.1576e+01, 8.6178e+01, 8.2770e+01,\n",
      "        9.9882e+01, 4.0172e+01, 0.0000e+00, 1.0921e-08, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.5964e+02, 7.4893e+02, 4.0085e+02, 6.2428e+02, 6.3985e+02, 7.8462e+02,\n",
      "        3.6076e+02, 2.0415e+03, 3.4082e+02, 6.2168e+02, 3.8963e+02, 3.7149e+02,\n",
      "        2.9257e+03, 1.4920e+00, 6.1014e+00, 7.0677e+00, 2.8639e+00, 3.9483e+00,\n",
      "        1.5707e+00, 1.6745e+00, 1.2686e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True, False,  True, False,  True,  True,\n",
      "         True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([82.9381, 96.3561, 70.0514,  0.0000,  0.0000, 19.9115,  0.0000,  0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True, False,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([72.0314,  0.0000, 61.7596,  0.0000,  0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False,  True, False,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([116.2385, 151.6720, 136.4891, 168.1893,  96.2291,  52.4204,  91.3261,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.4700e+02, 2.2982e+02, 2.7902e+02, 2.3865e+02, 2.9584e+02, 2.1121e+02,\n",
      "        1.1814e+02, 2.5513e+02, 2.1516e+02, 2.5012e+02, 1.5506e+02, 1.8910e+02,\n",
      "        1.9638e+02, 1.8502e+02, 2.0229e+02, 1.0078e+02, 3.4479e-02, 9.5625e-02,\n",
      "        1.8869e-02], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "pruning 54 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    4\n",
      "     ║    ╠════╝\n",
      "     ║    22\n",
      "     ║    ╠════╗\n",
      "     ║    ║    2\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    25\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   31\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  32\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 16\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :6 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 62 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     18\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    10\n",
      "     ║    ╠════╝\n",
      "     ║    29\n",
      "     ║    ╠════╗\n",
      "     ║    ║    10\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    34\n",
      "    ╠════╗\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   36\n",
      "   ╠════╗\n",
      "   ║    15\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  35\n",
      "  ╠════╗\n",
      "  ║    13\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 22\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 23.040176391601562)\n",
      "Mean, Std: (1.0, 2.2476720809936523)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([1.1127e+02, 6.8763e+01, 1.5265e+02, 1.0926e+02, 8.1531e+01, 1.3156e+02,\n",
      "        8.3777e+01, 2.0288e+02, 1.4439e+02, 5.3398e+01, 1.2734e-08, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 89.5970,  91.3802,  86.5699, 146.5575, 166.3271, 117.1570,  98.8524,\n",
      "         72.7619, 128.6155, 156.8259,  99.2768,  92.5978,  93.1379,  98.1659,\n",
      "         66.1172,  68.8519, 145.0811,  70.5284,  98.8250,  72.4270,  71.2520,\n",
      "         66.5892,  47.8317,   0.0000,  61.2359,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True, False,  True,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.5262e+03, 1.1597e+03, 1.2937e+03, 6.9124e+02, 9.0954e+02, 1.0525e+03,\n",
      "        3.9251e+02, 1.1312e+03, 1.0042e+03, 3.0831e+03, 9.1825e+02, 4.0757e+03,\n",
      "        3.9528e+03, 8.9039e+03, 5.9574e+02, 2.8504e+02, 8.1497e+01, 1.2784e+01,\n",
      "        1.0893e+02, 1.6096e+02, 5.3131e+02, 1.4432e+02, 6.2129e+00, 4.0743e+00,\n",
      "        5.6535e+01, 9.2809e+00, 2.0401e+00, 5.0859e+00, 3.3983e+00, 1.7012e+00,\n",
      "        2.3216e+00, 2.5014e+00, 4.6509e+00, 1.3552e+00, 8.0008e-01, 2.0097e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False,  True, False,  True, False,  True,  True,\n",
      "         True,  True, False,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.1744e+03, 7.8159e+02, 1.2909e+03, 1.2790e+03, 2.6848e+03, 3.0556e+03,\n",
      "        5.1980e+02, 4.8731e+02, 4.9829e+02, 1.3477e+03, 1.2148e+03, 2.4114e+02,\n",
      "        9.6916e+02, 8.0675e+02, 6.0860e+02, 1.5364e+02, 3.0179e+02, 2.6487e+01,\n",
      "        1.9417e+01, 3.4626e+01, 1.6313e+01, 1.5330e+00, 7.7323e+00, 5.4704e+00,\n",
      "        5.9474e+00, 8.7289e+00, 1.2832e+01, 4.1501e+00, 1.7864e+00, 3.6242e+00,\n",
      "        7.1161e+00, 1.6174e+00, 8.3093e+00, 2.6115e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True,  True, False, False, False,  True, False,  True,  True,\n",
      "        False,  True, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([3.3553e+02, 1.1205e+03, 1.0673e+03, 1.1106e+03, 1.0777e+03, 4.0333e+02,\n",
      "        7.4508e+02, 3.9603e+02, 9.1598e+02, 9.6639e+02, 2.1982e+02, 6.5739e+02,\n",
      "        3.0761e+02, 3.8167e+02, 7.7509e+02, 8.2187e+02, 1.8779e+03, 3.0696e+03,\n",
      "        1.6590e+03, 1.5702e+02, 3.5431e+02, 1.3636e+03, 2.2103e+02, 1.3663e+01,\n",
      "        1.4820e+00, 3.3037e+00, 8.5723e+00, 1.1486e+01, 1.1857e+00, 1.3230e+01,\n",
      "        1.5899e+01, 1.0450e+01, 1.1055e+01, 2.6431e+01, 4.9326e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True, False, False,  True, False,\n",
      "        False, False, False, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([146.2637,  84.8945, 124.5891,  66.1473,  51.9438,  72.5523,  70.0562,\n",
      "         84.6408,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.0721e+02, 7.7785e+02, 4.9543e+02, 5.8820e+02, 5.5611e+02, 6.7941e+02,\n",
      "        3.6578e+02, 1.7636e+03, 3.6287e+02, 5.8281e+02, 3.8556e+02, 4.1201e+02,\n",
      "        3.2688e+03, 1.2164e+01, 2.0902e+00, 3.1587e+00, 3.1060e+00, 6.8006e-01],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([8.5580e+01, 8.7320e+01, 6.5409e+01, 4.5571e+01, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5681e-08], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([83.1071, 53.1131,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, 34.7534], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 97.9759,  98.4030, 113.3446, 114.8837, 107.1357,  60.7408,  73.6271,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.3774e+02, 2.2953e+02, 3.0312e+02, 2.4663e+02, 2.9156e+02, 2.1612e+02,\n",
      "        1.4269e+02, 2.6710e+02, 2.1671e+02, 2.7242e+02, 1.5795e+02, 2.0305e+02,\n",
      "        1.9414e+02, 1.8484e+02, 2.1190e+02, 9.2540e+01, 9.4942e-03, 1.6510e-02,\n",
      "        1.0760e-02, 7.3516e-03, 3.6727e-03, 4.7167e-02], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True], device='cuda:1')\n",
      "pruning 68 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     13\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    4\n",
      "     ║    ╠════╝\n",
      "     ║    23\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    27\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   24\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 16\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :7 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 61 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     17\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    14\n",
      "     ║    ╠════╝\n",
      "     ║    33\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    39\n",
      "    ╠════╗\n",
      "    ║    11\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   29\n",
      "   ╠════╗\n",
      "   ║    15\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  34\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 21\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 22.557880401611328)\n",
      "Mean, Std: (0.9999999403953552, 2.2036283016204834)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([112.3475,  93.9263, 141.7354, 143.5968,  92.1721, 117.9649,  94.9520,\n",
      "        206.7144, 136.6927,  69.5966,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 72.8301,  80.0874, 100.6143, 149.8204, 143.0757, 127.2081,  88.8898,\n",
      "         56.1996, 109.8906, 142.6557,  92.9365,  86.6533,  92.4400, 116.0074,\n",
      "         74.5244,  68.9961, 132.5067,  75.6870,  79.5147,  77.9006,  61.3333,\n",
      "         77.8547,  66.6902,   0.0000,  41.3150,   0.0000,  51.5874,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,  60.9351], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False,  True, False,  True,  True,  True,\n",
      "         True,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.6230e+03, 1.3820e+03, 9.3234e+02, 5.9562e+02, 1.2937e+03, 4.3034e+02,\n",
      "        1.2914e+03, 3.3375e+03, 8.1090e+02, 3.6642e+03, 3.9744e+03, 8.7265e+03,\n",
      "        4.5112e+02, 3.6389e+02, 2.5490e+02, 1.0977e+02, 1.0235e+02, 8.4824e+02,\n",
      "        1.3747e+02, 2.4081e+02, 2.7958e+00, 6.6584e+01, 1.7263e+00, 1.9129e+00,\n",
      "        5.1383e+00, 2.9697e+00, 1.3199e+01, 5.9878e+00, 3.6575e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False, False, False, False,\n",
      "         True, False,  True,  True, False,  True, False, False,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([9.8984e+02, 7.6038e+02, 9.5557e+02, 1.4532e+03, 2.0010e+03, 2.6239e+03,\n",
      "        4.9624e+02, 5.6494e+02, 7.4409e+02, 1.3596e+03, 1.1088e+03, 5.8059e+02,\n",
      "        1.1923e+03, 7.4732e+02, 5.4364e+02, 1.1403e+02, 6.1979e+02, 4.6998e+01,\n",
      "        6.9917e-01, 1.0276e+02, 1.5477e+01, 6.6148e+00, 9.0652e+00, 9.4697e+00,\n",
      "        1.3622e+01, 2.5438e+01, 1.6943e+00, 1.0786e+01, 3.2201e+00, 1.0573e+01,\n",
      "        1.9093e+01, 3.3751e-01, 1.5216e+01, 1.6324e+01, 2.4122e+00, 2.8438e+00,\n",
      "        2.9323e+00, 2.4268e+00, 2.4840e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False,  True, False,  True, False,\n",
      "        False,  True, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([4.0739e+02, 7.8279e+02, 1.1110e+03, 1.1035e+03, 5.9102e+02, 8.4534e+02,\n",
      "        4.4310e+02, 7.3813e+02, 1.2370e+03, 1.9541e+02, 7.9928e+02, 3.9834e+02,\n",
      "        2.8987e+02, 6.8693e+02, 8.5577e+02, 1.7911e+03, 3.3351e+03, 6.9657e+02,\n",
      "        1.0529e+02, 5.3728e+02, 1.4441e+03, 7.1687e+02, 5.1345e+01, 7.0829e+01,\n",
      "        1.3051e+01, 6.3500e+00, 2.2800e+01, 2.6270e+01, 2.7209e+00, 2.0117e+01,\n",
      "        1.1856e+01, 5.0906e+00, 7.1104e+00, 4.3259e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False,  True, False, False,  True, False,\n",
      "        False, False, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([151.5474, 147.1670,  95.8754,  89.1427,  60.0660,  99.5044,  68.9711,\n",
      "        108.6764,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.0035e+02, 7.6163e+02, 5.0547e+02, 5.7115e+02, 7.3886e+02, 4.0582e+02,\n",
      "        1.8235e+03, 4.6189e+02, 6.6626e+02, 4.7080e+02, 3.6203e+02, 3.0124e+03,\n",
      "        4.2362e+00, 1.2316e+00, 7.2663e-01, 8.8145e+00, 6.2554e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([93.8707, 99.2723, 69.3789, 61.2596,  0.0000,  0.0000,  0.0000, 51.6930,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([74.8711, 57.9812, 66.5701,  0.0000,  0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 90.1251, 106.1861, 109.8038, 119.5042, 148.7046,  53.4118, 113.2816,\n",
      "          0.0000,   0.0000,   0.0000,  27.5521], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True,  True,  True,\n",
      "        False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.3925e+02, 2.4310e+02, 3.2998e+02, 2.4374e+02, 2.8885e+02, 2.2739e+02,\n",
      "        1.4307e+02, 2.5405e+02, 2.1952e+02, 2.6164e+02, 1.8601e+02, 1.9746e+02,\n",
      "        1.8840e+02, 1.8852e+02, 2.0852e+02, 8.8081e+01, 1.2984e-02, 3.5969e-02,\n",
      "        2.0352e-02, 9.7459e-03, 1.0513e-01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True], device='cuda:1')\n",
      "pruning 55 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ║    25\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    29\n",
      "    ╠════╗\n",
      "    ║    8\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   23\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  30\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 16\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :8 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 62 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     19\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    7\n",
      "     ║    ╠════╝\n",
      "     ║    30\n",
      "     ║    ╠════╗\n",
      "     ║    ║    8\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    41\n",
      "    ╠════╗\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   29\n",
      "   ╠════╗\n",
      "   ║    15\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  35\n",
      "  ╠════╗\n",
      "  ║    16\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 20\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 22.752670288085938)\n",
      "Mean, Std: (1.0000001192092896, 2.2363901138305664)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([9.1075e+01, 8.4135e+01, 1.4588e+02, 1.2174e+02, 1.0235e+02, 8.6245e+01,\n",
      "        6.8009e+01, 2.0543e+02, 1.4088e+02, 1.0420e+02, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 2.1327e-06], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False, False, False, False,\n",
      "         True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 77.6482,  75.2734, 108.2488, 163.1247, 137.6931,  93.4287,  82.5547,\n",
      "        116.7818, 149.1440,  96.6406,  87.6055, 127.1000,  99.3537,  83.0750,\n",
      "         69.3045, 139.4339,  87.8741,  87.2554,  63.1198,  70.7928,  75.7027,\n",
      "         68.4605,  58.1535,  69.8878,  49.8485,   0.0000,   0.0000,  84.1729,\n",
      "          0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True, False,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.4998e+03, 1.4569e+03, 1.0430e+03, 7.7487e+02, 1.1111e+03, 4.1748e+02,\n",
      "        1.1797e+03, 3.4988e+03, 9.6422e+02, 3.9954e+03, 4.0335e+03, 8.7511e+03,\n",
      "        4.1564e+02, 2.6716e+02, 2.1326e+02, 1.4938e+02, 7.8268e+02, 2.0253e+02,\n",
      "        4.5966e+02, 1.0212e+02, 1.6679e+00, 2.9397e+00, 1.1612e+01, 2.9350e+00,\n",
      "        2.2606e+00, 6.4711e+00, 9.9050e-01, 1.6984e+01, 2.7890e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False,  True, False,\n",
      "         True,  True, False,  True,  True, False,  True, False,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([9.8871e+02, 8.2496e+02, 1.5108e+03, 2.3391e+03, 3.0218e+03, 6.0084e+02,\n",
      "        6.8205e+02, 8.7495e+02, 1.8646e+03, 1.2514e+03, 5.8563e+02, 1.0835e+03,\n",
      "        1.2253e+03, 7.8040e+02, 1.4337e+02, 1.1475e+03, 1.6817e+02, 3.7908e+02,\n",
      "        2.7612e+01, 3.2162e+00, 1.0067e+01, 2.4653e+01, 5.8681e+00, 1.1626e+01,\n",
      "        8.8944e+00, 5.1036e+00, 8.6074e+00, 5.1171e+01, 3.0153e+00, 2.2008e+00,\n",
      "        4.4953e+00, 7.3254e+00, 6.2464e+00, 3.3175e-01, 3.8573e+00, 2.8997e+00,\n",
      "        6.1575e+00, 3.0138e+00, 4.3478e+00, 2.8157e+00, 6.0119e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "        False, False, False,  True, False,  True, False,  True, False,  True,\n",
      "        False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([4.3621e+02, 9.6775e+02, 1.0334e+03, 5.2340e+02, 1.1796e+03, 4.4396e+02,\n",
      "        5.0358e+02, 1.1339e+03, 3.9931e+02, 6.7827e+02, 5.6122e+02, 2.7979e+02,\n",
      "        7.0444e+02, 1.0702e+03, 1.1778e+03, 3.1568e+03, 8.6695e+02, 4.5715e+02,\n",
      "        1.4747e+03, 1.0437e+03, 1.1466e+02, 7.6463e+00, 4.6807e+00, 1.0009e+01,\n",
      "        1.4639e+01, 1.5599e+01, 3.0650e+00, 7.0874e+00, 2.9763e+00, 3.4005e+00,\n",
      "        7.4015e+00, 5.5068e+00, 4.2046e+00, 1.0358e+01, 1.9542e+01],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False,  True, False,  True,  True,\n",
      "        False, False, False, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([9.4849e+01, 7.2127e+01, 8.3390e+01, 8.0727e+01, 6.1812e+01, 9.8081e+01,\n",
      "        6.6466e+01, 1.1999e+02, 0.0000e+00, 2.3319e-08, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([6.9694e+02, 7.5093e+02, 5.5299e+02, 5.8243e+02, 8.1610e+02, 4.3378e+02,\n",
      "        1.8662e+03, 4.6969e+02, 6.1605e+02, 5.6118e+02, 3.6058e+02, 2.8926e+03,\n",
      "        6.3578e+00, 6.1598e+00, 4.1336e+00, 4.2378e-01, 2.1352e+00, 1.3811e+00,\n",
      "        4.3046e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([90.5679, 88.8740, 77.0824, 70.1908, 58.3415,  0.0000,  0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([67.1380, 68.5749, 79.0452,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([114.7216, 104.0917, 145.1458, 101.5237, 107.5147,  56.9015,  78.3134,\n",
      "         27.7541,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.3926e+02, 2.3633e+02, 3.2667e+02, 2.4113e+02, 3.1104e+02, 2.0829e+02,\n",
      "        1.3426e+02, 2.7871e+02, 2.2220e+02, 2.6809e+02, 1.6155e+02, 1.7817e+02,\n",
      "        1.9310e+02, 1.9462e+02, 2.2420e+02, 1.0217e+02, 3.1925e-02, 1.3386e-01,\n",
      "        1.1014e-01, 1.0261e-01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "pruning 65 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    4\n",
      "     ║    ╠════╝\n",
      "     ║    25\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    32\n",
      "    ╠════╗\n",
      "    ║    8\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   20\n",
      "   ╠════╗\n",
      "   ║    9\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    7\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 15\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :9 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 61 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     22\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    8\n",
      "     ║    ╠════╝\n",
      "     ║    36\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    42\n",
      "    ╠════╗\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   25\n",
      "   ╠════╗\n",
      "   ║    12\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  37\n",
      "  ╠════╗\n",
      "  ║    13\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 18\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 17.77925682067871)\n",
      "Mean, Std: (0.9999999403953552, 2.096022605895996)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([ 90.6244,  64.3105, 161.3813, 149.8152, 105.4896,  93.6739, 222.1402,\n",
      "        139.2642,  81.2016,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False,  True,\n",
      "         True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 88.6409,  89.9345,  98.4030, 141.4196, 117.0563,  72.8438,  92.1835,\n",
      "        149.2239, 101.8112,  77.9020, 100.8430, 107.5597,  70.2835,  88.0499,\n",
      "        132.2865,  68.2294,  78.7926,  85.0261,  62.8560,  81.7997,  69.5182,\n",
      "         54.5790,  59.9023,  55.4661, 112.3033,  36.1374,  45.9790,  53.9059,\n",
      "         72.8384,   0.0000,  58.9319,   0.0000,   0.0000,   0.0000,  51.2019,\n",
      "          0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False, False,  True,  True,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False,  True,  True,  True, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.2387e+03, 1.5190e+03, 1.0221e+03, 8.0633e+02, 1.3210e+03, 1.1900e+03,\n",
      "        3.7292e+03, 1.0172e+03, 4.2706e+03, 4.1085e+03, 6.8692e+03, 4.4745e+02,\n",
      "        3.3353e+02, 2.3729e+02, 1.3471e+03, 2.3262e+02, 2.0899e+02, 1.7141e+00,\n",
      "        3.2380e+00, 6.3597e+00, 4.3006e+00, 2.3837e+00, 1.1101e+01, 1.9279e+00,\n",
      "        3.8870e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True, False,\n",
      "        False,  True, False,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.0452e+03, 8.0472e+02, 1.9908e+03, 2.6790e+03, 2.4541e+03, 7.3040e+02,\n",
      "        9.3527e+02, 2.1824e+03, 1.0414e+03, 6.5984e+02, 1.3471e+03, 1.1235e+03,\n",
      "        6.6758e+02, 1.4609e+03, 1.9961e+02, 4.8335e+02, 7.2951e+01, 7.1984e+00,\n",
      "        3.9673e+00, 3.6822e+00, 4.8162e+00, 8.7563e+00, 3.2520e+01, 4.6036e+02,\n",
      "        1.8266e+02, 1.2498e+01, 6.1026e+00, 1.0516e+00, 7.8900e+00, 9.2910e+00,\n",
      "        2.9733e+00, 5.4992e+00, 3.7598e+00, 6.9600e+00, 1.1201e+01, 1.4607e+01,\n",
      "        5.9725e+00, 1.5432e+00, 4.1735e+00, 2.2522e+00, 1.7202e+00, 1.1535e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False,  True, False,  True,\n",
      "         True, False,  True, False, False, False, False,  True, False,  True,\n",
      "         True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([5.4725e+02, 8.8263e+02, 9.3166e+02, 5.6960e+02, 3.9628e+02, 4.4742e+02,\n",
      "        3.7348e+02, 1.3559e+03, 4.7468e+02, 5.8595e+02, 6.8958e+02, 5.8750e+02,\n",
      "        5.1480e+02, 9.6774e+02, 9.6261e+02, 3.2374e+03, 9.3393e+02, 3.7351e+02,\n",
      "        1.3656e+03, 1.3345e+03, 1.4377e+02, 1.6087e+01, 1.4353e+01, 2.8387e+00,\n",
      "        3.3207e+01, 6.7003e+00, 1.8010e+00, 9.1366e+00, 5.4342e+00, 1.1084e+01,\n",
      "        3.4251e+01, 4.4594e+00, 1.2298e+00, 6.7310e-01, 3.9565e+00, 6.9759e+00,\n",
      "        4.8615e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True,  True, False,\n",
      "        False, False, False,  True, False, False,  True, False, False, False,\n",
      "        False,  True,  True,  True, False, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([107.9333, 109.0763,  66.3200,  83.8905,  56.4930,  65.5943, 116.2112,\n",
      "         75.1862,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.9127e+02, 7.9494e+02, 6.6754e+02, 4.3856e+02, 7.4805e+02, 1.8848e+03,\n",
      "        4.6713e+02, 6.9229e+02, 5.8309e+02, 3.6530e+02, 2.8635e+03, 2.7678e+01,\n",
      "        5.4199e+01, 2.4367e+00, 6.7860e+00, 3.0964e+00, 7.1807e+00, 4.9007e+00,\n",
      "        1.6789e+00, 2.3025e+00, 4.2569e+00, 1.6751e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False,  True, False,  True, False, False,  True,  True,\n",
      "        False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([101.6391,  73.5475,  85.0265,  63.9150,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([69.6685, 64.2200, 74.1428,  0.0000,  0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 79.3374, 124.5271, 132.2176, 100.8390, 122.5825,  61.1881,  91.7750,\n",
      "         64.5176,   0.0000,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.5223e+02, 2.4948e+02, 3.1878e+02, 2.5423e+02, 2.7394e+02, 1.2316e+02,\n",
      "        2.5855e+02, 2.4501e+02, 2.4160e+02, 2.0620e+02, 2.0708e+02, 2.1483e+02,\n",
      "        1.9657e+02, 2.8750e+02, 1.0024e+02, 1.3928e-02, 4.7501e-03, 6.9146e-03],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "pruning 60 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     16\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    4\n",
      "     ║    ╠════╝\n",
      "     ║    27\n",
      "     ║    ╠════╗\n",
      "     ║    ║    3\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    33\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   20\n",
      "   ╠════╗\n",
      "   ║    8\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  30\n",
      "  ╠════╗\n",
      "  ║    7\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 15\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :10 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 61 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     19\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    10\n",
      "     ║    ╠════╝\n",
      "     ║    34\n",
      "     ║    ╠════╗\n",
      "     ║    ║    9\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    41\n",
      "    ╠════╗\n",
      "    ║    11\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   24\n",
      "   ╠════╗\n",
      "   ║    10\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  39\n",
      "  ╠════╗\n",
      "  ║    14\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 20\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 19.23774528503418)\n",
      "Mean, Std: (1.0, 2.1339967250823975)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([ 88.6658,  59.1313, 143.0866, 127.1732, 113.5610,  72.3312, 158.3365,\n",
      "         73.0174,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 75.3125,  74.8912, 105.2363, 151.1791,  93.7133, 112.7107, 158.0107,\n",
      "        120.6189, 111.3839,  69.5860,  76.0826, 122.7881,  97.8868, 120.1333,\n",
      "         79.8921,  76.6804,  78.3421,  67.7133,  62.0340,  56.3173, 143.7096,\n",
      "         86.8324,  51.9417,  50.2623,  91.8781,  82.3249,  81.6464,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.2389e+03, 1.5760e+03, 8.6151e+02, 8.6656e+02, 1.3335e+03, 1.0560e+03,\n",
      "        4.2328e+03, 1.0869e+03, 4.2077e+03, 3.7597e+03, 7.5247e+03, 5.4853e+02,\n",
      "        4.7479e+02, 2.3086e+02, 1.4994e+03, 2.2655e+02, 7.3261e+00, 1.1833e+00,\n",
      "        6.8700e+00, 5.6186e+00, 1.3916e+01, 5.2567e+00, 1.9137e+00, 7.9505e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.8052e+02, 7.7464e+02, 2.0086e+03, 2.5719e+03, 2.2309e+03, 7.8167e+02,\n",
      "        8.1827e+02, 1.7883e+03, 9.8935e+02, 8.7386e+02, 1.2652e+03, 1.0225e+03,\n",
      "        4.9552e+02, 1.5708e+03, 4.4983e+02, 5.8600e+02, 4.3978e+01, 2.5133e+00,\n",
      "        8.3343e+00, 4.4857e+00, 1.0662e+00, 6.4647e+01, 5.5570e+02, 2.0848e+02,\n",
      "        5.0255e+00, 2.3412e+00, 3.4656e+00, 8.3941e-01, 1.1576e+01, 8.7877e-01,\n",
      "        3.9434e+00, 6.4021e-01, 5.5093e+00, 1.4031e+00, 4.9591e-01, 2.9194e+00,\n",
      "        1.6026e+01, 2.9550e+00, 2.8167e+01, 6.9278e+00, 1.2938e+01],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True, False, False,  True, False, False,\n",
      "         True, False, False, False,  True,  True,  True,  True, False,  True,\n",
      "        False,  True, False,  True,  True,  True, False,  True, False, False,\n",
      "        False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([6.6239e+02, 1.2066e+03, 9.4481e+02, 8.8514e+02, 7.4463e+02, 5.2057e+02,\n",
      "        6.8986e+02, 1.5484e+03, 8.0414e+02, 7.7240e+02, 7.1454e+02, 7.7886e+02,\n",
      "        5.9397e+02, 1.2370e+03, 6.1929e+02, 3.1154e+03, 1.1848e+03, 1.6417e+03,\n",
      "        2.9020e+02, 5.8233e+01, 1.4230e+02, 4.5485e+01, 4.4976e-01, 3.1895e-01,\n",
      "        3.2262e+00, 6.8081e+00, 1.2461e+00, 3.0789e+00, 5.5235e+00, 2.0589e+01,\n",
      "        2.3611e+01, 1.1187e+01, 3.8630e+00, 6.0666e+00, 1.6267e+01, 1.2943e+01,\n",
      "        2.8897e+00, 3.6141e+00, 7.5819e-01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True,  True, False,  True,  True, False, False,\n",
      "        False, False,  True, False, False, False,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.0815e+02, 1.0247e+02, 5.3003e+01, 7.9191e+01, 5.7056e+01, 7.9211e+01,\n",
      "        1.0952e+02, 0.0000e+00, 9.5226e-08, 9.9316e+01, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 8.5084e+01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True,  True, False,\n",
      "         True,  True,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([8.5886e+02, 7.5032e+02, 8.0256e+02, 4.0234e+02, 7.0850e+02, 2.2871e+03,\n",
      "        4.6648e+02, 6.7028e+02, 3.6848e+02, 2.8496e+03, 2.3100e+02, 1.4430e+02,\n",
      "        9.2018e+00, 6.2264e+00, 8.3803e+00, 2.2681e+01, 7.1824e+00, 8.0313e-01,\n",
      "        3.0154e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([84.4884, 79.1290, 80.6745, 64.7012, 38.6732,  0.0000,  0.0000, 37.3818,\n",
      "         0.0000,  0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([63.2840, 57.4975, 65.6403,  0.0000,  0.0000, 22.0889,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True, False,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 91.8796, 133.2846,  79.2584, 140.6335,  67.5670,  77.4984,  74.1342,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.5230e+02, 2.3307e+02, 3.1199e+02, 2.5168e+02, 2.6786e+02, 1.2846e+02,\n",
      "        2.4350e+02, 2.4067e+02, 2.4990e+02, 1.9044e+02, 1.8685e+02, 2.2695e+02,\n",
      "        2.0644e+02, 2.8046e+02, 1.1841e+02, 1.3961e-02, 1.0701e+00, 5.1768e-03,\n",
      "        5.1522e-02, 6.7732e-03], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "pruning 64 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     17\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ║    25\n",
      "     ║    ╠════╗\n",
      "     ║    ║    4\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    27\n",
      "    ╠════╗\n",
      "    ║    6\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   22\n",
      "   ╠════╗\n",
      "   ║    8\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  30\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 14\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :11 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 61 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     22\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    14\n",
      "     ║    ╠════╝\n",
      "     ║    34\n",
      "     ║    ╠════╗\n",
      "     ║    ║    13\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    34\n",
      "    ╠════╗\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   25\n",
      "   ╠════╗\n",
      "   ║    13\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  32\n",
      "  ╠════╗\n",
      "  ║    11\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 17\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 20.660566329956055)\n",
      "Mean, Std: (1.0, 2.1662309169769287)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([140.7701,  50.2694, 157.0005, 141.5433, 110.2916,  93.7142, 157.9667,\n",
      "         73.7391,   0.0000,   0.0000,   0.0000,  58.1273,   0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False,  True,  True,\n",
      "         True, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 92.9999,  76.1069,  97.5313, 170.5297,  96.8282, 115.3328, 130.9091,\n",
      "        127.0524,  85.6237,  72.1285, 164.7677,  86.9439, 120.7230,  84.5213,\n",
      "         78.1698,  93.1379,  90.1261,  69.6008,  69.5768, 124.7410,  86.6866,\n",
      "         57.6905, 117.2479, 115.9167,  82.0331,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.9600e+03, 1.5164e+03, 8.9760e+02, 8.6504e+02, 1.3406e+03, 9.2207e+02,\n",
      "        4.5677e+03, 8.2766e+02, 4.3270e+03, 3.7044e+03, 8.3376e+03, 5.9140e+02,\n",
      "        5.2067e+02, 2.3682e+02, 1.4580e+03, 2.2924e+02, 1.4422e+01, 1.7398e+00,\n",
      "        4.3773e+01, 1.4483e+01, 1.3169e+01, 2.4509e+00, 6.4937e+00, 1.3301e+00,\n",
      "        6.4006e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False, False,\n",
      "        False,  True, False,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([8.3857e+02, 9.0097e+02, 2.1713e+03, 2.2609e+03, 1.9942e+03, 7.8194e+02,\n",
      "        9.1235e+02, 1.8495e+03, 1.2603e+03, 1.1075e+03, 1.2092e+03, 1.4160e+03,\n",
      "        5.8288e+02, 5.0433e+02, 5.8474e+01, 6.6170e+01, 7.1500e+00, 9.4281e+01,\n",
      "        4.5774e+02, 2.9420e+02, 4.4325e+00, 2.3376e+00, 4.2360e+00, 1.4365e+01,\n",
      "        7.9544e+00, 1.3633e+01, 4.1586e+00, 3.8766e+00, 3.4071e+00, 2.1144e+00,\n",
      "        5.3554e+00, 1.5984e+01, 1.1863e+01, 8.5506e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False,  True, False, False,  True,  True,  True,\n",
      "        False, False,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([9.2421e+02, 9.0410e+02, 8.9234e+02, 6.2362e+02, 1.6317e+03, 4.7548e+02,\n",
      "        9.5352e+02, 1.2144e+03, 7.4169e+02, 8.4357e+02, 5.2468e+02, 7.2782e+02,\n",
      "        6.4446e+02, 1.6203e+03, 5.9999e+02, 2.8824e+03, 1.4984e+03, 1.9992e+03,\n",
      "        4.6241e+02, 8.0700e+01, 4.4557e+02, 2.1576e+02, 6.0898e+00, 5.1591e+00,\n",
      "        1.1674e+01, 7.3821e+00, 4.2032e+01, 9.5761e+00, 4.7850e+00, 3.0143e+00,\n",
      "        1.0331e+01, 7.8317e-01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False,  True,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False,  True, False,  True,\n",
      "        False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([87.2121, 65.2282, 59.8407, 39.7707, 63.4982, 72.3496, 98.3619, 82.9771,\n",
      "        96.6030,  0.0000, 46.0030], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False,  True,\n",
      "        False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([8.4453e+02, 7.2220e+02, 7.4905e+02, 4.1125e+02, 8.3973e+02, 1.9825e+03,\n",
      "        4.5358e+02, 7.7763e+02, 3.9155e+02, 2.9494e+03, 3.1927e+02, 1.8250e+02,\n",
      "        8.2527e+00, 5.2984e+00, 5.5529e+00, 6.7317e+00, 2.2570e+00, 6.3943e-01,\n",
      "        9.4677e-01, 1.8838e+00, 4.5697e+00, 6.1636e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "        False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([91.2921, 77.3735, 65.6836, 64.2203, 57.5011,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.2608e+01, 6.5239e+01, 8.7919e+01, 3.8923e+01, 2.1332e+01, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 3.8615e-10, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        2.0134e+01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "         True,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.4853e+02, 9.8742e+01, 1.8906e+02, 6.4365e+01, 6.4740e+01, 7.9182e+01,\n",
      "        0.0000e+00, 2.0515e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        6.0011e+01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.8590e+02, 3.2054e+02, 3.2301e+02, 2.8834e+02, 3.1214e+02, 1.3453e+02,\n",
      "        2.7815e+02, 2.7033e+02, 2.4853e+02, 2.0364e+02, 2.5575e+02, 2.5425e+02,\n",
      "        3.0900e+02, 1.0763e+02, 1.9682e-02, 5.8699e-04, 2.1223e-02],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True,  True], device='cuda:1')\n",
      "pruning 67 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     17\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ║    24\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    27\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   21\n",
      "   ╠════╗\n",
      "   ║    8\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  25\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :12 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 61 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     22\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    13\n",
      "     ║    ╠════╝\n",
      "     ║    36\n",
      "     ║    ╠════╗\n",
      "     ║    ║    12\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    29\n",
      "    ╠════╗\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   26\n",
      "   ╠════╗\n",
      "   ║    12\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  27\n",
      "  ╠════╗\n",
      "  ║    15\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 17\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 16.947755813598633)\n",
      "Mean, Std: (1.0, 2.093942880630493)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([8.9637e+01, 1.5941e+02, 1.1170e+02, 1.2495e+02, 1.4060e+02, 1.5518e+02,\n",
      "        1.1948e+02, 6.0152e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1297e-09],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 71.6554, 113.8118, 160.5847, 113.9152, 136.2536, 135.3795, 138.1048,\n",
      "         88.1483,  80.4882, 155.7105,  85.9650, 126.6956,  78.3717,  68.2055,\n",
      "         86.0290,  83.4847,  77.3517,  63.8648, 125.0864,  92.6626,  94.0094,\n",
      "        131.3896, 104.8044,  97.8324,  51.7598,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,  72.2858,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "         True, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.2302e+03, 1.3965e+03, 9.3502e+02, 9.8935e+02, 1.4697e+03, 1.0610e+03,\n",
      "        4.0345e+03, 3.9173e+03, 4.6367e+03, 7.3918e+03, 6.2521e+02, 5.4448e+02,\n",
      "        3.6318e+02, 1.1001e+03, 3.4147e+02, 8.2252e+00, 9.0051e+01, 3.9311e+01,\n",
      "        7.0094e+00, 4.1895e+00, 1.3477e+01, 1.5993e+01, 6.9668e+00, 7.5204e+00,\n",
      "        8.3203e+00, 2.7276e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([9.2188e+02, 9.7867e+02, 1.6892e+03, 2.3445e+03, 1.7818e+03, 8.2484e+02,\n",
      "        1.0670e+03, 1.9536e+03, 1.6507e+03, 1.4001e+03, 1.5094e+03, 5.9455e+02,\n",
      "        2.8522e+02, 6.9364e+01, 2.1240e+02, 2.5569e+01, 1.9482e+02, 6.6164e+02,\n",
      "        3.6930e+02, 2.0684e+01, 2.0170e+01, 4.2585e+00, 1.1701e+01, 2.8895e+00,\n",
      "        7.8641e+00, 6.3453e-01, 2.8258e+01, 2.5596e+00, 4.7781e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False,  True, False,  True,  True,  True, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([1323.7996, 1488.3237, 1324.9365,  903.8651,  544.7142, 1473.3527,\n",
      "        1371.9860,  643.4703, 1631.2961, 1619.0203, 2299.4587,  611.2580,\n",
      "        3193.9756, 3686.8665, 4129.0229, 1166.9414,  477.2850,  273.2995,\n",
      "         130.7989,    6.8555,   27.7191,   48.0670,  132.7307,   14.1980,\n",
      "          33.2637,   23.4231,   27.5887], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([138.5130,  73.6187,  70.6680, 105.1444, 149.5215,  62.2445, 135.5990,\n",
      "        138.2519,  40.9870,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False,  True, False, False, False,  True,\n",
      "         True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.4796e+02, 7.3738e+02, 5.8753e+02, 4.7151e+02, 1.0144e+03, 2.6827e+03,\n",
      "        4.9951e+02, 7.2960e+02, 4.5301e+02, 3.6939e+02, 2.4722e+02, 2.0506e+01,\n",
      "        1.8793e+02, 4.6972e+01, 9.3585e+00, 1.4031e+01, 1.0164e+01, 9.1924e+00,\n",
      "        3.8332e+00, 6.6608e+00, 2.3559e+00, 2.1015e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "         True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([107.1191,  84.6713,  83.9542,  63.0762,  71.0712,   0.0000,   0.0000,\n",
      "         54.0780,   0.0000,  33.1962,   0.0000,   0.0000,   0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False,  True,  True, False,  True, False,\n",
      "         True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 99.0157, 126.9183,  47.4305,  55.5841,  19.8180,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "         True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.4495e+02, 9.7065e+01, 1.3451e+02, 6.5777e+01, 6.0476e+01, 9.3343e+01,\n",
      "        9.6339e+01, 0.0000e+00, 0.0000e+00, 8.1776e-10, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.5411e+02, 3.2106e+02, 3.3911e+02, 2.8398e+02, 1.4871e+02, 2.4589e+02,\n",
      "        3.6024e+02, 2.5365e+02, 2.0768e+02, 2.7112e+02, 3.0399e+02, 3.3603e+02,\n",
      "        1.1290e+02, 6.1771e-02, 3.1799e-02, 4.1189e-02, 3.0211e-02],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True], device='cuda:1')\n",
      "pruning 62 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     18\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    6\n",
      "     ║    ╠════╝\n",
      "     ║    23\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    24\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   24\n",
      "   ╠════╗\n",
      "   ║    8\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  25\n",
      "  ╠════╗\n",
      "  ║    7\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :13 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 61 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     23\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    16\n",
      "     ║    ╠════╝\n",
      "     ║    30\n",
      "     ║    ╠════╗\n",
      "     ║    ║    15\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    26\n",
      "    ╠════╗\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   31\n",
      "   ╠════╗\n",
      "   ║    11\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  27\n",
      "  ╠════╗\n",
      "  ║    12\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 17\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 13.774394035339355)\n",
      "Mean, Std: (1.0, 2.0582263469696045)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([111.1837, 174.8285, 157.2640,  88.7119, 129.9240, 171.4053, 107.7543,\n",
      "         97.0204,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False,  True,  True,\n",
      "         True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 89.2126, 106.0459, 144.3273,  91.8505, 116.6088, 131.9015, 137.6464,\n",
      "        113.0211,  68.6078, 115.7073,  97.5902,  70.1661,  96.7642,  74.7831,\n",
      "         59.3024, 147.4494,  94.2256,  90.0166, 128.6143, 111.6548, 114.7443,\n",
      "         59.9420,  94.1645,   0.0000,   0.0000,   0.0000,   0.0000,  75.8858,\n",
      "          0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True, False,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.2391e+03, 1.3406e+03, 1.0322e+03, 1.0273e+03, 1.4973e+03, 1.1784e+03,\n",
      "        3.7152e+03, 4.1020e+03, 5.0889e+03, 6.3149e+03, 6.7439e+02, 5.7146e+02,\n",
      "        3.9792e+02, 1.4584e+03, 3.9736e+02, 2.1437e+01, 3.4435e+02, 4.3183e+01,\n",
      "        6.0066e-01, 5.8716e+00, 1.0350e+00, 3.6559e+00, 4.4480e+00, 7.4828e+00,\n",
      "        5.0899e+00, 6.0836e-01, 1.8960e+00, 1.4036e+00, 3.4787e+00, 7.3056e+00,\n",
      "        1.5664e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False,  True, False,\n",
      "         True,  True,  True, False, False,  True,  True,  True,  True, False,\n",
      "         True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 916.1050,  953.6473, 1732.5317, 2487.8235, 1590.8861,  790.6722,\n",
      "         961.0099, 2491.9836, 1441.0977, 1465.1199, 1335.9841,  599.1745,\n",
      "         294.4517,  135.2994,  428.9849,   54.0916,  144.0916,  626.2948,\n",
      "         390.1754,    5.5835,   12.0227,   33.1480,    6.5756,    6.6297,\n",
      "          11.1984,    2.8371], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1378.2134, 1438.0934, 1448.2466,  500.0898, 1278.4384, 1740.8643,\n",
      "        1038.8081, 1757.4720, 1853.8905, 3345.2634,  508.7013, 2866.7517,\n",
      "        4442.9785, 4622.6626, 1434.7640, 1047.7603,  214.2125,  282.0714,\n",
      "          57.2819,   25.8506,  273.8840,   23.3425,   19.1341,  236.3220,\n",
      "          11.3685,   35.4298,   11.5977], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False, False, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([123.2087,  83.2919,  66.3228,  91.5117, 118.4242, 124.8618,  66.7332,\n",
      "        100.5096,   0.0000,   0.0000,   0.0000,  53.6786], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True, False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.7045e+02, 6.9173e+02, 6.3040e+02, 5.3743e+02, 8.4945e+02, 3.6583e+03,\n",
      "        5.9909e+02, 7.8303e+02, 4.2071e+02, 2.8791e+02, 7.3009e+00, 2.3316e+02,\n",
      "        8.1830e+00, 9.3741e+01, 1.0195e+00, 4.0571e+00, 8.4109e+00, 9.8818e+00,\n",
      "        4.0390e+00, 4.9727e+00, 1.1079e+01, 3.0984e+00, 3.5777e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False,  True,  True,  True, False, False,  True, False,\n",
      "        False,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([89.0221, 91.1617, 93.2885, 66.6670, 62.5105, 63.0286,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, 48.2908,  0.0000, 30.2958,  0.0000,  0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True, False,  True, False,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([98.7295, 97.2312, 51.4587, 33.5635, 41.5047,  0.0000,  0.0000,  0.0000,\n",
      "         2.2539,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.3561e+02, 1.5896e+02, 1.5747e+02, 6.7246e+01, 7.2064e+01, 8.6078e+01,\n",
      "        9.1847e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.1672e-08], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.4254e+02, 3.0627e+02, 3.4884e+02, 3.0040e+02, 1.5012e+02, 2.6788e+02,\n",
      "        3.3520e+02, 2.7743e+02, 2.0509e+02, 2.6723e+02, 2.9880e+02, 3.4202e+02,\n",
      "        1.2021e+02, 3.6375e-02, 3.1107e-02, 5.3914e-02, 4.1809e-02],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False,  True,  True,  True,  True], device='cuda:1')\n",
      "pruning 62 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     16\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    8\n",
      "     ║    ╠════╝\n",
      "     ║    24\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    24\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   21\n",
      "   ╠════╗\n",
      "   ║    7\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  26\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 12\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :14 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 61 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     21\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    15\n",
      "     ║    ╠════╝\n",
      "     ║    37\n",
      "     ║    ╠════╗\n",
      "     ║    ║    9\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    27\n",
      "    ╠════╗\n",
      "    ║    9\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   24\n",
      "   ╠════╗\n",
      "   ║    14\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  34\n",
      "  ╠════╗\n",
      "  ║    16\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 14\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 12.469921112060547)\n",
      "Mean, Std: (0.9999998807907104, 1.9778850078582764)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([8.3868e+01, 1.5237e+02, 1.2632e+02, 9.9345e+01, 1.5770e+02, 9.2768e+01,\n",
      "        8.5545e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.9472e+01, 9.4052e-11,\n",
      "        0.0000e+00, 0.0000e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True,  True,  True,\n",
      "        False,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([9.8326e+01, 1.1846e+02, 1.3673e+02, 1.1237e+02, 1.2636e+02, 1.2577e+02,\n",
      "        1.4667e+02, 9.0272e+01, 7.4104e+01, 1.0974e+02, 8.8592e+01, 7.1180e+01,\n",
      "        9.6694e+01, 6.8800e+01, 7.2637e+01, 1.4008e+02, 6.2077e+01, 8.0827e+01,\n",
      "        1.1892e+02, 1.0187e+02, 1.0762e+02, 5.8567e+01, 8.3044e+01, 5.5164e+01,\n",
      "        0.0000e+00, 4.1677e+01, 0.0000e+00, 4.3325e+01, 3.0002e+01, 6.2450e-08,\n",
      "        1.0666e-09, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False,  True, False, False,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.2040e+03, 1.3443e+03, 9.2205e+02, 1.2412e+03, 1.5233e+03, 1.0112e+03,\n",
      "        3.4957e+03, 3.8897e+03, 4.4273e+03, 5.4869e+03, 5.5193e+02, 5.0464e+02,\n",
      "        4.7481e+02, 1.6416e+03, 3.5908e+02, 1.7177e+01, 8.8331e+01, 2.4471e+00,\n",
      "        6.5158e+00, 1.6444e+00, 1.1287e+01, 2.1254e+00, 8.5811e-01, 2.7007e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True, False,  True,\n",
      "        False,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.1626e+02, 9.2454e+02, 1.7768e+03, 1.9513e+03, 1.3304e+03, 7.6520e+02,\n",
      "        8.1362e+02, 2.1613e+03, 1.5592e+03, 1.4880e+03, 1.3495e+03, 6.1652e+02,\n",
      "        2.6706e+02, 1.2250e+02, 5.7780e+02, 1.2513e+02, 1.3041e+02, 5.5798e+02,\n",
      "        3.5366e+02, 1.9832e+01, 1.4849e+01, 1.6794e+01, 2.4162e+00, 1.5927e+00,\n",
      "        3.8436e+00, 5.9903e+00, 2.4511e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False,  True, False, False,\n",
      "        False, False,  True,  True,  True, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([1.1100e+03, 1.5879e+03, 1.2731e+03, 4.8546e+02, 1.3509e+03, 1.8899e+03,\n",
      "        1.0364e+03, 1.9039e+03, 1.9733e+03, 3.3308e+03, 9.9007e+02, 2.6281e+03,\n",
      "        4.1651e+03, 3.8425e+03, 1.5874e+03, 1.0672e+03, 5.0176e+02, 3.9023e+02,\n",
      "        2.0980e+02, 2.4606e+02, 1.8870e+01, 2.3220e+01, 1.8846e+02, 1.6277e+00,\n",
      "        1.1469e+01, 3.3367e+00, 1.0164e+00, 9.4152e+00, 3.7971e+00, 3.2934e-01,\n",
      "        4.9021e+00, 7.9464e-01, 1.0852e+01, 1.2063e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True, False,  True,  True, False,  True,  True,\n",
      "        False,  True, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 97.9341,  45.7544,  98.0311,  87.3993, 138.7327,  82.3455,  88.7520,\n",
      "         84.4813, 117.2730,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False, False, False,  True,\n",
      "         True,  True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.3681e+02, 6.9035e+02, 5.7785e+02, 5.4073e+02, 7.1977e+02, 3.4373e+03,\n",
      "        4.7047e+02, 7.7744e+02, 4.1954e+02, 3.5266e+02, 2.8222e+02, 5.6213e+00,\n",
      "        4.2908e+00, 3.9198e+00, 1.9348e+00, 1.6281e+00, 4.5344e+00, 8.9203e-01,\n",
      "        7.4320e+00, 8.8564e+00, 9.4920e-01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True,  True,  True, False,  True, False,  True,\n",
      "         True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([89.6377, 81.1258, 71.0213, 74.2342, 70.9202, 50.0215, 64.9944, 44.4447,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([5.6872e+01, 7.2934e+01, 4.4981e+01, 4.9746e+01, 2.5290e+01, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.0149e-09], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([123.8298, 121.6955, 141.2094,  64.1629,  67.1950, 110.9346, 130.1518,\n",
      "          0.0000,  14.0722], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False,  True, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.3312e+02, 3.1117e+02, 3.7163e+02, 2.7207e+02, 1.5894e+02, 2.5714e+02,\n",
      "        3.5875e+02, 2.9714e+02, 2.8100e+02, 2.8719e+02, 3.5016e+02, 1.4568e+02,\n",
      "        1.9380e-01, 1.4904e-01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True], device='cuda:1')\n",
      "pruning 67 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     14\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    8\n",
      "     ║    ╠════╝\n",
      "     ║    26\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    21\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    8\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  26\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 12\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :15 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 60 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     19\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    20\n",
      "     ║    ╠════╝\n",
      "     ║    41\n",
      "     ║    ╠════╗\n",
      "     ║    ║    8\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    24\n",
      "    ╠════╗\n",
      "    ║    10\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   20\n",
      "   ╠════╗\n",
      "   ║    11\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  31\n",
      "  ╠════╗\n",
      "  ║    14\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 15\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 13.671238899230957)\n",
      "Mean, Std: (1.0, 2.0171220302581787)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([7.5602e+01, 1.6226e+02, 1.3389e+02, 1.2159e+02, 1.7914e+02, 1.0454e+02,\n",
      "        7.3399e+01, 7.8792e+01, 4.6738e-05, 0.0000e+00, 0.0000e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False,  True,  True,\n",
      "         True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([8.4572e+01, 1.3995e+02, 1.1388e+02, 1.1733e+02, 1.3247e+02, 1.4674e+02,\n",
      "        1.2306e+02, 7.8818e+01, 1.1624e+02, 8.6084e+01, 8.4461e+01, 8.2416e+01,\n",
      "        6.1654e+01, 7.7372e+01, 1.5545e+02, 9.6501e+01, 7.8555e+01, 1.2633e+02,\n",
      "        1.1034e+02, 1.2510e+02, 5.7706e+01, 8.8463e+01, 5.8602e+01, 4.9310e+01,\n",
      "        7.4365e+01, 5.3432e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        4.8938e-08, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.8259e+01, 2.4172e-09],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.6633e+03, 1.6599e+03, 7.9890e+02, 1.3803e+03, 1.2394e+03, 3.2169e+03,\n",
      "        4.0790e+03, 5.2974e+03, 6.7196e+03, 7.7788e+02, 6.5547e+02, 6.3834e+02,\n",
      "        2.1788e+03, 5.0118e+02, 3.0432e+02, 1.3256e+02, 5.3752e+00, 5.2170e+00,\n",
      "        5.6829e+00, 1.3989e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.1771e+02, 9.0345e+02, 1.2486e+03, 2.7681e+03, 1.3890e+03, 8.2237e+02,\n",
      "        9.6870e+02, 2.2275e+03, 1.7831e+03, 1.2362e+03, 1.2653e+03, 5.9053e+02,\n",
      "        3.2327e+02, 3.9571e+02, 5.1268e+02, 1.6036e+02, 5.1087e+02, 1.9674e+01,\n",
      "        6.4999e+01, 8.2130e+00, 2.1382e+00, 4.9596e+00, 1.1229e+01, 3.6505e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False, False,  True, False, False, False, False, False,\n",
      "         True, False, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.1413e+03, 1.4067e+03, 2.2304e+03, 4.9017e+02, 1.1381e+03, 1.7167e+03,\n",
      "        9.3507e+02, 2.5886e+03, 1.7915e+03, 2.8941e+03, 1.2343e+03, 2.9860e+03,\n",
      "        4.6584e+03, 4.1494e+03, 1.8881e+03, 1.1328e+03, 4.1158e+02, 3.0217e+02,\n",
      "        4.6649e+02, 5.5998e+02, 2.9529e+01, 1.5368e+01, 1.4523e+00, 3.5241e+00,\n",
      "        1.0699e+01, 1.1449e+01, 2.2167e+00, 9.4563e-01, 5.6441e+00, 3.0708e+01,\n",
      "        1.2301e+01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False,  True,  True, False, False,  True,  True, False, False,\n",
      "        False], device='cuda:1')\n",
      "Significance:\n",
      "tensor([9.2106e+01, 7.0359e+01, 6.8022e+01, 1.2912e+02, 1.2753e+02, 4.7800e+01,\n",
      "        1.1218e+02, 1.0844e+02, 0.0000e+00, 0.0000e+00, 8.2171e-05, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([6.7491e+02, 7.2039e+02, 5.5219e+02, 4.9265e+02, 7.5149e+02, 4.1614e+03,\n",
      "        5.7566e+02, 7.4598e+02, 4.7481e+02, 3.3672e+02, 3.5620e+02, 1.0960e+00,\n",
      "        4.5449e+00, 2.4171e+00, 7.3692e+00, 4.7206e+00, 3.4892e+00, 6.4671e+00,\n",
      "        9.3191e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True,  True,  True, False,  True,  True, False, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([9.1943e+01, 7.8334e+01, 8.6245e+01, 6.6596e+01, 7.4618e+01, 5.7814e+01,\n",
      "        5.5735e+01, 6.4860e+01, 9.1196e-11, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        5.2568e+01, 3.9664e-07], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 60.3108, 102.3322,  40.2430,  63.4345,  46.4178,   0.0000,   0.0000,\n",
      "          0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([139.7468, 135.6270, 143.2432,  70.7300,  65.2401,  90.4795,  47.6070,\n",
      "          0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.1417e+02, 3.1502e+02, 3.9206e+02, 2.8813e+02, 1.5961e+02, 2.8486e+02,\n",
      "        4.0903e+02, 3.2434e+02, 2.8239e+02, 2.9904e+02, 3.6299e+02, 1.4811e+02,\n",
      "        4.6664e-02, 3.1718e-02, 1.4753e-01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False,  True,  True,  True], device='cuda:1')\n",
      "pruning 64 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     14\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    8\n",
      "     ║    ╠════╝\n",
      "     ║    26\n",
      "     ║    ╠════╗\n",
      "     ║    ║    5\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    20\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   18\n",
      "   ╠════╗\n",
      "   ║    7\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  25\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 11\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :16 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "Adding 60 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     18\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    19\n",
      "     ║    ╠════╝\n",
      "     ║    31\n",
      "     ║    ╠════╗\n",
      "     ║    ║    19\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    20\n",
      "    ╠════╗\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   23\n",
      "   ╠════╗\n",
      "   ║    11\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  27\n",
      "  ╠════╗\n",
      "  ║    15\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 14\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 11.485472679138184)\n",
      "Mean, Std: (0.9999999403953552, 1.9641337394714355)\n",
      "remove_below 0.01\n",
      "Significance:\n",
      "tensor([ 68.7425, 115.4435, 142.5041, 179.9917, 100.3661,  59.8333,  58.4049,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 95.3142, 127.8739, 102.5909, 121.5006, 158.9276, 102.8667,  70.7694,\n",
      "        122.6499,  96.0683, 103.1068,  85.0918,  70.2642,  56.7229, 136.4816,\n",
      "         85.8466,  93.3174, 121.6153, 117.3690, 120.6511,  70.3654, 115.7325,\n",
      "         86.1337,  60.7677, 106.0025,  70.6737,  64.6906,   0.0000,  61.0408,\n",
      "          0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False,  True, False,  True,  True,\n",
      "         True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.8369e+03, 1.8345e+03, 7.6147e+02, 1.8107e+03, 1.2070e+03, 3.2352e+03,\n",
      "        5.2046e+03, 5.0335e+03, 6.1455e+02, 4.5779e+02, 6.6040e+02, 2.2862e+03,\n",
      "        4.2431e+02, 1.3161e+02, 2.9641e+02, 2.5319e+02, 2.4212e+02, 4.0467e+02,\n",
      "        2.9909e+00, 9.4124e+00, 6.9381e+00, 6.9892e-01, 4.2121e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.7501e+02, 8.2912e+02, 1.7727e+03, 2.6856e+03, 1.4617e+03, 9.2006e+02,\n",
      "        1.1356e+03, 3.3437e+03, 2.2699e+03, 1.0392e+03, 5.2063e+02, 5.3283e+02,\n",
      "        4.1666e+02, 1.7650e+02, 5.4760e+02, 9.6357e+01, 1.2508e+02, 5.9526e+01,\n",
      "        5.9680e+00, 1.7378e+00], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 787.7318, 2176.3362, 2233.0857,  591.2551, 1480.3494, 1494.1030,\n",
      "         917.2188, 1825.5300, 2038.6204,  772.8368, 2560.4006, 3958.9827,\n",
      "        4319.1685, 1199.9651,  378.7594,  509.8649,  920.5667, 1179.2255,\n",
      "         193.6355,  199.9028,   52.6882,   18.2390,    6.5939,   65.8028,\n",
      "          15.2758,   13.7416,   23.6522], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([100.1851,  52.9041, 103.0247, 106.0890, 113.9566,  48.5313, 106.8613,\n",
      "         82.8666,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True,\n",
      "         True,  True,  True,  True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([6.3944e+02, 7.0274e+02, 5.3230e+02, 4.1883e+02, 7.2002e+02, 3.5867e+03,\n",
      "        5.8776e+02, 7.2097e+02, 4.3300e+02, 3.6360e+02, 3.8258e+02, 9.1459e+00,\n",
      "        4.6243e+00, 5.0677e+00, 1.1975e+01, 6.2491e+00, 9.6371e+00, 3.3651e+00],\n",
      "       device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([ 85.4127, 100.3829,  84.4730,  73.7465,  52.7901,  56.6972,  78.7076,\n",
      "         53.7228,  49.4678,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([7.4792e+01, 9.0857e+01, 5.1742e+01, 5.0989e+01, 4.0807e+01, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5338e-07, 0.0000e+00, 0.0000e+00,\n",
      "        5.0935e+01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "       device='cuda:1')\n",
      "Significance:\n",
      "tensor([123.9712, 144.0238, 171.5425,  61.4232,  89.6768, 109.2564,  43.0641,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000], device='cuda:1')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True,  True], device='cuda:1')\n",
      "Significance:\n",
      "tensor([2.5062e+02, 3.0714e+02, 4.1574e+02, 3.6667e+02, 2.4371e+02, 2.9106e+02,\n",
      "        3.3683e+02, 2.9123e+02, 2.9523e+02, 3.3843e+02, 2.4292e+02, 2.0534e-02,\n",
      "        7.1641e-02, 1.9143e+01], device='cuda:1')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True,  True, False], device='cuda:1')\n",
      "pruning 56 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 60 Neurons\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     23\n",
      "     ╠════╗\n",
      "     ║    ╠════╗\n",
      "     ║    ║    15\n",
      "     ║    ╠════╝\n",
      "     ║    31\n",
      "     ║    ╠════╗\n",
      "     ║    ║    11\n",
      "     ║    ╠════╝\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    19\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   27\n",
      "   ╠════╗\n",
      "   ║    11\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  28\n",
      "  ╠════╗\n",
      "  ║    10\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 16\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "trainer.loop(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.beta_del_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.randn(10)*10).abs().pow(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accs_all, label=\"train\")\n",
    "plt.plot(accs_test, label=\"test\")\n",
    "ymin, ymax = plt.gca().get_ylim()\n",
    "plt.text(0, 0.8*ymin+0.2*ymax, f\"Train-> max:{max(accs_all):.3f} end:{accs_all[-1]:.3f} \\nTest-> max:{max(accs_test):.3f} end:{accs_test[-1]:.3f}\")\n",
    "                    \n",
    "plt.legend()\n",
    "plt.savefig(\"files/05_dycnn_cifar10_2_9.3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.non_linearity.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.root_net.residual.fc0.residual.fc1.residual.activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
