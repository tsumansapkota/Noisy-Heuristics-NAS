{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "import time\n",
    "import sys, io\n",
    "\n",
    "import mylibrary.datasets as datasets\n",
    "import mylibrary.nnlib as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_hidden_neuron_number(i, o):\n",
    "    return (max(i,o)*(min(i,o)**2))**(1/3)\n",
    "\n",
    "\n",
    "class Shortcut_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=(3,3), stride=1):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self._kernel = np.array(kernel, dtype=int)\n",
    "        self._padding = tuple(((self._kernel-1)/2).astype(int))\n",
    "        self._stride = stride\n",
    "        _wd = nn.Conv2d(input_dim, output_dim, self._kernel, stride=self._stride,\n",
    "                        padding=self._padding, bias=False).weight.data\n",
    "        ## Shape = OutputDim, InputDim, Kernel0, Kernel1\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "        del _wd\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.shape[1] > 0 and self.weight.shape[0] > 0:\n",
    "            return F.conv2d(x, self.weight, stride=self._stride, padding=self._padding)\n",
    "        ### output dim is 0\n",
    "        elif self.weight.shape[0] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###       #out_dim #inp_dim            #kernel\n",
    "            w = torch.zeros(1, 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return torch.zeros(o.shape[0], 0, o.shape[2], o.shape[3], dtype=x.dtype, device=x.device)\n",
    "        ### input dim is 0\n",
    "        elif x.shape[1] == 0:\n",
    "            ###             #num_inp  #inp_dim    #feature\n",
    "            x = torch.zeros(x.shape[0], 1, x.shape[2], x.shape[3], dtype=x.dtype, device=x.device)\n",
    "            ###             #out_dim            #inp_dim            #kernel\n",
    "            w = torch.zeros(self.weight.shape[0], 1, self.weight.shape[2], self.weight.shape[3], dtype=x.dtype, device=x.device)\n",
    "            o = F.conv2d(x, w, stride=self._stride, padding=self._padding)\n",
    "            return o.data\n",
    "        else:\n",
    "            raise(f\"Unknown shape of input {x.shape} or weight {self.weight.shape}\")\n",
    "\n",
    "#     def decay_std_ratio(self, factor):\n",
    "#         self.weight.data = self.weight.data - self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "        \n",
    "#     def decay_std_ratio_grad(self, factor):\n",
    "#         self.weight.grad = self.weight.grad + self.tree.decay_rate_std*factor.t()*self.weight.data\n",
    "    \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        pass\n",
    "    \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "        pass\n",
    "     \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "        _w = self.weight.data[remaining, :]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "        _w = self.weight.data[:, remaining]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        _w = torch.cat((self.weight.data, torch.zeros(o, num, k0, k1, dtype=self.weight.data.dtype,\n",
    "                                                      device=self.weight.data.device)), dim=1)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i, k0, k1 = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i)\n",
    "        _new = torch.empty(num, i, k0, k1, dtype=self.weight.data.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        _w = torch.cat((self.weight.data, _new), dim=0)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)        \n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}S▚:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NonLinearity_Conv(nn.Module):\n",
    "\n",
    "#     def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "#         super().__init__()\n",
    "#         self.tree = tree\n",
    "#         self.bias = nn.Parameter(torch.zeros(io_dim))\n",
    "#         self.actf = actf_obj\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.actf(x+self.bias.view(1,-1,1,1))\n",
    "\n",
    "#     def add_neuron(self, num):\n",
    "#         _b = torch.cat((self.bias.data, torch.zeros(num, dtype=self.bias.data.dtype,\n",
    "#                                                     device=self.bias.data.device)))\n",
    "#         del self.bias\n",
    "#         self.bias = nn.Parameter(_b)\n",
    "        \n",
    "#     def remove_neuron(self, remaining):\n",
    "#         _b = self.bias.data[remaining]\n",
    "#         del self.bias\n",
    "#         self.bias = nn.Parameter(_b)\n",
    "\n",
    "class NonLinearity_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.bn = nn.BatchNorm2d(io_dim)\n",
    "        self.actf = actf_obj\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## if empty tensor forward that to next layer.\n",
    "        if x.shape[1] < 1:\n",
    "            return x\n",
    "        return self.actf(self.bn(x))\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        ####https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#BatchNorm2d\n",
    "        ## running_mean\n",
    "        _rm = self.bn.running_mean\n",
    "        _rm = torch.cat((_rm, torch.zeros(num, dtype=_rm.dtype, device=_rm.device)))\n",
    "        self.bn.running_mean = _rm\n",
    "        \n",
    "        ## running_var\n",
    "        _rv = self.bn.running_var\n",
    "        _rv = torch.cat((_rv, torch.ones(num, dtype=_rv.dtype, device=_rv.device)))\n",
    "        self.bn.running_var = _rv\n",
    "        \n",
    "        ## weight\n",
    "        _w = self.bn.weight.data\n",
    "        _w = torch.cat((_w, torch.ones(num, dtype=_w.dtype, device=_w.device)))\n",
    "        del self.bn.weight\n",
    "        self.bn.weight = nn.Parameter(_w)\n",
    "        \n",
    "        ## bias\n",
    "        _b = self.bn.bias.data\n",
    "        _b = torch.cat((_b, torch.zeros(num, dtype=_b.dtype, device=_b.device)))\n",
    "        del self.bn.bias\n",
    "        self.bn.bias = nn.Parameter(_b)\n",
    "        \n",
    "        self.bn.num_features += num\n",
    "        return\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        ## running_mean\n",
    "        _rm = self.bn.running_mean[remaining]\n",
    "        self.bn.running_mean = _rm\n",
    "        \n",
    "        ## running_var\n",
    "        _rv = self.bn.running_var[remaining]\n",
    "        self.bn.running_var = _rv\n",
    "        \n",
    "        ## weight\n",
    "        _w = self.bn.weight.data[remaining]\n",
    "        del self.bn.weight\n",
    "        self.bn.weight = nn.Parameter(_w)\n",
    "        \n",
    "        ## bias\n",
    "        _b = self.bn.bias.data[remaining]\n",
    "        del self.bn.bias\n",
    "        self.bn.bias = nn.Parameter(_b)\n",
    "        \n",
    "        self.bn.num_features = len(remaining)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinearity(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, io_dim, actf_obj=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.bias = nn.Parameter(torch.zeros(io_dim))\n",
    "        self.actf = actf_obj\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actf(x+self.bias)\n",
    "\n",
    "    def add_neuron(self, num):\n",
    "        _b = torch.cat((self.bias.data, torch.zeros(num, dtype=self.bias.data.dtype,\n",
    "                                                    device=self.bias.data.device)))\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)\n",
    "        \n",
    "    def remove_neuron(self, remaining):\n",
    "        _b = self.bias.data[remaining]\n",
    "        del self.bias\n",
    "        self.bias = nn.Parameter(_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, hidden_dim, output_dim, stride=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "#         self.stride = stride\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = HierarchicalResidual_Conv(self.tree, input_dim, hidden_dim, stride=stride, activation=activation) \n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = HierarchicalResidual_Conv(self.tree, hidden_dim, output_dim, activation=activation)\n",
    "        self.fc1.shortcut.weight.data *= 0.\n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "            self.apnz = self.apnz/self.count\n",
    "            self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "#         self.count = None\n",
    "\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "#             self.significance += z.pow(2).sum(dim=0)\n",
    "            self.significance += z.abs().sum(dim=0)\n",
    "#             self.significance += z.abs().pow(0.8).sum(dim=0)\n",
    "#             print(f\"SIG ACT:\\n{float(self.activations.abs().mean())}\")\n",
    "#             print(f\"GRAD Mean, Std:\\n{float(grad_output[0].data.abs().mean()), float(grad_output[0].data.std())}\")\n",
    "\n",
    "            if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "                self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "        #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "                self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below=None, above=None, mask=None):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(self.significance.numel(), dtype=torch.bool)\n",
    "        if below:\n",
    "            mask = torch.logical_or(mask,self.significance<=below)\n",
    "        if above:\n",
    "            mask = torch.logical_or(mask,self.significance>above)\n",
    "            \n",
    "        print(f\"Significance:\\n{self.significance}\\nPrune:\\n{mask}\")\n",
    "            \n",
    "        self.to_remove = torch.nonzero(mask).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "#         max_dim = np.ceil((self.tree.parent_dict[self].input_dim+\\\n",
    "#             self.tree.parent_dict[self].output_dim)/2)\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None:\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10, 3)*0.\n",
    "a.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, stride=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.stride = 1\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, stride=self.stride).to(self.tree.device)\n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None ## this can be Residual Layer or None\n",
    "        ##### only one of shortcut or residual can be None at a time\n",
    "        self.forward = self.forward_shortcut\n",
    "        \n",
    "        self.std_ratio = 0. ## 0-> all variation due to shortcut, 1-> residual\n",
    "        self.target_std_ratio = 0. ##\n",
    "    \n",
    "    def forward_both(self, r):\n",
    "        s = self.shortcut(r)\n",
    "        r = self.residual(r)\n",
    "\n",
    "        if self.residual.hook is None: ### dont execute when computing significance\n",
    "            s_std = torch.std(s, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            r_std = torch.std(r, dim=(0,2,3), keepdim=True).reshape(1, -1)\n",
    "            stdr = r_std/(s_std+r_std)\n",
    "\n",
    "            self.std_ratio = self.tree.beta_std_ratio*self.std_ratio + (1-self.tree.beta_std_ratio)*stdr.data\n",
    "            if r_std.min() > 1e-9:\n",
    "                ## recover for the fact that when decaying neurons, target ratio should also be reducing\n",
    "                if self.tree.total_decay_steps:\n",
    "                    i, o = self.shortcut.weight.shape[1],self.shortcut.weight.shape[0]\n",
    "                    if self.shortcut.to_remove is not None:\n",
    "                        i -= len(self.shortcut.to_remove)\n",
    "                    if self.shortcut.to_freeze is not None:\n",
    "                        o -= len(self.shortcut.to_freeze)\n",
    "                    h = self.residual.hidden_dim\n",
    "                    if self.residual.to_remove is not None:\n",
    "                        h -= len(self.residual.to_remove)\n",
    "                    \n",
    "#                     tr = h/np.ceil((i+o)/2 +1)\n",
    "                    tr = h/_get_hidden_neuron_number(i, o)\n",
    "                    self.compute_target_std_ratio(tr)\n",
    "                else:\n",
    "                    self.compute_target_std_ratio()\n",
    "                self.get_std_loss(stdr)\n",
    "        return s+r\n",
    "    \n",
    "    def forward_shortcut(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def forward_residual(self, x):\n",
    "        self.compute_target_std_ratio()\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def compute_target_std_ratio(self, tr = None):\n",
    "        if tr is None:\n",
    "#             tr = self.residual.hidden_dim/np.ceil((self.input_dim+self.output_dim)/2 +1)\n",
    "            tr = self.residual.hidden_dim/_get_hidden_neuron_number(self.input_dim, self.output_dim)\n",
    "#             tr = self.residual.hidden_dim/np.ceil(self.output_dim/2 +1)\n",
    "\n",
    "        tr = np.clip(tr, 0., 1.)\n",
    "        self.target_std_ratio = self.tree.beta_std_ratio*self.target_std_ratio +\\\n",
    "                                (1-self.tree.beta_std_ratio)*tr\n",
    "        pass        \n",
    "    \n",
    "    def get_std_loss(self, stdr):\n",
    "        del_std = self.target_std_ratio-stdr\n",
    "        del_std_loss = (del_std**2 + torch.abs(del_std)).mean()\n",
    "#         del_std_loss = (del_std**2).mean()\n",
    "        self.tree.std_loss += del_std_loss\n",
    "        return\n",
    "            \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_freezing_connection(to_freeze)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.start_decaying_connection(to_remove)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_freezed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.remove_freezed_connection(remaining)\n",
    "            if self.shortcut: self.std_ratio = self.std_ratio[:, remaining]\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        if self.shortcut:\n",
    "            self.shortcut.remove_decayed_connection(remaining)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_input_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        if self.shortcut:\n",
    "            self.shortcut.add_output_connection(num)\n",
    "        if self.residual:\n",
    "            self.residual.fc1.add_output_connection(num)\n",
    "            # if torch.is_tensor(self.std_ratio):\n",
    "            if self.shortcut:\n",
    "                self.std_ratio = torch.cat((self.std_ratio, torch.zeros(1, num, device=self.tree.device)), dim=1)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        \n",
    "        if self.residual is None:\n",
    "            # print(f\"Adding {num} hidden units.. in new residual_layer\")\n",
    "            self.residual = Residual_Conv(self.tree, self.input_dim,\n",
    "                                          num, self.output_dim, stride=self.stride,\n",
    "                                          activation=self.activation).to(self.tree.device)\n",
    "            \n",
    "            self.tree.parent_dict[self.residual] = self\n",
    "            if self.shortcut is None:\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            else:\n",
    "                self.forward = self.forward_both\n",
    "                self.std_ratio = torch.zeros(1, self.output_dim, device=self.tree.device)\n",
    "                \n",
    "        else:\n",
    "            # print(f\"Adding {num} hidden units..\")\n",
    "            self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            if self.std_ratio.min()>0.98 and self.target_std_ratio>0.98:\n",
    "                del self.tree.parent_dict[self.shortcut]\n",
    "                del self.shortcut\n",
    "                self.shortcut = None\n",
    "                self.forward = self.forward_residual\n",
    "                self.std_ratio = 1.\n",
    "            \n",
    "        elif self.target_std_ratio<0.95:\n",
    "            self.shortcut = Shortcut_Conv(self.tree, self.input_dim, self.output_dim, stride=self.stride)\n",
    "            self.shortcut.weight.data *= 0.\n",
    "            self.forward = self.forward_both\n",
    "            \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        if self.residual is None: return\n",
    "        \n",
    "        if self.residual.hidden_dim < 1:\n",
    "            del self.tree.parent_dict[self.residual]\n",
    "            del self.residual\n",
    "            ### its parent (Residual_Conv) removes it from dynamic list if possible\n",
    "            self.residual = None\n",
    "            self.forward = self.forward_shortcut\n",
    "            self.std_ratio = 0.\n",
    "            return\n",
    "        \n",
    "#         max_dim = np.ceil((self.input_dim+self.output_dim)/2)\n",
    "        # max_dim = min((self.input_dim, self.output_dim))+1\n",
    "        max_dim = _get_hidden_neuron_number(self.input_dim, self.output_dim) + 1 \n",
    "        # print(\"MaxDIM\", max_dim, self.residual.hidden_dim)\n",
    "        if self.residual.hidden_dim > max_dim:\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc0)\n",
    "            self.tree.DYNAMIC_LIST.add(self.residual.fc1)\n",
    "            # print(\"Added\", self.residual)\n",
    "            \n",
    "        # self.residual.fc0.morph_network()\n",
    "        # self.residual.fc1.morph_network()\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        stdr = self.std_ratio\n",
    "        if torch.is_tensor(self.std_ratio):\n",
    "            stdr = self.std_ratio.min()\n",
    "            \n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{self.target_std_ratio}, s:{stdr}\")\n",
    "        if self.shortcut:\n",
    "            self.shortcut.print_network_debug(depth+1)\n",
    "        if self.residual:\n",
    "            self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        if self.residual is None:\n",
    "            return\n",
    "        \n",
    "        if self.shortcut:\n",
    "            print(f\"{pre_string}╠════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}║    \")\n",
    "            print(f\"{pre_string}╠════╝\")\n",
    "        else:\n",
    "            print(f\"{pre_string}╚════╗\")\n",
    "            self.residual.print_network(f\"{pre_string}     \")\n",
    "            print(f\"{pre_string}╔════╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Conv Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Conv_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation, hidden_dim, post_activation=None):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.del_neurons = 0.\n",
    "        self.neurons_added = 0\n",
    "        self.post_activation = post_activation\n",
    "\n",
    "        ## Shortcut or Hierarchical Residual Layer\n",
    "        self.fc0 = hrnet0\n",
    "        self.non_linearity = NonLinearity_Conv(self.tree, hidden_dim, activation)\n",
    "        self.fc1 = hrnet1\n",
    "        \n",
    "        self.tree.parent_dict[self.fc0] = self\n",
    "        self.tree.parent_dict[self.fc1] = self\n",
    "        self.tree.parent_dict[self.non_linearity] = self\n",
    "        \n",
    "        self.hook = None\n",
    "        self.activations = None\n",
    "        self.significance = None\n",
    "        self.count = None\n",
    "        self.apnz = None\n",
    "        self.to_remove = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.non_linearity(x)\n",
    "        self.activations = x.data\n",
    "        if self.post_activation:\n",
    "            x = self.post_activation(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def start_computing_significance(self):\n",
    "        self.significance = 0.\n",
    "        self.count = 0\n",
    "        self.apnz = 0\n",
    "        self.hook = self.non_linearity.register_backward_hook(self.compute_neuron_significance)\n",
    "        pass\n",
    "            \n",
    "    def finish_computing_significance(self):\n",
    "        self.hook.remove()\n",
    "        self.significance = self.significance#/self.count\n",
    "#         print(f\"Significance before rethinking(apnz)\\n{self.significance}\")\n",
    "#         print(f\"Apnz\\n{self.apnz}\")\n",
    "        if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "            self.apnz = self.apnz/self.count\n",
    "            self.significance = self.significance*(1-self.apnz) * 4 ## tried on desmos.\n",
    "#         print(f\"Significance after rethinking(apnz)\\n{self.significance}\")\n",
    "#         self.count = None\n",
    "\n",
    "        self.hook = None\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def compute_neuron_significance(self, _class, grad_input, grad_output):\n",
    "        with torch.no_grad():\n",
    "            z = torch.sum(grad_output[0].data*self.activations, dim=(2,3))\n",
    "#             self.significance += z.pow(2).sum(dim=0)\n",
    "            self.significance += z.abs().sum(dim=0)\n",
    "#             self.significance += z.abs().pow(0.8).sum(dim=0)\n",
    "#             print(\"Current Significance \\n\", self.significance)\n",
    "#             print(f\"SIG ACT:\\n{float(self.activations.abs().mean())}\")\n",
    "#             print(f\"GRAD Mean, Std:\\n{float(grad_output[0].data.abs().mean()), float(grad_output[0].data.std())}\")\n",
    "\n",
    "            if isinstance(self.non_linearity.actf, nn.ReLU):\n",
    "                self.count += grad_output[0].shape[0]*grad_output[0].shape[2]*grad_output[0].shape[3]\n",
    "        #         self.apnz += torch.count_nonzero(self.activations.data, dim=0)\n",
    "                self.apnz += torch.sum(self.activations > 0., dim=(0,2,3), dtype=z.dtype).to(z.device)\n",
    "        pass\n",
    "    \n",
    "    def identify_removable_neurons(self, below=None, above=None, mask=None):\n",
    "        if self.to_remove is not None:\n",
    "            print(\"First remove all previous less significant neurons\")\n",
    "            return\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(self.significance.numel(), dtype=torch.bool)\n",
    "        if below:\n",
    "            mask = torch.logical_or(mask,self.significance<=below)\n",
    "        if above:\n",
    "            mask = torch.logical_or(mask,self.significance>above)\n",
    "            \n",
    "        print(f\"Significance:\\n{self.significance}\\nPrune:\\n{mask}\")\n",
    "        \n",
    "        self.to_remove = torch.nonzero(mask).reshape(-1)\n",
    "        if len(self.to_remove)>0:\n",
    "            self.fc0.start_freezing_connection(self.to_remove)\n",
    "            self.fc1.start_decaying_connection(self.to_remove)\n",
    "            self.tree.remove_neuron_residual.add(self)\n",
    "            return len(self.to_remove)\n",
    "        \n",
    "        self.to_remove = None\n",
    "        return 0\n",
    "\n",
    "    def remove_decayed_neurons(self):\n",
    "        remaining = []\n",
    "        for i in range(self.hidden_dim):\n",
    "            if i not in self.to_remove:\n",
    "                remaining.append(i)\n",
    "        \n",
    "        self.non_linearity.remove_neuron(remaining)\n",
    "        self.fc0.remove_freezed_connection(remaining)\n",
    "        self.fc1.remove_decayed_connection(remaining)\n",
    "        \n",
    "        self.neurons_added -= len(self.to_remove)\n",
    "        self.hidden_dim = len(remaining)\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def compute_del_neurons(self):\n",
    "        self.del_neurons = (1-self.tree.beta_del_neuron)*self.neurons_added \\\n",
    "                            + self.tree.beta_del_neuron*self.del_neurons\n",
    "        self.neurons_added = 0\n",
    "        return\n",
    "    \n",
    "    def add_hidden_neuron(self, num):\n",
    "        self.fc0.add_output_connection(num)\n",
    "        self.non_linearity.add_neuron(num)\n",
    "        self.fc1.add_input_connection(num)\n",
    "        \n",
    "        self.hidden_dim += num\n",
    "        self.neurons_added += num\n",
    "        pass\n",
    "\n",
    "    def morph_network(self):\n",
    "        self.fc0.morph_network()\n",
    "        self.fc1.morph_network()\n",
    "        max_dim = _get_hidden_neuron_number(self.tree.parent_dict[self].input_dim,\n",
    "            self.tree.parent_dict[self].output_dim)+1\n",
    "        if self.hidden_dim <= max_dim:\n",
    "            if self.fc0.residual is None:\n",
    "                if self.fc0 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc0)\n",
    "            if self.fc1.residual is None:\n",
    "                if self.fc1 in self.tree.DYNAMIC_LIST:\n",
    "                    self.tree.DYNAMIC_LIST.remove(self.fc1)\n",
    "        return \n",
    "\n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'║     '*depth}R▚:{depth}[{self.hidden_dim}|{self.non_linearity.bias.data.shape[0]}]\")\n",
    "        self.fc0.print_network_debug(depth+1)\n",
    "        self.fc1.print_network_debug(depth+1)\n",
    "        \n",
    "    def print_network(self, pre_string):\n",
    "        self.fc0.print_network(pre_string)\n",
    "        print(f\"{pre_string}{self.hidden_dim}\")\n",
    "        self.fc1.print_network(pre_string)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-96a8be50163c>:3: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  torch.nonzero(torch.logical_and(a,b))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [8]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10)<0 \n",
    "b = torch.randn(10) > 0.5\n",
    "torch.nonzero(torch.logical_and(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalResidual_Connector(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, hrnet0, hrnet1, activation=nn.ReLU(), post_activation=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = hrnet0.input_dim\n",
    "        self.output_dim = hrnet1.output_dim\n",
    "        \n",
    "        ## this can be Shortcut Layer or None\n",
    "        self.shortcut = None\n",
    "        self.residual = Residual_Conv_Connector(self.tree, hrnet0, hrnet1,\n",
    "                                                activation, hrnet0.output_dim, post_activation)\n",
    "        self.tree.parent_dict[self.residual] = self\n",
    "            \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.residual(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.residual.fc1.start_freezing_connection(to_freeze)\n",
    "        pass\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.residual.fc0.start_decaying_connection(to_remove)\n",
    "        pass\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.residual.fc1.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.residual.fc0.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.residual.fc0.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.residual.fc1.add_output_connection(num)\n",
    "        \n",
    "    def add_hidden_neuron(self, num):\n",
    "        if num<1: return\n",
    "        self.residual.add_hidden_neuron(num)\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):  \n",
    "        self.residual.fc0.maintain_shortcut_connection()\n",
    "        self.residual.fc1.maintain_shortcut_connection()\n",
    "        \n",
    "    def morph_network(self):\n",
    "        self.residual.morph_network()\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.residual.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        print(f\"{pre_string}╚╗\")\n",
    "        self.residual.print_network(f\"{pre_string} \")\n",
    "        print(f\"{pre_string}╔╝\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut only Hierarchical Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.tree = tree\n",
    "        _wd = nn.Linear(input_dim, output_dim, bias=False).weight.data\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty_like(_wd).copy_(_wd)\n",
    "        )\n",
    "    \n",
    "        ## for removing and freezing neurons\n",
    "        self.to_remove = None\n",
    "        self.to_freeze = None\n",
    "        self.initial_remove = None\n",
    "        self.initial_freeze = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## input_dim        ## output_dim\n",
    "        if x.shape[1] + self.weight.shape[1] > 0:\n",
    "            return x.matmul(self.weight.t())\n",
    "        else:\n",
    "            # print(x.shape, self.weight.shape)\n",
    "            # print(x.matmul(self.weight.t()))\n",
    "            if x.shape[1] + self.weight.shape[1] == 0:\n",
    "                return torch.zeros(x.shape[0], self.weight.shape[0], dtype=x.dtype, device=x.device)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.initial_remove = self.weight.data[:, to_remove]\n",
    "        self.to_remove = to_remove\n",
    "        self.tree.decay_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.initial_freeze = self.weight.data[to_freeze, :]\n",
    "        self.to_freeze = to_freeze\n",
    "        self.tree.freeze_connection_shortcut.add(self)\n",
    "        pass\n",
    "    \n",
    "    def freeze_connection_step(self):#, to_freeze):\n",
    "        self.weight.data[self.to_freeze, :] = self.initial_freeze\n",
    "        pass\n",
    "    \n",
    "    def decay_connection_step(self):#, to_remove):\n",
    "        self.weight.data[:, self.to_remove] = self.initial_remove*self.tree.decay_factor\n",
    "        pass\n",
    "            \n",
    "     \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing freezed; \", self.to_freeze)\n",
    "        _w = self.weight.data[remaining, :]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_freeze = None\n",
    "        self.to_freeze = None\n",
    "        pass\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        # print(self.weight.data.shape, \"removing decayed; \", self.to_remove)\n",
    "        _w = self.weight.data[:, remaining]\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        self.initial_remove = None\n",
    "        self.to_remove = None\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        _w = torch.cat((self.weight.data, torch.zeros(o, num, dtype=self.weight.data.dtype,\n",
    "                                                      device=self.weight.data.device)), dim=1)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)\n",
    "        pass\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        # print(self.weight.data.shape)\n",
    "        o, i = self.weight.data.shape\n",
    "        stdv = 1. / np.sqrt(i)\n",
    "        _new = torch.empty(num, i, dtype=self.bias.weight.dtype,\n",
    "                           device=self.weight.data.device).uniform_(-stdv, stdv)\n",
    "        \n",
    "        _w = torch.cat((self.weight.data, _new), dim=0)\n",
    "        del self.weight\n",
    "        self.weight = nn.Parameter(_w)\n",
    "        # print(self.weight.data.shape)        \n",
    "        pass\n",
    "    \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}S:{depth}[{self.weight.data.shape[1]},{self.weight.data.shape[0]}]\")\n",
    "\n",
    "\n",
    "class HierarchicalResidual_Shortcut(nn.Module):\n",
    "\n",
    "    def __init__(self, tree, input_dim, output_dim, kernel=None, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tree = tree\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        ## this can be Shortcut Layer or None\n",
    "        if kernel is None:\n",
    "            self.shortcut = Shortcut(tree, self.input_dim, self.output_dim) \n",
    "        else:\n",
    "            self.shortcut = Shortcut_Conv(tree, self.input_dim, self.output_dim, kernel, stride) \n",
    "        self.tree.parent_dict[self.shortcut] = self\n",
    "        \n",
    "        self.residual = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x)\n",
    "    \n",
    "    def start_freezing_connection(self, to_freeze):\n",
    "        self.shortcut.start_freezing_connection(to_freeze)\n",
    "        \n",
    "    def start_decaying_connection(self, to_remove):\n",
    "        self.shortcut.start_decaying_connection(to_remove)\n",
    "    \n",
    "    def remove_freezed_connection(self, remaining):\n",
    "        self.shortcut.remove_freezed_connection(remaining)\n",
    "        self.output_dim = len(remaining)\n",
    "    \n",
    "    def remove_decayed_connection(self, remaining):\n",
    "        self.shortcut.remove_decayed_connection(remaining)\n",
    "        self.input_dim = len(remaining)\n",
    "        pass\n",
    "    \n",
    "    def add_input_connection(self, num):\n",
    "        self.input_dim += num\n",
    "        self.shortcut.add_input_connection(num)\n",
    "\n",
    "    def add_output_connection(self, num):\n",
    "        self.output_dim += num\n",
    "        self.shortcut.add_output_connection(num)\n",
    "\n",
    "    def add_hidden_neuron(self, num):\n",
    "        print(\"Cannot Add Hidden neuron to Shortcut Only Layer\")\n",
    "        return\n",
    "    \n",
    "    def maintain_shortcut_connection(self):\n",
    "        pass\n",
    "        \n",
    "    def morph_network(self):\n",
    "        pass\n",
    "        \n",
    "    def print_network_debug(self, depth):\n",
    "        print(f\"{'|     '*depth}H:{depth}[{self.input_dim},{self.output_dim}]\"+\\\n",
    "              f\"σ[t:{None}, s:{None}\")\n",
    "        self.shortcut.print_network_debug(depth+1)\n",
    "        pass\n",
    "    \n",
    "    def print_network(self, pre_string=\"\"):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree and Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree_State():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.DYNAMIC_LIST = set() ## residual parent is added, to make code effecient.\n",
    "        ## the parents which is not intended to have residual connection should not be added.\n",
    "        self.beta_std_ratio = None\n",
    "        self.beta_del_neuron = None\n",
    "        self.device = 'cpu'\n",
    "    \n",
    "        self.parent_dict = {}\n",
    "    \n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual:set = None\n",
    "        self.freeze_connection_shortcut:set = None\n",
    "        self.decay_connection_shortcut:set = None\n",
    "\n",
    "        self.decay_rate_std = 0.001\n",
    "\n",
    "        self.add_to_remove_ratio = 2.\n",
    "        pass\n",
    "    \n",
    "    def get_decay_factor(self):\n",
    "        ratio = self.current_decay_step/self.total_decay_steps\n",
    "#         self.decay_factor = np.exp(-2*ratio)*(1-ratio)\n",
    "        self.decay_factor = (1-ratio)**2\n",
    "        pass\n",
    "    \n",
    "    def clear_decay_variables(self):\n",
    "        self.total_decay_steps = None\n",
    "        self.current_decay_step = None\n",
    "        self.decay_factor = None\n",
    "        self.remove_neuron_residual = None\n",
    "        self.freeze_connection_shortcut = None\n",
    "        self.decay_connection_shortcut = None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constructing Hierarchical Residual CNN (Resnet Inspired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, device, input_dim = 1, hidden_dims = [8, 16, 32, 64], output_dim = 10, final_activation=None,\n",
    "                 num_stat=5, num_std=100, decay_rate_std=0.001):\n",
    "        super().__init__()\n",
    "        self.tree = Tree_State()\n",
    "        self.tree.beta_del_neuron = (num_stat-1)/num_stat\n",
    "        self.tree.beta_std_ratio = (num_std-1)/num_std\n",
    "        self.tree.decay_rate_std = decay_rate_std\n",
    "        self.tree.device = device\n",
    "        \n",
    "        self.root_net = None\n",
    "        self._construct_root_net(input_dim, hidden_dims, output_dim)\n",
    "        \n",
    "#         self.tree.DYNAMIC_LIST.add(self.root_net)\n",
    "        self.tree.parent_dict[self.root_net] = None\n",
    "        \n",
    "        if final_activation is None:\n",
    "            final_activation = lambda x: x\n",
    "        self.non_linearity = NonLinearity(\"Root\", output_dim, final_activation)\n",
    "        \n",
    "        self.neurons_added = 0\n",
    "\n",
    "        self._remove_below = None ## temporary variable\n",
    "        \n",
    "    def _construct_root_net(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 8, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 8, 8)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 8, 16, stride=2)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2)\n",
    "        hrn3 = HierarchicalResidual_Conv(self.tree, 32, 32, stride=2)\n",
    "\n",
    "#         actf = lambda x: x\n",
    "        actf = nn.ReLU()\n",
    "    \n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1, actf)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2, actf)\n",
    "        hrnR0123 = HierarchicalResidual_Connector(self.tree, hrnR012, hrn3, actf)\n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 32, 10)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR0123fc = HierarchicalResidual_Connector(self.tree, hrnR0123, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR0123fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "        morphables = [self.root_net, hrnR0123, hrnR012, hrnR01, hrnR0, hrn3, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def _construct_root_net2(self, input_dim, hidden_dims, output_dim):\n",
    "        \n",
    "        hrnR = HierarchicalResidual_Shortcut(self.tree, 3, 16, kernel=(3,3), stride=1)\n",
    "        hrn0 = HierarchicalResidual_Conv(self.tree, 16, 16)\n",
    "        hrn1 = HierarchicalResidual_Conv(self.tree, 16, 32, stride=2)\n",
    "        hrn2 = HierarchicalResidual_Conv(self.tree, 32, 64, stride=2)\n",
    "\n",
    "        actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "    \n",
    "        hrnR0 = HierarchicalResidual_Connector(self.tree, hrnR, hrn0)\n",
    "        hrnR01 = HierarchicalResidual_Connector(self.tree, hrnR0, hrn1, actf)\n",
    "        hrnR012 = HierarchicalResidual_Connector(self.tree, hrnR01, hrn2, actf)\n",
    "        hrnfc = HierarchicalResidual_Shortcut(self.tree, 64, 10)\n",
    "        \n",
    "        def pool_and_reshape(x):\n",
    "            x = F.adaptive_avg_pool2d(x, (1,1))\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            return x\n",
    "        \n",
    "#         actf = lambda x: x\n",
    "#         actf = nn.ReLU()\n",
    "\n",
    "        hrnR012fc = HierarchicalResidual_Connector(self.tree, hrnR012, hrnfc,\n",
    "                                                   activation=actf, post_activation=pool_and_reshape)\n",
    "        self.root_net = hrnR012fc\n",
    "        \n",
    "        ## make every hierarchical Layer Morphable\n",
    "#         morphables = [hrn2, hrn1, hrn0]\n",
    "        morphables = [self.root_net, hrnR012, hrnR01, hrnR0, hrn2, hrn1, hrn0]\n",
    "#         morphables = [self.root_net, hrn0123, hrn012, hrn01]\n",
    "        for hr in morphables:\n",
    "            self.tree.DYNAMIC_LIST.add(hr)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.non_linearity(self.root_net(x))\n",
    "\n",
    "    def add_neurons(self, num):\n",
    "        num_stat = num//2\n",
    "        num_random = num - num_stat\n",
    "        \n",
    "        DL = list(self.tree.DYNAMIC_LIST)\n",
    "        if num_random>0:\n",
    "            rands = torch.randint(high=len(DL), size=(num_random,))\n",
    "            index, count = torch.unique(rands, sorted=False, return_counts=True)\n",
    "            for i, idx in enumerate(index):\n",
    "                DL[idx].add_hidden_neuron(int(count[i]))\n",
    "\n",
    "        if num_stat>0:\n",
    "            del_neurons = []\n",
    "            for hr in DL:\n",
    "                if hr.residual:\n",
    "                    del_neurons.append(hr.residual.del_neurons)#+1e-7)\n",
    "                else:\n",
    "                    del_neurons.append(0.)#1e-7) ## residual layer yet not created \n",
    "            \n",
    "            prob_stat = torch.tensor(del_neurons)\n",
    "            prob_stat = torch.log(torch.exp(prob_stat)+1.)\n",
    "            m = torch.distributions.multinomial.Multinomial(total_count=num_stat,\n",
    "                                                            probs= prob_stat)\n",
    "            count = m.sample()#.type(torch.long)\n",
    "            for i, hr in enumerate(DL):\n",
    "                if count[i] < 1: continue\n",
    "                hr.add_hidden_neuron(int(count[i]))\n",
    "        \n",
    "        self.neurons_added += num \n",
    "        pass\n",
    "\n",
    "    def identify_removable_neurons(self, num=None, threshold_min=0., threshold_max=1.):\n",
    "        \n",
    "        all_sig = []\n",
    "        self.all_sig_ = []\n",
    "        \n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                all_sig.append(hr.residual.significance)\n",
    "                \n",
    "        all_sigs = torch.cat(all_sig)\n",
    "        del all_sig\n",
    "        \n",
    "#         print(\"All_sigs\", all_sigs)\n",
    "        \n",
    "#         print(\"Normalization\", (all_sigs/all_sigs.sum()).sum())\n",
    "        \n",
    "        ### Normalizes such that importance 1 is average importance\n",
    "        normalizer = float(torch.sum(all_sigs))/len(all_sigs)\n",
    "        all_sig = all_sigs/normalizer\n",
    "\n",
    "        ### Normalizes to range [0, 1]\n",
    "#         max_sig = all_sigs.max()\n",
    "#         all_sig = all_sigs/(max_sig+1e-9)\n",
    "#         print(\"All_sig\", all_sig)\n",
    "#         print(\"Sig sum\", all_sig.sum())\n",
    "        print(f\"Significance Stat:\\nMin, Max: {float(all_sig.min()), float(all_sig.max())}\")\n",
    "        print(f\"Mean, Std: {float(all_sig.mean()), float(all_sig.std())}\")\n",
    "        all_sig = all_sig[all_sig<threshold_max]\n",
    "        if len(all_sig)<1: ## if all significance is above threshold max \n",
    "            return 0, None, all_sigs\n",
    "        all_sig = torch.sort(all_sig)[0] ### sorted significance scores\n",
    "        \n",
    "        self.all_sig_ = all_sig\n",
    "        \n",
    "        if not num:num = int(np.ceil(self.neurons_added/self.tree.add_to_remove_ratio))\n",
    "        ## reset the neurons_added number if decay is started\n",
    "\n",
    "        remove_below = threshold_min\n",
    "        if num>len(all_sig):\n",
    "            remove_below = float(all_sig[-1])\n",
    "        elif num>0:\n",
    "            remove_below = float(all_sig[num-1])\n",
    "        \n",
    "        ### sig < threshold_min is always removed; whatsoever\n",
    "        if remove_below < threshold_min:\n",
    "            remove_below = threshold_min\n",
    "            \n",
    "        print(\"remove_below\", remove_below)\n",
    "        remove_below *= normalizer\n",
    "#         remove_below *= max_sig\n",
    "#         print(\"remove_below\", remove_below)\n",
    "\n",
    "        self._remove_below = remove_below\n",
    "#         self._remove_above = remove_above*normalizer\n",
    "        self._remove_above = None\n",
    "\n",
    "        return remove_below, all_sigs\n",
    "\n",
    "    def decay_neuron_start(self, decay_steps=1000):\n",
    "        if self._remove_below is None: return 0\n",
    "        \n",
    "        self.neurons_added = 0 ## resetting this variable\n",
    "        \n",
    "        self.tree.total_decay_steps = decay_steps\n",
    "        self.tree.current_decay_step = 0\n",
    "        self.tree.remove_neuron_residual = set()\n",
    "        self.tree.freeze_connection_shortcut = set()\n",
    "        self.tree.decay_connection_shortcut = set()\n",
    "        \n",
    "        count_remove = 0\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                ### always prune 1 % of the neurons randomly. It might overlap with less significant neurons\n",
    "                mask = torch.bernoulli(torch.ones_like(hr.residual.significance)*0.05).type(torch.bool)\n",
    "                count_remove += hr.residual.identify_removable_neurons(below=self._remove_below,\n",
    "                                                                       above=self._remove_above,\n",
    "                                                                       mask = mask\n",
    "                                                                      )\n",
    "        if count_remove<1:\n",
    "            self.tree.clear_decay_variables()\n",
    "        return count_remove\n",
    "    \n",
    "    def decay_neuron_step(self):\n",
    "        if self.tree.total_decay_steps is None:\n",
    "            return\n",
    "        \n",
    "        self.tree.current_decay_step += 1\n",
    "        \n",
    "        if self.tree.current_decay_step < self.tree.total_decay_steps:\n",
    "            self.tree.get_decay_factor()\n",
    "            for sh in self.tree.decay_connection_shortcut:\n",
    "                sh.decay_connection_step()\n",
    "            for sh in self.tree.freeze_connection_shortcut:\n",
    "                sh.freeze_connection_step()\n",
    "        else:\n",
    "            for rs in self.tree.remove_neuron_residual:\n",
    "                rs.remove_decayed_neurons()\n",
    "            \n",
    "            self.tree.clear_decay_variables()\n",
    "            \n",
    "    def compute_del_neurons(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.compute_del_neurons()\n",
    "    \n",
    "    def maintain_network(self):\n",
    "        self.root_net.maintain_shortcut_connection()\n",
    "        self.root_net.morph_network()\n",
    "        \n",
    "    def start_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.start_computing_significance()\n",
    "\n",
    "    def finish_computing_significance(self):\n",
    "        for hr in self.tree.DYNAMIC_LIST:\n",
    "            if hr.residual:\n",
    "                hr.residual.finish_computing_significance()\n",
    "            \n",
    "    def print_network_debug(self):\n",
    "        self.root_net.print_network_debug(0)\n",
    "        \n",
    "    def print_network(self):\n",
    "        print(self.root_net.input_dim)\n",
    "        self.root_net.print_network()\n",
    "        print(\"│\")\n",
    "        print(self.root_net.output_dim)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(1, 0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bernoulli(torch.ones(10)*0.01).type(torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dycnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = transforms.Compose([\n",
    "    transforms.RandomCrop(size=32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "cifar_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465], # mean=[0.5071, 0.4865, 0.4409] for cifar100\n",
    "        std=[0.2023, 0.1994, 0.2010], # std=[0.2009, 0.1984, 0.2023] for cifar100\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"../../_Datasets/cifar10/\", train=True, download=True, transform=cifar_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"../../_Datasets/cifar10/\", train=False, download=True, transform=cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "# learning_rate = 0.00003\n",
    "\n",
    "dynet = Dynamic_CNN(device).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563, 313)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameters\n",
    "num_add_neuron = 50#25#10\n",
    "num_decay_steps = int(len(train_loader)*3)#3\n",
    "\n",
    "remove_above = 10\n",
    "threshold_max = 1\n",
    "threshold_min = 0.001\n",
    "\n",
    "train_epoch_min = 1 #1\n",
    "train_epoch_max = 6 #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.add_to_remove_ratio = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4689"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decay_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoTrainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_func = None\n",
    "        self.adding_func = None\n",
    "        self.pruning_func = None\n",
    "        self.maintainance_func = None\n",
    "        self.extra_func = None\n",
    "        \n",
    "        self.log_func = None\n",
    "        \n",
    "    def loop(self, count = 15):\n",
    "        cb = count\n",
    "        for i in range(count):\n",
    "            if i>-0.1:\n",
    "                self.adding_func()\n",
    "            else:\n",
    "                global optimizer, warmup\n",
    "                dynet.print_network()    \n",
    "#                 optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "                optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "                warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader))\n",
    "            \n",
    "            self.training_func()\n",
    "            \n",
    "            if i>-0.1:\n",
    "                self.pruning_func()\n",
    "            \n",
    "            self.maintainance_func()\n",
    "            \n",
    "            self.log_func(i)\n",
    "            \n",
    "            if self.extra_func:\n",
    "                self.extra_func()\n",
    "            \n",
    "            print(f\"=====================\")\n",
    "            print(f\"===LOOPS FINISHED :{i} ===\")\n",
    "            print(f\"Pausing for 2 second to give user time to STOP PROCESS\")\n",
    "            time.sleep(2)\n",
    "        self.training_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when to stop training functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coeff(num_iter, coeff0, coeff1, coeff2, coeff_opt, loss_list):\n",
    "    if len(loss_list)<10: return np.array([0]), np.array([0]), float(coeff0.data.cpu()[0])\n",
    "    \n",
    "    _t = torch.tensor(loss_list)\n",
    "    _t = (_t - _t[-1])/(_t[0]-_t.min()) ## normalize to make first point at 1 and last at 0 \n",
    "    _t = torch.clamp(_t, -1.1, 1.1)\n",
    "    _x = torch.linspace(0, 1, steps=len(_t))\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        coeff_opt.zero_grad()\n",
    "        _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "        _loss = ((_y - _t)**2).mean()\n",
    "        _loss.backward()\n",
    "        coeff_opt.step()\n",
    "\n",
    "        coeff0.data = torch.clamp(coeff0.data, -20., 20.)\n",
    "        coeff1.data = torch.clamp(coeff1.data, 0.7, 2.)\n",
    "        coeff2.data = torch.clamp(coeff2.data, -0.2,0.1)\n",
    "        \n",
    "    if torch.isnan(coeff0.data[0]):\n",
    "        coeff0.data[0] = 0.\n",
    "        coeff1.data[0] = 0.\n",
    "        coeff2.data[0] = 1. ## this gives signal\n",
    "        \n",
    "    _y = torch.exp(coeff0*_x)*(1-_x)*coeff1 + coeff2\n",
    "\n",
    "    return _x.numpy(), _t.numpy(), _y.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global variables\n",
    "optimizer = None\n",
    "warmup = None\n",
    "coeff_opt = None\n",
    "\n",
    "loss_all = []\n",
    "accs_all = []\n",
    "accs_test = []\n",
    "\n",
    "## for adam optimizer = \n",
    "# learning_rate *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupLR_Polynomial():\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_epoch, num_batch_in_epoch, power=5):\n",
    "        self.warmup_epoch = warmup_epoch\n",
    "        self.optimizer = optimizer\n",
    "        self.num_batch = num_batch_in_epoch\n",
    "        self.steps = 0\n",
    "        self.power = power\n",
    "        \n",
    "    def step(self):\n",
    "        steps = self.steps/self.num_batch\n",
    "        self.steps += 1\n",
    "        \n",
    "        factor = 1\n",
    "        warming = False\n",
    "        if steps<self.warmup_epoch:\n",
    "            factor = (steps/self.warmup_epoch)**self.power\n",
    "            warming = True\n",
    "            \n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] *= factor\n",
    "        \n",
    "        return warming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_neurons_func():\n",
    "    global optimizer, warmup\n",
    "    dynet.add_neurons(num_add_neuron)\n",
    "    dynet.print_network()    \n",
    "#     optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader), power=1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_network_func():\n",
    "    global optimizer, warmup, loss_all, accs_all\n",
    "    \n",
    "    coeff0 = torch.zeros(1, requires_grad=True)\n",
    "    coeff1 = torch.zeros(1, requires_grad=True)\n",
    "    coeff2 = torch.zeros(1, requires_grad=True)\n",
    "    coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "    loss_list = []\n",
    "    prev_loss = None\n",
    "    beta_loss = (1000-1)/1000\n",
    "    loss_ = []\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    breakall=False\n",
    "    \n",
    "\n",
    "    steps_ = -1\n",
    "    for epoch in range(train_epoch_max):\n",
    "        \n",
    "        train_acc = 0\n",
    "        train_count = 0\n",
    "        for train_x, train_y in train_loader:\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "            steps_ += 1\n",
    "            \n",
    "            dynet.decay_neuron_step()\n",
    "            dynet.tree.std_loss = 0.    \n",
    "\n",
    "            yout = dynet(train_x)\n",
    "            loss = criterion(yout, train_y) + dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "            \n",
    "            if steps_>100:\n",
    "                prev_loss = (1-beta_loss)*float(loss)+beta_loss*prev_loss\n",
    "                loss_list.append(prev_loss)\n",
    "            elif steps_ == 100:\n",
    "                loss_.append(float(loss))\n",
    "                prev_loss = np.mean(loss_)\n",
    "                loss_ = []\n",
    "            else:\n",
    "                loss_.append(float(loss))\n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=False)\n",
    "            warmup.step()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "            targets = train_y.data.cpu().numpy()\n",
    "\n",
    "            correct = (outputs == targets).sum()\n",
    "            train_acc += correct\n",
    "            train_count += len(outputs)\n",
    "\n",
    "            if steps_%100 == 0 and steps_>0:\n",
    "                if len(loss_list)>0:\n",
    "                    max_indx = np.argmax(loss_list)\n",
    "                    loss_list = loss_list[max_indx:]\n",
    "    #                 loss_all.append(float(loss))\n",
    "                \n",
    "                _x, _t, _y = update_coeff(50, coeff0, coeff1, coeff2, coeff_opt, loss_list)\n",
    "                _c = float(coeff0.data.cpu()[0])\n",
    "    #             if coeff2.data[0] > 0.5: ## this is a signal to reset optimizer\n",
    "                coeff_opt = torch.optim.Adam([coeff0, coeff1, coeff2], lr=0.8)\n",
    "                _info = f'ES: {epoch}:{steps_}, coeff:{_c:.3f}/{-5}, \\nLoss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "\n",
    "                ax.clear()\n",
    "                if len(_x)>0:\n",
    "                    ax.plot(_x, _t, c='c')\n",
    "                    ax.plot(_x, _y, c='m')\n",
    "                xmin, xmax = ax.get_xlim()\n",
    "                ymin, ymax = ax.get_ylim()\n",
    "                ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                ax2.clear()\n",
    "                if len(accs_all)>0:\n",
    "                    acc_tr = accs_all\n",
    "                    acc_te = accs_test\n",
    "                    if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                    if len(acc_te)>20: acc_te = acc_te[-20:]\n",
    "                    ax2.plot(acc_tr, label=\"train\")\n",
    "                    ax2.plot(acc_te, label=\"test\")\n",
    "                    ax2.legend(loc=\"lower right\")\n",
    "                    \n",
    "                    ymin, ymax = ax2.get_ylim()\n",
    "                    ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                    ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_te):.3f} end{acc_te[-1]:.3f}\")\n",
    "\n",
    "                \n",
    "                fig.canvas.draw()\n",
    "                plt.savefig(\"./files/_temp_train_plot.png\")\n",
    "\n",
    "                if _c < -5 and epoch>train_epoch_min: \n",
    "                    breakall=True\n",
    "                    break\n",
    "                    \n",
    "        if not breakall:\n",
    "            accs_all.append(train_acc/train_count*100.)\n",
    "            with torch.no_grad():\n",
    "                corrects = 0\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x  = test_x.to(device)\n",
    "                    yout = dynet.forward(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                    correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                    corrects += correct\n",
    "                accs_test.append(corrects/len(test_dataset)*100)\n",
    "    plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_func():\n",
    "    global optimizer, warmup\n",
    "#     optimizer = torch.optim.Adam(dynet.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.SGD(dynet.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    warmup = WarmupLR_Polynomial(optimizer, 0, len(train_loader), power=0.5)\n",
    "    \n",
    "    \n",
    "    dynet.start_computing_significance()\n",
    "\n",
    "    for train_x, train_y in train_loader:\n",
    "        train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "        dynet.tree.std_loss = 0.    \n",
    "        yout = dynet(train_x)\n",
    "#         yout.backward(gradient=torch.ones_like(yout))\n",
    "        loss = criterion(yout, train_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=False)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    dynet.finish_computing_significance()\n",
    "    dynet.identify_removable_neurons(num=None,\n",
    "                                 threshold_min = threshold_min,\n",
    "                                 threshold_max = threshold_max)\n",
    "    num_remove = dynet.decay_neuron_start(decay_steps=num_decay_steps)\n",
    "    if num_remove > 0:\n",
    "        print(f\"pruning {num_remove} neurons.\")\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,4))\n",
    "        ax = fig.add_subplot(121)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        \n",
    "        loss_list = []\n",
    "        steps_ = -1\n",
    "        breakall=False\n",
    "        for epoch in range(train_epoch_max):\n",
    "            loss_ = []\n",
    "            train_acc = 0\n",
    "            train_count = 0\n",
    "            for train_x, train_y in train_loader:\n",
    "                train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "                steps_ += 1\n",
    "\n",
    "                dynet.decay_neuron_step()\n",
    "                dynet.tree.std_loss = 0.    \n",
    "\n",
    "                yout = dynet(train_x)\n",
    "                loss = criterion(yout, train_y) + dynet.tree.decay_rate_std*dynet.tree.std_loss\n",
    "\n",
    "                loss_.append(float(loss))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=False)\n",
    "                warmup.step()\n",
    "                optimizer.step()\n",
    "\n",
    "                outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                targets = train_y.data.cpu().numpy()\n",
    "                correct = (outputs == targets).sum()\n",
    "                train_acc += correct\n",
    "                train_count += len(outputs)\n",
    "\n",
    "                dynet.decay_neuron_step()\n",
    "                \n",
    "                if steps_%50 == 0 and steps_>0:\n",
    "                    loss = np.mean(loss_)\n",
    "                    loss_ = []\n",
    "                    loss_list.append(loss)\n",
    "                \n",
    "                if steps_%100 == 0 and steps_>0:\n",
    "                    \n",
    "                    _info = f'ES: {epoch}:{steps_}, Loss:{float(loss):.3f}, Acc:{correct/len(outputs)*100:.3f}%'\n",
    "#                     print(_info)\n",
    "                    ax.clear()\n",
    "                    out = (yout.data.cpu().numpy()>0.5).astype(int)\n",
    "                    ax.plot(loss_list)\n",
    "                    \n",
    "                    xmin, xmax = ax.get_xlim()\n",
    "                    ymin, ymax = ax.get_ylim()\n",
    "                    ax.text(xmin, ymin, _info)\n",
    "                    \n",
    "                    ax2.clear()\n",
    "                    if len(accs_all)>0:\n",
    "                        acc_tr = accs_all\n",
    "                        acc_te = accs_test\n",
    "                        if len(acc_tr)>20: acc_tr = acc_tr[-20:]\n",
    "                        if len(acc_te)>20: acc_te = acc_te[-20:]\n",
    "                        ax2.plot(acc_tr, label=\"train\")\n",
    "                        ax2.plot(acc_te, label=\"test\")\n",
    "                        ax2.legend(loc=\"lower right\")\n",
    "\n",
    "                        ymin, ymax = ax2.get_ylim()\n",
    "                        ax2.text(0, 0.1*ymin+0.9*ymax, f\"TR:max{max(acc_tr):.3f} end{acc_tr[-1]:.3f}\")\n",
    "                        ax2.text(0, 0.2*ymin+0.8*ymax, f\"TE:max{max(acc_te):.3f} end{acc_te[-1]:.3f}\")\n",
    "\n",
    "                    \n",
    "                    fig.canvas.draw()\n",
    "                    plt.savefig(\"./files/_temp_prune_plot.png\")\n",
    "#                     plt.pause(0.01)\n",
    "#                     print(\"\\n\")\n",
    "                    \n",
    "                if steps_>num_decay_steps+int(num_decay_steps/2): breakall=True\n",
    "                if breakall: break\n",
    "            if breakall: break\n",
    "                \n",
    "        if not breakall:\n",
    "            accs_all.append(train_acc/train_count*100.)\n",
    "            with torch.no_grad():\n",
    "                corrects = 0\n",
    "                for test_x, test_y in test_loader:\n",
    "                    test_x  = test_x.to(device)\n",
    "                    yout = dynet.forward(test_x)\n",
    "                    outputs = tnn.Logits.logit_to_index(yout.data.cpu().numpy())\n",
    "                    correct = (outputs == test_y.data.cpu().numpy()).sum()\n",
    "                    corrects += correct\n",
    "                accs_test.append(corrects/len(test_dataset)*100)\n",
    "        plt.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maintain_network():\n",
    "    dynet.compute_del_neurons()\n",
    "    dynet.maintain_network()\n",
    "    dynet.print_network()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network_stat(loop_indx):\n",
    "    stdout = sys.stdout\n",
    "    s = io.StringIO(newline=\"\")\n",
    "    sys.stdout = s\n",
    "    dynet.print_network()\n",
    "    sys.stdout = stdout\n",
    "    s.seek(0)\n",
    "    # prints = s.read()\n",
    "    architecture = s.getvalue()\n",
    "    s.close()\n",
    "    \n",
    "    ### number of neurons\n",
    "    count = 0\n",
    "    for hr in dynet.tree.DYNAMIC_LIST:\n",
    "        if hr.residual:\n",
    "            count += hr.residual.hidden_dim\n",
    "    \n",
    "    with open(\"05_dynamic_CNN_log_7.txt\", \"a+\") as f:\n",
    "        if loop_indx == 0:\n",
    "            ### Print the configuration at top.\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "            \n",
    "            from datetime import datetime\n",
    "            now = datetime.now()\n",
    "            dt_string = now.strftime(\"%B %d, %Y @ %H:%M:%S\")\n",
    "            f.write(f\"DateTime: {dt_string}\")\n",
    "            \n",
    "            f.write(f\"num_add_neuron :{num_add_neuron}\\n add_to_remove_ratio :{dynet.tree.add_to_remove_ratio}\\n\")\n",
    "            f.write(f\"learning_rate :{learning_rate}\\n num_decay_steps :{num_decay_steps}\\n\")\n",
    "            f.write(f\"threshold_max :{threshold_max}\\n threshold_min :{threshold_min}\\n\")\n",
    "            f.write(f\"train_epoch_min :{train_epoch_min}\\n threshold_max :{train_epoch_max}\\n\")\n",
    "            f.write(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")\n",
    "        \n",
    "        f.write(f\"####################| Loop:{loop_indx} | Epoch: {len(accs_all)} \\n\")\n",
    "        num_params = sum(p.numel() for p in dynet.parameters())\n",
    "        num_trainable = sum(p.numel() for p in dynet.parameters() if p.requires_grad)\n",
    "        f.write(f\"| Dynamic Neurons:{count} | Total Parameters: {num_params} | Trainable Parameters: {num_trainable}\\n\")\n",
    "        f.write(f\"| Train Acc:{accs_all[-1]:.3f} | Test Acc: {accs_test[-1]:.3f}\\n\")\n",
    "        f.write(architecture)\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set all functions and begin automated loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AutoTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.adding_func = add_neurons_func\n",
    "trainer.training_func = training_network_func\n",
    "trainer.pruning_func = pruning_func\n",
    "trainer.maintainance_func = maintain_network\n",
    "trainer.log_func = save_network_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_neurons_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     8\n",
      "    ╔╝\n",
      "    8\n",
      "   ╔╝\n",
      "   16\n",
      "  ╔╝\n",
      "  32\n",
      " ╔╝\n",
      " 32\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, p in enumerate(list(dynet.parameters())):\n",
    "#     print(i, p.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = list(dynet.parameters())[3]\n",
    "# print(p.shape)\n",
    "# p[8:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = list(dynet.parameters())[4]\n",
    "# print(p.shape)\n",
    "# p[0][8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = list(dynet.parameters())[7]\n",
    "# print(p.shape)\n",
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.add_hidden_neuron(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.residual.fc0.shortcut.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.residual.fc1.shortcut.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.residual.fc0.shortcut.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.residual.fc1.shortcut.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.add_hidden_neuron(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.residual.fc0.shortcut.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.residual.fc0.shortcut.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc0.residual.fc1.shortcut.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc0.residual.fc1.shortcut.weight[32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.shortcut.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynet.root_net.residual.fc0.residual.fc1.shortcut.weight[:, 32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOG\n",
    "# adding neuron on the hr_conv layer is okay.\n",
    "# adding new neuron on a hr_connector layer, makes outgoing weights zero; and incoming weights non-zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tx, ty = iter(train_loader).next()\n",
    "# tx, ty = tx.to(device), ty.to(device)\n",
    "# yy = dynet(tx)\n",
    "# criterion(yy, ty).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    8\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    11\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   22\n",
      "   ╠════╗\n",
      "   ║    6\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  37\n",
      "  ╠════╗\n",
      "  ║    6\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 37\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.17728270590305328, 4.0797600746154785)\n",
      "Mean, Std: (1.0, 0.632587730884552)\n",
      "remove_below 0.4829329550266266\n",
      "Significance:\n",
      "tensor([ 898.0045, 2072.7407, 2934.4043, 1729.2085, 2674.5906, 1982.6914],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1290.5321,  993.4836, 2898.4451,  709.8898, 1962.1218, 1389.1333,\n",
      "        1964.4255], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 380.3583,  362.3417,  511.8503,  711.2408,  749.9832,  408.8514,\n",
      "        1246.7322,  498.2433], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True,  True,  True, False, False,  True, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1683.6466, 2308.0210, 3011.7043, 2480.4141, 1673.0116, 2328.3342],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True,  True], device='cuda:0')\n",
      "pruning 43 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     3\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    11\n",
      "    ╠════╗\n",
      "    ║    7\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   19\n",
      "   ╠════╗\n",
      "   ║    6\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  33\n",
      "  ╠════╗\n",
      "  ║    4\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 17\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :0 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     10\n",
      "     ╠════╗\n",
      "     ║    6\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    14\n",
      "    ╠════╗\n",
      "    ║    19\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   23\n",
      "   ╠════╗\n",
      "   ║    15\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  41\n",
      "  ╠════╗\n",
      "  ║    8\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 17\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0, 3.048577308654785)\n",
      "Mean, Std: (1.0, 0.5648202896118164)\n",
      "remove_below 0.6021400094032288\n",
      "Significance:\n",
      "tensor([ 817.1741, 1833.5781, 3267.0542, 1868.8225, 1101.5653, 1722.5977,\n",
      "         832.1143,  836.8541, 1312.8260,  994.4023,  954.7563,  398.9840,\n",
      "         500.5301,  760.1508,  942.2321], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False,  True,  True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 812.8997,  276.3831, 1375.2360,  824.9877,  836.5590,  390.0852,\n",
      "         801.9414,  430.0223,  753.0369,  718.4088,  389.3776,  763.4777,\n",
      "        1345.2190,  630.5540,  379.7127, 1315.4418, 1333.4362, 1108.6669,\n",
      "         218.3485], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False,  True, False,  True, False, False,\n",
      "         True, False, False,  True,  True, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([   0.0000,    0.0000,    0.0000,  355.3613,  815.6992, 1444.6908],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True,  True,  True,  True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([2543.1001, 2535.0010, 3369.0139, 2427.7861, 2399.1880,  696.0203,\n",
      "        1438.0330,  462.7264], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False,  True],\n",
      "       device='cuda:0')\n",
      "pruning 42 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     5\n",
      "     ╠════╗\n",
      "     ║    2\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    6\n",
      "    ╠════╗\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   19\n",
      "   ╠════╗\n",
      "   ║    12\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  34\n",
      "  ╠════╗\n",
      "  ║    6\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 15\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :1 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     5\n",
      "     ╠════╗\n",
      "     ║    6\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    9\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    11\n",
      "    ║    ╠════╝\n",
      "    ║    19\n",
      "    ║    ╠════╗\n",
      "    ║    ║    3\n",
      "    ║    ╠════╝\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   27\n",
      "   ╠════╗\n",
      "   ║    18\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  38\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 16\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.024352198466658592, 2.924123764038086)\n",
      "Mean, Std: (1.0, 0.6248453259468079)\n",
      "remove_below 0.3823193609714508\n",
      "Significance:\n",
      "tensor([ 696.1313, 1772.3239, 2698.7642, 1447.1895,  869.8157, 1068.7300,\n",
      "         911.4336,  776.8536, 1324.5549,  943.2979,  731.7587, 1329.9952,\n",
      "         269.4884,  316.3798,  849.4632,  422.7477,  564.3968,  433.7671],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True, False,  True, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([339.2242, 383.4949, 291.8705], device='cuda:0')\n",
      "Prune:\n",
      "tensor([True, True, True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 571.3657, 1318.4161,  816.2036,  897.2897,  653.2733,  772.4572,\n",
      "         770.2641, 1042.6178, 1781.0704, 1157.4489, 1333.3452, 1151.6747,\n",
      "         267.1854,  374.5277,  462.9829,  272.3737,  372.5574,  326.8368,\n",
      "         347.7688], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True, False,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([662.8244, 974.9036, 364.4622, 388.0502, 300.9094, 199.6427],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([334.2647, 256.1010, 414.7150, 268.9792, 437.9789, 316.5119, 221.9438,\n",
      "        196.7856, 255.0935, 230.9880, 383.1648], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "         True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([2369.2256, 2314.5295, 3349.8293, 2609.0369, 2945.2747, 1654.9338,\n",
      "         534.6340, 1020.1339,  566.1270], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "pruning 37 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     3\n",
      "     ╠════╗\n",
      "     ║    2\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    8\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    1\n",
      "    ║    ╠════╝\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   25\n",
      "   ╠════╗\n",
      "   ║    14\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  36\n",
      "  ╠════╗\n",
      "  ║    9\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 14\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :2 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     7\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    12\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    4\n",
      "    ║    ╠════╝\n",
      "    ║    18\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   34\n",
      "   ╠════╗\n",
      "   ║    23\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  42\n",
      "  ╠════╗\n",
      "  ║    15\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 16\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.0185698252171278, 3.3930130004882812)\n",
      "Mean, Std: (1.0, 0.7519991993904114)\n",
      "remove_below 0.30358996987342834\n",
      "Significance:\n",
      "tensor([ 613.7237, 1237.3594, 3364.4116, 1562.6116, 1019.5898, 1079.4172,\n",
      "        1050.4451,  775.4992, 1652.4553,  913.3198,  708.8718, 1043.0243,\n",
      "        1020.2179,  586.4817,  146.3750,  208.6599,  152.5631,  213.8128,\n",
      "         174.1495,  127.8471,  196.1047,  219.2418,  182.3828],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1306.5653,  728.1747,  702.0540,  744.5306,  480.9519,  586.3679,\n",
      "         876.3928, 1525.9312,  870.6986, 1260.7559, 1195.0459,  422.4419,\n",
      "         280.2548,  393.8407,  178.7463,  290.7425,  286.3595,  381.0347],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False,  True, False,  True,  True,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 347.4928, 1123.1890,  406.7620], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([616.4751, 296.3036, 491.8574, 230.3813], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1968.1510, 1981.5135, 3620.6428, 3492.1001, 2695.9324, 2000.6035,\n",
      "         732.1705, 1839.9364, 1488.0353,  148.6641,  332.6518,  353.8037,\n",
      "         109.9627,  327.3910,  241.3596], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False,  True, False,  True], device='cuda:0')\n",
      "pruning 42 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     4\n",
      "     ╠════╗\n",
      "     ║    2\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    11\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    1\n",
      "    ║    ╠════╝\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   25\n",
      "   ╠════╗\n",
      "   ║    13\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  38\n",
      "  ╠════╗\n",
      "  ║    12\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :3 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     9\n",
      "     ╠════╗\n",
      "     ║    2\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    16\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    6\n",
      "    ║    ╠════╝\n",
      "    ║    15\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   31\n",
      "   ╠════╗\n",
      "   ║    24\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  45\n",
      "  ╠════╗\n",
      "  ║    16\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 18\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.008638309314846992, 3.461671829223633)\n",
      "Mean, Std: (1.0, 0.8169987201690674)\n",
      "remove_below 0.22620978951454163\n",
      "Significance:\n",
      "tensor([ 748.1453, 1409.8411, 2439.4749, 1618.7635, 1170.8351, 1178.7262,\n",
      "        1045.6472,  647.7746, 1088.8505,  957.8903, 1259.2872, 1128.2090,\n",
      "         583.5999,  158.2071,  180.7814,  173.8363,  224.3052,  206.0144,\n",
      "         150.9133,  160.4269,  189.4607,  204.2693,  260.0758,  250.2481],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 991.9886,  782.1993,  936.3351, 1047.0070,  612.0202,  635.0401,\n",
      "        1492.7817,  914.8799, 1284.1516, 1144.3641,  531.8806,  338.8566,\n",
      "         490.2596,  166.1589,  143.8226], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 280.4934, 1011.9731], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([757.6241, 232.7451, 272.2838, 152.9265, 190.0595, 220.3734],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([2439.1785, 1726.2499, 3685.1394, 3722.7444, 3092.7295, 2255.3721,\n",
      "         725.3721, 1844.3135, 1909.8876,  712.7315, 1414.0870,  600.8167,\n",
      "         317.2027,   96.7052,  192.3256,  117.5939], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False,  True, False,  True,  True,  True], device='cuda:0')\n",
      "pruning 47 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     7\n",
      "     ╠════╗\n",
      "     ║    2\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    10\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    1\n",
      "    ║    ╠════╝\n",
      "    ║    11\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   24\n",
      "   ╠════╗\n",
      "   ║    15\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  41\n",
      "  ╠════╗\n",
      "  ║    11\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :4 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     13\n",
      "     ╠════╗\n",
      "     ║    7\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    14\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    3\n",
      "    ║    ╠════╝\n",
      "    ║    16\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   27\n",
      "   ╠════╗\n",
      "   ║    23\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  48\n",
      "  ╠════╗\n",
      "  ║    17\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 17\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.00902148149907589, 4.015125751495361)\n",
      "Mean, Std: (1.0, 0.8189231157302856)\n",
      "remove_below 0.22074837982654572\n",
      "Significance:\n",
      "tensor([ 828.3716, 1848.1683, 2492.7644, 1655.2544, 1161.5878, 1194.1305,\n",
      "         862.0690,  758.5784, 1056.7717,  809.6968, 1460.1261, 1264.6060,\n",
      "         662.6800,  486.9928,  342.3105,  179.0278,   96.4885,  141.8468,\n",
      "         166.0930,  284.5920,  296.4919,  207.4317,  128.6293],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False,  True,  True,  True,  True, False,\n",
      "        False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1136.7017,  894.7581,  733.0323,  764.5341, 1754.5375,  831.0936,\n",
      "        1161.4521, 1248.0042,  607.1046,  508.5555,  513.7513,  184.4636,\n",
      "         172.6395,  289.2598,  155.5301,  175.4826], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True,  True, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([196.0467, 957.7581, 182.2316, 119.6488, 212.2915, 181.6430, 196.6751],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False,  True,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([961.0831, 225.6528, 365.3135], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([2432.3918, 1626.0656, 4213.0581, 3288.9319, 2489.1238,  908.4357,\n",
      "        1853.6819, 2613.7263, 1290.8403, 1345.7360,  668.4139,  374.2321,\n",
      "         134.1681,  186.9895,  187.2699,  186.9857,  311.4081],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False,  True,  True,  True,  True, False], device='cuda:0')\n",
      "pruning 38 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     7\n",
      "     ╠════╗\n",
      "     ║    1\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    13\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    2\n",
      "    ║    ╠════╝\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   26\n",
      "   ╠════╗\n",
      "   ║    16\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  45\n",
      "  ╠════╗\n",
      "  ║    12\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :5 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    2\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    21\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    7\n",
      "    ║    ╠════╝\n",
      "    ║    18\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   31\n",
      "   ╠════╗\n",
      "   ║    20\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  53\n",
      "  ╠════╗\n",
      "  ║    18\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 16\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.00809790100902319, 4.3073906898498535)\n",
      "Mean, Std: (0.9999998807907104, 0.8752721548080444)\n",
      "remove_below 0.16655069589614868\n",
      "Significance:\n",
      "tensor([ 741.9041, 1643.3591, 2180.0820, 1664.0283, 1292.6836, 1300.1763,\n",
      "        1022.5442,  615.2161,  987.6257,  771.0814, 1132.3275,  644.6003,\n",
      "         670.3990,  622.8505,  606.8550,  381.6176,  183.9434,  124.0682,\n",
      "         141.8304,  164.8406], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1135.3959,  707.9219,  690.4526,  669.7335, 1775.8551,  939.3806,\n",
      "        1368.5219, 1143.7164,  575.9063,  581.1853,  480.3658,  303.3530,\n",
      "         125.4755,  112.3349,  128.0098,   87.8932,  104.0629,  135.7394],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([990.1594, 190.5852], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([931.4244, 511.3927,  82.6619, 188.7940, 117.0856, 174.6018, 173.0175],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False,  True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1966.2701, 1463.9125, 3727.2383, 4263.1646, 2847.0078,  947.1638,\n",
      "        1894.5082, 2487.1055, 1332.2841, 1174.3611,  704.7670,  527.7047,\n",
      "          49.7161,   92.9278,  159.8770,  226.0582,  117.4486,  141.4176],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False,  True,  True,  True, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "pruning 42 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     9\n",
      "     ╠════╗\n",
      "     ║    2\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    12\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    11\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   29\n",
      "   ╠════╗\n",
      "   ║    16\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  46\n",
      "  ╠════╗\n",
      "  ║    12\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :6 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    5\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    20\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    7\n",
      "    ║    ╠════╝\n",
      "    ║    20\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   32\n",
      "   ╠════╗\n",
      "   ║    18\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  57\n",
      "  ╠════╗\n",
      "  ║    18\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 16\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.009334301576018333, 3.882026433944702)\n",
      "Mean, Std: (1.0, 0.8782240152359009)\n",
      "remove_below 0.15716005861759186\n",
      "Significance:\n",
      "tensor([ 865.3012, 2033.8370, 1400.1626, 1223.8674, 1332.7015, 1006.9437,\n",
      "         721.7775,  980.5949,  770.7690, 1352.1111,  737.0886,  738.2188,\n",
      "         806.0518,  825.4750,  402.9032,  326.6036,   95.5956,   96.2774],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1217.4955,  729.8453,  843.2010, 1711.0054,  914.7177, 1180.6626,\n",
      "        1070.3218,  549.3522,  802.5867,  646.2640,  521.0829,  140.9322,\n",
      "         136.0925,  108.5315,  138.4059,  171.4047,  111.7592,   99.8312,\n",
      "         142.1471,  236.3086], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False,  True,  True,  True,  True, False,  True,  True,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1035.9332,  244.2937,  166.5912,  153.1187,   99.1547],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([898.7126, 725.3660, 243.6674, 319.7493, 213.0830, 125.5137,  67.8271],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1867.6245, 1551.3170, 3619.8267, 3744.3545, 2814.2593, 1082.7002,\n",
      "        2133.2869, 2883.1731, 1406.3571, 1696.8231,  881.3008,  526.6513,\n",
      "         130.9789,   94.1636,  179.4797,   99.0266,  108.0330,  156.7850],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True,  True,  True,  True, False],\n",
      "       device='cuda:0')\n",
      "pruning 43 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     10\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    13\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   28\n",
      "   ╠════╗\n",
      "   ║    15\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  50\n",
      "  ╠════╗\n",
      "  ║    13\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :7 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     17\n",
      "     ╠════╗\n",
      "     ║    8\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    10\n",
      "    ║    ╠════╝\n",
      "    ║    15\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   29\n",
      "   ╠════╗\n",
      "   ║    17\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  54\n",
      "  ╠════╗\n",
      "  ║    21\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 19\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.009417058899998665, 4.29927396774292)\n",
      "Mean, Std: (1.0, 0.9161144495010376)\n",
      "remove_below 0.12376765161752701\n",
      "Significance:\n",
      "tensor([ 697.0773, 2297.2673, 1593.3618, 1256.1357, 1372.7761, 1160.8372,\n",
      "         568.6141,  966.0280,  804.9869, 1398.6417,  652.2087, 1094.3069,\n",
      "         838.2501,  647.5839,  321.1261,  160.5830,  144.7468],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1292.6566,  787.4716,  826.0178, 1450.9531,  906.6274,  952.9746,\n",
      "        1018.8051,  934.2100,  638.6395,  515.6325,  395.2390,  369.9983,\n",
      "         180.4769,  106.8935,  125.4925], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([900.7426, 314.9414, 223.6245,  98.6344,  89.9439, 108.0093,  74.8605,\n",
      "         84.8153], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([969.5932, 648.3671, 367.2842, 350.2048, 527.1425,  57.4548,  56.4843,\n",
      "         65.5025, 100.3024,  63.7025], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([2171.8630, 1460.3148, 3869.3879, 4114.1792, 2606.4082, 1304.8929,\n",
      "        2262.1292, 2813.8711, 1329.8209, 1969.7247, 1070.7844,  667.9868,\n",
      "         306.6602,  109.6825,   71.7645,  158.9686,   55.1715,  118.4391,\n",
      "          85.7145,  144.5242,   96.0884], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True, False,  True,  True,  True, False,\n",
      "         True], device='cuda:0')\n",
      "pruning 42 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    17\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   28\n",
      "   ╠════╗\n",
      "   ║    16\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  48\n",
      "  ╠════╗\n",
      "  ║    15\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :8 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     14\n",
      "     ╠════╗\n",
      "     ║    7\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    20\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    11\n",
      "    ║    ╠════╝\n",
      "    ║    20\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   32\n",
      "   ╠════╗\n",
      "   ║    25\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  52\n",
      "  ╠════╗\n",
      "  ║    22\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 17\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.00558417709544301, 4.140828609466553)\n",
      "Mean, Std: (0.9999999403953552, 0.9123390913009644)\n",
      "remove_below 0.13262499868869781\n",
      "Significance:\n",
      "tensor([ 643.3175, 2229.0889, 1458.3195, 1435.1732, 1131.2849,  936.0681,\n",
      "         652.1494,  814.6802,  698.7136, 1255.3102,  581.1409, 1086.1619,\n",
      "         991.9681,  788.4633,  569.0468,  337.2917,  170.9162,   57.5551,\n",
      "         105.2896,   68.8020,  138.3468,  104.2696,  156.5506,  130.3314,\n",
      "          68.5765], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False,\n",
      "        False,  True, False, False, False, False, False,  True,  True,  True,\n",
      "        False,  True, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1262.7568,  651.9615, 1013.8611, 1581.6753,  897.9263,  987.2796,\n",
      "        1265.2826,  771.5546,  696.6613,  591.1388,  487.0457,  288.8581,\n",
      "         249.9987,  159.3234,  101.6674,   98.5565,   79.1045,   98.1766,\n",
      "         107.1839,   94.6904], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([955.8710, 292.9356, 334.2559,  85.7518,  81.6945,  87.2084,  83.0531],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([792.0067, 646.7440, 459.2658, 404.8866, 406.9062,  97.8391,  58.7758,\n",
      "         33.7529,  82.3332, 108.9908,  48.4089], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "         True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1956.8121, 1817.4288, 3684.0190, 3534.5601, 2383.4937,  989.0543,\n",
      "        2366.1462, 2468.5461, 1437.0375, 2185.1465, 1249.7043, 1053.9760,\n",
      "         384.9749,  324.0872,  296.8679,  133.5265,   97.5000,  118.4130,\n",
      "          92.4069,  117.9940,  203.9668,  105.3514], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False,  True, False, False, False,  True, False,  True, False,\n",
      "        False,  True], device='cuda:0')\n",
      "pruning 44 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    17\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   29\n",
      "   ╠════╗\n",
      "   ║    18\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  48\n",
      "  ╠════╗\n",
      "  ║    17\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :9 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     18\n",
      "     ╠════╗\n",
      "     ║    6\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    25\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    9\n",
      "    ║    ╠════╝\n",
      "    ║    20\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   35\n",
      "   ╠════╗\n",
      "   ║    22\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  54\n",
      "  ╠════╗\n",
      "  ║    22\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 15\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.00630120187997818, 4.451115131378174)\n",
      "Mean, Std: (1.0, 0.9436657428741455)\n",
      "remove_below 0.1309826374053955\n",
      "Significance:\n",
      "tensor([ 746.4993, 2451.5217, 1582.3801, 2012.7742, 1214.8898,  950.9582,\n",
      "         830.5200,  699.3372, 1357.0671,  639.6208, 1250.4902,  786.6595,\n",
      "         743.9493,  444.6445,  275.8586,  249.0003,  261.2527,  227.7664,\n",
      "          71.4609,   80.1131,  150.2061,  130.1680], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True,  True,\n",
      "        False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1361.1748,  727.4499,  944.5874, 1469.9797, 1144.4164, 1304.2340,\n",
      "        1167.5424,  814.4523,  709.5856,  742.8152,  514.6514,  345.9674,\n",
      "         278.4548,  249.1031,  138.2189,   71.1021,   97.1153,  143.5512,\n",
      "         109.3472,  101.1856], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True,  True, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([841.9194, 342.2052, 390.0247, 106.9341,  71.8298,  62.4662],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([822.2834, 684.7153, 503.2577, 537.5135, 455.8997,  97.9440,  75.5282,\n",
      "         46.4242,  43.2130], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1933.5905, 1554.8771, 3956.0247, 4096.7739, 3127.0322, 1156.4620,\n",
      "        2536.9937, 2904.6328, 2226.9727, 1465.5436, 1017.6394,  617.4799,\n",
      "         553.4744,  428.2974,  243.4089,  281.9446,  481.7891,   94.1106,\n",
      "         110.1956,  120.5555,  122.0060,   37.7579], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False,  True,  True,  True,\n",
      "        False,  True], device='cuda:0')\n",
      "pruning 43 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     10\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    15\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   31\n",
      "   ╠════╗\n",
      "   ║    20\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  52\n",
      "  ╠════╗\n",
      "  ║    18\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 11\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :10 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     16\n",
      "     ╠════╗\n",
      "     ║    7\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    25\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    9\n",
      "    ║    ╠════╝\n",
      "    ║    21\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   33\n",
      "   ╠════╗\n",
      "   ║    27\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  61\n",
      "  ╠════╗\n",
      "  ║    19\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 15\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.012862149626016617, 4.28962516784668)\n",
      "Mean, Std: (1.0, 0.9485341906547546)\n",
      "remove_below 0.10492093861103058\n",
      "Significance:\n",
      "tensor([ 875.2898, 1958.6147, 1631.1932, 1464.4668, 1168.6444, 1056.7628,\n",
      "         922.4215,  844.1456, 1539.3030,  719.6076,  981.0898,  816.7873,\n",
      "         818.8701,  443.8763,  473.0091,  215.9467,  352.2139,  303.5065,\n",
      "         250.4952,  307.3192,   62.5695,  140.3067,   93.0580,   67.0351,\n",
      "          76.0050,  132.1094,  114.9016], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False,  True, False, False, False, False, False, False,\n",
      "         True, False,  True,  True,  True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1133.6014,  783.1365, 1101.0811, 1471.5137, 1051.0088, 1109.4323,\n",
      "        1173.8184,  923.5275,  646.5618,  853.1426,  547.2592,  361.7073,\n",
      "         297.7804,  353.4988,  160.7487,   71.5082,   48.2854,  108.8916,\n",
      "          86.2135,   63.4995,   69.3166], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "         True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([916.2412, 321.9484, 402.8784,  76.7723,  73.2945,  48.7340,  75.2124],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([823.4134, 701.9843, 593.5953, 510.3965, 472.7253,  42.5933,  75.7884,\n",
      "         51.0098,  48.7280], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([2007.0845, 1478.5807, 3768.3594, 3171.5154, 2872.1985, 1108.9147,\n",
      "        2783.5750, 2909.1829, 2290.8342, 1559.9730, 1194.0757,  777.4613,\n",
      "         685.3380,  792.5509,  422.6452,  562.7277, 1393.7032,  357.6915,\n",
      "         120.9180], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "pruning 46 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    17\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   32\n",
      "   ╠════╗\n",
      "   ║    21\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  55\n",
      "  ╠════╗\n",
      "  ║    19\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 11\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :11 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     16\n",
      "     ╠════╗\n",
      "     ║    10\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    20\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    6\n",
      "    ║    ╠════╝\n",
      "    ║    18\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   38\n",
      "   ╠════╗\n",
      "   ║    26\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  64\n",
      "  ╠════╗\n",
      "  ║    24\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 15\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.005656352266669273, 4.694596767425537)\n",
      "Mean, Std: (1.0000001192092896, 0.9744656085968018)\n",
      "remove_below 0.12937185168266296\n",
      "Significance:\n",
      "tensor([ 672.0234, 1692.7627, 1503.2406, 1701.1927, 1226.4794, 1019.9957,\n",
      "         835.4008,  870.8058, 1137.7792,  570.8507,  791.3997,  753.2341,\n",
      "         554.9210,  383.9170,  619.0195,  449.5045,  286.0958,  385.5111,\n",
      "         165.4128,  185.3199,  212.0459,  112.0095,   85.9036,  162.5189,\n",
      "          98.9515,   66.9470], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 790.5215, 1527.4812, 1119.4344, 1074.0652, 1039.9315,  911.4099,\n",
      "         658.8517, 1028.7522,  487.1140,  452.2464,  348.3507,  436.6772,\n",
      "         269.6214,   90.7366,   65.7010,   85.4761,   72.4622,   73.3595],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([855.3646, 321.9905, 321.4976,  92.0045,  57.1985,  51.0594,  49.1619,\n",
      "         56.6802,  78.1043,  61.5414], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([746.5469, 711.5803, 444.7428, 563.2755, 464.9813,  91.3714],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1864.6001, 1380.2966, 3266.8967, 3974.0791, 2741.4744, 1050.0886,\n",
      "        2690.7627, 2649.8135, 2078.9697, 1677.0579, 1322.3018,  945.9785,\n",
      "         831.8760, 1017.1191,  601.5018,  717.8658, 1531.2289,  482.7266,\n",
      "         222.2325,  120.4416,   83.2545,  139.8062,   58.2411,   82.7483],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False,  True, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False,  True,  True], device='cuda:0')\n",
      "pruning 37 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   32\n",
      "   ╠════╗\n",
      "   ║    23\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  63\n",
      "  ╠════╗\n",
      "  ║    20\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 11\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :12 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    6\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    25\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    10\n",
      "    ║    ╠════╝\n",
      "    ║    17\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   39\n",
      "   ╠════╗\n",
      "   ║    30\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  70\n",
      "  ╠════╗\n",
      "  ║    26\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 12\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.012546311132609844, 4.73205041885376)\n",
      "Mean, Std: (1.0, 0.9745222926139832)\n",
      "remove_below 0.10413166135549545\n",
      "Significance:\n",
      "tensor([ 654.7438, 1599.4681, 1579.0981, 1895.8883, 1097.9674,  958.2072,\n",
      "         820.4698,  761.9340, 1473.8549,  650.9595, 1006.1651, 1047.5763,\n",
      "         632.2491,  404.3083,  518.4943,  424.2011,  314.0775,  565.1163,\n",
      "         254.4457,  292.3591,  256.4493,  122.2819,  164.5404,   66.3431,\n",
      "          81.3256,   78.5139,   77.2326,   91.3500,   79.6315,   90.2023],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 895.1967, 1718.3281,  914.1301, 1299.9042, 1330.8571,  948.8861,\n",
      "         826.8829, 1103.1116,  485.9718,  501.5545,  387.3016,  475.7220,\n",
      "         307.9386,  106.6247,  107.0888,   39.1402,   59.9519],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([925.0014, 288.6850, 319.7041,  54.9252,  56.1176,  53.8469],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([916.3804, 699.7527, 564.8533, 546.9882, 499.8255,  44.9480,  47.6179,\n",
      "         24.8349,  31.9382,  36.1160], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1746.1799, 1642.7030, 3577.1216, 3782.1235, 3586.9370, 1307.2587,\n",
      "        2476.4453, 2587.1016, 2034.4894, 1542.1018, 1174.5867,  984.2099,\n",
      "        1217.3582,  703.7230, 1407.2570, 2070.2407,  798.0863,  440.3514,\n",
      "         408.8619, 1017.2849,   63.1707,   84.4880,   46.9098,  124.3842,\n",
      "          61.5463,  128.2729], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True,  True, False,  True, False], device='cuda:0')\n",
      "pruning 42 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    20\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   35\n",
      "   ╠════╗\n",
      "   ║    23\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  64\n",
      "  ╠════╗\n",
      "  ║    22\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 10\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :13 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     14\n",
      "     ╠════╗\n",
      "     ║    7\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    25\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    9\n",
      "    ║    ╠════╝\n",
      "    ║    23\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   40\n",
      "   ╠════╗\n",
      "   ║    27\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  69\n",
      "  ╠════╗\n",
      "  ║    29\n",
      "  ║    ╠════╗\n",
      "  ║    ║    5\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 10\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.040508802980184555, 5.007160663604736)\n",
      "Mean, Std: (1.0, 0.9793387651443481)\n",
      "remove_below 0.11281215399503708\n",
      "Significance:\n",
      "tensor([ 624.5986, 1690.1794, 1457.9795, 1399.4822,  938.0096, 1001.0503,\n",
      "        1009.8926,  795.0087, 1467.3690,  557.3108,  947.0222,  891.5639,\n",
      "         536.6244,  691.8363,  508.5527,  364.0461,  541.3203,  302.3116,\n",
      "         371.9444,  317.5846,  212.6880,  332.5167,  163.1908,   73.3301,\n",
      "          61.6878,   65.1163,  111.9075], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 936.2307, 1575.8151, 1124.1689, 1068.5948,  811.1607,  739.8170,\n",
      "        1056.8407,  555.1723,  539.7064,  440.8735,  526.9584,  327.7441,\n",
      "         178.3702,  210.7782,   82.9518,   73.9127,   92.6700,   69.2025,\n",
      "          54.6805,   54.2010,   67.0990,   85.4002,   49.3225],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True, False,  True,  True,  True,\n",
      "         True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([877.1197, 386.0641, 323.1274,  66.0972,  48.9004,  33.1715,  65.0962],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 70.3454, 476.1066, 140.4181, 128.8790,  92.3788], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([875.3047, 707.7881, 655.1418, 538.2146, 501.6648,  48.0645,  40.9034,\n",
      "         54.9796,  39.3968], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1898.7610, 1491.1685, 3184.7839, 3661.2715, 3067.5327, 1261.1680,\n",
      "        2780.1106, 2489.1926, 1862.6345, 1506.6115, 1245.7738, 1200.2544,\n",
      "        1505.9631,  861.6597, 1582.4875, 1699.5452,  986.6890,  512.4201,\n",
      "         697.5635, 1171.8739,  271.7500,  310.4543,   94.7325,   97.4310,\n",
      "         128.7973,   92.2515,  156.9156,   64.0344,  114.6428],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False,  True, False],\n",
      "       device='cuda:0')\n",
      "pruning 43 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    15\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   37\n",
      "   ╠════╗\n",
      "   ║    22\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  63\n",
      "  ╠════╗\n",
      "  ║    27\n",
      "  ║    ╠════╗\n",
      "  ║    ║    3\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 10\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :14 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    8\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    23\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    6\n",
      "    ║    ╠════╝\n",
      "    ║    23\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   39\n",
      "   ╠════╗\n",
      "   ║    27\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  71\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    3\n",
      "  ║    ╠════╝\n",
      "  ║    32\n",
      "  ║    ╠════╗\n",
      "  ║    ║    7\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 11\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.007080761715769768, 5.060304164886475)\n",
      "Mean, Std: (1.0, 1.004782795906067)\n",
      "remove_below 0.0715106725692749\n",
      "Significance:\n",
      "tensor([ 710.0060, 1678.6310, 1396.1838, 1430.0109, 1192.8301, 1056.3234,\n",
      "         926.7335,  851.3503, 1180.8619,  615.4006, 1191.8508,  869.1806,\n",
      "         680.1047,  702.0142,  521.4858,  520.6774,  383.8362,  458.8117,\n",
      "         397.3395,  266.4661,  342.0667,  149.8586,   47.3427,   67.9098,\n",
      "          56.5495,   48.2236,   51.4074], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False,  True, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([44.8779, 41.4055, 91.4987], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 920.7888, 1856.9299, 1377.3641, 1172.7189,  893.6493,  772.6704,\n",
      "        1018.0031,  639.8459,  547.2729,  499.0603,  612.7226,  483.4504,\n",
      "         232.1707,  226.9057,  117.4724,   47.1754,   29.7686,   40.8732,\n",
      "          42.4977,   41.2345,   27.2242,   42.9274,   29.3098],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([960.7648, 414.6373, 343.6420,  45.0899,  40.9965,  40.5517,  27.6310,\n",
      "         34.6588], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([896.8329, 209.0905, 199.3788,  62.6791,  34.2930,  61.1105,  77.3841],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([913.5425, 652.3800, 680.9047, 626.7334, 576.3093,  25.0602],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1787.9719, 1445.6941, 3584.6526, 3799.4846, 3440.4778, 1232.7137,\n",
      "        2783.4429, 2220.1465, 1976.0006, 1671.4183, 1228.7205, 1359.1543,\n",
      "        1431.8560, 1130.1028, 1680.4060, 2118.5710, 1217.7775,  410.0294,\n",
      "         843.8169, 1492.4115,  382.8079,  399.0871,  189.2541,  135.6926,\n",
      "         151.1765,  256.2126,  124.3624,   59.1704,   56.6590,   99.0963,\n",
      "          87.8622,   54.5626], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False,  True, False,  True,  True,  True,\n",
      "        False,  True], device='cuda:0')\n",
      "pruning 47 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    15\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   36\n",
      "   ╠════╗\n",
      "   ║    22\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  66\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    1\n",
      "  ║    ╠════╝\n",
      "  ║    26\n",
      "  ║    ╠════╗\n",
      "  ║    ║    6\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :15 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     14\n",
      "     ╠════╗\n",
      "     ║    4\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    7\n",
      "    ║    ╠════╝\n",
      "    ║    16\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   42\n",
      "   ╠════╗\n",
      "   ║    27\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  76\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    5\n",
      "  ║    ╠════╝\n",
      "  ║    36\n",
      "  ║    ╠════╗\n",
      "  ║    ║    10\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.028828877955675125, 5.811820030212402)\n",
      "Mean, Std: (1.0, 1.0004948377609253)\n",
      "remove_below 0.15573427081108093\n",
      "Significance:\n",
      "tensor([ 651.8415, 1354.7307, 1596.0361, 1809.1260, 1012.1599,  868.1746,\n",
      "         911.7117,  966.8702, 1581.8174,  727.9106,  920.7743, 1083.9657,\n",
      "         613.3975,  484.1778,  624.1909,  607.6440,  466.4558,  633.2399,\n",
      "         521.3671,  574.0431,  232.7302,   52.3337,  102.5502,   87.4208,\n",
      "          66.4457,   90.9637,   74.4485], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True,  True,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([233.8874, 138.9070, 140.2928, 115.0575,  78.1496], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 879.4760, 1376.0757, 1144.1119, 1127.9436, 1154.4562,  710.1083,\n",
      "        1178.2067,  658.2896,  596.2222,  566.9958,  510.0865,  544.9044,\n",
      "         223.1676,  341.3467,  177.0553,  103.3991], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([826.6307, 324.3346, 390.1872, 107.5271], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1141.4968,  357.2399,  142.6158,  486.5991,  195.7640,  253.1755,\n",
      "         111.1032,  167.4507,  125.4606,  134.1229], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([817.3928, 737.4119, 685.6229, 552.8758, 539.6323,  24.4494,  39.8147],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1565.7303, 1350.6849, 3417.9729, 3633.2488, 3755.3250, 1407.4109,\n",
      "        3040.8945, 2355.4453, 1702.7385, 1715.8163,  946.6722, 1367.0123,\n",
      "        1652.8694, 1046.2816, 1934.2581, 2085.5754, 1326.6666,  693.0211,\n",
      "         953.5868, 1140.6777,  793.8316,  302.5611,  340.2641,  309.9218,\n",
      "         246.7647,  594.8462,  163.0162,  156.4953,   79.1494,  179.2697,\n",
      "         132.0765,  158.2298,   71.3417,  166.6816,   88.1277,   69.7205],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "         True, False,  True, False,  True,  True], device='cuda:0')\n",
      "pruning 42 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    5\n",
      "    ║    ╠════╝\n",
      "    ║    15\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   38\n",
      "   ╠════╗\n",
      "   ║    19\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  66\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    2\n",
      "  ║    ╠════╝\n",
      "  ║    30\n",
      "  ║    ╠════╗\n",
      "  ║    ║    8\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :16 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     13\n",
      "     ╠════╗\n",
      "     ║    4\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    24\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    8\n",
      "    ║    ╠════╝\n",
      "    ║    22\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   40\n",
      "   ╠════╗\n",
      "   ║    22\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  75\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    6\n",
      "  ║    ╠════╝\n",
      "  ║    35\n",
      "  ║    ╠════╗\n",
      "  ║    ║    14\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.01466328278183937, 5.617064952850342)\n",
      "Mean, Std: (1.0, 1.0016145706176758)\n",
      "remove_below 0.10502930730581284\n",
      "Significance:\n",
      "tensor([ 696.3304, 1693.4917, 1443.7988, 1174.0927,  720.5539,  722.3438,\n",
      "        1491.3480,  686.1413, 1029.2986,  944.9296,  725.2300,  587.5289,\n",
      "         585.7934,  656.4918,  412.9059,  594.2347,  503.8421,  511.2825,\n",
      "         388.6514,   54.0007,   60.2230,   80.5630], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([693.7828, 506.2255,  55.0464, 101.4687,  69.3568,  66.0450],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 977.0852, 1473.4038, 1002.8178, 1218.5195,  992.7953,  720.9008,\n",
      "        1053.7271,  570.8850,  505.8222,  502.4388,  568.5537,  428.7441,\n",
      "         301.4713,  420.8625,  214.3579,   72.3651,   83.8696,   46.6220,\n",
      "          56.1566,   47.7256,   33.9749,   74.5192], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False,\n",
      "        False, False, False,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([785.8667, 270.5306, 342.4438,  67.8892], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1350.3984,  704.7839,  219.8331,  809.9929,  158.4398,  432.9262,\n",
      "         293.6143,  176.3409,  119.0832,   52.4782,   98.1829,   70.9589,\n",
      "          65.8694,   89.9048], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False,  True,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([835.4218, 759.4409, 674.0890, 583.8674, 533.7130,  25.8093,  37.8888,\n",
      "         28.9687], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True,  True, False,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1550.1589, 1597.2267, 3882.5808, 3688.3906, 3475.2722, 1308.7832,\n",
      "        2359.7227, 2364.3254, 1889.9868, 1831.8002,  968.2089, 1697.5751,\n",
      "         915.0408, 1759.7400, 1846.1152, 1410.8286,  810.0345, 1179.8079,\n",
      "        1341.9956,  927.8527,  353.2041,  460.2045,  425.9108,  438.3170,\n",
      "         716.6789,  282.2555,  351.4807,  318.1005,  479.1021,  173.9869,\n",
      "         149.1125,   75.0035,   88.7220,  122.9553,  134.8784],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True,  True, False, False], device='cuda:0')\n",
      "pruning 45 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    3\n",
      "    ║    ╠════╝\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   38\n",
      "   ╠════╗\n",
      "   ║    19\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  71\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    3\n",
      "  ║    ╠════╝\n",
      "  ║    32\n",
      "  ║    ╠════╗\n",
      "  ║    ║    11\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :17 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     14\n",
      "     ╠════╗\n",
      "     ║    4\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    7\n",
      "    ║    ╠════╝\n",
      "    ║    18\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   44\n",
      "   ╠════╗\n",
      "   ║    20\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  78\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    6\n",
      "  ║    ╠════╝\n",
      "  ║    42\n",
      "  ║    ╠════╗\n",
      "  ║    ║    17\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 12\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.017211509868502617, 5.451505184173584)\n",
      "Mean, Std: (1.0, 0.989340603351593)\n",
      "remove_below 0.1322869509458542\n",
      "Significance:\n",
      "tensor([ 683.9506, 1212.5449, 1437.4371,  806.9630,  808.2052,  918.0369,\n",
      "        1250.0664,  717.2881, 1145.8562, 1059.5875,  813.4459,  664.3979,\n",
      "         540.1343,  704.5132,  438.5923,  684.0811,  441.3345,  456.3360,\n",
      "         373.4854,   61.8673], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([786.2094, 751.3486, 248.4203,  73.7029,  57.8151,  51.8225],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1059.0393, 1517.6322,  968.1737, 1023.6349,  889.6240,  753.7378,\n",
      "         633.3228,  538.4717,  513.0595,  559.6148,  461.0776,  350.3893,\n",
      "         302.6073,  109.7796,  173.9187,  102.1113,   50.7625,   83.2967],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True,  True, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([772.7712, 263.4719, 359.1540,  54.6683], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1187.1515,  860.0687,  285.2109,  912.9670,  327.1884,  401.4818,\n",
      "         347.9258,  244.6433,  213.9651,  250.9936,  214.3780,  101.9424,\n",
      "         107.9257,   69.8181,  138.7888,   77.4018,   77.1994],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False,  True, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([839.8339, 672.3690, 591.9053,  48.8763, 108.9196,  59.2267, 100.3848],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1597.9288, 1165.7698, 3500.8560, 3411.7849, 3945.2441, 1154.2175,\n",
      "        2483.8779, 2245.6265, 1866.5891, 1217.0677, 1642.2231, 1299.4729,\n",
      "        1883.2367, 2056.0247, 1501.0352,  884.1503, 1383.1267, 1312.8914,\n",
      "         936.2889,  507.7893,  521.5129,  670.8566,  509.7428,  832.7664,\n",
      "         537.8506,  502.2138,  455.9871,  505.0902,  386.3604,  220.5902,\n",
      "         223.9519,  243.6664,  105.6829,   71.9510,   87.8177,  120.1361,\n",
      "         126.9711,   76.0143,  104.2249,   78.1375,  134.3285,   87.6138],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True,  True, False, False,  True,  True,  True,\n",
      "        False,  True], device='cuda:0')\n",
      "pruning 49 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     10\n",
      "     ╠════╗\n",
      "     ║    2\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    3\n",
      "    ║    ╠════╝\n",
      "    ║    12\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   38\n",
      "   ╠════╗\n",
      "   ║    17\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  72\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    2\n",
      "  ║    ╠════╝\n",
      "  ║    35\n",
      "  ║    ╠════╗\n",
      "  ║    ║    13\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :18 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    4\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    24\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    8\n",
      "    ║    ╠════╝\n",
      "    ║    15\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   41\n",
      "   ╠════╗\n",
      "   ║    21\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  80\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    5\n",
      "  ║    ╠════╝\n",
      "  ║    41\n",
      "  ║    ╠════╗\n",
      "  ║    ║    20\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 12\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.00990935880690813, 5.753238677978516)\n",
      "Mean, Std: (0.9999999403953552, 0.9738532304763794)\n",
      "remove_below 0.13060277700424194\n",
      "Significance:\n",
      "tensor([ 575.0534, 1594.3556, 1500.6677,  887.9788, 1050.8840,  636.0921,\n",
      "        1099.5513, 1032.3688,  839.9304,  623.2218,  570.8563,  821.7245,\n",
      "         519.7740,  668.5380,  641.0596,  486.6303,  438.6740,   83.9370,\n",
      "          87.5377,   75.1532,   73.7405], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([907.7875, 495.0290,  95.8657,  93.5848,  91.3998], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1038.5289, 1579.1703, 1095.5959,  827.2882,  715.2675,  601.4227,\n",
      "         456.6571,  593.0463,  445.7393,  676.6536,  212.7295,  316.2306,\n",
      "         136.5477,   51.6161,   44.1278], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([814.8931, 295.8309, 111.8569, 107.1980], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1221.5293, 1107.4536,  379.1003, 1131.0349,  582.7593,  750.2133,\n",
      "         500.4430,  438.1472,  314.9989,  265.3688,  380.7089,  227.5180,\n",
      "         241.1586,   67.4497,   96.8840,   90.2704,   99.4766,   60.9124,\n",
      "          61.3784,   79.6517], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1086.6731,  704.2925,  548.5781,   87.0323,   61.4147,   41.3339,\n",
      "          68.3152,   35.3698], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1803.8832, 1063.4019, 3813.9321, 2992.8755, 3296.9976, 1167.1578,\n",
      "        2501.5981, 2028.9436, 1795.9265, 1065.2532, 1735.5332, 1258.4514,\n",
      "        1658.0656, 2198.3467, 1495.7635,  857.2649, 1104.0573, 1366.2706,\n",
      "        1047.9510,  607.1365,  786.6133,  630.5878,  638.0917, 1128.5696,\n",
      "         644.6237,  972.5190,  736.5846,  606.8231,  401.1343,  259.4680,\n",
      "         390.4178,  482.6344,  319.5289,  233.7665,  303.8538,   94.6099,\n",
      "         114.4529,   76.5018,   81.8225,   79.0646,   78.3409],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False,  True,  True,  True,\n",
      "         True], device='cuda:0')\n",
      "pruning 46 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     10\n",
      "     ╠════╗\n",
      "     ║    4\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    3\n",
      "    ║    ╠════╝\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   36\n",
      "   ╠════╗\n",
      "   ║    16\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  79\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    2\n",
      "  ║    ╠════╝\n",
      "  ║    34\n",
      "  ║    ╠════╗\n",
      "  ║    ║    12\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :19 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    9\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    7\n",
      "    ║    ╠════╝\n",
      "    ║    20\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   43\n",
      "   ╠════╗\n",
      "   ║    16\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  85\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    7\n",
      "  ║    ╠════╝\n",
      "  ║    38\n",
      "  ║    ╠════╗\n",
      "  ║    ║    14\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.010469826869666576, 5.4794840812683105)\n",
      "Mean, Std: (1.0, 0.958852231502533)\n",
      "remove_below 0.12688834965229034\n",
      "Significance:\n",
      "tensor([ 681.3856, 1392.3093, 1522.9077,  832.0654, 1078.9862,  578.7966,\n",
      "        1176.4587,  987.3512,  801.3004,  577.1782,  737.6802,  586.2250,\n",
      "         609.1733,  608.3537,  602.6613,  452.6319], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([789.3921, 648.4312,  61.3627,  77.0067,  69.4882,  86.3765, 145.3312],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True,  True,  True,  True,  True,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 889.0674, 1510.5707, 1120.0782,  708.6851,  848.9234,  540.2680,\n",
      "         467.8992,  552.7089,  521.7120,  833.2946,  273.8119,  428.1227,\n",
      "         222.0546,   94.3343,   57.4913,   49.2027,   75.2299,   92.2343,\n",
      "          91.0495,   79.6315], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True, False, False, False, False,\n",
      "        False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([767.6359, 304.7273, 151.0902, 182.0159, 111.3654,  86.3727,  97.0575,\n",
      "         55.6398,  46.9569], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1287.0593, 1128.2446, 1164.4243,  783.4076,  650.5211,  629.4401,\n",
      "         492.7696,  513.6254,  313.3285,  507.2764,  325.2967,  318.4528,\n",
      "          72.6273,   69.0752], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1005.4359,  679.2040,  469.2718,   51.3939,   59.4690,   28.9853,\n",
      "          30.2500], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1542.7079, 1172.9983, 3336.9368, 2879.9436, 3131.0435, 2452.1685,\n",
      "        2149.2239, 1838.1292, 1227.8815, 1766.4956, 1330.2764, 2067.8977,\n",
      "        1990.3998, 1287.5042,  884.5632, 1079.8030, 1431.7957, 1154.5907,\n",
      "         673.6466, 1038.3218,  702.1589,  696.7081,  781.3820,  910.8385,\n",
      "         739.7595,  713.3115,  543.2376,  372.1541,  695.2535,  489.5156,\n",
      "         757.2770,  386.1698,  544.2122,  123.5830,   95.0991,  140.6342,\n",
      "          43.8304,  100.2786], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False,\n",
      "         True, False, False,  True, False, False, False, False, False, False,\n",
      "        False,  True, False, False,  True, False,  True, False],\n",
      "       device='cuda:0')\n",
      "pruning 52 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    5\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    3\n",
      "    ║    ╠════╝\n",
      "    ║    11\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   39\n",
      "   ╠════╗\n",
      "   ║    16\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  76\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    1\n",
      "  ║    ╠════╝\n",
      "  ║    32\n",
      "  ║    ╠════╗\n",
      "  ║    ║    12\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :20 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     17\n",
      "     ╠════╗\n",
      "     ║    9\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    8\n",
      "    ║    ╠════╝\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   42\n",
      "   ╠════╗\n",
      "   ║    18\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  82\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    6\n",
      "  ║    ╠════╝\n",
      "  ║    38\n",
      "  ║    ╠════╗\n",
      "  ║    ║    15\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.017992274835705757, 5.624022483825684)\n",
      "Mean, Std: (1.0, 0.9610334038734436)\n",
      "remove_below 0.13808569312095642\n",
      "Significance:\n",
      "tensor([ 701.9426, 1218.3721, 1476.5048,  755.5029, 1014.6136,  658.7032,\n",
      "         992.5352, 1078.7761,  628.9646,  520.4408,  720.2898,  566.2520,\n",
      "         657.9679,  608.4172,  591.1443,  586.9557,   62.0215,   82.1970],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([241.7309,  97.3322,  94.0726, 129.7639,  87.8636,  63.1700],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True, False,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 830.4510, 1330.2649, 1065.3685,  668.9263,  842.9927,  432.5456,\n",
      "         553.2325,  547.4887, 1054.6211,  343.7544,  652.2706,   98.1102,\n",
      "         138.8637,   59.6565], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True,  True, False, False, False,  True, False, False, False,\n",
      "        False,  True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([687.2631, 338.7005, 180.1044, 247.8743, 200.3036,  47.9381,  43.8003,\n",
      "         68.6159,  96.3486], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1598.1024, 1182.7831, 1342.4116,  850.7043,  822.2039,  679.2552,\n",
      "         574.6738,  666.6367,  327.2227,  669.8450,  542.0825,  456.1451,\n",
      "         110.9390,   85.9157,  122.7605], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True,  True, False,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([789.8892, 668.0854, 356.6216,  64.6539,  53.1710,  58.9576,  79.6494,\n",
      "         77.7007], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1352.2515, 1178.8514, 3187.3928, 2927.8721, 2882.0337, 2259.6526,\n",
      "        1777.3331, 1664.0179,  940.4371, 1735.5452, 1328.0518, 2711.8293,\n",
      "        2088.2524, 1433.7850,  949.5782, 1236.0176, 1461.1462,  703.3177,\n",
      "        1244.9435,  741.6424,  943.4165,  922.3250,  900.0478,  736.4429,\n",
      "         423.3495,  932.3002,  561.2059,  701.4802,  745.2027,  185.9674,\n",
      "         183.5314,  138.3186,   88.5898,  106.1867,  149.9321,  103.6503,\n",
      "         116.9085,  119.9988], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
      "        False,  True, False, False, False,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False,  True, False, False, False, False, False],\n",
      "       device='cuda:0')\n",
      "pruning 54 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    5\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    3\n",
      "    ║    ╠════╝\n",
      "    ║    9\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   38\n",
      "   ╠════╗\n",
      "   ║    16\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  74\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    2\n",
      "  ║    ╠════╝\n",
      "  ║    34\n",
      "  ║    ╠════╗\n",
      "  ║    ║    12\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 8\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :21 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    12\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    21\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    8\n",
      "    ║    ╠════╝\n",
      "    ║    11\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   46\n",
      "   ╠════╗\n",
      "   ║    18\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  78\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    8\n",
      "  ║    ╠════╝\n",
      "  ║    37\n",
      "  ║    ╠════╗\n",
      "  ║    ║    17\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.062172722071409225, 5.934531211853027)\n",
      "Mean, Std: (1.0, 0.9511427879333496)\n",
      "remove_below 0.14929640293121338\n",
      "Significance:\n",
      "tensor([ 637.6920, 1207.0863, 1288.4221,  984.1046, 1001.7503,  568.1708,\n",
      "        1165.9518,  932.5760,  677.5652,  571.6624,  791.0074,  591.9333,\n",
      "         862.7142,  587.5717,  502.9075,  688.1050,   87.4914,   50.7626],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True, False,  True, False,\n",
      "        False, False, False, False, False, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([481.2516, 318.9326,  69.4243, 127.4409, 104.9380, 110.3335, 114.4143,\n",
      "        159.7136], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False,  True,  True,  True, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([ 814.9189,  766.8204,  786.1184,  406.9306,  527.3959, 1256.4185,\n",
      "         550.2092,  771.6215,  408.6630,  188.4180,  206.5762],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False, False, False,  True, False, False, False, False,\n",
      "        False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([640.3857, 433.2291, 271.6938, 256.4814, 289.4236,  99.9407,  82.2806,\n",
      "         83.6809,  61.5610,  90.4724,  48.0554,  75.4869], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False,  True, False,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1830.6090, 1235.1462, 1583.5389, 1148.8336, 1174.4601, 1019.8016,\n",
      "         611.4813, 1199.5481,  439.8093,  917.3810,  309.8547,  407.4741,\n",
      "          86.8924,  202.4968,   77.0821,  173.2040,   79.7636],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False,  True, False,  True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([854.3823, 707.8509, 377.6630,  69.9654,  52.5398,  62.1075,  49.1769,\n",
      "         85.1878], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1850.9358, 1316.8928, 2238.8438, 3193.4263, 2655.9861, 1867.0304,\n",
      "        1760.3986, 1358.9363, 1863.8212, 1303.6177, 1966.3112, 1899.6454,\n",
      "         734.9190, 1212.2516,  807.4984, 1626.9874,  815.9819, 1210.0410,\n",
      "        1465.2433, 1041.7091,  800.3487,  491.3422, 1295.6986,  827.7577,\n",
      "         865.4896,  904.7118,  255.5553,  884.3073,  376.0578,  326.6872,\n",
      "         173.4126,  226.4433,  212.7357,  162.0435,  142.8992,  138.0375,\n",
      "         135.5526], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False], device='cuda:0')\n",
      "pruning 51 neurons.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    16\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    3\n",
      "    ║    ╠════╝\n",
      "    ║    9\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   38\n",
      "   ╠════╗\n",
      "   ║    14\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  72\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    4\n",
      "  ║    ╠════╝\n",
      "  ║    36\n",
      "  ║    ╠════╗\n",
      "  ║    ║    13\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :22 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     14\n",
      "     ╠════╗\n",
      "     ║    10\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    6\n",
      "    ║    ╠════╝\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   41\n",
      "   ╠════╗\n",
      "   ║    19\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  76\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    10\n",
      "  ║    ╠════╝\n",
      "  ║    40\n",
      "  ║    ╠════╗\n",
      "  ║    ║    18\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 10\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.02567756548523903, 5.587857246398926)\n",
      "Mean, Std: (0.9999998807907104, 0.9226308465003967)\n",
      "remove_below 0.1617218255996704\n",
      "Significance:\n",
      "tensor([ 776.2334, 1114.7450, 1611.7050,  716.5328, 1020.3411,  604.3719,\n",
      "         881.6364,  587.4540,  849.1017,  600.0496,  765.2828,  817.4978,\n",
      "         931.3103,  772.1747,  122.3510,  106.9771,   81.7637,   57.5007,\n",
      "         115.8922], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([438.8152, 362.3233, 176.8105, 427.3284, 103.1351,  94.3082,  84.1668,\n",
      "         93.9745, 126.2442,  70.7233], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False,  True,  True,  True,  True, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([765.9520, 836.4816, 393.4675, 612.3769, 691.1053, 811.5626, 422.0270,\n",
      "        368.4019, 574.7599, 182.7485, 157.6126, 438.4074, 167.7471],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([639.0278, 380.9890, 348.7333, 108.5875,  88.3391,  61.0299, 107.7421,\n",
      "        136.1268,  66.3848,  78.3547], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True, False,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1692.9735, 1849.4873, 1487.9292, 1615.6094, 1142.1860, 1119.5072,\n",
      "         704.7185, 1253.8013,  918.6854,  476.2523,  549.4457,  362.4704,\n",
      "         252.3878,   96.4235,  106.1102,   48.4786,  138.9911,   74.0757],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([832.4359, 607.5881, 401.5187,  61.3029,  71.8971,  52.2171],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1896.3271, 1144.6178, 2569.2061, 2357.3481, 1947.3276, 1802.0012,\n",
      "        1222.4495, 1652.6490, 1323.7928, 2284.7026, 1727.5588,  939.7637,\n",
      "        1581.7947,  988.1708, 1533.0870,  856.3225, 1039.9832, 1499.2750,\n",
      "        1447.0212,  824.9280,  613.1334, 1709.7808,  878.4265, 1110.9865,\n",
      "        1116.7365,  526.7228, 1163.2302,  465.6263,  448.2576,  263.2776,\n",
      "        1434.5903,  319.9896,  373.1021,  221.1103,  241.8560,  438.1999,\n",
      "          95.2377,  142.6046,  197.5769,  157.2975], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False,  True, False,\n",
      "        False, False, False, False, False, False,  True, False, False, False],\n",
      "       device='cuda:0')\n",
      "pruning 40 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    4\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    20\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    3\n",
      "    ║    ╠════╝\n",
      "    ║    13\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   38\n",
      "   ╠════╗\n",
      "   ║    14\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  70\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    5\n",
      "  ║    ╠════╝\n",
      "  ║    38\n",
      "  ║    ╠════╗\n",
      "  ║    ║    14\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :23 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    8\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    22\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    4\n",
      "    ║    ╠════╝\n",
      "    ║    19\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   41\n",
      "   ╠════╗\n",
      "   ║    18\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  74\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    8\n",
      "  ║    ╠════╝\n",
      "  ║    50\n",
      "  ║    ╠════╗\n",
      "  ║    ║    17\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 13\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "Significance Stat:\n",
      "Min, Max: (0.015047763474285603, 5.851343631744385)\n",
      "Mean, Std: (1.0, 0.9343194365501404)\n",
      "remove_below 0.12384568154811859\n",
      "Significance:\n",
      "tensor([ 793.0929, 1123.8048, 1151.1165,  788.3045, 1000.2540,  613.9762,\n",
      "         895.9512,  492.2863,  846.4983,  598.1959,  786.6909,  785.5085,\n",
      "         938.4085,  817.3321,   70.3447,   56.5826,   55.7105,   44.8533],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([572.5323, 564.7500, 264.7414, 548.6755, 284.7244,  93.3401, 105.6639,\n",
      "         61.6617], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([894.8800, 763.9335, 377.0278, 534.2031, 609.4509, 705.4596, 409.9662,\n",
      "        415.3210, 657.1380, 171.9415, 273.0700, 379.0419, 157.3884,  65.7674,\n",
      "         59.6651,  73.8835,  46.3717, 106.2694, 122.7051], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "         True, False, False,  True,  True,  True,  True, False, False],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([652.3599, 319.0452, 297.8032, 144.1094,  60.6608,  60.7440,  60.7711,\n",
      "         53.0464], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([1575.3363, 1677.9352, 1579.9860, 1686.8035, 1037.8867, 1322.9894,\n",
      "         988.2817, 1613.8608,  870.7235,  645.8920,  712.1020,  410.5692,\n",
      "         337.8218,  186.0370,   44.0846,   69.1116,   71.1785],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([772.9502, 547.2627, 447.9912,  69.6909], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1571.0305, 1230.4540, 2719.4260, 2165.5940, 2061.1252, 1837.6558,\n",
      "        1337.4348, 1697.3273, 1196.6525, 2093.0103, 1569.0735,  999.5712,\n",
      "        1224.3417,  900.5647, 1584.1986,  887.9603, 1191.1407, 1665.3206,\n",
      "        1192.6714,  824.2717,  791.2948, 1683.5675, 1077.4166,  923.6907,\n",
      "        1103.5740,  654.9090, 1169.3784,  426.1478,  194.3819, 1703.9869,\n",
      "         400.3066,  336.7543,  343.0982,  453.5039,  535.7841,  256.0392,\n",
      "         207.3416,  237.7000,  121.2839,  131.4284,  112.4754,   98.9996,\n",
      "         130.2106,  192.5183,  104.6468,   60.9929,  106.8655,  136.8079,\n",
      "         120.7354,   91.2772], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False, False, False,  True],\n",
      "       device='cuda:0')\n",
      "pruning 47 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     11\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    19\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    3\n",
      "    ║    ╠════╝\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   37\n",
      "   ╠════╗\n",
      "   ║    13\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  68\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    6\n",
      "  ║    ╠════╝\n",
      "  ║    47\n",
      "  ║    ╠════╗\n",
      "  ║    ║    13\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 8\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :24 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     14\n",
      "     ╠════╗\n",
      "     ║    6\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    24\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    7\n",
      "    ║    ╠════╝\n",
      "    ║    17\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   42\n",
      "   ╠════╗\n",
      "   ║    16\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  72\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    9\n",
      "  ║    ╠════╝\n",
      "  ║    53\n",
      "  ║    ╠════╗\n",
      "  ║    ║    23\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significance Stat:\n",
      "Min, Max: (0.0407443568110466, 5.590255260467529)\n",
      "Mean, Std: (0.9999998807907104, 0.9474864602088928)\n",
      "remove_below 0.1202586218714714\n",
      "Significance:\n",
      "tensor([ 908.8502, 1211.4331, 1315.3469,  778.0433, 1088.5835,  699.1904,\n",
      "        1053.1127,  663.7358,  831.5013,  781.3604,  862.7773, 1310.0144,\n",
      "         983.2526,   61.0560,   52.1943,   66.1321], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([502.8575, 690.7938, 438.5308, 581.8721, 135.5365, 129.0453,  63.0406,\n",
      "         79.6278,  64.0942], device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False,  True,  True,  True],\n",
      "       device='cuda:0')\n",
      "Significance:\n",
      "tensor([837.6137, 903.3618, 385.0215, 649.8495, 666.9908, 769.9155, 476.0679,\n",
      "        406.4576, 987.7696, 245.7992, 481.3058, 219.4333, 121.2480, 152.2625,\n",
      "         40.8089,  77.9403,  97.6441], device='cuda:0')\n",
      "Prune:\n",
      "tensor([ True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False,  True,  True, False], device='cuda:0')\n",
      "Significance:\n",
      "tensor([768.2402, 297.6380, 266.3687,  75.3566,  70.9451,  52.3326],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1989.0969, 1701.5023, 1839.3330, 2249.9199, 1102.1222, 1441.7275,\n",
      "         830.3557, 1319.7582, 1217.5356,  925.5803,  729.8388,  672.4354,\n",
      "         320.8019,   83.2431,   64.1804,   46.4306,   79.5662,   59.8365,\n",
      "          57.5858,  130.0771,   74.9682,  103.9798,   68.0229],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True,  True,  True, False,\n",
      "         True, False,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([789.5702, 592.5338, 551.1545,  51.0158,  37.5145,  35.2750,  32.4296],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False,  True,  True,  True,  True], device='cuda:0')\n",
      "Significance:\n",
      "tensor([1903.7213, 1150.0485, 3035.2722, 2655.2688, 1889.1630, 1851.7291,\n",
      "        1385.1792, 2012.6290, 1422.6840, 2841.5051, 1590.1029, 1313.0199,\n",
      "        1873.0673, 1018.0747, 1560.1304,  976.8954, 1170.2808, 1799.7220,\n",
      "        1637.6702,  837.5156,  714.7330, 1212.6201, 1204.1843, 1241.5618,\n",
      "         714.1497, 1254.1580,  505.1701,  321.9698, 2105.0007,  422.9983,\n",
      "         377.5427,  564.3844,  620.7292,  718.4584,  353.9005,  340.2579,\n",
      "         182.9441,  141.1915,  215.3819,  216.3923,  164.6881,  285.8557,\n",
      "         311.2299,  227.6443,  188.4473,  164.7053,  186.4798,  159.3867,\n",
      "          95.7172,  107.8231,  145.1750,  147.3963,  114.7641],\n",
      "       device='cuda:0')\n",
      "Prune:\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False], device='cuda:0')\n",
      "pruning 44 neurons.\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     12\n",
      "     ╠════╗\n",
      "     ║    3\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    18\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    3\n",
      "    ║    ╠════╝\n",
      "    ║    14\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   36\n",
      "   ╠════╗\n",
      "   ║    13\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  66\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    6\n",
      "  ║    ╠════╝\n",
      "  ║    53\n",
      "  ║    ╠════╗\n",
      "  ║    ║    15\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 9\n",
      "╔╝\n",
      "│\n",
      "10\n",
      "=====================\n",
      "===LOOPS FINISHED :25 ===\n",
      "Pausing for 2 second to give user time to STOP PROCESS\n",
      "3\n",
      "╚╗\n",
      " ╚╗\n",
      "  ╚╗\n",
      "   ╚╗\n",
      "    ╚╗\n",
      "     15\n",
      "     ╠════╗\n",
      "     ║    5\n",
      "     ╠════╝\n",
      "    ╔╝\n",
      "    27\n",
      "    ╠════╗\n",
      "    ║    ╠════╗\n",
      "    ║    ║    4\n",
      "    ║    ╠════╝\n",
      "    ║    19\n",
      "    ╠════╝\n",
      "   ╔╝\n",
      "   37\n",
      "   ╠════╗\n",
      "   ║    16\n",
      "   ╠════╝\n",
      "  ╔╝\n",
      "  66\n",
      "  ╠════╗\n",
      "  ║    ╠════╗\n",
      "  ║    ║    11\n",
      "  ║    ╠════╝\n",
      "  ║    66\n",
      "  ║    ╠════╗\n",
      "  ║    ║    21\n",
      "  ║    ╠════╝\n",
      "  ╠════╝\n",
      " ╔╝\n",
      " 11\n",
      "╔╝\n",
      "│\n",
      "10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-036bb0f9319e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-2ed0e5a805ae>\u001b[0m in \u001b[0;36mloop\u001b[0;34m(self, count)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mwarmup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWarmupLR_Polynomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-ebaec8ea0ad1>\u001b[0m in \u001b[0;36mtraining_network_func\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function flush_figures at 0x7f54f704de50> (for post_execute):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mflush_figures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# ignore the tracking, just draw and close all figures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# safely show traceback if in IPython, else raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             display(\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2193\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m                     bbox_inches = self.figure.get_tightbbox(\n\u001b[0m\u001b[1;32m   2196\u001b[0m                         renderer, bbox_extra_artists=bbox_extra_artists)\n\u001b[1;32m   2197\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpad_inches\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   2504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2505\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2506\u001b[0;31m             \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2507\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2508\u001b[0m                 \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[0m\n\u001b[1;32m   4151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4153\u001b[0;31m                     bb_xaxis = self.xaxis.get_tightbbox(\n\u001b[0m\u001b[1;32m   4154\u001b[0m                         renderer, for_layout_only=for_layout_only)\n\u001b[1;32m   4155\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, for_layout_only)\u001b[0m\n\u001b[1;32m   1115\u001b[0m                     ticks_to_draw, renderer)\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_offset_text_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticklabelBoxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticklabelBoxes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffsetText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_offset_text_position\u001b[0;34m(self, bboxes, bboxes2)\u001b[0m\n\u001b[1;32m   2104\u001b[0m                 \u001b[0mbottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2106\u001b[0;31m                 \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2107\u001b[0m                 \u001b[0mbottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbottom\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOFFSETTEXTPAD\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m72\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(bboxes)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# can remove once we are at numpy >= 1.16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxmin\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxmax\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymin\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# can remove once we are at numpy >= 1.16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxmin\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxmax\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymin\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AllFiles/ProgramFiles/miniconda3/lib/python3.8/site-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mxmin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;34m\"\"\"The left edge of the bounding box.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.loop(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.tree.beta_del_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.randn(10)*10).abs().pow(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accs_all, label=\"train\")\n",
    "plt.plot(accs_test, label=\"test\")\n",
    "ymin, ymax = plt.gca().get_ylim()\n",
    "plt.text(0, 0.8*ymin+0.2*ymax, f\"Train-> max:{max(accs_all):.3f} end:{accs_all[-1]:.3f} \\nTest-> max:{max(accs_test):.3f} end:{accs_test[-1]:.3f}\")\n",
    "                    \n",
    "plt.legend()\n",
    "plt.savefig(\"files/05_dycnn_cifar10_7.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(accs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.non_linearity.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynet.root_net.residual.fc0.residual.fc1.residual.activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
